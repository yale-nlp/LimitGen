{
    "title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention",
    "abstract": "This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block.\nWe demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Memory serves as a cornerstone of intelligence, as it enables efficient computations tailored to specific contexts. However, Transformers (Vaswani et al., 2017  ###reference_b54###) and Transformer-based LLMs (Brown et al., 2020  ###reference_b6###; Touvron et al., 2023  ###reference_b53###; Anil et al., 2023  ###reference_b1###; Groeneveld et al., 2024  ###reference_b20###) have a constrained context-dependent memory, due to the nature of the attention mechanism.\n###figure_1### The attention mechanism in Transformers exhibits quadratic complexity in both memory footprint and computation time.\nFor example, the attention Key-Value (KV) states have 3TB memory footprint for a 500B model with batch size 512 and context length 2048 (Pope et al., 2023  ###reference_b39###).\nIndeed, scaling LLMs to longer sequences (i.e. 1M tokens) is challenging with the standard Transformer architectures and serving longer and longer context models becomes costly financially.\nCompressive memory systems promise to be more scalable and efficient than the attention mechanism for extremely long sequences (Kanerva, 1988  ###reference_b24###; Munkhdalai et al., 2019  ###reference_b37###). Instead of using an array that grows with the input sequence length, a compressive memory primarily maintains a fixed number of parameters to store and recall information with a bounded storage and computation costs. In the compressive memory, new information is added to the memory by changing its parameters with an objective that this information can be recovered back later on. However, the LLMs in their current state have yet to see an effective, practical compressive memory technique that balances simplicity along with quality.\nIn this work, we introduce a novel approach that enables Transformer LLMs to effectively process infinitely long inputs with bounded memory footprint and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention (Figure 1  ###reference_###). The Infini-attention incorporates a compressive memory into the vanilla attention mechanism (Bahdanau et al., 2014  ###reference_b3###; Vaswani et al., 2017  ###reference_b54###) and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block.\nSuch a subtle but critical modification to the Transformer attention layer enables a natural extension of existing LLMs to infinitely long contexts via continual pre-training and fine-tuning.\nOur Infini-attention reuses all the key, value and query states of the standard attention computation for long-term memory consolidation and retrieval. We store old KV states of the attention in the compressive memory, instead of discarding them like in the standard attention mechanism. We then retrieve the values from the memory by using the attention query states when processing subsequent sequences. To compute the final contextual output, the Infini-attention aggregates the long-term memory-retrieved values and the local attention contexts.\nIn our experiments, we show that our approach outperforms baseline models on long-context language modeling benchmarks while having  114x comprehension ratio in terms of memory size. The model achieves even better perplexity when trained with 100K sequence length. A 1B LLM naturally scales to 1M sequence length and solves the passkey retrieval task when injected with Infini-attention. Finally, we show that a 8B model with Infini-attention reaches a new SOTA result on a 500K length book summarization task after continual pre-training and task fine-tuning.\nIn summary, our work makes the following contributions:\nWe introduce a practical and yet powerful attention mechanism \u2013 Infini-attention with long-term compressive memory and local causal attention for efficiently modeling both long and short-range contextual dependencies.\nInfini-attention introduces minimal change to the standard scaled dot-product attention and supports plug-and-play continual pre-training and long-context adaptation by design.\nOur approach enables Transformer LLMs to scale to infinitely long context with a bounded memory and compute resource by processing extremely long inputs in a streaming fashion."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "###figure_2### Figure 2  ###reference_### compares our model, Infini-Transformer, and Transformer-XL (Dai et al., 2019  ###reference_b14###).\nSimilar to Transformer-XL, Infini-Transformer operates on a sequence of segments. We compute the standard causal dot-product attention context within each segment. So the dot-product attention computation is local in a sense that it covers a total  number of tokens of the current segment with index  ( is the segment length).\nThe local attention (Dai et al., 2019  ###reference_b14###), however, discards the attention states of the previous segment when processing the next one. In Infini-Transformers, instead of leaving out the old KV attention states, we propose to reuse them to maintain the entire context history with a compressive memory. So each attention layer of Infini-Transformers has both global compressive and local fine-grained states. We call such an efficient attention mechanism Infini-attention, which is illustrated in Figure 1  ###reference_### and described formally in the following sections."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Infini-attention",
            "text": "As shown Figure 1  ###reference_###, our Infini-attention computes both local and global context states and combine them for its output. Similar to multi-head attention (MHA), it maintains  number of parallel compressive memory per attention layer ( is the number of attention heads) in addition to the dot-product attention."
        },
        {
            "section_id": "2.1.1",
            "parent_section_id": "2.1",
            "section_name": "2.1.1 Scaled Dot-product Attention",
            "text": "The multi-head scaled dot-product attention (Vaswani et al., 2017  ###reference_b54###), specially its self-attention variant (Munkhdalai et al., 2016  ###reference_b36###; Cheng et al., 2016  ###reference_b10###), has been the main building block in LLMs. The MHA\u2019s strong capability to model context-dependent dynamic computation and its conveniences of temporal masking have been leveraged extensively in the autoregressive generative models.\nA single head in the vanilla MHA computes its attention context  from sequence of input segments  as follows. First, it computes attention query, key, and value states:\nHere,  are trainable projection matrices.\nThen, the attention context is calculated as a weighted average of all other values as\nFor MHA, we compute  number of attention context vectors for each sequence element in parallel, concatenate them along the second dimension and then finally project the concatenated vector to the model space to obtain attention the output."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Memory and Effective Context Window",
            "text": "Our Infini-Transformer enables an unbounded context window with a bounded memory footprint. To illustrate this, Table 1  ###reference_### lists the previous segment-level memory models with their context-memory footprint and effective context length defined in terms of model parameters and input segment length. Infini-Transformer has a constant memory complexity of  for storing compressed context in  and  for each head in single layer while for the other models, the complexity grows along with the sequence dimension - the memory complexity depends either on the cache size for Transformer-XL (Dai et al., 2019  ###reference_b14###), Compressive Transformer (Rae et al., 2019  ###reference_b41###) and Memorizing Transformers (Wu et al., 2022  ###reference_b55###) or on the soft-prompt size for RTM (Bulatov et al., 2022  ###reference_b7###) and AutoCompressors (Ge et al., 2023  ###reference_b18###).\n###figure_3### Transformer-XL computes attention over KV states cached from the last segment in addition to the current states. Since this is done for each layer, Transformer-XL extends the context window from  to  tokens with an additional memory footprint of . Compressive Transformer adds a second cache to Transformer-XL and stores compressed representations of past segment activations. So it extends the Transformer-XL\u2019s context window by  but still has a large context-memory complexity. Taking the idea further, Memorizing Transformers opt to store the entire KV states as context for input sequences. Since the storage becomes prohibitively expensive in this case, they restrict the contextual computation to a single layer only. By utilizing a fast kNN retriever, Memorizing Transformers then build a context window covering the entire sequence history of length  at an increased cost of storage. Our experiments show that Infini-Transformer LM can achieve more than 100x compression rate on top of Memorizing Transformers while further improving the perplexity score.\nRMT and AutoCompressors allow for a potentially infinite context length since they compress the input into summary vectors and then pass them as extra soft-prompt inputs for the subsequent segments. However, in practice the success of those techniques highly depends on the size of soft-prompt vectors. Namely, it is necessary to increase the number of soft-prompt (summary) vectors to achieve a better performance with AutoCompressors (Chevalier et al., 2023  ###reference_b11###) and with that, the memory and compute complexity grow quickly resulting in diminished efficiency. It was also observed in AutoCompressors (Chevalier et al., 2023  ###reference_b11###) that an efficient compression objective is needed for training such prompt compression techniques (Ge et al., 2023  ###reference_b18###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We evaluated our Infini-Transformer models on benchmarks involving extremely long input sequences: long-context language modeling, 1M length passkey context block retrieval and 500K length book summarization tasks. For the language modeling benchmark, we train our models from scratch while for the passkey and book summarization tasks, we continually pre-train existing LLMs in order to highlight a plug-and-play long-context adaptation capability of our approach."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Long-context Language Modeling",
            "text": "We trained and evaluated small Infini-Transformer models on PG19 (Rae et al., 2019  ###reference_b41###) and Arxiv-math (Wu et al., 2022  ###reference_b55###) benchmarks.\nOur setup closely resembles that of Memorizing Transformers (Wu et al., 2022  ###reference_b55###). Namely, all our models have 12 layers and 8 attention heads of dimension 128 each and FFNs with hidden layer 4096.\n###table_1### We set the Infini-attention segment length  to 2048 for all attention layers and the input sequence length to 32768 for training. This allows the Infini-attention to unroll over 16 steps w.r.t its compressive memory states. For the RMT baseline, we performed several runs with summary prompt lengths 50, 100 and 150 and sequence lengths 4096, 8196 and 32768. RMT with 100 summary vectors gave the best result when trained on 8196 length sequences.\nThe main results from the language modeling experiments are summarized in Table 2  ###reference_###. Our Infini-Transformer outperforms both Transformer-XL (Dai et al., 2019  ###reference_b14###) and Memorizing Transformers (Wu et al., 2022  ###reference_b55###) baselines while maintaining 114x less memory parameters than the Memorizing Transformer model with a vector retrieval-based KV memory with length of 65K at its  layer.\n100K length training.\nWe further increased the training sequence length to 100K from 32K and trained the models on Arxiv-math dataset. 100K training further decreased the perplexity score to 2.21 and 2.20 for  and  models.\nGating score visualization. Figure 3  ###reference_### visualizes the gating score,  for the compressive memory for all attention heads in each layer. There are two types of heads emerged in Infini-attention after training: specialized heads with a gating score near 0 or 1 and mixer heads with a score close to 0.5. The specialized heads either process contextual information via the local attention computation or retrieve from the compressive memory whereas the mixer heads aggregate both current contextual information and long-term memory content together into a single output. Interestingly, each layer has at least a single short-range head, allowing a forward-propagation of input signal up until the output layer. We also observed an interleaving of long and short-term content retrievals throughout the forward computation."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "LLM Continual Pre-training",
            "text": "We performed a lightweight continual pre-training for long-context adaptation of existing LLMs. The pre-training data includes the PG19 and Arxiv-math corpus as well as C4 text (Raffel et al., 2020  ###reference_b42###) with length more than 4K tokens. The segment length  was set to 2K throughout our experiments.\n1M passkey retrieval benchmark. We replaced the vanilla MHA in a 1B LLM with Infini-attention and continued to pre-train on inputs with length of 4K. The model was trained for 30K steps with batch size of 64 before fine-tuning on the passkey retrieval task (Mohtashami & Jaggi, 2024  ###reference_b32###).\nThe passkey task hides a random number into a long text and asks it back at the model output. The length of the distraction text is varied by repeating a text chunk multiple times. The previous work (Chen et al., 2023a  ###reference_b8###) showed that a 8B LLaMA model can solve the task up to 32K length when fine-tuned with the same 32K length inputs with Position Interpolation. We take this challenge further and fine-tune on only 5K length inputs to test on 1M length regime.\n###figure_4### Table 3  ###reference_### reports the token-level accuracy for test subsets with input lengths ranging from 32K to 1M. For each test subset, we controlled the position of the passkey so that it is either located around the beginning, middle or the end of the input sequence.\nWe reported both zero-shot accuracy and fine-tuning accuracy. Infini-Transformers solved the task with up to 1M context length after fine-tuning on 5K length inputs for 400 steps.\n500K length book summarization (BookSum). We further scaled our approach by continuously pre-training a 8B LLM model with 8K input length for 30K steps. We then fine-tuned on a book summarization task, BookSum (Kry\u015bci\u0144ski et al., 2021  ###reference_b27###) where the goal is to generate a summary of an entire book text.\nWe set the input length to 32K for fine-tuning and increase to 500K for evaluating. We use a generation temperature of 0.5 and  and set the number of decoding steps to 1024 to generate a summary of each book.\nTable 4  ###reference_### compares our model against the encoder-decoder models that were built particularly for the summarization task (Lewis et al., 2019  ###reference_b28###; Xiao et al., 2021  ###reference_b57###) and their retrieval-based long-context extension (Bertsch et al., 2024  ###reference_b5###). Our model outperforms the previous best results and achieves a new SOTA on BookSum by processing the entire text from book. We have also plotted the overall Rouge score on validation split of BookSum data in Figure 4  ###reference_###. There is a clear trend showing that with more text provided as input from books, Our Infini-Transformers improves its summarization performance metric."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Compressive memory. Inspired by the plasticity in biological neurons (Munkhdalai & Yu, 2017a  ###reference_b34###; Miconi et al., 2018  ###reference_b31###), compressive memory approaches cast parameterized functions as memory to store and retrieve information (Hinton & Plaut, 1987  ###reference_b22###; Schmidhuber, 1992  ###reference_b47###; Ba et al., 2016  ###reference_b2###; Munkhdalai et al., 2019  ###reference_b37###). Unlike the Transformer KV memory array (Vaswani et al., 2017  ###reference_b54###; Wu et al., 2022  ###reference_b55###), which grows with input sequence length, compressive memory systems maintain a constant number of memory parameters for computational efficiency. The parameters are modified with an update rule to store information, which is then retrieved via a memory reading mechanism (Graves et al., 2014  ###reference_b19###; Sukhbaatar et al., 2015  ###reference_b51###; Munkhdalai & Yu, 2017b  ###reference_b35###).\nCompressed input representations can be viewed as a summary of past sequence segments (Rae et al., 2019  ###reference_b41###; Chevalier et al., 2023  ###reference_b11###). Along this direction, more recent works have been utilizing a Transformer LLM itself to compress input sequence for efficient long-context modeling (Bulatov et al., 2022  ###reference_b7###; Chevalier et al., 2023  ###reference_b11###; Ge et al., 2023  ###reference_b18###; Mu et al., 2024  ###reference_b33###). However, the previous segment-level compression methods, including Compressive Transformers (Rae et al., 2019  ###reference_b41###) still discard the memory entries of old segments in order to free up space for the new ones, limiting their context window to the most recent segments. This is in contrast to our Infini-attention that computes incremental memory updates to a fixed amount of memory parameters in a recurrent fashion.\nLong-context continual pre-training. There is a line of work that extends the do-product attention layers and continues to train LLMs for long-context (Xiong et al., 2023  ###reference_b58###; Fu et al., 2024  ###reference_b17###). The attention extensions include incorporating sparsity into the attention layer (Chen et al., 2023b  ###reference_b9###; Ratner et al., 2022  ###reference_b43###; Mohtashami & Jaggi, 2024  ###reference_b32###) as well as manipulating the position encodings (Chen et al., 2023a  ###reference_b8###; Peng et al., 2023  ###reference_b38###) Although the position encoding-based methods such as position interpolation techniques (Chen et al., 2023a  ###reference_b8###) can be data efficient as they only adjust the positional bias in the attention layer, they are still costly for inference.\nThe attention mechanism is also prone to the issues of attention sink (Xiao et al., 2023  ###reference_b56###) and lost-in-the-middle (Liu et al., 2024  ###reference_b30###). Consequently, they struggle in a regime where context length is longer than what was observed during training (Press et al., 2021  ###reference_b40###; Kazemnejad et al., 2024  ###reference_b26###). The proposed Infini-attention addresses those issues by enabling a segment-level streaming computation over long sequences with a fixed local attention window. Our Infini-Transformers successfully extrapolate to 1M input length regimes when trained on 32K and even 5K length sequences.\nEfficient attention. The efficient attention techniques attempt to improve the efficiency of the dot-product attention with an approximation or a system-level optimization. Multiple directions have been explored for different forms of efficient attention approximation, including sparsity-based (Child et al., 2019  ###reference_b12###; Beltagy et al., 2020  ###reference_b4###; Sukhbaatar et al., 2021  ###reference_b52###; Ding et al., 2023  ###reference_b16###) and linear attention approximation (Shen et al., 2018  ###reference_b49###; Katharopoulos et al., 2020  ###reference_b25###; Schlag et al., 2021  ###reference_b46###).\nAmong those, the linear attention variants are closely related to the associative memory matrix (Schlag et al., 2020  ###reference_b45###; 2021  ###reference_b46###) and the metalearned neural memory (Munkhdalai et al., 2019  ###reference_b37###), where KV bindings (Smolensky, 1990  ###reference_b50###) are stored in Fast-Weights (Hinton & Plaut, 1987  ###reference_b22###; Schmidhuber, 1992  ###reference_b47###; Ba et al., 2016  ###reference_b2###) that are modified in with respect to new contextual information.\nMore recently, system-level optimization techniques have been proposed by leveraging specific hardware architecture to make the exact attention computation more efficient (Dao et al., 2022  ###reference_b15###; Liu et al., 2023  ###reference_b29###)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "An effective memory system is crucial not just for comprehending long contexts with LLMs, but also for reasoning, planning, continual adaptation for fresh knowledge, and even for learning how to learn. This work introduces a close integration of compressive memory module into the vanilla dot-product attention layer. This subtle but critical modification to the attention layer enables LLMs to process infinitely long contexts with bounded memory and computation resources. We show that our approach can naturally scale to a million length regime of input sequences, while outperforming the baselines on long-context language modeling benchmark and book summarization tasks. We also demonstrate a promising length generalization capability of our approach. 1B model that was fine-tuned on up to 5K sequence length passkey instances solved the 1M length problem."
        }
    ],
    "url": "http://arxiv.org/html/2404.07143v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "4"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.1.1",
            "2.1.2",
            "2.2"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1",
            "3.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.07143v1",
        "paper_title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention",
        "research_background": "### Motivation:\nThe primary motivation of this paper lies in addressing the constrained context-dependent memory of traditional Transformer models. Transformers and large language models (LLMs) have demonstrated impressive capabilities, but their reliance on the attention mechanism results in quadratic complexity in both memory and computation time. This limitation becomes apparent when dealing with extremely long sequences, making it challenging and financially costly to scale these models efficiently. By introducing a novel attention mechanism, the authors aim to enable efficient processing of infinitely long inputs while maintaining a bounded memory footprint and computation cost.\n\n### Research Problem:\nThe central research problem addressed in this paper involves overcoming the memory and computational inefficiencies inherent in the standard attention mechanism used in Transformers. Specifically, the challenge is to design a method that allows Transformer LLMs to handle extremely long sequences effectively (up to 1M tokens) without prohibitive resource requirements. The authors propose Infini-attention, a new attention technique that incorporates compressive memory, to achieve this objective.\n\n### Relevant Prior Work:\n1. **Transformers and Large Language Models**:\n    - Vaswani et al., 2017: Introduction of the Transformer model, which employs the attention mechanism.\n    - Brown et al., 2020; Touvron et al., 2023; Anil et al., 2023; Groeneveld et al., 2024: Development and scaling of LLMs based on Transformer architectures.\n\n2. **Memory and Attention Complexity**:\n    - Pope et al., 2023: Highlighted the substantial memory footprint (3TB for a 500B model) and computational cost associated with the attention mechanism in long sequences.\n\n3. **Compressive Memory Systems**:\n    - Kanerva, 1988; Munkhdalai et al., 2019: Exploration of compressive memory systems that promise scalability and efficiency for extremely long sequences by maintaining a fixed number of parameters with bounded storage and computation costs.\n\n4. **Attention Mechanism and Extensions**:\n    - Bahdanau et al., 2014: Introduction of the vanilla attention mechanism.\n    - Vaswani et al., 2017: Establishment of the standard scaled dot-product attention in Transformer models.\n   \nBy building on the concepts of compressive memory and refining the attention mechanism, the authors seek to balance simplicity with effectiveness in Transformer LLMs, addressing the limitations of existing methods in handling long-range contextual dependencies.",
        "methodology": "Infini-Transformer aims to innovate upon the conventional Transformer-XL model by proposing a new attention mechanism named Infini-attention, which efficiently maintains the entire context history over extended sequences.\n\nKey components:\n1. **Segment-based Operation**: Similar to Transformer-XL, Infini-Transformer operates on sequences divided into segments. Causal dot-product attention is applied within each segment to process the tokens locally.\n\n2. **Local Attention Computation**: Within a segment indexed \\(i\\), attention computations remain local, involving only the tokens present in the current segment of length \\(l\\).\n\n3. **Global Compressive Memory**: Instead of disregarding the preceding segments\u2019 attention states as done in Transformer-XL, Infini-Transformer reuses these states by maintaining a compressive memory. This innovation ensures that the full context history is preserved.\n\n4. **Infini-Attention Mechanism**: Infini-attention combines both the local fine-grained states (current segment\u2019s attention) and the global compressive states (historical context) within each attention layer. This dual-state approach enables efficient utilization of the past context without the computational burden of processing it in its entirety.\n\nThe Infini-attention mechanism is designed to retain the benefits of Transformer-XL's segment-wise processing while overcoming its limitations by keeping a comprehensive yet manageable memory of past contexts. This results in more efficient and effective handling of long sequences, aiming to significantly improve performance in tasks requiring extensive context comprehension.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Main Experiment Setup\nWe evaluated our Infini-Transformer models on benchmarks that involve extremely long input sequences across three distinct tasks:\n\n1. **Long-Context Language Modeling**: \n   - In this benchmark, we train our models from scratch.\n   \n2. **1M Length Passkey Context Block Retrieval**:\n   - For this task, we continually pre-train existing LLMs to highlight the plug-and-play long-context adaptation capability of our approach.\n   \n3. **500K Length Book Summarization**:\n   - Similar to the passkey context block retrieval task, we continually pre-train existing LLMs for the book summarization task.\n\n#### Datasets\n- The specific datasets used in each task (e.g., individual books, specific language corpora) are not clearly listed in the provided segment but involve extremely long text sequences as indicated by the 1M length for passkey retrieval and 500K length for book summarization.\n\n#### Baselines\n- The baselines used for comparison are not explicitly detailed in the provided text, but they typically involve existing large language models (LLMs) which our approach builds upon through continual pre-training for adaptation to long contexts.\n\n#### Evaluation Metrics\n- While the specific metrics for each task are not detailed in the provided segment, typically:\n  - **Language Modeling**: Perplexity or accuracy in predicting the next token in sequence.\n  - **Context Block Retrieval**: Precision and recall in identifying relevant blocks of text in an extremely large context.\n  - **Book Summarization**: ROUGE, BLEU, and other summarization quality metrics.\n\n#### Main Experimental Results\nThe results of these experiments highlight the efficient handling and processing of extremely long input sequences using our Infini-Transformer models. Key points inferred about the results:\n- **Language Modeling**: The models effectively trained from scratch suggests good performance in language modeling on long sequences.\n- **Passkey Context Block Retrieval & Book Summarization**: The continual pre-training adaptation showcases the model's ability to integrate long-context sequences flexibly and efficiently, enabling high performance in retrieval and summarization tasks.\n\nThe specific numerical results and comparative performance levels are not included in the provided segment, but the overall implication is a successful demonstration of the Infini-Transformer's capabilities across these challenging benchmarks."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the performance of Infini-Transformer on long-context language modeling benchmarks and understand the gating score behavior of its compressive memory mechanism.",
            "experiment_process": "We trained and evaluated small Infini-Transformer models on the PG19 and Arxiv-math benchmarks. The models consisted of 12 layers, 8 attention heads of dimension 128 each, and FFNs with hidden layer size 4096. Infini-attention segment length was 2048, and the input sequence length was 32768 for training, allowing Infini-attention to unroll over 16 steps with respect to its compressive memory states. For comparison, the RMT baseline was tested with multiple summary prompt lengths and sequence lengths. The main results were summarized in Table 2. Additionally, the training sequence length was increased to 100K for further experiments on the Arxiv-math dataset. Gating scores were visualized to show the behavior of compressive memory across different attention heads.",
            "result_discussion": "The Infini-Transformer outperformed both Transformer-XL and Memorizing Transformers while maintaining significantly lower memory parameters. Increasing the training sequence length to 100K further reduced perplexity scores. Visualizations revealed two types of attention heads: specialized heads and mixer heads, with distinct gating scores representing their roles in processing contextual and long-term memory information.",
            "ablation_id": "2404.07143v1.No1"
        },
        {
            "research_objective": "To test the continual pre-training capability of Infini-Transformer for long-context adaptation in language models and evaluate its performance on tasks requiring long-context understanding.",
            "experiment_process": "The continual pre-training involved the PG19, Arxiv-math, and C4 corpora with segment lengths of 2K. For the 1M passkey retrieval benchmark, a 1B LLM with Infini-attention replaced vanilla MHA and was trained for 30K steps with batch size 64 before fine-tuning. The passkey task required models to retrieve hidden random numbers from long texts, with input lengths ranging up to 1M. Zero-shot and fine-tuning accuracies were reported. For the 500K length book summarization task, an 8B LLM was continuously pre-trained for 30K steps and fine-tuned on BookSum with input lengths increasing up to 500K.",
            "result_discussion": "The Infini-Transformer achieved up to 1M context length retrieval in the passkey task post fine-tuning on 5K length inputs. In the book summarization task, it achieved state-of-the-art results, outperforming previous models by processing the entire text of books, with performance improving as more text was provided as input. The Rouge score showed a clear trend of improvement with longer input contexts.",
            "ablation_id": "2404.07143v1.No2"
        }
    ]
}