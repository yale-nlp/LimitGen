{
    "title": "Prompt Public Large Language Models to Synthesize Data for Private On-device Applications",
    "abstract": "Pre-training on public data is an effective method to improve the performance for federated learning (FL) with differential privacy (DP). This paper investigates how large language models (LLMs) trained on public data can improve the quality of pre-training data for the on-device language models trained with DP and FL. We carefully design LLM prompts to filter and transform existing public data, and generate new data to resemble the real user data distribution. The model pre-trained on our synthetic dataset achieves relative improvement of 19.0% and 22.8% in next word prediction accuracy compared to the baseline model pre-trained on a standard public dataset, when evaluated over the real user data in Gboard (Google Keyboard, a production mobile keyboard application). Furthermore, our method achieves evaluation accuracy better than or comparable to the baseline during the DP FL fine-tuning over millions of mobile devices, and our final model outperforms the baseline in production A/B testing. Our experiments demonstrate the strengths of LLMs in synthesizing data close to the private distribution even without accessing the private data, and also suggest future research directions to further reduce the distribution gap.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "While recent advances of machine learning models significantly benefit from scaling up both training data and model size [24  ###reference_b24###, 2  ###reference_b2###, 32  ###reference_b32###, 15  ###reference_b15###, 39  ###reference_b39###], smaller models have advantages in practical deployment due to inference latency, service cost, and privacy benefits when hosted on the local devices. User data are particularly effective to improve the performance of relatively small models targeting a specific task [18  ###reference_b18###, 47  ###reference_b47###, 7  ###reference_b7###]. Privacy-preserving methods are necessary for training these models using real user data [3  ###reference_b3###]. Differential privacy (DP) [11  ###reference_b11###, 12  ###reference_b12###], a mathematical guarantee applied to characterize the learning process, is a widely acknowledged method to prevent models from memorizing individual user\u2019s information in the training data. Cross-device federated learning (FL) [29  ###reference_b29###, 23  ###reference_b23###], where devices collaboratively learn a model without transferring user data, is popular for limiting data access.\n###figure_1### The usage of large-scale public and private data is important to achieve both privacy and utility for privacy-preserving methods. Training DP models from scratch to achieve state of the art utility with meaningful guarantees is challenging [40  ###reference_b40###]. Recent work [27  ###reference_b27###, 49  ###reference_b49###] show promising results with much better privacy utility trade-off by combining pre-training language models (LMs) on public data and fine-tuning on private data. Public pre-training has become a standard technique for DP training, FL [31  ###reference_b31###], and the combination of DP and FL [47  ###reference_b47###]. In this paper, we focus on Gboard (Google Keyboard, a production mobile keyboard application), where small on-device LMs are trained and deployed [47  ###reference_b47###]. The training pipeline has two stages (see Figure 1  ###reference_###): pre-training using server-side public data, and private fine-tuning over private user data with DP FL. The trained LM is deployed on the users\u2019 mobile devices to support features such as next word prediction, smart compose, smart completion and suggestion to improve the users\u2019 typing experience.\nPre-training on the server-side public data is particularly helpful in reducing the number of training rounds (and hence, the communication and computation cost) needed by DP FL over the private user data. Intuitively, pre-training allows a model to learn knowledge shared by the public and private domain, so that the privacy budget can be efficiently utilized during the private fine-tuning phase to learn important features specific to the private domain. Therefore, the closer the distribution between the public pre-training data and the private user data, the more savings in the privacy budget used by the DP FL. In this work, we explore whether the powerful large LMs (LLMs) can be used to improve the quality of the server-side pre-training data for Gboard.\nLLMs with billions of parameters have achieved impressive performance in the general language generation and understanding tasks (see, e.g., [2  ###reference_b2###, 32  ###reference_b32###, 15  ###reference_b15###, 39  ###reference_b39###] and the references therein). As LLMs are a strong representation of their training data111Pre-trained LLMs are considered to be public because their training data do not contain the on-device user data in Gboard. The privacy concerns of LLMs and their training data is an important independent topic [5  ###reference_b5###, 41  ###reference_b41###]., Wang et al. [42  ###reference_b42###] asked Can Public Large Language Models Help Private Cross-device Federated Learning, and explored two approaches: 1) knowledge distillation (from the teacher LLM to student on-device LM) in pre-training, which significantly reduces the public data size and slightly improves the final performance after DP FL fine-tuning; and 2) distribution matching, which splits the privacy budget in two phases, and uses an LLM and a FL-trained LM from the first phase to filter the public data for the second phase. However, both approaches in [42  ###reference_b42###] require non-trivial changes of the current public pre-training and DP FL fine-tuning pipeline. Moreover, Wang et al. [42  ###reference_b42###] did not fully exploit LLMs\u2019 emergent ability to generate long text sequences.\nIn this paper, we propose a simple yet effective method to improve the public data quality by exploiting the strong generative ability of LLMs. As shown in Figure 1  ###reference_###, we carefully design the prompts to guide LLMs to generate data closer to the target domain. In our case, the target domain of Gboard is the private user typing data on their mobile phones. We investigate three types of LLM prompts: 1) filter and 2) transform the public C4 data [36  ###reference_b36###], and 3) generate diverse chat data by chain-of-thought [45  ###reference_b45###] style prompting. The synthesized data can be directly used as the server-side pre-training data without extra changes to the DP FL phase, and hence, is simple to deploy in practice. Furthermore, the synthetic data can be potentially used as the proxy data for other tasks (e.g., server-side evaluation) or models, which is another advantage over the previous methods proposed in [42  ###reference_b42###].\nThe quality of our LLM generated data is assessed by running production FL experiments over the real user data from millions of mobile devices. Compared to the baseline C4 pre-training data [47  ###reference_b47###], the LM pre-trained on our data gives relative improvement of 19.0% and 22.8% in the next word prediction accuracy when evaluated on the real user data (see Table 3  ###reference_###). The LM also achieves superior performance in A/B testing after fine-tuning with DP FL (see Figure 4  ###reference_###). Finally, we show that distribution gap between the public and private data can be further reduced, if a privately trained LM is available to filter the data."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "Federated Learning (FL) and Differential Privacy (DP). In cross-device FL, clients such as mobile devices collaboratively learn a model using the decentralized data. The Federated Averaging (FedAvg) algorithm and its variants  [29  ###reference_b29###, 43  ###reference_b43###] are widely used in practice.\nIn each training round (see Figure 2  ###reference_###), the server first broadcasts a global model to a subset of clients; each client then updates their local model with local data, typically by an SGD optimizer, and sends back the model delta by subtracting the learned and initial local model weights; the model deltas are aggregated and used as pseudo gradient on the server to update the global model. After training typically thousands of rounds, the final model will be deployed on mobile devices for inference.\n###figure_2### DP provides a quantifiable measurement of the privacy risk of models memorizing the individual user\u2019s information in the training data. Combining DP with FL gives an advanced privacy-preserving training method. DP is achieved by two operations [30  ###reference_b30###, 22  ###reference_b22###, 8  ###reference_b8###, 47  ###reference_b47###]: 1) clipping the  norm of each client\u2019s model delta to control their contribution, and 2) adding noise to the aggregated deltas on the server.\nIn this paper, we use a production FL system similar to [4  ###reference_b4###] to run DP FL algorithm to train an on-device LM (see Section 4  ###reference_### for the setup).\nWe fix the privacy and optimization parameters for fine-tuning with DP FL and only study the effectiveness of different pre-training public (proxy) data. See Appendix B  ###reference_### for a mathematical description of the DP definition and details of the algorithms, and Appendix C  ###reference_### for hyperparameters and DP guarantees.\nSynthetic Data Generation. Using LLMs to generate synthetic data has shown promising results in many applications.\nTaori et al. [38  ###reference_b38###] used self-instruct [44  ###reference_b44###] to fine-tune LLaMA 7B [39  ###reference_b39###] with synthetic instructions and answers generated by the large text-davinci-003 model [33  ###reference_b33###] with few-shot prompting.\nEldan and Li [13  ###reference_b13###], Gunasekar et al. [17  ###reference_b17###], Li et al. [28  ###reference_b28###] used GPT-3.5 and GPT-4 models [34  ###reference_b34###, 1  ###reference_b1###] to generate data to train smaller models of fewer than 2 billion parameters for coherent storytelling, coding in Python, and common sense reasoning.\nYu et al. [52  ###reference_b52###] prompted LLMs with attributes to generate synthetic data similar to an existing attributed dataset.\nZhu et al. [55  ###reference_b55###], Shu et al. [37  ###reference_b37###] used LLMs to filter and transform given text for the rewriting task. In this work, we use LLMs to synthesize data for Gboard, where the target domain distribution is user typing data on mobiles that are different from the public data on the web.\nPrivate data can be used in various DP methods to guide LLMs to generate synthetic data close to the private distribution.\nThese methods assume direct access to the private data for either directly fine-tuning an LLM [26  ###reference_b26###, 53  ###reference_b53###, 51  ###reference_b51###, 50  ###reference_b50###] or measuring the distance between the generated data and private distribution [46  ###reference_b46###].\nIt is challenging to apply these methods in a cross-device FL system due to the on-device resource limitations and privacy concerns of mobile users.\nZhang et al. [54  ###reference_b54###] proposed to prompt LLMs with classification labels to generate synthetic pre-training data for FL. The method is designed for classification tasks and is studied for image and speech data, which cannot be directly applied to learn a language model."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Prompt LLMs to Synthesize Private-like Data",
            "text": "We design proper prompts to guide LLMs to process or generate data, so that the resulting data can be closer to the private target domain, compared to the baseline C4 dataset [36  ###reference_b36###] used by the current production [47  ###reference_b47###]. Our target domain distribution is formed by real-users\u2019 mobile keyboard typing data stored on their mobile devices. For privacy protection, we cannot directly collect or access the on-device examples. Instead, we use common sense knowledge to design the LLM prompts. While we focus on English, our approach can be easily applied to other languages. An instruction-tuned PaLM 2-S [2  ###reference_b2###] is used as the LLM throughout the paper.\nThree types of data are synthesized by the LLM: filtered C4, generated chat, and converted C4. While prompting LLMs to synthesize data has been explored previously as discussed in Section 2  ###reference_###, to the best of our knowledge, we are the first to study this in a production FL application for private data, and validate its effectiveness by extensive experiments over millions of mobile phones."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Filter: Is the Public Example Likely Typed On Mobile Phones?",
            "text": "The public C4 dataset has over 360 million examples (782GB on disk) and is used as the pre-training data by the current production. Each example contains a paragraph of text extracted from a webpage. For each example, the LLM is prompted to output a binary answer: \u201cDetermine whether the following topic is likely to be discussed by people on their mobile phones. Give a score of 0 or 1, where 1 means very likely, and 0 means unlikely.\u201d"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Prompt to Directly Generate Chat",
            "text": "We exploit the generative ability of LLMs to directly generate synthetic chat data.\nThe key challenge is to ensure that the generated data are diverse [17  ###reference_b17###, 13  ###reference_b13###, 52  ###reference_b52###]. To improve diversity, we design the following prompts with seven variables:\n\u201cImagine you are a [GENDER] at age [AGE]. You are using the [CHAT-APP] APP to message [RECEIVER] on your mobile phone on the [TIME] of a [DAY]. You want to chat about the following topic: [TOPIC]. Generate the conversation between you and your message receiver. Do not include information other than the conversation.\u201d.\nAmong the seven variables, five of them are sampled from a predefined set of categorical values, and two (RECEIVER and TOPIC) are self-generated by the LLM inspired by the chain-of-thought prompting [45  ###reference_b45###]. As shown in Figure 3  ###reference_###, for given values of (AGE, GENDER, TIME, DAY, CHAT-APP), we first use LLM to generate a list of message receivers. Then we set RECEIVER to each value in the generated list, and use LLM again to generate a list of message topics. We post-process the generated list of receivers and topics to remove any duplications. Finally, we loop over the TOPIC in the generated list, and use LLM to generate the conversations.\n###figure_3### We describe the predefined values of the five variables, and LLM prompts used to generate the other two variables (RECEIVER and TOPIC).\nAGE: Uniformly sampled from 15 to 55, and 3 age groups: \u201cbetween 55 and 59\u201d, \u201cbetween 60 and 64\u201d, and \u201cover 65\u201d.\nGENDER: \u201cmale\u201d, and \u201cfemale\u201d.\nTIME: \u201cmorning\u201d, \u201cafternoon\u201d, and \u201cnight\u201d.\nDAY: 11 common holidays (e.g., \u201cNew Year\u2019s Day\u201d), a special \u201cvacation day\u201d, and 28 values in the format \u201c[WEEKDAY] in the [SEASON]\u201d where \u201cWEEKDAY\u201d can take 7 values from \u201cMonday\u201d to \u201cSunday\u201d, and \u201cSEASON\u201d can take 4 values from \u201cspring\u201d to \u201cwinter\u201d.\nCHAT-APP: \u201cAndroid Messages\u201d, \u201cFacebook Messenger\u201d, \u201cSnapchat\u201d, \u201cInstagram\u201d, \u201cWhatsApp\u201d, \u201cDiscord\u201d, and \u201cTelegram\u201d. As a sanity check of LLM\u2019s knowledge, we\u2019ve asked the LLM to describe the differences between those popular chat apps, and verified that the answers are reasonable.\nRECEIVER: Given values for AGE, GENDER, TIME, DAY, and CHAT-APP, we ask the LLM to generate a list of message receivers: \u201cImagine you are a [GENDER] at age [AGE]. You are using the [CHAT-APP] APP to message someone on your mobile phone on the [TIME] of a [DAY]. Generate a list of potential message receivers.\u201d.\nTOPIC: Given values for AGE, GENDER, TIME, DAY, CHAT-APP, and a RECEIVER value generated by the LLM, we ask the LLM again to generate a list of topics: \u201cImagine you are a [GENDER] at age [AGE]. You are using the [CHAT-APP] APP to message [RECEIVER] on your mobile phone on the [TIME] of a [DAY]. Generate a list of potential message topics.\u201d.\nWe use top-k sampling [14  ###reference_b14###] with  and temperature 0.2. A higher temperature usually gives a longer list of candidate receivers and topics. Because of the resource limitations, we choose a fixed temperature and leave the exploration of other sampling parameters or methods such as [20  ###reference_b20###] as future work. The generated chat data has 19GB on disk (around 4B tokens), containing about 30 million multi-turn conversations (see Table 5  ###reference_### in the Appendix for a few examples)."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Transform: Convert Public Example into Chat on Mobile Phones",
            "text": "###table_1### Despite carefully designing the LLM prompts, the chat data generated by directly prompting the LLM (Section 3.2  ###reference_###) can be less diverse than the C4 dataset. In Table 2  ###reference_###, we compute the percentage of words in the vocabulary that have appeared in the dataset. The vocabulary contains 30K words and is used by the on-device LM over the en-US (United States) population. If a word does not appear in the pre-training data, then the corresponding word embedding will not be learned during the pre-training phase. Therefore, a higher vocabulary coverage is usually desired. The synthetic chat data given by directly prompting the LLM has a lower vocabulary coverage 79.3% than the raw and filtered C4 datasets (both have 99.6%).\nMotivated by this observation, we apply another approach to generate synthetic chat data: transform the LLM filtered C4 dataset (obtained in Section 3.1  ###reference_###) into conversations. For each filtered C4 example, we ask the LLM to:\n\u201cConvert the following article to a conversation that you may message over your mobile phone. Generate the conversation. Include as many details as possible.\u201d.\nDue to the resource constraints, only 20% of the filtered C4 examples are converted to conversations. The resulting dataset has about 10GB size (around 2B tokens) and 10 million multi-turn conversations (see Table 6  ###reference_### in the Appendix for a few examples). As shown in Table 6  ###reference_###, despite its small size, this dataset inherits a good vocabulary coverage 99.0% from the original C4."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Combine Filtered, Generated and Transformed Synthetic Data",
            "text": "We combine the 19GB data directly generated by LLM in Section 3.2  ###reference_### and the 10GB data transformed from C4 in Section 3.3  ###reference_###, and obtain a chat dataset of 29GB with about 40 million multi-turn conversations. We name it LLM-syn-chat-29G. It exploits the generative ability of LLMs to synthesize chat to resemble the private user data in Gboard. Intuitively, the generated chat may have a distribution closer to the target distribution than the C4 data (see some evidence in Section 5  ###reference_###). However, as we will show in Section 4.1  ###reference_###, the LM trained on the LLM-syn-chat-29G alone achieves slightly lower accuracy than LLM-filter-C4-136G when evaluated on the real user data, possibly due to the diversity issue. Therefore, we combine LLM-syn-chat-29G and LLM-filter-C4-136G into LLM-mix-166G.\nWe also tried different ratios when combining the two datasets (e.g., half from synthetic chat and half from filtered C4), and found that simply combining all the data works the best."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "As described in [18  ###reference_b18###, 47  ###reference_b47###], our on-device LM is a one-layer LSTM [19  ###reference_b19###, 16  ###reference_b16###] with 670 hidden units, embedding dimension 96, and a 30K-size word-level vocabulary. The LM has about 6M parameters. As shown in Figure 1  ###reference_###, we follow [47  ###reference_b47###] to train the LM in two steps: 1) server-side pre-training, and 2) fine-tuning with FL and DP. In DP FL fine-tuning, we run on a production FL system similar to [4  ###reference_b4###] with the real user data. Experiments are performed over two populations of the mobile devices: United States (i.e., the \u201cen-US\u201d population) and India (i.e., the \u201cen-IN\u201d population).\nTo participate in a training round in cross-device FL system, the mobile devices have to satisfy local criteria such as being charging and connecting to unmetered network [4  ###reference_b4###, 21  ###reference_b21###], and minimum separation time across rounds [47  ###reference_b47###]. The total number of available devices are estimated to be around 13M for en-US and 8M for en-IN. Note that the exact population size is unknown as devices are not tracked or logged in the system."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Pre-training with Public Data on the Server",
            "text": "We use the standard cross entropy loss, and train the LM with Adam optimizer [25  ###reference_b25###] using 0.001 learning rate and  epsilon. The LM is trained for around 150K steps with batch size 20K. See Appendix C  ###reference_### for more hyperparameter tuning details.\nWe evaluate the pre-trained LM on the decentralized user data in the en-US and en-IN populations. We use federated evaluation to aggregate metrics from multiple rounds of participated mobile devices. In each evaluation round, a subset of devices will receive the pre-trained LM and run evaluation on their data, and then the server aggregates the evaluation metrics from these devices. Minimum separation time criteria is enforced to guarantee different devices are chosen across rounds. The next word prediction (NWP) evaluation accuracy is reported in Table 3  ###reference_### with the following observations:\nPre-training on the synthetic chat data LLM-syn-chat-29G gives a lower accuracy than pre-training on the filtered C4 \u201cLLM-filter-C4-136G\u201d, potentially because that the synthetic chat has smaller size and lower diversity as discussed in Section 3.3  ###reference_###.\nCombining the filtered C4 and the synthetic chat data gives the best pre-training dataset \u201cLLM-mix-166G\u201d, which achieves 22.8% and 19.0% relative improvement over the baseline C4 data for the en-US and en-IN populations, respectively.\nDiscussion on the quality measure. Evaluating the quality of the generated data is a common challenge. It is even more challenging when the target domain is the private user data, because we need to be extremely careful on privacy protection. In this work, we measure the data quality by training an on-device LM and using federated evaluation to aggregate a single scalar value of NWP accuracy. Developing practical privacy-preserving methods to measure the data quality between the server-side data and the private on-device data is an important future work."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Fine-tuning with Differentially Private Federated Learning",
            "text": "###figure_4### After pre-training on the server in Section 4.1  ###reference_###, we follow the recommended practices in [47  ###reference_b47###] to fine-tune the LM using a production FL system [4  ###reference_b4###]. For more details about the federated training algorithm and DP accounting, see Appendix B  ###reference_###.\nAs the FL training proceeds, we run federated evaluation of the trained LMs over a different set of devices (i.e., the holdout set) in the same population, and report the NWP evaluation metrics in Figure 4  ###reference_###. We highlight the following observations:\nCompared to the baseline (C4 pre-trained LM), the LM pre-trained on LLM data has a higher accuracy at training round 0. This is consistent with the metrics reported in Table 3  ###reference_###, which are potentially aggregated from more devices across multiple federated evaluation rounds.\nDuring the FL training, the LM pre-trained on the LLM data maintains superior (over the en-US population) or comparable evaluation accuracy (over the en-IN population). Specifically, to reach the 0.17 accuracy on the en-US population, our method needs around 600 rounds while the the baseline needs around 1100 rounds (i.e., almost 2x more), giving a significant saving in the communication and computation cost and an improvement in the privacy guarantees.\nA/B testing. After fine-tuned with DP FL, we conduct live A/B testing to measure the LM performance in the production environment. Specifically, we measure two metrics: WMR (Word Modified Ratio, i.e., the ratio of words being modified during typing or after committed) and WPM (Word Per Minute, i.e., the number of committed words per minute). For fairness, the LMs in A/B testing have the same privacy guarantees, achieved by using the same noise multiplier, same number of clients per round, same minimum separation time, and same number of training rounds (see Section C.3  ###reference_### for more details). For en-US, the FL fine-tuned English LM pre-trained on LLM data improves over the baseline WMR by 0.64% and WPM by 0.11%. For en-IN, the FL fine-tuned English LM pre-trained on LLM data improves over the baseline WMR by 0.05% and WPM by 0.04%. These improvements, especially in en-US, are significant for improving the users\u2019 mobile typing experience.\nDiscussion on en-US vs en-IN. Our LLM synthetic data is more effective in the en-US population than the en-IN population. This indicates that the synthetic data may be less similar to the real user typing data in India. An interesting direction of future work is to see if using target country/region in the LLM prompts can help synthesize better data."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Improve Synthetic Data with Fine-tuned On-device LM",
            "text": "So far we only use the common sense knowledge to prompt the LLM to synthesize data closer to the private distribution of user typing data in Gboard. While the LLM-based synthetic data provide impressive gains compared to the C4 baseline in pre-training as shown in Table 3  ###reference_###, the on-device LM still benefits a lot from fine-tuning over the real user data as shown in Figure 4  ###reference_###. This indicates a gap between the distribution of the LLM-based synthetic data and that of the real user data. In this section, we present a preliminary study of a simple strategy to close the gap, without changing the privacy-preserving way of accessing private data. Specifically, we ask the following question: can we reduce the distribution gap by using a fine-tuned on-device LM from Section 4.2  ###reference_### to filter the data?\nNote that the goal of this study is to use the fined-tuned LMs to filter the synthetic dataset to get better proxy data for the private domain. We only pre-train on the new proxy data and measure the quality improvement following Section 4.1  ###reference_###, but do not further fine-tune the pre-trained model with DP FL as in Section 4.2  ###reference_### and in [42  ###reference_b42###]. Training two models with DP FL on the same population not only complicates the standard production pipeline, but also needs to carefully allocate privacy budgets because the two models are accessing the same set of users (see [42  ###reference_b42###] for more discussions). Although the new proxy data is not used for improving the on-device LMs for Gboard, it can be of independent interest for other tasks. For example, the proxy data can be used to facilitate research simulation in the datacenter, and improve server-side models that cannot be easily fine-tuned with DP FL.\nFor each example in the LLM-mix-166G dataset, we compute 3 values: 1) OOV rate: the percentage of tokens not in the LM vocabulary; 2) pre-trained LM score: the average log-likelihood computed by the LM trained on LLM-mix-166G as in Section 4.1  ###reference_###; 3) fine-tuned LM score: the average log-likelihood computed by the LM pre-trained on LLM-mix-166G and fine-tuned over real user data with DP FL as in Section 4.2  ###reference_###. Examples satisfying the following conditions are kept in the filtered LLM data (see Appendix C  ###reference_### for more details): OOV rate  0.6; fine-tuned LM score  5; fine-tuned LM score  pre-trained LM score.\nAfter filtering LLM-mix-166G, the data size is decreased to 32GB (around 7B tokens). We use LLM-prox-32G to represent this new proxy dataset. We follow the same step described in Section 4.1  ###reference_### to measure the quality of LLM-prox-32G: train an LM from scratch on the data, and then perform federated evaluation over the real user data. As shown in Table 4  ###reference_###, on the en-US population, LLM-prox-32G further improves the accuracy from 0.1452 to 0.1509 compared to LLM-mix-166G, and achieves much higher accuracy than the baseline C4. This indicates that the filtered data have higher quality, despite only having 19% of the original LLM-mix-166G data. Table 4  ###reference_### also shows that a larger fraction of LLM filtered C4 examples are filtered out compared to the synthetic chat data, which potentially indicates that the majority of the LLM filtered C4 are less similar to the private user data."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Inspired by recent advances in generative LLMs, this work studies whether LLMs can help differentially private federated learning for training small on-device models. In particular, we focus on Gboard (Google Keyboard, a production mobile keyboard application) where the goal is to learn a small on-device LM using the private user typing data. Our work provides evidence that even with no access to the private data, common sense knowledge and careful prompt design can help guide LLMs to synthesize data similar to the target domain. Effectiveness of our method is verified by extensive FL experiments over the real-world user data from the millions of mobile devices.\nOur results suggest a few interesting directions of future work, including developing more privacy-preserving quality measures, investigating other LLM prompting strategies such as adding the country/region information, and different sampling methods, as already pointed out in Section 4.1  ###reference_###, Section 4.2  ###reference_###, and Section 3  ###reference_###. Moreover, given the promising results from the preliminary study in Section 5  ###reference_###, further investigation on improving the quality of proxy data with guaranteed privacy-preserving methods can be a rewarding direction."
        }
    ],
    "url": "http://arxiv.org/html/2404.04360v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "5"
        ]
    },
    "research_context": {
        "paper_id": "2404.04360v1",
        "paper_title": "Prompt Public Large Language Models to Synthesize Data for Private On-device Applications",
        "research_background": "### Motivation and Research Problem\nThe paper addresses the challenge of improving the performance of small on-device language models (LMs) used in applications like Gboard (Google Keyboard) while ensuring user data privacy. Small models are desirable in on-device scenarios due to their lower inference latency, reduced service costs, and privacy benefits. However, training these models to perform well requires high-quality data, which raises privacy concerns, especially when using real user data.\n\nDifferential privacy (DP) and federated learning (FL) are popular methods to preserve privacy while training models. Pre-training on server-side public data followed by private fine-tuning using DP FL is a common approach. The first stage helps reduce the computational and communication costs of the second stage by allowing the models to learn generalizable features from public data. However, the effectiveness of this pipeline depends significantly on how closely the public data aligns with the domain of the private user data.\n\nThe research problem, therefore, is to improve the quality of the public pre-training data so that it closely represents the private user data domain, thus maximizing the utility of the privacy budget during the fine-tuning phase. This paper explores whether large language models (LLMs), known for their strong generative capabilities, can generate high-quality synthetic data that better matches the target domain of user typing data on mobile phones.\n\n### Relevant Prior Work\n1. **Scaling Models and Data**: Previous advances have shown that scaling up both training data and model size leads to significant improvements in ML models [2, 15, 24, 32, 39].\n \n2. **On-device Models and Privacy**: Smaller models benefit from using specific user data to improve performance for targeted tasks, but require privacy-preserving methods [3, 7, 18, 47].\n\n3. **Differential Privacy (DP) and Federated Learning (FL)**: DP provides mathematical guarantees to prevent models from memorizing individual user\u2019s information [11, 12, 40]. FL allows collaborative model training across devices without sharing user data [23, 29, 31].\n\n4. **Pre-training and Fine-tuning Pipelines**: Combining pre-training on public data and fine-tuning on private data has shown a promising privacy-utility trade-off for DP training and FL [27, 47, 49]. \n\n5. **Use of LLMs**: Recent studies evaluated whether LLMs can assist private cross-device FL, showing they can reduce public data size and slightly improve performance after DP FL fine-tuning [42]. However, they did not fully leverage LLMs' capability to generate long text sequences.\n\n### Contributions and Findings\nThis paper proposes a method to generate public pre-training data using LLMs by designing prompts that guide these models to generate data more representative of private user data domains like those used in Gboard. Three types of prompts are investigated: filtering and transforming existing public data and generating diverse chat data using chain-of-thought prompting.\n\nThe synthetic data generated through this approach has demonstrated significant improvements in performance when used for pre-training. In production FL experiments on real user data, the models pre-trained on this synthetic data showed improvements in next word prediction accuracy and performed better in A/B testing after fine-tuning with DP FL, compared to the baseline performance with traditional public data (C4 dataset).\n\nUltimately, this work illustrates that leveraging LLMs for data synthesis can effectively enhance pre-training pipelines without necessitating substantial changes to existing DP FL methods, offering a practical and superior approach for privacy-preserving on-device applications.",
        "methodology": "The proposed method aims to use Large Language Models (LLMs) to generate data that closely resembles private, on-device user data, specifically data from mobile keyboard typing. Below are the key components and innovations of the methodology:\n\n### Key Components:\n\n1. **Prompts Design**:\n   - **Purpose**: To guide LLMs to generate data that is similar to the target domain, which consists of real users' mobile keyboard typing data.\n   - **Approach**: Use common-sense knowledge to design these prompts due to the inability to directly collect or access on-device data for privacy reasons.\n\n2. **Data Types**:\n   - **Filtered C4**: Synthesized data where parts of the C4 dataset are filtered based on certain criteria.\n   - **Generated Chat**: Data generated by the LLM in response to chat-like prompts.\n   - **Converted C4**: Data from the C4 dataset that has been modified through prompt-based LLM processing to better resemble the target domain.\n\n3. **LLM Utilized**:\n   - **PaLM 2-S**: An instruction-tuned version of the PaLM 2 model is used exclusively throughout the paper for synthesizing data.\n\n### Innovations:\n\n1. **Private Target Domain Emulation**:\n   - Innovatively uses prompts to synthesize data that mimics private, on-device data without compromising user privacy.\n\n2. **Domain Application**:\n   - First to explore the use of prompt-based data synthesis in a production Federated Learning (FL) application for private data. \n\n3. **Validation**:\n   - The effectiveness of this methodology is validated by extensive experiments conducted on millions of mobile phones, providing a robust evaluation of the approach.\n\n### Summary:\n\nThe method leverages specially designed prompts to guide LLMs in generating or processing data that closely mirrors real users' private typing data, without compromising privacy. This is achieved using an instruction-tuned PaLM 2-S model and encompasses generating three types of synthesized data: filtered C4, generated chat, and converted C4. The approach stands out for its application in a production environment for FL and its extensive validation, marking a pioneering effort in synthesizing private data for on-device applications.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Setup:**\n\n1. **Model Details:**\n   - **Type:** One-layer LSTM\n   - **Hidden Units:** 670\n   - **Embedding Dimension:** 96\n   - **Vocabulary Size:** 30,000 (word-level)\n   - **Total Parameters:** Approximately 6 million\n\n2. **Training Process:**\n   - **Step 1:** Server-side pre-training\n   - **Step 2:** Fine-tuning with Federated Learning (FL) and Differential Privacy (DP)\n\n3. **Federated Learning Environment:**\n   - **Production FL System:** Similar to the setup in the referenced works [4, 47]\n   - **Population for Training:** \n     - United States (en-US) mobile devices\n     - India (en-IN) mobile devices\n   - **Device Participation Criteria:** Devices must be charging, connected to an unmetered network, and satisfy a minimum time interval between rounds [4, 21, 47].\n\n4. **Data:**\n   - Real user data is used for training in the FL system.\n   - Estimated device availability:\n     - ~13 million for en-US\n     - ~8 million for en-IN\n   - Exact population sizes are not tracked or logged.\n\n**Results:**\n- Unfortunately, the actual results of the experiments are not provided in the provided text. The text focuses on describing the setup of the main experiment without delineating specific outcomes or performance metrics.\n\n**Note:** Based on the information provided, this description encapsulates the setup and conditions under which the main experiment was conducted, including model specifics, training methodology, environment, and datasets used. For an accurate depiction of experimental results, the original paper or additional sections would need to be consulted."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The goal of this ablation study is to evaluate the impact of pre-training on synthetic data versus filtered public data and their combination on the performance of a language model for next word prediction.",
            "experiment_process": "The language model was pre-trained using the Adam optimizer with a learning rate of 0.001 and trained for around 150K steps with a batch size of 20K. The pre-trained models were evaluated on decentralized user data from the en-US and en-IN populations using federated evaluation. The federated evaluation aggregated metrics from multiple rounds of participated mobile devices, with different subsets of devices chosen in each round. The next word prediction (NWP) evaluation accuracy was reported.",
            "result_discussion": "The results showed that pre-training on synthetic chat data led to lower accuracy compared to filtered C4 data, likely due to the smaller size and lower diversity of the synthetic chat data. Combining filtered C4 and synthetic chat data produced the best pre-training dataset (LLM-mix-166G), achieving a 22.8% and 19.0% relative improvement over the baseline C4 data for the en-US and en-IN populations, respectively.",
            "ablation_id": "2404.04360v1.No1"
        },
        {
            "research_objective": "The study aims to understand how fine-tuning a language model with Differentially Private Federated Learning (DP FL) after pre-training on synthetic data can impact its performance, particularly in production environments.",
            "experiment_process": "Following pre-training, the language model was fine-tuned using a production federated learning system. Federated evaluation was conducted using a different set of devices from the same populations used during pre-training. The next word prediction (NWP) accuracy at various training rounds was compared between the baseline model pre-trained on C4 data and the model pre-trained on LLM data. Additionally, live A/B testing was performed to measure performance metrics such as Word Modified Ratio (WMR) and Words Per Minute (WPM) in a production environment.",
            "result_discussion": "The fine-tuned model pre-trained on LLM data showed higher accuracy at training round 0 and maintained superior or comparable accuracy throughout the FL training compared to the baseline. Specifically, for the en-US population, the method required significantly fewer rounds to reach the same accuracy level as the baseline, reducing communication and computation costs and improving privacy guarantees. A/B testing results also indicated improvements in WMR and WPM, particularly in the en-US population, enhancing the user typing experience.",
            "ablation_id": "2404.04360v1.No2"
        },
        {
            "research_objective": "This ablation study investigates whether using a fine-tuned on-device language model to filter synthetic datasets can reduce the distribution gap between synthetic and real user data, thereby improving the quality of the proxy data for private domains.",
            "experiment_process": "A fine-tuned on-device language model was employed to filter the synthetic dataset LLM-mix-166G. Each example in the dataset was evaluated based on three criteria: OOV rate, pre-trained LM score, and fine-tuned LM score. Examples that met specified thresholds for these values were retained. The resulting filtered dataset (LLM-prox-32G) was then used to train a new language model, which was evaluated using federated evaluation on real user data, following the same procedure described in the pre-training step.",
            "result_discussion": "Filtering the LLM-mix-166G dataset reduced its size by 81%, resulting in the LLM-prox-32G dataset. Despite the reduction in data size, the new dataset led to higher accuracy (from 0.1452 to 0.1509) in the en-US population compared to the original LLM-mix-166G dataset. The study indicates that filtering synthetic data with a fine-tuned model can enhance data quality, even if the dataset is significantly smaller. Additionally, more examples from the filtered C4 dataset were removed compared to synthetic chat data, suggesting that LLM filtered C4 examples were less similar to private user data.",
            "ablation_id": "2404.04360v1.No3"
        }
    ]
}