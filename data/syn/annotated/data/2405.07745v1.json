{
    "title": "LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language",
    "abstract": "Despite advancements in English-dominant generative large language models, further development is needed for low-resource languages to enhance global accessibility. The primary methods for representing these languages are monolingual and multilingual pretraining. Monolingual pretraining is expensive due to hardware requirements, and multilingual models often have uneven performance across languages. This study explores an alternative solution by adapting large language models, primarily trained on English, to low-resource languages. We assess various strategies, including continual training, instruction fine-tuning, task-specific fine-tuning, and vocabulary extension. The results show that continual training improves language comprehension, as reflected in perplexity scores, and task-specific tuning generally enhances performance of downstream tasks. However, extending the vocabulary shows no substantial benefits. Additionally, while larger models improve task performance with few-shot tuning, multilingual models perform worse than their monolingual counterparts when adapted.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### The performance of proprietary generative large language models (LLMs) is better than open-source ones in most cases as this article is written Xu et al. (2022  ###reference_b61###); Sun et al. (2024  ###reference_b49###), though there are efforts to develop open-source generative LLMs in terms of high performance and human ethics alignment Touvron et al. (2023a  ###reference_b53###); Jiang et al. (2023  ###reference_b26###); Almazrouei et al. (2023  ###reference_b3###).\nThe progress is more significant in the English language compared to other languages as the aforementioned open-source models are mostly trained by English corpora Wang et al. (2023  ###reference_b58###); Zhang et al. (2023a  ###reference_b65###). To make natural language processing technology more inclusive and accessible globally, research and development should be dedicated to the techniques that improve the performance of large language models in low-resource languages.\nMonolingual Yang et al. (2023b  ###reference_b63###); Nagoudi et al. (2023  ###reference_b37###); Uludo\u011fan et al. (2024  ###reference_b55###); Corr\u00eaa et al. (2024  ###reference_b13###); Kesgin et al. (2024  ###reference_b29###) and multilingual pretraining Shliazhko et al. (2023  ###reference_b48###); Scao et al. (2022  ###reference_b45###); Lin et al. (2024b  ###reference_b35###) of generative LLMs are two main solutions for representing low-resource languages. However, monolingual pretraining is too costly due to hardware requirements for generative LLMs Zhao et al. (2023a  ###reference_b68###). On the other hand, multilingual LLMs have uneven performance across different languages mostly due to imbalanced training corpus Zhang et al. (2023a  ###reference_b65###); Qin et al. (2024  ###reference_b41###). Our proposed solution is to adapt open-source generative LLMs for low-resource languages, illustrated in Figure 1  ###reference_###.\nIn this regard, this study examines how to adapt open-source LLMs for low-resource languages in a systematic way. We focus on the benefits of using different methodologies, both individually and together, including continual training, supervised fine-tuning, and vocabulary extension, to adapt generative LLMs for low-resource languages.\nFor the sake of efficiency, we use Llama Touvron et al. (2023a  ###reference_b53###) in the experiments. We select the Turkish language as a low-resource language. We therefore refer to the model family used in this study as LlamaTurk. The model size and language selection are affordable when the number of experiments is considered in this study111Two NVIDIA RTX 2080Tis and four A4000s are employed in the experiments.. Also, Llama is trained mostly with English data, which can provide better investigation for adapting non-English languages. The Turkish language can be categorized under low-resource languages when training corpus of open-source generative LLMs are considered Touvron et al. (2023a  ###reference_b53###), yet the recipes given in this study can also be used for other low-resource languages since the methods are independent of language itself.\nWe further examine adaptation in terms of two more aspects: Model size and multilinguality. Model size is important for scalability and performance Zhao et al. (2023a  ###reference_b68###); Yang et al. (2023a  ###reference_b62###). We provide an analysis of the adaptation of Llama-7b and 13b in this respect. Moreover, multilingual LLMs, such as BLOOM Scao et al. (2022  ###reference_b45###), Yi AI et al. (2024  ###reference_b2###), Aya \u00dcst\u00fcn et al. (2024  ###reference_b71###), and MaLA Lin et al. (2024a  ###reference_b34###), can provide an opportunity to adapt low-resource languages easier than English-dominant ones due to multilingual corpus and vocabulary. Since BLOOM and Yi do not involve Turkish in training and Aya is larger than MaLA in terms of model parameters, we use MaLA for an analysis of multilingual LLMs.\nThe main contributions of this study can be summarized as follows. We (i) analyze the adaptation of generative LLMs for low-resource language systematically to understand advantages and disadvantages in terms of continual training, instruction fine-tuning, task-specific fine-tuning, and vocabulary extension, (ii) investigate model size and multilingual models for adaptation, and (iii) publish all resources including source codes, datasets, and generative models reported in the experiments222https://github.com/metunlp/llamaturk."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Generative LLMs are either proprietary or open-source. Although proprietary LLMs have currently outstanding performance Sun et al. (2024  ###reference_b49###), there are also efforts to develop competitive open-source models Touvron et al. (2023a  ###reference_b53###); Jiang et al. (2023  ###reference_b26###).\nThe majority language of open-source generative LLMs is English. Their pretraining text corpus mostly includes text in the English language. For adapting LLMs pretrained with English data for low-resource languages, the following methods are examined. (i) The training phase is continued using non-English raw data to learn the language properties of the new language Larcher et al. (2023  ###reference_b32###); Cui et al. (2024  ###reference_b15###); Zhao et al. (2024  ###reference_b67###); Acikgoz et al. (2024  ###reference_b1###). (ii) The knowledge of large language model is transferred by supervised fine-tuning on a non-English instruction or downstream-task dataset Santilli and Rodol\u00e0 (2023  ###reference_b44###); Holmstr\u00f6m and Doostmohammadi (2023  ###reference_b22###); Kohli et al. (2023  ###reference_b30###); Zhao et al. (2024  ###reference_b67###); Garcia et al. (2024  ###reference_b20###); Kuulmets et al. (2024  ###reference_b31###). (iii) The vocabulary of large language model is extended to include non-English tokens Cui et al. (2023  ###reference_b14###); Zhao et al. (2024  ###reference_b67###).\nThese methods are employed in different studies and languages, resulting in a lack of understanding advantages and disadvantages of each in a controlled experimental framework.\nDifferent from these studies, we provide a comprehensive experimental setup on the benefits of different methodologies for adapting generative LLMs for low-resource languages. Moreover, we focus on model size and multilingual models for adaptation."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Adaptation Methods",
            "text": "In this section, we explain the methods to adapt open-source generative LLMs for low-resource languages in detail."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Continual Training",
            "text": "Continual training is the process of extending the pretraining phase of LLMs by incorporating new data corpus Gupta et al. (2023  ###reference_b21###). The main objective is to minimize the loss on this new data while having relatively lower loss scores on previous data since continual training is open to catastrophic forgetting French (1999  ###reference_b18###); Li and Lee (2024  ###reference_b33###). Continual training can therefore capture implicit language structures and text semantics.\nPrevious studies Qin et al. (2022  ###reference_b42###) show that continual training improves the performance of domain adaptation for BERT-like encoder-based LLMs Devlin et al. (2019  ###reference_b17###). It is also used for adapting decoder-based generative LLMs to low-resource Cui et al. (2023  ###reference_b14###); Zhao et al. (2024  ###reference_b67###), code-mixed Owen et al. (2024  ###reference_b39###), non-Latin Husain et al. (2024  ###reference_b24###), and multilingual Lin et al. (2024a  ###reference_b34###) settings.\nIn this study, similar to previous studies, we employ Low-Rank Adaptation (LoRA) Hu et al. (2021  ###reference_b23###) for efficient training due to limited resources. We use a raw Wikipedia corpus333https://huggingface.co/datasets/wikipedia from November 2023 with a size of 534,988 Turkish articles.\nWe set the input sequence length as 512 tokens and the batch size as 128 instances. We use 32 gradient accumulation steps and 100 linear warmup steps. We train with a learning rate of 3e-4 for a single epoch. LoRA\u2019s R is set to 8, alpha to 16, and dropout to 0.05. Since continual training is costly and the study has a limited budget, we employ continual training for only Llama-7b444https://huggingface.co/huggyllama/llama-7b with 8-bit quantization. A single run of continual training takes approximately 206 hours with these settings using four NVIDIA RTX A4000s."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Instruction Fine-tuning",
            "text": "Instruction tuning is a supervised fine-tuning method that improves the ability of LLMs to follow instructions Wei et al. (2021  ###reference_b60###); Ouyang et al. (2022  ###reference_b38###); Zhang et al. (2024  ###reference_b64###). During training, the model is presented with many pairs of instructions and corresponding responses. The main objective is to teach the model to generate accurate responses based on the given instructions, rather than continuing from the previous text.\nDifferent from previous instruction-tuning efforts, Stanford\u2019s Alpaca Taori et al. (2023  ###reference_b50###) is a leading model that shows major improvements by instruction fine-tuning an open-source generative LLM, namely Touvron et al. (2023a  ###reference_b53###). While Alpaca and similar models such as Vicuna Chiang et al. (2023  ###reference_b11###) have an instruction set constructed by prompting proprietary LLMs, other models such as Dolly Conover et al. (2023  ###reference_b12###) employ human labor for constructing a more reliable instruction set. The majority of these efforts are for the English language, yet there are instruction-tuned models to adapt English-supported LLMs for low-resource settings Cui et al. (2023  ###reference_b14###); Zhao et al. (2024  ###reference_b67###); Azime et al. (2024  ###reference_b6###).\nIn this study, we construct an instruction set by translating Alpaca\u2019s 52k instructions from English to Turkish by using Google Translate555https://translate.google.com. The quality of the translated set is inadequate for training since we observe many issues such as translation errors (e.g. missing letters and untranslated words), keyword translations (e.g. reserved keywords specific to programming languages should not be translated), and semantic mismatching (e.g. original instruction asks for a phrase with five words, but correct translation has less than five words). We therefore manually validate and correct the quality of the instruction set. We publish our instruction set666https://github.com/metunlp/llamaturk. We also provide a prompting example for instruction fine-tuning in Appendix A.1  ###reference_###.\nWe employ instruction tuning for all LLMs examined in this study, namely Llama-7b777https://huggingface.co/huggyllama/llama-7b, Llama-13b888https://huggingface.co/huggyllama/llama-13b, and MaLA-10b999https://huggingface.co/MaLA-LM/mala-500-10b-v1. We use 8-bit quantization with LoRA (resulting in training 12.4% of LLM parameters) and the same hyperparameters as in continual training, except that we use a smaller input sequence length (256 tokens) and train for two epochs. A single run of instruction tuning takes approximately 17.5 hours for Llama-7b with these settings using two NVIDIA RTX 2080Tis."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Task-Specific Fine-tuning",
            "text": "Task-specific tuning is a type of instruction tuning, where a fine-tuning set involves task-related instructions and ground-truth answers Budzianowski and Vuli\u0107 (2019  ###reference_b9###); Wang et al. (2024  ###reference_b59###), rather than adapting a general-purpose instruction set. Task-specific tuning of generative LLMs is proven to be successful in different domains including text editing Raheja et al. (2023  ###reference_b43###), sentiment analysis Inserte et al. (2024  ###reference_b25###), and machine translation Zheng et al. (2024  ###reference_b70###). However, task-specific tuning have the potential of deteriorating the language capabilities of LLMs Zhang et al. (2023b  ###reference_b66###); Zhao et al. (2023b  ###reference_b69###).\nWe follow instruction fine-tuning with a task-specific dataset for the downstream task of sentiment analysis. We choose sentiment analysis since it is a widely applicable task that represents a fundamental natural language processing capability Liu (2012  ###reference_b36###). For this purpose, we create an instruction set for sentiment analysis. To create a balanced set, we downsample 2,500 instances for both negative and positive sentiment classes, a total of 5k instances from the TRSAv1 dataset Aydo\u011fan and Kocaman (2023  ###reference_b5###). We then use a prompt manually crafted for the task of sentiment analysis101010We run prompts from existing resources Bach et al. (2022  ###reference_b7###) but decided to use a manually crafted one by observing better performance in preliminary experiments.. We provide the prompt in Appendix A.2  ###reference_###.\nWe employ task-specific tuning for all LLMs examined in this study. We use all models in 8-bit quantization. We also use LoRA (resulting in training 12.4% of LLM parameters) and the same hyperparameters as in instruction tuning. A single run of task-specific tuning takes approximately 2.5 hours for Llama-7b with these settings using two NVIDIA RTX 2080Tis."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Combinations",
            "text": "A summary of data statistics used for the adaptation methods is given in Table 1  ###reference_###. In addition to a single examination of these methods, we also report the results of using them in combination to leverage better performance. We particularly employ the following combinations using Llama-7b with LoRA. Hyperparameters are set the same as explained in the previous subsections.\nContinual Training with Instruction Fine-tuning: We first obtain a model by continual training using raw Wiki data as explained in Section 3.1  ###reference_###. We then apply instruction fine-tuning as explained in Section 3.2  ###reference_###. The motivation is to boost the potential of instruction tuning when the backbone model is trained with low-resource raw text beforehand.\nContinual Training with Task-Specific Fine-tuning: With a similar motivation to the previous approach, we first obtain a model by continual training using raw Wiki data. We then apply task-specific fine-tuning as explained in Section 3.3  ###reference_###.\nContinual Training with Instruction and Task-Specific Fine-tuning: The motivation is to boost the performance of task-specific tuning when the model is trained by both raw text and instruction-set in low-resource language beforehand. We first obtain a model by continual training using raw Wiki data. We then apply instruction tuning and task-specific fine-tuning respectively.\nInstruction and Task-Specific Fine-tuning: This approach avoids continual training but examines using both instruction and then task-specific tuning respectively. The motivation is to boost the performance of task-specific tuning when the model is trained by only instruction-set in low-resource language beforehand.\nVocabulary Extension with Instruction Fine-tuning: We extend the vocabulary with low-resource language tokens as explained in Section 3.4  ###reference_###. We then apply instruction tuning to understand the impact of vocabulary extension on instruction tuning.\nVocabulary Extension with Task-Specific Fine-tuning: With a similar motivation to the previous approach, we extend the vocabulary with low-resource language tokens and then apply task-specific tuning to understand the impact of vocabulary extension on task-specific tuning.\nVocabulary Extension with Continual Training: We extend the vocabulary with low-resource language tokens and then apply continual training to understand its impact on continual training."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we evaluate the performance of different methods to adapt generative large language models for low-resource language. We particularly conduct both intrinsic and extrinsic evaluations in order to understand the performance of both language comprehension and downstream tasks. We also run benchmark LLM evaluation by using appropriate datasets. This section further involves the results of using varying model sizes and applying multilingual models for the adaptation.\nIn all cases, perplexity scores are improved by continual training (LlamaTurk-7b-c). The lowest perplexity scores are also obtained by continual training in the majority of cases (three of four data collections). A possible reason is that the model could gradually accumulate language knowledge as it is exposed to more raw text. This incremental learning process can allow the model to become more robust and adaptable.\nPerplexity scores are improved by instruction tuning (LlamaTurk-7b-i). The only exception is xquad-context, yer instruction tuning has still a very close perplexity score to the original Llama-7b. Our instruction-tuning set is based on Alpaca, which has general-purpose instructions and responses. On the other hand, task-specific tuning (LlamaTurk-7b-t) deteriorates perplexity scores in all cases. We argue that, by training on task-specific instructions, generative LLMs might become overly specialized and optimized for those specific instructions, rather than maintaining a more general understanding of language.\nThe combinations that include task-specific tuning have poor perplexity scores. On the other hand, continual training and instruction tuning improve perplexity. We therefore expect to have a better performance by using them together (LlamaTurk-7b-c-i) but perplexity scores get worse than the case when they are applied alone. However, when perplexity is measured on an instruction set (databricks-instruction), continual training together with instruction tuning has the lowest perplexity score. This observation can support that generative LLMs adapt to different data types, and one should consider target data type before selecting adaptation method.\nIn all models where vocabulary extension is applied (Llama-7b-v), perplexity scores get higher than the original (Llama-7b). We argue that without sufficient training data and fine-tuning, the model can struggle to effectively incorporate the new vocabulary into its internal representations and learning processes. Similarly, (Zhao et al., 2024  ###reference_b67###) observes negative impact of vocabulary extension, and also suggests that vocabulary extension might not be a suitable choice for small-scale continual training such as in our continual training with 0.2 billion tokens of the training data. Another reason could be the number of additional tokens in vocabulary (28k tokens), merged with the original tokenizer (32k tokens). More experimentation is needed to understand if a different number of new tokens in vocabulary works better in adaptation.\nWe find that task-specific tuning cannot help improve perplexity scores previously. However, our extrinsic evaluation shows that task-specific tuning improves the performance of sentiment analysis. Specifically, we observe that task-specific tuned model (LllamaTurk-7b-t) is good at zero-shot inference, suggesting that task-specific instructions provide sufficient knowledge for zero-shot evaluation.\nWhen instruction tuning is employed alone, it has no significant impact on the performance of downstream task. However, we find that the highest accuracy score is obtained when instruction tuning and task-specific tuning are together employed (LllamaTurk-7b-i-t). Moreover, LllamaTurk-7b-i-t has a better few-shot performance compared to other methods including task-specific tuning.\nWhen continual training is employed alone (LllamaTurk-7b-c), we observe no significant improvement in the performance of downstream task. However, the performance is promising when it is used together with task-specific tuning (LllamaTurk-7b-c-t). This suggests further examination of continual training with task-specific tuning in different downstream tasks and datasets.\nSimilar to the perplexity experiments, we observe that vocabulary extension has no improvement on the performance of downstream task.\nIn both cases of applying instruction or task-specific tuning, we find that LlamaTurk-13b improves perplexity scores in all cases. However, task-specific tuning (LlamaTurk-13b-t) is still outperformed by the original Llama model Llama-13b in most cases.\nWe find that LlamaTurk-13b improves the performance of downstream task when it is applied with task-specific tuning and few-shot evaluation. On the other hand, the adaptation of a larger model with instruction tuning has no significant impact on the performance of downstream task.\n###figure_2### ###figure_3### ###figure_4### ###figure_5### Perplexity and accuracy scores of the original MaLA-7b model are improved by adapting MaLATurk-7b in both instruction and task-specific tuning. However, the perplexity of adapting a monolingual model LlamaTurk-7b is still better than adapting a multilingual model in all cases. Similarly, monolingual adaptation has better accuracy scores of task-specific tuning in most cases. The only benefit of adapting multilingual LLM is observed when instruction tuning is applied.\n###figure_6### ###figure_7### ###figure_8### ###figure_9###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Intrinsic Evaluation",
            "text": "Intrinsic evaluation of generative LLMs involves a perplexity score that represents how well a language model can predict the next word in a sequence of text Jurafsky and Martin (2009  ###reference_b27###):\nwhere  is the total number of words and  is the probability assigned by the model to the -th word given the preceding text context.\nA lower perplexity score indicates that language model is better able to predict the next word, and thus has a better understanding of the language.\n###table_1### We calculate the perplexity scores on different data collections than the ones used in Section 3  ###reference_###. Specifically, we use the Turkish question and context subsets of xquad Artetxe et al. (2019  ###reference_b4###), and the instruction and response subsets of databricks-dolly-15k Conover et al. (2023  ###reference_b12###) using a Turkish translated version111111https://huggingface.co/datasets/atasoglu/databricks-dolly-15k-tr. The detailed statistics of the data used for calculating perplexity scores are given in Table 2  ###reference_###. The reason for reporting the perplexity scores for different subsets is that the characteristics of each subset can be helpful to understand the applied method\u2019s impact on the adaptation. For instance, xquad-question has instances of questions while xquad-context has longer paragraphs of task descriptions. Similarly, databricks-instruction has instruction-type questions, while databricks-response has answers or responses to those questions.\nIn Table 3  ###reference_###, we provide the perplexity scores. The main observations can be summarized as follows.\nIn all cases, perplexity scores are improved by continual training (LlamaTurk-7b-c). The lowest perplexity scores are also obtained by continual training in the majority of cases (three of four data collections). A possible reason is that the model could gradually accumulate language knowledge as it is exposed to more raw text. This incremental learning process can allow the model to become more robust and adaptable.\nPerplexity scores are improved by instruction tuning (LlamaTurk-7b-i). The only exception is xquad-context, yer instruction tuning has still a very close perplexity score to the original Llama-7b. Our instruction-tuning set is based on Alpaca, which has general-purpose instructions and responses. On the other hand, task-specific tuning (LlamaTurk-7b-t) deteriorates perplexity scores in all cases. We argue that, by training on task-specific instructions, generative LLMs might become overly specialized and optimized for those specific instructions, rather than maintaining a more general understanding of language.\nThe combinations that include task-specific tuning have poor perplexity scores. On the other hand, continual training and instruction tuning improve perplexity. We therefore expect to have a better performance by using them together (LlamaTurk-7b-c-i) but perplexity scores get worse than the case when they are applied alone. However, when perplexity is measured on an instruction set (databricks-instruction), continual training together with instruction tuning has the lowest perplexity score. This observation can support that generative LLMs adapt to different data types, and one should consider target data type before selecting adaptation method.\nIn all models where vocabulary extension is applied (Llama-7b-v), perplexity scores get higher than the original (Llama-7b). We argue that without sufficient training data and fine-tuning, the model can struggle to effectively incorporate the new vocabulary into its internal representations and learning processes. Similarly, (Zhao et al., 2024  ###reference_b67###  ###reference_b67###) observes negative impact of vocabulary extension, and also suggests that vocabulary extension might not be a suitable choice for small-scale continual training such as in our continual training with 0.2 billion tokens of the training data. Another reason could be the number of additional tokens in vocabulary (28k tokens), merged with the original tokenizer (32k tokens). More experimentation is needed to understand if a different number of new tokens in vocabulary works better in adaptation."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Extrinsic Evaluation",
            "text": "Generative LLMs employ human evaluations as an evaluation method to align with human judgments Ouyang et al. (2022  ###reference_b38###). However, human-based evaluation is labor-intensive, making it costly and less feasible for low-resource languages. On the other hand, LLM evaluation benchmarks offer reliable evaluation for downstream NLP tasks such as GLUE Wang et al. (2018  ###reference_b57###) and SuperGLUE Wang et al. (2019  ###reference_b56###). Similarly, there are evaluation frameworks and tools such as LM Evaluation Harness Gao et al. (2023  ###reference_b19###) and MLflow121212https://github.com/mlflow/mlflow. However, they mostly support English benchmark datasets. Although multilingual datasets are published by some benchmarks, either they do not include the language used in this study, or the data size is small for task-specific tuning. We therefore craft an evaluation on sentiment analysis in this subsection131313We also provide a benchmark evaluation for available datasets from LLM benchmarks in Section 4.3  ###reference_###..\nFor this purpose, we extract 100 instances (50 instances for both positive and negative classes) from the Turkish sentiment analysis dataset used in task-specific tuning Aydo\u011fan and Kocaman (2023  ###reference_b5###). We avoid selecting from 5k instances used in task-specific tuning explained in Section 3.3  ###reference_###. Since inference is time costly, we use a small subset of this dataset for the evaluation. We also craft inference prompts for different scenarios including zero-shot to few-shot prompts. We check the generated text if it equals to positive or negative, and calculate the accuracy score accordingly. We measure accuracy since the inference dataset is fully balanced. We provide the inference prompts in Appendix A.3  ###reference_###.\nDuring inference, we load the models with 8-bit quantization due to limited hardware. Generation configuration involves the following hyperparameters. The temperature is set to 0.2. Beam search is applied with four beams, and top-p is set to 0.75. A single run of inference takes approximately from six hours (zero-shot) to eight hours (3-shot) for Llama-7b with these settings using two NVIDIA RTX 2080Tis.\nIn Table 4  ###reference_###, we provide the perplexity scores for all methods. The main observations are as follows.\nWe find that task-specific tuning cannot help improve perplexity scores previously. However, our extrinsic evaluation shows that task-specific tuning improves the performance of sentiment analysis. Specifically, we observe that task-specific tuned model (LllamaTurk-7b-t) is good at zero-shot inference, suggesting that task-specific instructions provide sufficient knowledge for zero-shot evaluation.\nWhen instruction tuning is employed alone, it has no significant impact on the performance of downstream task. However, we find that the highest accuracy score is obtained when instruction tuning and task-specific tuning are together employed (LllamaTurk-7b-i-t). Moreover, LllamaTurk-7b-i-t has a better few-shot performance compared to other methods including task-specific tuning.\nWhen continual training is employed alone (LllamaTurk-7b-c), we observe no significant improvement in the performance of downstream task. However, the performance is promising when it is used together with task-specific tuning (LllamaTurk-7b-c-t). This suggests further examination of continual training with task-specific tuning in different downstream tasks and datasets.\nSimilar to the perplexity experiments, we observe that vocabulary extension has no improvement on the performance of downstream task."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Benchmark Evaluation",
            "text": "In this subsection, we report the performance results on benchmark datasets. Since LLM evaluation benchmarks mostly include English datasets, we examine multilingual datasets in available LLM benchmarks. For this purpose, we use the Turkish subsets of XCOPA Ponti et al. (2020  ###reference_b40###) and Belebele Bandarkar et al. (2023  ###reference_b8###) datasets provided by LM Evaluation Harness Gao et al. (2023  ###reference_b19###). XCOPA is a benchmark to evaluate the ability of machine learning models to transfer commonsense reasoning. Belebele is a multiple-choice machine reading comprehension dataset, and each question has four multiple-choice. We modify the default prompts given in LM Evaluation Harness to align with our instruction prompting. We provide the inference prompts in Appendix A.4  ###reference_### and A.5  ###reference_###.\nSince the dataset sizes are small, we are not able to apply task-specific tuning in these benchmark datasets. Specifically, we observe almost no change in performance scores when XCOPA\u2019s 600 and Belebele\u2019s 900 instances are fine-tuned for the Turkish language, while the performance is improved in Section 4.2  ###reference_### with 5k instances. We thereby report the results for instruction tuning and related methods. Table 5  ###reference_### reports the accuracy scores on the XCOPA and Belebele datasets.\nThe results show that instruction tuning (LlamaTurk-7b-i) improves the performance of downstream task in both datasets. However, continual training and vocabulary extension have no significant benefits on the results. The results thereby align with the results of sentiment analysis reported in Section 4.2  ###reference_###."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Model Size",
            "text": "We provide an analysis of the impact of model size on adapting generative LLMs. For this purpose, we employ Llama models with 7b and 13b parameters. Figure 2  ###reference_### shows a histogram depicting the comparison between the fine-tuned models for instruction tuning (LlamaTurk-7b-i and LlamaTurk-13b-i) and task-specific tuning (LlamaTurk-7b-t and LlamaTurk-13b-t).\nIn both cases of applying instruction or task-specific tuning, we find that LlamaTurk-13b improves perplexity scores in all cases. However, task-specific tuning (LlamaTurk-13b-t) is still outperformed by the original Llama model Llama-13b in most cases.\nWe find that LlamaTurk-13b improves the performance of downstream task when it is applied with task-specific tuning and few-shot evaluation. On the other hand, the adaptation of a larger model with instruction tuning has no significant impact on the performance of downstream task.\n###figure_10### ###figure_11### ###figure_12### ###figure_13###"
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Multilingual Models",
            "text": "We also provide an analysis for the impact of multilingual generative LLMs on adapting generative LLMs. For this purpose, we fine-tune a multilingual model, MaLA-500 Lin et al. (2024b  ###reference_b35###). MaLA is developed to cover 534 languages by using vocabulary extension and continual training on Llama2 Touvron et al. (2023b  ###reference_b54###). Analyzing a multilingual LLM with an enriched vocabulary can provide more insights into LLM adaptation for low-resource languages.\nFigure 3  ###reference_### shows a histogram depicting the comparison between the fine-tuned models for instruction tuning (LlamaTurk-7b-i and MaLATurk-7b-i) and task-specific tuning (LlamaTurk-7b-t and MaLATurk-7b-t).\nPerplexity and accuracy scores of the original MaLA-7b model are improved by adapting MaLATurk-7b in both instruction and task-specific tuning. However, the perplexity of adapting a monolingual model LlamaTurk-7b is still better than adapting a multilingual model in all cases. Similarly, monolingual adaptation has better accuracy scores of task-specific tuning in most cases. The only benefit of adapting multilingual LLM is observed when instruction tuning is applied.\n###figure_14### ###figure_15### ###figure_16### ###figure_17###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This study examines different methods for adapting English-dominant generative large language models to low-resource languages.\nThe results show that continual training with raw text can improve perplexity, while vocabulary extension has no significant impact on adaptation performance. We also find that the adaptation with general-purpose instruction tuning has promising results in both perplexity and accuracy scores, while downstream task performance can be boosted by task-specific tuning. Furthermore, adapting a larger model with 13b parameters improves task performance with few-shot tuning. However, we observe no significant improvements by adapting a multilingual model.\nIn future work, we plan to adapt other open-source language models such as Llama2 Touvron et al. (2023b  ###reference_b54###) and Gemini Team et al. (2024  ###reference_b51###) to generalize our results to different models. Other adaptation methods can also be studied such as modification of model architecture since different model layers and tokenization algorithms might change the outcomes."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "This study employs a particular family of generative large language models (Llama and MaLA) for adapting open-source generative monolingual and multilingual LLMs to a low-resource language. Using other generative models might have different results in the experiments. Similarly, we use the Turkish language for the target of adaptation. Other languages might have different experimental results depending on the tuning and inference datasets with prompt examples. We therefore acknowledge the effect of the instruction set and prompting templates in the results.\nMoreover, benchmark evaluation is limited to multilingual datasets in this study due to the availability of benchmark datasets for the target language. Lastly, we would like to emphasize the limited hardware resources the experiments were conducted, which restricts using a variety of models including larger sizes (higher than 13b) and different model types (rather than Llama)."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Ethical Concerns",
            "text": "This study employs a low-resource language, Turkish, and our findings can guide to other researchers studying low-resource languages. We also provide both intrinsic and extrinsic performance evaluations that can be considered for deploying generative LLMs in similar tasks.\nTo provide transparency, we explain all details regarding text collections used in pretraining and fine-tuning our generative language models. Moreover, we report the details of the models and configurations with hyperparameters.\nSince the training corpus of generative LLMs involves a huge amount of raw text from different resources including the world wide web, it is inevitable to observe a risk of cultural and ethical bias towards different individuals and communities in the generated text of the published models in this study Kasneci et al. (2023  ###reference_b28###); Cetinkaya et al. (2024  ###reference_b10###). Moreover, training texts are contaminated with more problematic biases and polluted with a large amount of synthetic text generated by LLMs Denning and Rousse (2024  ###reference_b16###). Possible bias can be removed by filtering the corpus, however, we leave the study of such filtering to future work since it would require a dedicated effort but the scope of this study is to compare the adaptation methods of generative LLMs for low-resource languages.\nLastly, we estimate the carbon footprint of our study based on the energy usage of GPUs. We consider execution time in hours and electrical energy consumption in kWh, and assume that power consumption during training is equal to the maximum power drain of GPUs by operating at maximum power utilization (0.25 MW for 2080Ti, and 0.14 MW for A4000). We assume that 1 MWh is equivalent to 0.439 ton CO2eq141414https://enerji.gov.tr/evced-cevre-ve-iklim-elektrik-uretim-tuketim-emisyon-faktorleri. Our estimation ignores the carbon footprint of CPU utilization and the manufacturing costs of the hardware.\nSocial carbon cost is approximately 50.64, 3.84, and 0.55 kg CO2eq for a single run of continual training, instruction tuning, and task-specific tuning, respectively."
        }
    ],
    "url": "http://arxiv.org/html/2405.07745v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.5",
            "4"
        ]
    },
    "research_context": {
        "paper_id": "2405.07745v1",
        "paper_title": "LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language",
        "research_background": "### Motivation\n\nThe paper is motivated by the need to make natural language processing (NLP) technology more inclusive and accessible globally. It highlights the significant progress in open-source generative large language models (LLMs) for the English language, pointing out the disparity when it comes to low-resource languages. The motivation is further driven by the challenges these low-resource languages face, including cost-prohibitive monolingual pretraining and uneven performance in multilingual LLMs due to imbalanced training corpora.\n\n### Research Problem\n\nThe central research problem the paper addresses is how to systematically adapt open-source generative LLMs for low-resource languages. Specifically, it aims to explore different methodologies \u2014 continual training, supervised fine-tuning, and vocabulary extension \u2014 to assess their effectiveness individually and in combination for these languages. Another aspect of the research problem includes examining the impact of model size and multilingual capacity on the adaptability and performance of generative LLMs in low-resource language contexts.\n\n### Relevant Prior Work\n\nThe paper references a range of prior work to build its foundation:\n1. **Performance Discrepancy**: Xu et al. (2022) and Sun et al. (2024) note that proprietary LLMs often outperform open-source ones, though efforts like Touvron et al. (2023a), Jiang et al. (2023), and Almazrouei et al. (2023) are pushing the boundaries of open-source LLM performance and ethical considerations.\n2. **Language Resource Imbalance**: Studies by Wang et al. (2023) and Zhang et al. (2023a) show that open-source models predominantly use English training corpora, leading to better performance in English over other languages.\n3. **Monolingual and Multilingual Pretraining**: Monolingual approaches (Yang et al. (2023b), Nagoudi et al. (2023), Uludo\u011fan et al. (2024), Corr\u00eaa et al. (2024), Kesgin et al. (2024)) are computationally expensive (Zhao et al. (2023a)), while multilingual LLMs (Shliazhko et al. (2023), Scao et al. (2022), Lin et al. (2024b)) suffer from imbalanced performance across languages (Zhang et al. (2023a), Qin et al. (2024)).\n4. **Model Size and Multilingual Models**: Zhao et al. (2023a) and Yang et al. (2023a) discuss the relevance of model size for scalability and performance. Multilingual models like BLOOM (Scao et al. (2022)), Yi AI (2024), Aya (\u00dcst\u00fcn et al. (2024)), and MaLA (Lin et al. (2024a)) offer a framework for easier adaptation of low-resource languages owing to their diverse corpora and vocabularies, though BLOOM and Yi AI lack Turkish training data, and Aya is larger than MaLA.\n  \nThe convergence of these studies underscores the necessity for a focused investigation into adapting open-source generative LLMs for low-resource languages, which this paper seeks to address through a systematic approach using Turkish as a case study within the experimental confines of the Llama and MaLA models.",
        "methodology": "\n### Methodology\nIn this section, we explain the methods to adapt open-source generative LLMs (Large Language Models) for low-resource languages in detail.\n\n1. **Data Collection and Preprocessing**: \n   - **Source Identification**: Identify sources of text in the low-resource language, such as online articles, books, and social media.\n   - **Data Cleaning**: Apply text cleaning procedures, which may include removing URLs, special characters, and irrelevant content.\n   - **Normalization**: Normalize the text to have a consistent format, handling issues like varying spelling and punctuation.\n   - **Tokenization**: Use language-specific tokenizers that respect the linguistic properties of the low-resource language.\n\n2. **Model Selection and Adaptation**:\n   - **Base Model Selection**: Choose a suitable open-source generative LLM that can be effectively adapted. Factors such as model architecture, size, and pre-existing multilingual capabilities are considered.\n   - **Transfer Learning**: Fine-tune the selected base model using the preprocessed data from the low-resource language. This involves continuing the training of the model where it left off, using the new language-specific datasets.\n   - **Subword Units**: Implement subword units for handling unknown words and improving the model's ability to generate coherent text in the low-resource language.\n\n3. **Training Process**:\n   - **Hyperparameter Tuning**: Adjust hyperparameters such as learning rate, batch size, and number of epochs to optimize the model\u2019s performance on the low-resource language.\n   - **Validation and Testing**: Use validation and test sets specific to the low-resource language to evaluate the performance of the model. Metrics for evaluation may include perplexity, BLEU score, and human judgment.\n   - **Early Stopping**: Implement early stopping to prevent overfitting, monitoring validation loss to determine when to halt training.\n\n4. **Output Generation and Evaluation**:\n   - **Sample Generation**: Generate text samples in the low-resource language to qualitatively assess the model\u2019s performance.\n   - **Human Evaluation**: Conduct human evaluations to assess the fluency, coherence, and overall quality of the generated text. This may involve native speakers rating the text samples.\n   - **Comparison with Baselines**: Compare the adapted model\u2019s performance against baseline models or previous methods to demonstrate improvements.\n\n5. **Model Deployment**:\n   - **Resource Efficiency**: Ensure that the adapted model is resource-efficient and can be deployed on accessible hardware.\n   - **User Feedback Loop**: Implement mechanisms for user feedback to iteratively improve the model over time based on real-world usage.\n\nBy following these steps, we bring the capabilities of open-source generative LLMs to low-resource languages, thereby enhancing their accessibility and promoting linguistic diversity in AI research.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiment Setup\n\n**Main Objectives:** To evaluate methods for adapting generative large language models (LLMs) for low-resource languages, focusing on both intrinsic language comprehension and extrinsic downstream task performance.\n\n**Datasets and Evaluation Metrics:**\n1. **Intrinsic Evaluation:** Perplexity scores were used to gauge language comprehension.\n2. **Extrinsic Evaluation:** Accuracy scores were used to measure performance on downstream tasks such as sentiment analysis.\n\n**Baselines and Model Variants:**\n1. **Original models:** Llama-7b and Llama-13b, serving as baseline.\n2. **Variants:**\n   - **Continual Training (LlamaTurk-7b-c, LlamaTurk-13b-c):** Incremental training to expose the model to more raw text.\n   - **Instruction Tuning (LlamaTurk-7b-i, LlamaTurk-13b-i):** Trained with a set based on Alpaca's general-purpose instructions and responses.\n   - **Task-Specific Tuning (LlamaTurk-7b-t, LlamaTurk-13b-t):** Focused on specific tasks like sentiment analysis.\n   - **Combinations:**\n     - **Continual + Instruction Tuning (LlamaTurk-7b-c-i):** To explore if combining methods offers superior performance.\n     - **Instruction + Task-Specific Tuning (LlamaTurk-7b-i-t):** To evaluate combined tuning effects on tasks.\n   - **Vocabulary Extension (Llama-7b-v):** Adding new tokens to the model's vocabulary.\n\n#### Main Experimental Results\n\n1. **Perplexity Scores:**\n   - **Continual Training:** Lowest perplexity scores in the majority of cases, indicating improved language comprehension through incremental learning.\n   - **Instruction Tuning:** Improved perplexity scores except for xquad-context; close to baseline in that case.\n   - **Task-Specific Tuning:** Deteriorated perplexity scores, likely due to overspecialization.\n   - **Vocabulary Extension:** Increased perplexity scores in all cases, indicating ineffective incorporation of additional tokens.\n\n2. **Downstream Task Performance:**\n   - **Task-Specific Tuning:** Improved sentiment analysis performance and excelled in zero-shot inference.\n   - **Instruction Tuning:** Alone showed no significant improvement in downstream tasks; best accuracy achieved when combined with task-specific tuning.\n   - **Continual Training:** Alone provided no significant downstream task performance improvement but showed promise when combined with task-specific tuning.\n   - **Vocabulary Extension:** No improvement in downstream task performance observed.\n\n3. **Model Size Analysis:**\n   - **LlamaTurk-13b:** \n     - Perplexity improvement across all cases, yet task-specific tuning (LlamaTurk-13b-t) was still usually outperformed by the original Llama-13b.\n     - Improved downstream task performance when combined, instruction tuning alone had no significant effect.\n\n4. **Monolingual vs. Multilingual Models:**\n   - **Monolingual (LlamaTurk-7b):** Better perplexity and most task-specific tuning accuracy scores.\n   - **Multilingual:** Slight advantage only when instruction tuning is applied.\n\nIn summary, continual training and instruction tuning individually improve model performance for low-resource languages, whereas task-specific tuning improves specific downstream tasks but deteriorates general language understanding. Combining methods can be beneficial but requires careful calibration based on the dataset and task. Vocabulary extension was found to be ineffective without sufficient training data."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To assess the effectiveness of different combinations of adaptation methods on improving low-resource language performance in large language models.",
            "experiment_process": "Using the Llama-7b model with LoRA, the study examines several combinations of adaptation methods. The methods and combinations include: (1) Continual Training followed by Instruction Fine-tuning using raw Wiki data, (2) Continual Training followed by Task-Specific Fine-tuning, (3) Continual Training followed by both Instruction and Task-Specific Fine-tuning, (4) Instruction followed by Task-Specific Fine-tuning without Continual Training, (5) Vocabulary Extension followed by Instruction Fine-tuning, (6) Vocabulary Extension followed by Task-Specific Fine-tuning, and (7) Vocabulary Extension followed by Continual Training. Hyperparameters are kept consistent as explained in the earlier sections.",
            "result_discussion": "The experiments reveal that combinations involving Continual Training and Instruction Fine-tuning generally improve perplexity scores, while combinations involving Task-Specific Fine-tuning result in poorer perplexity scores. Applying both Continual Training and Instruction Fine-tuning together does not always outperform their individual applications, except when measured on specific instruction sets. Vocabulary Extension consistently results in higher perplexity scores, suggesting it struggles to integrate new tokens without sufficient training data. Task-specific tuning improves sentiment analysis performance, especially in zero-shot settings, while Instruction Fine-tuning alone shows no significant impact on downstream tasks. However, combined Instruction and Task-Specific tuning yield the highest downstream task accuracy. Larger models (LlamaTurk-13b) further improve performance in both perplexity and downstream tasks when task-specific tuning is applied.",
            "ablation_id": "2405.07745v1.No1"
        },
        {
            "research_objective": "To evaluate the effects of different adaptation methods on the generation performance and downstream tasks in low-resource languages using various model sizes and multilingual adaptations.",
            "experiment_process": "The section involves intrinsic and extrinsic evaluations to measure language comprehension (via perplexity scores) and downstream task performance. It considers varying model sizes (LlamaTurk-7b and LlamaTurk-13b) and applies different adaptation strategies (Continual Training, Instruction Tuning, Task-Specific Tuning, and Vocabulary Extension) on both monolingual and multilingual models. Evaluations are carried out using benchmark datasets with assessments focusing on perplexity and accuracy scores.",
            "result_discussion": "Continual Training leads to improved perplexity scores across most datasets. Instruction Tuning improves perplexity except for one dataset, where it remains close to the original model's score. Task-Specific Tuning, however, generally increases perplexity scores, indicating potential over-specialization. Vocabulary Extension increases perplexity scores, likely due to insufficient training data for effectively incorporating new tokens. Despite poorer perplexity scores, Task-Specific Tuning enhances downstream task performance, especially in zero-shot sentiment analysis. The highest accuracy for downstream tasks is achieved by combining Instruction and Task-Specific Tuning. Larger models (LlamaTurk-13b) show consistent improvements in perplexity scores and downstream tasks, except when compared to the monolingual adaptation, which generally outperforms multilingual adaptation. Multilingual model benefits are observed mainly with instruction tuning.",
            "ablation_id": "2405.07745v1.No2"
        }
    ]
}