{
    "title": "A Japanese-Chinese Parallel Corpus Using Crowdsourcing for Web Mining",
    "abstract": "Using crowdsourcing, we collected more than 10,000 URL pairs\n(parallel top page pairs) of bilingual websites that contain\nparallel documents and created a Japanese-Chinese parallel corpus of\n4.6M sentence pairs from these websites. We used a Japanese-Chinese\nbilingual dictionary of 160K word pairs for document and sentence\nalignment. We then used high-quality 1.2M Japanese-Chinese sentence\npairs to train a parallel corpus filter based on statistical\nlanguage models and word translation probabilities. We compared the\ntranslation accuracy of the model trained on these 4.6M sentence\npairs with that of the model trained on Japanese-Chinese sentence\npairs from CCMatrix (12.4M) Schwenk et al. (2021b), a\nparallel corpus from global web mining. Although our corpus is only\none-third the size of CCMatrix, we found that the accuracy of the\ntwo models was comparable and confirmed that it is feasible to use\ncrowdsourcing for web mining of parallel data.111Work in\nprogress",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Parallel data is vital in machine translation for traditional\nencoder-decoders and recent large language models. From an analysis of\nPalm\u2019s training data, Briakou et al. (2023  ###reference_b2###) showed that\nlarge language models could translate because their training data\ncontain parallel data incidentally.\nRelatively small large language models of around 10B parameters have\npoor translation accuracy. However, Xu et al. (2024  ###reference_b36###) proposed\nALMA, a method of fine-tuning an LLM on a large amount of monolingual\ndata, followed by fine-tuning on a small amount of high-quality\nbilingual data. They achieved translation accuracy comparable to GPT-3\nusing relatively small LLMs. Guo et al. (2024  ###reference_b9###) showed that\ncontinuous pre-training of large language models on large amounts of\nparallel data before fine-tuning on high-quality bilingual data\nimproves translation accuracy over ALMA.\nThis paper discusses a method for collecting Japanese and Chinese\nparallel sentence pairs from the web. Translation between Japanese and\nChinese is considered one of the most important non-English language\npairs in terms of the number of speakers and the scale of the economy.\nWe specifically report on the effectiveness of crowdsourcing in\ncollecting URLs of websites containing parallel data.\nMorishita et al. (2022b  ###reference_b17###) proposed a method of collecting\nparallel URLs (parallel page pairs) using cloud workers for domain\nadaptation of machine translation. We collected parallel top page URL\npairs of bilingual websites by specifying only language pairs to the\ncrowd workers, with no particular restriction on the target domain.\nThe experiment results show crowdsourced websites can collect more\nparallel sentence pairs with less crawling than automatically\ncollected bilingual websites using Common Crawl. We also show that\nthe translation accuracy achieved using the Japanese-Chinese parallel\ncorpus created using crowdsourcing is comparable to that achieved\nusing Japanese-Chinese pairs of CCMatrix\nSchwenk et al. (2021b  ###reference_b27###), a parallel corpus created by global\nweb mining. It is worth noting that our corpus only contains\none-third of the data present in CCMatrix.\nWe release a 4.6M Japanese-Chinese parallel corpus created using\ncrowdsourcing as JParaCrawl Chinese v2.0 for research purposes only.\n222https://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/  ###reference_rawl/###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Web mining for parallel data",
            "text": "Research on collecting bilingual data by mining the web began around\n2000 Resnik (1999  ###reference_b24###); Uszkoreit et al. (2010  ###reference_b32###). We can\nbroadly divide current research into hierarchical mining (local\nmining) and global mining.\nIn hierarchical mining (or local mining), based on the web\u2019s\nhierarchical structure, we first search for websites that include\nparallel documents, then search for parallel document pairs within a\nwebsite, and then search for parallel sentence pairs within a parallel\ndocument pairs. In global mining, we consider the web a flat, massive\nset of sentences. We use similarity based on a multilingual sentence\nembedding model to find a sentence\u2019s translation among all sentences\nin the web in different languages. ParaCrawl\nBa\u00f1\u00f3n et al. (2020  ###reference_b1###) is a prime example of the former, and\nCCMatrix Schwenk et al. (2021b  ###reference_b27###) is a prime example of the\nlatter.\nMost previous parallel corpora are created by local mining. Like\nEuroParl Koehn (2005  ###reference_b10###) and OpenSubtitles\nLison and Tiedemann (2016  ###reference_b12###), we first identify a\nwebsite that contains parallel documents, extract bilingual document\npairs based primarily on metadata, and then perform sentence\nalignment.\nThe first successful example of global mining is WikiMatrix\nSchwenk et al. (2021a  ###reference_b26###), which collected bilingual text\npairs from Wikipedia in various languages using\nLASER333https://github.com/facebookresearch/LASER  ###reference_###, a\nmultilingual sentence embedding model. CCMatrix\nSchwenk et al. (2021b  ###reference_b27###) applies global mining to CCNet\nWenzek et al. (2020  ###reference_b35###), a monolingual corpus of various\nlanguages extracted from Common Crawl. In the No Language Left Behind\n(NLLB) project Team et al. (2022  ###reference_b29###), they extended CCMatrix to over 200\nlanguages.\nPerforming global mining for the entire web requires enormous\ncomputational resources. We consider local mining a more realistic\napproach to collecting bilingual data for specific language pairs, as\nin the case of JParaCrawl\nMorishita et al. (2020  ###reference_b16###, 2022a  ###reference_b15###),\nwhich collected parallel data between Japanese and English.\nIn local mining, many previous works on document and sentence\nalignment exist, but little research has been done on how to find\nwebsites that contain parallel data. ParaCrawl\nBa\u00f1\u00f3n et al. (2020  ###reference_b1###) used the Common Crawl archives to\ncollect websites that contain bilingual text. They applied a language\ndetector on each site\u2019s web pages and looked for sites that contained\napproximately the same amount of text in the language pair to be\ncollected. CCAligned El-Kishky et al. (2020  ###reference_b6###) analyzed\nthe Common Crawl archives using language-identifiable strings in URLs\nas clues and collected parallel URL pairs.\nMorishita et al. (2022b  ###reference_b17###) used a cloud worker to collect\nparallel URLs for domain adaptation of machine translation."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Japanese-Chinese parallel corpora",
            "text": "The most widely used Japanese-Chinese parallel corpus for research is\nJapanese-Chinese ASPEC (Asian Scientific Paper Excerpt Corpus), which\nhas 0.68 million sentence pairs consisting of abstracts of Japanese\nscientific papers and their manual translation into Chinese\nNakazawa et al. (2016  ###reference_b18###). The JPO-NICT Chinese-Japanese\nparallel\ncorpus444https://alaginrc.nict.go.jp/jpo-outline.html  ###reference_ml###,\nwhich has about 130 million Japanese-Chinese patent sentence pairs,\nextracted from patent applications in Japan and China based on patent\nfamilies. ASPEC and JPO-NICT are parallel corpora for specific fields\nand unsuitable for general translation between Japanese and Chinese.\nWCC-JC 3.0 Zhang et al. (2022  ###reference_b38###, 2023  ###reference_b39###) is\na Japanese-Chinese parallel corpus of approximately 3M sentence pairs\ncollected from the web, including movie and TV subtitles, lyrics, and\nnews articles.\nIt is available for research purposes by sending an email request to\nthe authors.\nJapanese-Chinese bilingual data collected from Wikipedia include\nLinguaTools-WikiTitles v2014 (1.7M sentence pairs), which is contained\nin OPUS Tiedemann (2012  ###reference_b31###), a collection of open parallel\ncorpora, WikiMatrix (1.3M sentence pairs)\nSchwenk et al. (2021a  ###reference_b26###), and Wikipedia Chinese-Japanese\nParallel Corpus (0.13M sentence pairs)\nChu et al. (2014  ###reference_b3###, 2015  ###reference_b4###). OpenSubtitles\nv2018\nLison and Tiedemann (2016  ###reference_b12###); Lison et al. (2018  ###reference_b13###),\ncollected from movie subtitles, contains 1.1M Japanese-Chinese\nsentence pairs.\nThe largest publicly available Japanese-Chinese parallel data\ncollected from the web is CCMatrix Schwenk et al. (2021b  ###reference_b27###),\nwith approximately 12M sentence pairs. JParaCrawl Chinese v1.0\nMorishita et al. (2020  ###reference_b16###) contains 83K Japanese-Chinese\nsentence pairs. The Asian Language Treebank\nThu et al. (2016  ###reference_b30###) translates English Wikinews into\nJapanese, Chinese, and other Asian languages and contains\napproximately 20,000 sentences divided into train, dev, and test sets."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Parallel website mining",
            "text": "Our procedure for collecting parallel data is the same as ParaCrawl\nBa\u00f1\u00f3n et al. (2020  ###reference_b1###) and follows the pipeline of\nBitextor555https://github.com/bitextor/bitextor  ###reference_###, which\nconsists of web crawling, document alignment, sentence alignment, and\nparallel corpus filtering.\nIn ParaCrawl, they determine which websites to crawl by analyzing the\nCommon Crawl archives. They first apply CLD2\n666https://github.com/CLD2Owners/cld2  ###reference_### to each web page\nto identify its language and extract websites containing approximately\nthe same target language pair texts. They then crawl the extracted\nwebsites with Heritrix\n777https://github.com/internetarchive/heritrix3  ###reference_x3###.\nIn this study, we analyzed 12 sets of Common Crawl archives (104TB in\ntotal) published from September 2021 to June 2023 using the language\ndetector CLD2 and enumerated about 40,000 websites that contain\nroughly equal amounts of Japanese and Chinese text in order of total\ntext volume in a website.\nWe used Extractor888https://github.com/paracrawl/extractor  ###reference_### from the ParaCrawl project for this procedure.\nMorishita et al. (2022b  ###reference_b17###) proposed a method of collecting\nparallel URLs (parallel web pages) using cloud workers for domain\nadaptation of machine translation. We asked crowd workers to collect\nwebsites containing parallel pages, specifying only the language\npairs, and report the pair of URLs of the Japanese and Chinese top\npages for each website.\nFor both parallel top page URL pairs collected using crowdsourcing and\nbilingual website URLs obtained from Common Crawl, we used Heritrix to\ncrawl each website for up to 48 hours, crawling Word and PDF files as\nwell as HTML."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Parallel corpus filtering",
            "text": "The ParaCrawl project has two parallel corpus filters, Bicleaner and\nBicleaner AI. Bicleaner\nS\u00e1nchez-Cartagena et al. (2018  ###reference_b25###); Ram\u00edrez-S\u00e1nchez et al. (2020  ###reference_b22###)\nextracts features using word translation probabilities and statistical\nlanguage models and trains a random-forest classifier to classify\nwhether a sentence pair is parallel. Bicleaner AI\nZaragoza-Bernabeu et al. (2022  ###reference_b37###) uses a pre-trained\nmultilingual language model to train a binary classifier. Both\nmethods require high-quality parallel data to train the classifier.\nIn this study, we used\nBicleaner131313https://github.com/bitextor/bicleaner  ###reference_### to\nminimize the use of external resources and improve computational\nefficiency. We used mecab and pkuseg\nLuo et al. (2019  ###reference_b14###)141414https://github.com/lancopku/pkuseg-python  ###reference_###\nfor Japanese and Chinese word segmentation to compute word frequency\nand obtain word alignment from high-quality parallel sentence pairs.\nWe used AWESOME-align Dou and Neubig (2021  ###reference_b5###) for word alignment\nto compute word translation probabilities.\nTo train the parallel corpus filter, we used an in-house\nJapanese-Chinese parallel corpus (1.2M sentence pairs) consisting of\ntravel conversations, dictionary examples, literary works, and\nnewspaper articles. Among the in-house Japanese-Chinese parallel\ncorpus, the Basic Travel Expression Corpus (BTEC, about 0.5M sentence\npairs) Takezawa et al. (2002  ###reference_b28###) is the largest, followed by\nthe dictionary example sentences (about 260,000 sentence pairs).\nWe used the Japanese-Chinese Bicleaner model to compute scores for\nbilingual sentence pairs and extract sentence pairs with a threshold\nvalue of 0.5 or higher. We further calculated each sentence\u2019s vector\nusing the multilingual sentence embedding model LaBSE\nFeng et al. (2022  ###reference_b8###) and filtered out sentence pairs with a\ncosine distance below a threshold of 0.7.\nFor URLs obtained from Common Crawl and URL pairs obtained from\ncrowdsourcing, Table 1  ###reference_### shows the number of\nsites we successfully crawled, the number of sites that yielded any\nparallel sentence pairs, and the total number of parallel sentence\npairs obtained. The percentage of successful parallel sentence pair\nextraction was considerably higher for websites obtained from\ncrowdsourcing, at 74.5 percent, compared to 27.2 percent for those\nobtained from Common Crawl. The total number of parallel sentence\npairs obtained for websites from Common Crawl and those from\ncrowdsourcing is 2.8M and 4.6M sentence pairs, respectively,\nindicating that crowdsourcing can collect more sentence pairs with\nless crawling than analyzing Common Crawl."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Translation experiments",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "We examined the accuracy of Japanese-to-Chinese and\nChinese-to-Japanese translations to assess the quality of parallel\nsentence pairs collected using crowdsourcing.\nTable 2  ###reference_### shows the Japanese-Chinese parallel\ndatasets used in the translation experiments.\nWe used CCMatrix Schwenk et al. (2021b  ###reference_b27###),\nWikiTitles Tiedemann (2012  ###reference_b31###),\nWikiMatrix Schwenk et al. (2021a  ###reference_b26###), and\nOpenSubtitles2018 Lison et al. (2018  ###reference_b13###) for\ncomparison because they have more than one million sentence pairs and\nare readily available.\nWe combined WikiTitles, WikiMatrix, and OpenSubtitles2018 into one\n(wt-wm-os), and trained three models from ccmatrix, wt-wm-os, and\ncrowdsourcing.\nWe further trained one model from all five corpora.\nFor the development set, we used\nnews-commentary-v18 (1,677 sentence\npairs)151515https://data.statmt.org/news-commentary/v18.1/  ###reference_8.1/###,\nthe dev set of Asian Language Treebank Parallel Corpus (1,000 sentence\npairs)161616https://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/index.html  ###reference_mutiyama/ALT/index.html###,\nand the dev set of FLORES-200 (997 sentence\npairs)171717https://github.com/facebookresearch/flores  ###reference_###.\nFor the test sets, we used the test set of Asian Language Treebank\n(1,000 sentence pairs), the test set of ASPEC-JC (2107 sentence pairs),\nthe devtest of FLORES-200 (1012 sentence pairs), and NTREX-128 (1997\nsentence pairs) Federmann et al. (2022  ###reference_b7###). We also used as our\ntest set 1,000 sentences randomly sampled from our in-house\nJapanese-Chinese parallel corpus (bitext_cj) and our in-house Chinese\ntranslations of news (495 sentences) and question answering (497\nsentences) from the WMT2023 Japanese-English test set (wmt2023j). The\nsource language of these test sets is Japanese for aspec-jc and\nwmt2023j, a mixture of Japanese and Chinese for bitext_cj, and\nEnglish for the others."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Experiment condition",
            "text": "We used fairseq Ott et al. (2019  ###reference_b19###) as the translation\nsoftware and transformer big Vaswani et al. (2017  ###reference_b34###) as the\ntranslation model.\nTable 3  ###reference_### shows the hyper parameters of Transformer.\nWe used sentencepiece Kudo and Richardson (2018  ###reference_b11###) to\ntokenize training, development, and test data.\nThe vocabulary size is 32K for both Japanese and Chinese.\nWe evaluated translation accuracy using sacreBLEU\nPapineni et al. (2002  ###reference_b20###); Post (2018  ###reference_b21###) and COMET\n(wmt22-comet-da) Rei et al. (2020  ###reference_b23###)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Translation accuracy",
            "text": "Table 4  ###reference_### shows the translation accuracy from Japanese to\nChinese, and Table 5  ###reference_### shows that from Chinese to Japanese.\nAmong the three translation models, ccmatrix, wt-wm-os, and\ncrowdsourcing, ccmatrix and crowdsourcing have about the same translation\naccuracy, while wt-wm-os is less accurate.\nBetween ccmatrix and crowdsourcing, crowdsourcing has higher\ntranslation accuracy from Japanese to Chinese, and ccmatrix has higher\naccuracy from Chinese to Japanese.\nCreating a single translation model from all bilingual data yields a\nhigher translation accuracy than these three models."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Crowdsourcing (4.6M) has only one-third of sentence pairs of CCMatrix\n(12.4M), but the translation accuracy is about the same. This\nindicates that parallel sentence pairs obtained using crowdsourcing\nhave higher quality than CCMatrix.\nFor Japanese-to-Chinese translation, the higher accuracy of our\nparallel corpus collected using crowdsourcing compared to CCMatrix is\nprobably due to the fact that the crowdsourcing was done in Japan by\nJapanese crowd workers. Many of the websites collected by Japanese\ncrowd workers are Japanese websites that include pages translated into\nChinese. More diversity may be needed when translating Chinese into\nJapanese.\nThis study evaluated the translation accuracy of parallel sentence\npairs collected using crowdsourcing (4.6M). However, we expect that\nadding parallel sentence pairs collected using Common Crawl (2.8M)\nwill increase diversity and improve translation accuracy from Chinese\nto Japanese. Another issue is that evaluating Chinese-to-Japanese\ntranslation accuracy could be more reliable if we had a test set whose\nsource sentences originated from Chinese and whose reference sentences\nare direct manual translations from Chinese to Japanese."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper describes an attempt to create a Japanese-Chinese parallel\ncorpus from the web by collecting URL pairs of parallel websites\nthrough crowdsourcing. We collected 4.6M sentence pairs and showed\nthat we could achieve the same level of translation accuracy as the\nCCMatrix (12.4M) with one-third of the data.\nIn the future, we will create an adult filter to filter the parallel\nsentence pairs (2.8M) collected using Common Crawl and add these to\nmake a Japanese-Chinese parallel corpus with more diverse content. We\nwill also train a machine translation model using the sentence pairs\ncreated with bilingual dictionary-based document and sentence\nalignment to perform machine translation-based document and sentence\nalignment, which could improve the quality of parallel sentence pairs."
        }
    ],
    "url": "http://arxiv.org/html/2405.09017v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.3",
            "4.1",
            "5"
        ]
    },
    "research_context": {
        "paper_id": "2405.09017v1",
        "paper_title": "A Japanese-Chinese Parallel Corpus Using Crowdsourcing for Web Mining",
        "research_background": "### Paper Motivation, Research Problem, and Relevant Prior Work:\n\n#### Motivation:\nThe paper is motivated by the need to enhance machine translation between Japanese and Chinese, a critical non-English language pair due to its substantial number of speakers and the economic significance of the regions where these languages are spoken. Despite advancements in large language models (LLMs), translation accuracy for relatively smaller LLMs remains insufficient. The paper aims to address this gap by developing a method to collect Japanese and Chinese parallel sentence pairs from the web, which is crucial for improving translation models.\n\n#### Research Problem:\nThe research problem is how to efficiently and effectively collect a high-quality Japanese-Chinese parallel corpus to improve machine translation. The paper investigates the use of crowdsourcing to collect URLs of websites containing parallel data. This approach is evaluated to see if it can outperform traditional web-mining techniques, such as those used in creating the CCMatrix corpus, considering both the quantity and quality of the collected data.\n\n#### Relevant Prior Work:\n1. **Parallel Data in Machine Translation:**\n   - Briakou et al. (2023) highlighted the importance of parallel data in the training of large language models, showing that incidental inclusion of parallel data can enable LLMs to perform translation tasks.\n\n2. **Fine-Tuning Techniques for Machine Translation:**\n   - Xu et al. (2024) proposed ALMA, a method that fine-tunes a large language model on extensive monolingual data followed by high-quality bilingual data, achieving translation accuracy comparable to GPT-3.\n   - Guo et al. (2024) showed that continuous pre-training on large amounts of parallel data before fine-tuning could further enhance translation accuracy beyond what ALMA achieved.\n\n3. **Crowdsourcing for Parallel Corpus Collection:**\n   - Morishita et al. (2022b) introduced a method of using cloud workers to collect parallel page pairs for domain adaptation in machine translation. This technique inspired the paper's approach to using crowdsourcing for the Japanese-Chinese language pair.\n\n4. **Existing Parallel Corpora:**\n   - Schwenk et al. (2021b) created CCMatrix, a parallel corpus generated through global web mining. The paper compares the newly created corpus with CCMatrix in terms of quantity and translation accuracy.\n\nBy building on these prior works, the paper contributes a novel crowdsourcing-based method for harvesting a large-scale, high-quality parallel Japanese-Chinese corpus, released as JParaCrawl Chinese v2.0, and demonstrates that this method can achieve competitive translation results efficiently.",
        "methodology": "In the paper \"A Japanese-Chinese Parallel Corpus Using Crowdsourcing for Web Mining,\" the methodology section outlines the procedures and methods used to collect and construct a parallel corpus, utilizing both established techniques and novel contributions. The key components and innovations of the proposed method or model are as follows:\n\n1. **Web Crawling**:\n   - The procedure for collecting parallel data follows the ParaCrawl approach (Ba\u00f1\u00f3n et al., 2020) and employs the Bitextor pipeline. This consists of several critical steps: web crawling, document alignment, sentence alignment, and parallel corpus filtering.\n   - Websites for crawling are determined by analyzing Common Crawl archives. The language of each web page is identified using CLD2 (Compact Language Detector 2). Websites containing roughly equal amounts of the target language pair (Japanese and Chinese, in this case) are extracted.\n   - Heritrix, an open-source web crawler, is utilized to crawl these identified websites.\n\n2. **Language Detection and Crawling Scope**:\n   - In this study, 12 sets of Common Crawl archives, totaling 104TB and dating from September 2021 to June 2023, were analyzed using CLD2. This process enumerated about 40,000 websites that had approximately equal amounts of Japanese and Chinese text.\n   - Extractor, another tool from the ParaCrawl project, was used to handle this procedure efficiently.\n\n3. **Crowdsourcing for URL Collection**:\n   - Inspired by Morishita et al. (2022b), the method includes a novel approach of using cloud workers to collect parallel URLs for domain adaptation of machine translation. Crowd workers were tasked with identifying and reporting pairs of URLs for the top pages in Japanese and Chinese for each website. This crowdsourcing method ensures a broader and possibly more accurate collection of parallel web pages by leveraging human intelligence.\n  \n4. **Extended Crawling**:\n   - For both crowdsourced URL pairs and bilingual website URLs obtained from Common Crawl, the thoroughness of the crawling process is emphasized. Each website is crawled for up to 48 hours, including diverse content types such as HTML pages, Word documents, and PDFs.\n\nTogether, these steps form a comprehensive approach to building a Japanese-Chinese parallel corpus, blending automated processes such as web crawling and language detection with the innovative use of crowdsourcing to refine and extend the collection of parallel web pages.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Experiment Setup:**\nWe examined the accuracy of Japanese-to-Chinese and Chinese-to-Japanese translations to assess the quality of parallel sentence pairs collected using crowdsourcing. The main experiment utilized the following datasets, baselines, and evaluation metrics:\n\n**Datasets:**\n1. **Training Datasets:**\n   - **CCMatrix** (Schwenk et al., 2021b)\n   - **WikiTitles** (Tiedemann, 2012)\n   - **WikiMatrix** (Schwenk et al., 2021a)\n   - **OpenSubtitles2018** (Lison et al., 2018)\n   - Combined dataset of **WikiTitles**, **WikiMatrix**, and **OpenSubtitles2018** (wt-wm-os)\n   - Crowdsourced parallel sentence pairs\n   - All five corpora combined\n\n2. **Development Sets:**\n   - **news-commentary-v18** (1,677 sentence pairs)\n   - **Asian Language Treebank Parallel Corpus dev set** (1,000 sentence pairs)\n   - **FLORES-200 dev set** (997 sentence pairs)\n\n3. **Test Sets:**\n   - **Asian Language Treebank test set** (1,000 sentence pairs)\n   - **ASPEC-JC test set** (2,107 sentence pairs)\n   - **FLORES-200 devtest set** (1,012 sentence pairs)\n   - **NTREX-128** (1,997 sentence pairs)\n   - **In-house Japanese-Chinese parallel corpus** (1,000 randomly sampled sentences - bitext_cj)\n   - **In-house Chinese translations of news** (495 sentences from WMT2023 Japanese-English test set - wmt2023j)\n   - **In-house Chinese translations of question answering** (497 sentences from WMT2023 Japanese-English test set - wmt2023j)\n\n**Baselines:**\nThe following models were trained and compared:\n1. Model trained on **CCMatrix**\n2. Model trained on **wt-wm-os** (combined WikiTitles, WikiMatrix, and OpenSubtitles2018)\n3. Model trained on **crowdsourced** parallel sentence pairs\n4. Model trained on **all five corpora combined**\n\n**Evaluation Metrics:**\nThe experiment mainly evaluated translation accuracy. However, the specific metrics used to assess accuracy such as BLEU scores, were not detailed in the excerpt provided.\n\n**Main Experimental Results:**\nThe main experimental results were not detailed in the provided excerpt. The focus was on the setup and the datasets used for the comparisons. The results might typically include comparative translation accuracy, demonstrating which training dataset(s) led to superior translation performance in Japanese-Chinese and Chinese-Japanese translations."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the efficiency of crowdsourcing in collecting high-quality Japanese-Chinese parallel sentences and its impact on translation accuracy compared to existing large-scale datasets like CCMatrix.",
            "experiment_process": "The study utilized the ParaCrawl project filters, Bicleaner and Bicleaner AI, to extract high-quality Japanese-Chinese parallel sentences. They used Japanese and Chinese word segmentation using mecab and pkuseg and calculated word translation probabilities via AWESOME-align. The in-house dataset comprising 1.2M sentence pairs in various categories was employed for training the parallel corpus filter. Sentence pairs were scored using Bicleaner and filtered using LaBSE for vector computation, retaining pairs with a cosine similarity above 0.7. URLs from Common Crawl and crowdsourcing were then processed to extract parallel sentence pairs. The study compared extraction effectiveness between Common Crawl and crowdsourcing, noting a significant difference in successful extraction rates: 74.5% for crowdsourcing and 27.2% for Common Crawl.",
            "result_discussion": "The ablation study revealed that the crowdsourced parallel corpus (4.6M sentence pairs) exhibited comparable translation accuracy to the much larger CCMatrix corpus (12.4M sentence pairs). Despite its smaller size, the quality of parallel pairs obtained via crowdsourcing was higher. The likely reason for such performance is the employment of Japanese crowd workers, leading to the capture of high-quality Japanese pages translated into Chinese. It suggests that adding more diverse datasets, such as those collected from Common Crawl, could further improve translation accuracy, especially for Chinese-to-Japanese translation.",
            "ablation_id": "2405.09017v1.No1"
        },
        {
            "research_objective": "To assess the quality of Japanese-Chinese parallel sentence pairs collected using crowdsourcing by comparing the translation accuracy with other publicly available datasets.",
            "experiment_process": "The datasets used in this comparison study include CCMatrix, WikiTitles, WikiMatrix, and OpenSubtitles2018, combined into one dataset (wt-wm-os). Three different models were trained separately using ccmatrix, wt-wm-os, and crowdsourcing data. A fourth model was trained using all five datasets. Development sets consisted of news-commentary-v18, the dev set of Asian Language Treebank Parallel Corpus, and the dev set of FLORES-200. Test sets included the test set of Asian Language Treebank, the test set of ASPEC-JC, the devtest of FLORES-200, NTREX-128, and additional test sets comprising randomly sampled sentences from the in-house parallel corpus and Chinese translations of news and question answering sets from WMT2023 Japanese-English test set.",
            "result_discussion": "The analysis demonstrated that despite the crowdsourced data being only a third of the size of CCMatrix, the translation accuracy for Japanese-to-Chinese translations was on par with the larger dataset. The study suggests the high quality of crowdsourced data could be attributed to the contextual relevancy and specificity managed by Japanese crowd workers. The findings indicate potential improvements in Chinese-to-Japanese translation accuracy by integrating additional parallel sentence pairs from more diverse sources, like Common Crawl.",
            "ablation_id": "2405.09017v1.No2"
        }
    ]
}