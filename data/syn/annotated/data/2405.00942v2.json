{
    "title": "LLaVA Finds Free Lunch: Teaching Human Behavior Improves Content Understanding Abilities Of LLMs",
    "abstract": "Communication is defined as \u201cWho says what to whom with what effect.\u201d A message from a communicator generates downstream receiver effects, also known as behavior. Receiver behavior, being a downstream effect of the message, carries rich signals about it. Even after carrying signals about the message, the behavior data is often ignored while training large language models. We show that training LLMs on receiver behavior can actually help improve their content-understanding abilities. Specifically, we show that training LLMs to predict the receiver behavior of likes and comments improves the LLM\u2019s performance on a wide variety of downstream content understanding tasks. We show this performance increase over 40 video and image understanding tasks over 23 benchmark datasets across both 0-shot and fine-tuning settings, outperforming many supervised baselines. Moreover, since receiver behavior, such as likes and comments, is collected by default on the internet and does not need any human annotations to be useful, the performance improvement we get after training on this data is essentially free-lunch. We release the receiver behavior cleaned comments and likes of 750k images and videos collected from multiple platforms along with our instruction-tuning data.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Communication is defined by five factors: sender, message, channel, receiver, and behavior [68  ###reference_b68###, 46  ###reference_b46###, 47  ###reference_b47###]. Lasswell encoded these five factors in the phrase, \u201cWho says what to whom with what effect.\u201d [68  ###reference_b68###, 46  ###reference_b46###, 47  ###reference_b47###]. Human behavior occurs as a downstream artifact in the process of communication. Behavior is produced by the receiver as a response to the message sent by the sender. Being a downstream effect, behavior can help us infer important signals about the message itself. These signals, if properly harnessed, should be able to increase performance on the message understanding tasks popular in NLP and CV, like question answering, sentiment analysis, topic classification, etc. Despite this, behavior data is considered noise and is ignored while training large language models [15  ###reference_b15###, 61  ###reference_b61###] and also large vision and language models [54  ###reference_b54###, 88  ###reference_b88###]. In this paper, we explore this line of thought more.\n###figure_1### Humans produce two kinds of behavioral signals upon observing a message [11  ###reference_b11###, 64  ###reference_b64###]: perceptual signals and actions as behavior. Perceptual signals, like seeing, touching, and hearing, help a receiver primarily sense the world around her, ultimately guiding her actions. Actions are how a receiver acts on the outside world.\nThe signals produced by the human receiver upon receiving a message carry information about the message itself (Fig. 1  ###reference_###). For instance, if a person\u2019s heartbeat rises upon watching a movie scene, it can help us infer that perhaps the scene was an exciting scene [24  ###reference_b24###]. Similarly, regressing while reading is indicative of important or confusing phrases [14  ###reference_b14###]. In these cases, perception behavior helps us derive inferences about content. In a similar vein, the actions a person performs after watching a movie, such as comments and likes, carry signals about the movie (Fig. 1  ###reference_###, 2  ###reference_###).\nExpanding on these ideas, prior literature has shown that harnessing perceptual signals, like eye movements, saliency, keystrokes, mouse movements, and FMRI, by modeling them together with content understanding tasks can improve both NLP and CV tasks. For instance, integration with perception signals causes performance improvement in tasks like visual and natural language question answering [60  ###reference_b60###, 42  ###reference_b42###, 71  ###reference_b71###], text and image sentiment analysis [42  ###reference_b42###, 10  ###reference_b10###, 26  ###reference_b26###], natural language inference [42  ###reference_b42###], part-of-speech identification [8  ###reference_b8###, 9  ###reference_b9###], named entity recognition [32  ###reference_b32###, 31  ###reference_b31###], syntactic parsing [63  ###reference_b63###], image captioning [21  ###reference_b21###], and visual object detection [74  ###reference_b74###, 44  ###reference_b44###].\nWhile the initial studies show that perceptual signals have much promise for improving downstream content understanding, they have a few significant issues due to which integrating human perception has not seen wide adoption in training LLMs. The human processing signals can only be collected in lab settings requiring specialized lab equipment and are thus expensive to collect and thus are also limited in number. For example, the largest datasets containing the human processing signals are SALICON [37  ###reference_b37###] and Cheng et al. [19  ###reference_b19###] for visual saliency (10k images each), CELER [12  ###reference_b12###] and Dundee corpus [39  ###reference_b39###] containing eye movements over 28k sentences and 20 news articles respectively, and Dhakal et al. [22  ###reference_b22###] containing keystroke patterns over 1.5k sentences. Clearly, these datasets, while making important contributions, do not scale to the level at which today\u2019s large language models are trained (trillions of natural language and image tokens).\nOn the other hand, actions (the other type of behavioral signals produced by a human receiver) are collected at a large scale in the form of digital analytics. Examples of this kind of data are likes, views, shares, comments, and purchase histories on images, tweets, videos, webpages, and other kinds of media. Action data has a much broader representation than is possible in lab settings and is much cheaper to collect than using specialized lab equipment. At the same time, actions have not been much investigated in the literature for their potential to improve downstream content understanding. Further, they are much more noisier than perceptual signals collected in the lab, owing to their collection in real-world settings.\nTherefore, in this paper, we make initial efforts to collect and understand digital analytics at scale with the aim of integrating them with LLMs to improve their downstream content understanding capabilities. We introduce methods for filtering and cleaning behavioral data and then propose tasks for large language and vision models, leading to improvements in language and visual content understanding tasks. For this, we look to Reddit and YouTube as two major sources of visual content and human behavior in the form of viewer comments, likes, and upvotes. From Reddit, we collect 5 million images and videos along with their upvotes and top-rated comments from two major subreddits (r/pics and r/videos). Similarly, from YouTube, we collect 2.2 million videos from 30000 channels along with their likes, views, and top user comments. After extensive filtering and cleaning, we are left with 730k samples of videos and images across the two platforms which we use for the next steps.\nAfter collecting user behavior over image and video content, next, we design tasks to teach large vision and language models (VLMs) to simulate user behavior. For this, we use an instruction fine-tuning format. Given a video or an image and the other metadata like time of post and channel, we ask the model to simulate user behavior of likes and comments. See Fig 2  ###reference_###, Listing 1  ###reference_### for examples. We choose LLaMA-Vid [52  ###reference_b52###] as our base model to teach it the user behavior. We call the resultant model Behavior-LLaVA (Large Language and Vision Assistant) [54  ###reference_b54###]. Note that all the vision language models [54  ###reference_b54###, 88  ###reference_b88###, 33  ###reference_b33###, 50  ###reference_b50###, 52  ###reference_b52###] are trained on image-text instructions where text is some kind of human annotations [53  ###reference_b53###, 69  ###reference_b69###] or generated by stronger LLMs like GPT-4. Both these options require a significant amount of money. It is noteworthy that our model does not add any extra annotation or API cost to generate data. Rather, we rely on sampling data from digital analytics databases, which are anyway being collected for various reasons.\nWe make the following contributions with this work:\n1) We explore the idea of learning human behavior, resulting in better content understanding. We test this for action-level behavior data corresponding to receiver comments and likes. We collect a dataset consisting of 400k images and 330k videos along with their receiver comments and likes. Then, LLaMA-Vid is trained for the task of predicting receiver comments and likes given a media (a video or an image) (Listing 1  ###reference_###). We show that using this simple task formulation and behavioral data collected in the wild, results in performance improvement over a hierarchy of tasks. We get improvements over the base LLaMA-Vid across 40 tasks over 23 benchmark datasets in both zero-shot and fine-tuned settings. We show this over low-level content understanding tasks like object and activity recognition and also over high-level tasks like topic and emotion detection. Through this, we propose a scalable approach to increase the content understanding abilities of VLMs, requiring minimal cost and no architectural changes.\n2) To disentangle the effect of training LLaVA-Vid on the additional 730k video and image samples from the effect of training on behavior data over those samples, we train LLaVA-Vid on 730k samples using the original task of LLaVA-Vid. We call this model Ad-LLaVA. We show that Ad-LLaVA shows slight performance improvements over LLaVA-Vid; however, Behavior-LLaVA still performs better than both Ad-LLaVA and LLaMA-Vid, thus highlighting the importance of behavior data and instruction fine-tuning on behavior data.\n3) We also show an ablation of Behavior-LLaVA across different kinds of behavior. We try out the perception behavior of saliency prediction over images and two types of action-level behavior (comments and likes) over images and videos. We find that perception-level behavior does not result in significant performance improvements; however, action-level behavior shows improvements across all the tasks. We posit that one reason for this could be due to the scale for which action-level data is available (Table 10  ###reference_0###). While perception behavior is mostly collected in lab settings, action-level behavior data can be collected in a scalable manner automatically and cheaply."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "In this section, we introduce our approach to train Behavior-LLaVA. Since no publicly available corpus consists of behavior together with image and video content, we first introduce our instruction fine-tuning dataset, \u201cBehavior-LLaVA Instruction Fine-Tuning dataset\u201d (BLIFT). Next, we introduce our methodology to train Behavior-LLaVA. Finally, we report the results of testing Behavior-LLaVA\u2019s capabilities on a hierarchy of tasks. The tasks cover low-level media understanding tasks like object and activity detection, high-level media understanding tasks like emotion, topic, and persuasion strategy classification."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "BLIFT Dataset",
            "text": "Given the abundance of media and behavioral data and its accessibility, our data collection relies on two primary sources: Reddit and YouTube. These platforms share similarities in terms of hosting media content (images and videos) and user engagement metrics such as Reddit upvotes and comments, and YouTube likes, views, and comments. Here, we delineate the process involved in constructing the instruction fine-tuning dataset, which we term as the Behavior-LLaVA Instruction Fine-Tuning (BLIFT) dataset."
        },
        {
            "section_id": "2.1.2",
            "parent_section_id": "2.1",
            "section_name": "2.1.2 Data from YouTube",
            "text": "Our data collection from YouTube begins with querying Wikidata [73  ###reference_b73###] for YouTube IDs to compile a list of channels. Wikidata, derived from Wikipedia, provides a curated selection of renowned channels, automatically filtering out noisy videos commonly found in datasets collected from diverse sources like user-generated videos. This initial step yielded a dataset of 2.2 million videos spanning the period from 2018 to 2023, sourced from approximately 6,000 channels collected from Wikidata.\nTo refine the dataset, manual filtering was employed to exclude certain categories deemed less relevant for our purposes. These categories included music and songs, gaming content, non-English videos, sports commentary, anime, memes, channels with disabled comments sections, and news-related content. Furthermore, videos exceeding a duration of 60 seconds were omitted, and only videos with a substantial viewership, defined as greater than 10,000 views, were retained. We observed that these videos usually have less noisy comments and likes.\nSubsequently, the top comments from each video, as ordered by YouTube (i.e., the most liked comments), were selected for inclusion in the dataset. To address redundancy in comments, a TF-IDF filter was applied with a threshold of 0.7, which proved effective in removing duplicate comments prevalent in YouTube data.\nComments were further filtered to include only those with a minimum of four words and a maximum of 100 words, ensuring a balance between relevance and conciseness. Additionally, to mitigate the presence of NSFW content, a vocabulary specific to NSFW terms [1  ###reference_b1###] was employed to filter out inappropriate posts. On average, we finally get 3.1 comments per video, providing a substantial corpus of user-generated content for analysis. After applying these filtering steps, the dataset was reduced to 250,000 videos, ensuring a curated and relevant collection for subsequent analysis and model training."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Instruction Fine-Tuning LLaMA-Vid",
            "text": "After collecting Reddit and YouTube media and user behavior, we formulate instruction fine-tuning tasks for training LLaMA-Vid. In the training instruction, given the media content and automatic speech recognition if available, we ask the model to simulate the scene-by-scene description and user likes/views and top-5 comments. This instruction training template is given in Listing 1  ###reference_###.\nTo generate the instruction data, first, frames are sampled using the 30-degree rule [70  ###reference_b70###, 5  ###reference_b5###, 27  ###reference_b27###], then the scene-by-scene description are obtained by concatenating automatically generated captions and tags from LLaVA-13B [54  ###reference_b54###], colors and tone through Qin et al. [65  ###reference_b65###]. This instruction format keeps the instructions similar to the instruction format for other VLMs like LLaVA [54  ###reference_b54###], MiniGPT-4 [88  ###reference_b88###], BLIP [49  ###reference_b49###], LLaMA-Vid [52  ###reference_b52###], etc., while additionally teaching the model to learn behavior. We keep the instruction fine-tuning template similar for both YouTube and Reddit. The complete instruction is given in Listing 1  ###reference_###.\nWe start with the trained LLaMA-Vid model. The LLaMA-Vid model uses two tokens to represent each video, which they call content and context tokens. While the context token encodes the overall image context based on user input, the content token encapsulates visual cues in each frame. For learning context tokens, the model uses attention queries that interact with previously generated image features in the designed attention module. To generate content tokens, the image features are average pooled.\nThis dual-token strategy significantly reduces the number of tokens needed to represent videos, thus enabling the model to scale to longer (hour-long) videos. To better support hour-long videos, LLaMA-Vid was trained on a 9k movie-level conversation instruction set containing plot reasoning and detail understanding questions.\nTaking the base LLaMA-Vid model, we finetune it further on behavioral data. We combine 730k instruction pairs from BLIFT with the original instruction tuning dataset consisting of 40K text conversations from ShareGPT, 625K single or multi-turn visual QA pairs, and 98K video QA pairs; all the modules except the Visual Encoder are kept frozen. We ablate on multiple sampling ratios from BLIFT. We train the LLaMA-Vid checkpoints with their original SFT mix along with BLIFT. We ablate different sampling ratios and found 1:1 to be empirically performing the best. We train the model for 2.2 epochs, keeping track of the 0-shot evaluation metrics and perplexity on comments in the eval set. For the best checkpoint, the perplexity on comments reduces from 6.22 to 3.05, and the  on likes/views goes from -5.1 to 0.45.\nTo disentangle the effect of training on additional data samples from the effect of training on behavioral data, we train LLaMA-Vid on BLIFT with the video and image verbalization and do not include receiver behavior of comments and likes. Then, the overall instruction template consists of scene-by-scene automatically generated verbalization similar to Listing 1  ###reference_### without the likes and comment simulation. We call the LLaMA-Vid fine-tuned on this data, Ad-LLaVA. We compare Behavior-LLaVA with Ad-LLaVA and LLaMA-Vid along with other state-of-the-art literature benchmarks on various tasks (Tables 1  ###reference_###-4  ###reference_###).\nAs an ablation experiment, we also try teaching the Behavior-LLaVA perceptual signals. For this, we take the largest perception signal dataset in the literature - Salicon10k [37  ###reference_b37###]. It consists of 10,000 MS COCO images [18  ###reference_b18###] with free-viewing eye gaze data collected through a novel mouse-based interface. The dataset has been widely used in many studies. We formulate two tasks using this data, (1) Salicon [Object]: estimating the saliency over the objects in the image and (2) Salicon [Region]: estimating the saliency over a region, where the regions tiles formed by breaking the image into a 3x3 grid. For both tasks we try to model two objectives, ranking and predicting, we found ranking to be much more effective. The instruction are given in Listings 5  ###reference_### and 6  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Results and Discussion",
            "text": "In the experimental results, we aim to showcase the diverse and emergent capabilities of our Behavior-LLaVA model through quantitative numbers on various tasks and qualitative examples. These abilities include generating detailed image and video descriptions, emotion and sentiment analysis, question answering, video understanding tasks like scene and action detection. Additionally, we present the ability of Behavior-LLaVA to transfer learn on other behaviors like memorability of a video - both short-term and long-term.\nModel\nscene\nway_speaking\nrelationship\nlike_ratio\nview_count\ndirector\ngenre\nwriter\nyear\n\nVideo4096-GPT-3.5 generated story + Flan-t5-xxl [13  ###reference_b13###]\n60.2\n39.07\n64.1\n0.061\n12.84\n69.9\n58.1\n52.4\n75.6\n\nVideo4096-GPT-3.5 generated story + GPT-3.5 classifier [13  ###reference_b13###]\n54.54\n32.95\n68.42\n0.031\n12.69\n75.26\n50.84\n32.16\n75.96\n\nLLaMA-Vid + GPT-3.5 Generated Story\n58.12\n35.5\n60.6\n0.314\n10.34\n65.34\n49.77\n34.23\n72.12\n\nAd-LLaVA\n59.05\n37.07\n61.2\n0.319\n10.37\n66.84\n55.13\n35.33\n77.34\n\nBehavior LLaVA + GPT-3.5 Generated Story\n66.43\n41.03\n64.21\n0.17\n5.12\n71.12\n63.45\n39.4\n79.3\n\nImprovement of Behavior LLaVA over LLaMA-Vid\n10.48%\n15.58%\n9.62%\n45.86%\n50.48%\n8.85%\n27.49%\n15.1%\n9.96%"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Evaluation",
            "text": "To test the effectiveness of Behavior-LLaVA, we conduct experiments involving 40 distinct tasks across 23 benchmark datasets. The diversity of tasks and datasets allows us to evaluate the performance and capabilities of Behavior-LLaVA thoroughly. Each of them is covered briefly next:\nVisual Question Answering (VQA): We evaluate the performance of visual question answering on the following benchmark datasets:\nThe Long-Video Understanding (LVU) benchmark by Wu and Krahenbuhl [76  ###reference_b76###] comprises nine distinct tasks aimed at assessing long video comprehension, incorporating over 1000 hours of video content. These tasks encompass diverse aspects such as content understanding (including \u2018relationship\u2019, \u2018speaking style\u2019, \u2018scene/place\u2019), prediction of user engagement (\u2018YouTube like ratio\u2019, \u2018YouTube popularity\u2019), and forecasting movie metadata (\u2018director\u2019, \u2018genre\u2019, \u2018writer\u2019, \u2018movie release year\u2019).\nThe Holistic Video Understanding (HVU) dataset by Diba et al. [23  ###reference_b23###] stands as the largest dataset for long video comprehension, comprising 572,000 samples. Encompassing a broad spectrum of semantic elements within videos, HVU tasks involve the classification of scenes, objects, actions, events, attributes, and concepts. Performance evaluation on HVU tasks is conducted using the mean average precision (mAP) metric on the validation set.\nWe also use MSVD-QA, MSRVTT-QA [17  ###reference_b17###, 79  ###reference_b79###], and ActivityNet-QA [16  ###reference_b16###] datasets. Their description is given in Appendix A  ###reference_###.\nVideo and Image Understanding Benchmarks: We use a wide variety of tasks to evaluate video and image understanding: topic, emotion, and persuasion strategy classification, action and reason retrieval and generation, and emotions. We briefly introduce the benchmarks:\nThe advertisements dataset by Hussain et al. [34  ###reference_b34###] contains 3,477 video advertisements and the corresponding annotations for emotion and topic tags and action-reason statements for each video. There are a total of 38 topics and 30 unique emotion tags per video. Further, we have 5 action-reason statements for each video for the action-reason generation task.\nPersuasion strategy dataset [13  ###reference_b13###] is a dataset consisting of 1002 video advertisements from popular brands and their persuasion strategy labels like social identity, anchoring and comparison, reciprocity, foot-in-the-door, etc.\nTraining\nModel\nTopic\nSentiment\nPersuasion\nAction\nReason\n\nClubbed\nAll labels\n\nRandom\nRandom\n2.63\n3.37\n14.3\n8.37\n3.34\n3.33\n\nZero-shot\nVideoChat [51  ###reference_b51###]\n9.07\n3.09\n5.1\n10.28\n-\n-\n\n\nVideo4096 - GPT-3.5 Generated Story + GPT-3.5 Classifier [13  ###reference_b13###]\n51.6\n11.68\n79.69\n35.02\n66.27\n59.59\n\n\nLCBM [40  ###reference_b40###]\n42.17\n7.08\n58.83\n32.83\n39.55\n27.91\n\n\nLLaMA-VID w/ only video\n10.11\n3.42\n5.75\n12.32\n29.61\n24.11\n\n\nLLaMA-VID w/ video + GPT-3.5 Story\n42.72\n11.05\n64.02\n32.07\n37.76\n42.33\n\n\nBehavior-LLaVA w/ only video\n22.65\n11.13\n60.04\n13.39\n42.66\n33.33\n\n\nBehavior-LLaVA w/ video + verbalization\n46.34\n11.7\n64.13\n33.33\n52.06\n52.03\n\n\nAd-LLaVA w/ video + GPT-3.5 story\n51.16\n11.33\n68.03\n33.11\n43.26\n51.45\n\n\nBehavior-LLaVA w/ video + GPT-3.5 story\n60.09\n12.84\n79.94\n36.12\n67.10\n79.18\n\nImprovement of Behavior-LLaVA over LLaMA-Vid\n40.66%\n16.2%\n24.86%\n12.62%\n77.7%\n87.05%\n\nFinetuned\nVideo4096- Generated Story + Roberta Classifier [13  ###reference_b13###]\n71.3\n33.02\n84.20\n64.67\n42.96\n39.09\n\n\nLLaMA-VID w/ video + verbalization\n59.13\n32.11\n79.15\n50.93\n50.32\n30.13\n\n\nLLaMA-VID w/ video + GPT-3.5 Story\n63.11\n35.01\n84.15\n55.01\n57.11\n45.73\n\n\nBehavior-LLaVA w/ only video\n58.03\n22.72\n84.41\n26.23\n59.33\n51.45\n\n\nBehavior-LLaVA w/ video + verbalization\n68.32\n33.92\n85.93\n64.72\n70.89\n75.34\n\n\nAd-LLaVA w/ video + GPT-3.5 story\n66.34\n36.24\n84.09\n58.31\n68.15\n78.15\n\n\nBehavior-LLaVA w/ video + GPT-3.5 story\n71.2\n39.55\n86.17\n65.03\n80.44\n81.67\n\nImprovement of Behavior-LLaVA over LLaMA-Vid\n12.82%\n12.97%\n2.4%\n18.21%\n40.85%\n78.59%\nFor emotion analysis, we use VideoEmotion-8 [7  ###reference_b7###], Ekman-6 [78  ###reference_b78###], CAER [48  ###reference_b48###], IAPSa [58  ###reference_b58###], Emotion6 [62  ###reference_b62###], EmoSet [83  ###reference_b83###], and Abstract [57  ###reference_b57###] datasets. A brief description for each of them is given in Appendix A  ###reference_###.\nImage Dense Captioning: Literature image captioning datasets such as MS-COCO [18  ###reference_b18###] reduce the inherently rich information and fine-grained semantics to simplistic captions, with very brief statements focussing only on salient objects. Behavior data such as user comments help a model learn much more information such as object and material properties, world knowledge, emotion, character understanding, spatial relationships, aesthetics, etc. (see Fig. 1  ###reference_###), enhancing the model\u2019s captioning capability. Therefore, we design a captioning task to test this capability and compare it with respect to LLaMA-Vid and LLaVA-34B (a 2.5x larger model). Since we do not have ground truth for this task, following the LLM-as-a-judge paradigm, we use GPT-4V as the judge for all the models. GPT-4V is asked to evaluate the dense captions on three metrics: Correctness (Listing 2  ###reference_###) evaluating the factuality and model hallucinations, Detail (Listing 3  ###reference_###) evaluating the number and depth of details captured by the generated captions, and Quality (Listing 4  ###reference_###) measuring the subjective quality of the concepts chosen to be highlighted by the captioning model and the arrangement, coherence, and the linking of various concepts.\nImage and Video Memorability Simulation: Behavior-LLaVA is trained on behavior along with the media. To check if training on behavior helps in solving other behavior tasks [40  ###reference_b40###], we test it over image and video memorability simulation. For this, we select seven benchmark datasets covering long-term and short-term memorability over images and videos: LaMem [41  ###reference_b41###], SUN [36  ###reference_b36###], and MemCat [28  ###reference_b28###] for images and\nMemento10k[59  ###reference_b59###], VideoMem [20  ###reference_b20###], MediaEval [43  ###reference_b43###], and LAMBDA [35  ###reference_b35###] for videos. We briefly cover each of them in Appendix A  ###reference_###.\nTraining\nDataset\nVideo Emotion-8\nCAER\nEkman-6\n\nRandom\nRandom\n12.5\n14.28\n16.67\n\n0-Shot\nLLaMA-Vid\n29.7\n27.2\n37.33\n\n\nBehavior-LLaVA\n41.35\n51.0\n49.33\n\n\nAd-LLaVA\n29.8\n27.3\n37.66\n\nImprovement of Behavior-LLaVA over LLaMA-Vid\n39.22%\n84.19%\n32.14%\n\nFinetuned\nZhao et al. [87  ###reference_b87###]\n54.5\n78.3\n55.3\n\n\nZhang et al. [86  ###reference_b86###]\n57.3\n80.1\n58.2\n\n\neMOTIONS [77  ###reference_b77###]\n-\n-\n53.12\n\n\nArevalo et al. [6  ###reference_b6###]\n53.7\n77.3\n54.2\n\n\nQiu et al. [66  ###reference_b66###]\n53.3\n-\n57.3\n\n\nXu et al. [78  ###reference_b78###]\n52.6\n77.9\n55.6\n\n\nLLaMA-Vid\n53.8\n75.6\n57.9\n\n\nAd-LLaVA\n54.1\n76.1\n57.8\n\n\nBehavior-LLaVA\n56.9\n79.3\n58.4\n\nImprovement of Behavior-LLaVA over LLaMA-Vid\n5.76%\n3.57%\n0.86%\nFor Tables 1  ###reference_###, 9  ###reference_###, and 2  ###reference_###, we follow the evaluation protocol of Video-4096 [13  ###reference_b13###], for Table 6  ###reference_### we follow the evaluation protocol of LLaVA and LLaMA-VID. For Tables 3  ###reference_###, 7  ###reference_###, and 4  ###reference_###, for 0-shot evaluation results, we use the logits of the next token from the given task vocabulary. For Table 4  ###reference_###, we use the evaluation protocol of SI et al. [70  ###reference_b70###].\nTraining\nModels\nImage Datasets\nVideo Datasets\n\nLamem\nMemcat\nSUN\nMemento10k\nVideoMem\nMediaEval\nLAMBDA\n\n\nHuman Consistency\n0.68\n0.78\n0.75\n0.73\n0.61\n-\n0.61\n\nFinetuned\n10-shot in-context learning GPT-3.5\n0.29\n0.18\n0.15\n0.07\n0.06\n0.06\n0.06\n\n\nViTMem [29  ###reference_b29###]\n0.71\n0.65\n0.63\n0.56\n0.51\n-\n0.08\n\n\nHenry trained with 25% data [35  ###reference_b35###]\n0.56\n0.64\n0.59\n0.62\n0.49\n0.32\n0.28\n\n\nHenry trained with 50% data [35  ###reference_b35###]\n0.65\n0.68\n0.67\n0.69\n0.55\n0.44\n0.40\n\n\nHenry trained with 75% data [35  ###reference_b35###]\n0.71\n0.75\n0.73\n0.74\n0.62\n0.49\n0.47\n\n\nHenry trained on all (combined) datasets [35  ###reference_b35###]\n0.72\n0.79\n0.76\n0.72\n0.60\n0.48\n0.52\n\n\nAd-LLaVA trained with 50% data\n0.67\n0.65\n0.61\n0.69\n0.56\n0.43\n0.47\n\n\nBehaviour LLaVA trained with 25% data\n0.67\n0.72\n0.69\n0.68\n0.53\n0.44\n0.50\n\n\nBehaviour LLaVA trained with 50% data\n0.72\n0.77\n0.73\n0.71\n0.59\n0.46\n0.51\n\n\nBehaviour LLaVA trained with 75% data\n0.73\n0.77\n0.74\n0.70\n0.60\n0.47\n0.50\n\n\nBehavior-LLaVA trained on all datasets\n0.73\n0.78\n0.74\n0.71\n0.60\n0.47\n0.52\n\nImprovement of Behavior-LLaVA over LLaMA-Vid (25% data)\n19.64%\n12.5%\n16.95%\n9.68%\n8.16%\n37.5%\n78.57%\n\nImprovement of Behavior-LLaVA over LLaMA-Vid (50% data)\n10.77%\n13.26%\n8.96%\n2.90%\n7.27%\n4.54%\n27.5%\n\n0-shot\nLLaMA-Vid\n0.13\n0.11\n0.05\n0.03\n0.05\n0.02\n0.05\n\n\nAd-LLaVA\n0.14\n0.13\n0.06\n0.06\n0.07\n0.04\n0.13\n\n\nBehavior-LLaVA\n0.21\n0.17\n0.13\n0.12\n0.08\n0.07\n0.16\n\nImprovement of Behavior-LLaVA over LLaMA-Vid\n61.5%\n54.5%\n160%\n300%\n160%\n350%\n219%\n###figure_3### ###figure_4###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Discussion",
            "text": "Tables 1  ###reference_###, 9  ###reference_###, and 6  ###reference_### contain the results for the visual question answering tasks, Tables 2  ###reference_###, 3  ###reference_###, 7  ###reference_### contain the results for video and image understanding tasks, Tables 8  ###reference_### contains the results for dense-captioning, and Table 4  ###reference_### contains the results for image and video memorability benchmarks. A common trend we observe across all the results is that Behavior-LLaVA performs better than the base model LLaMA-Vid and the finetuned model Ad-LLaVA on all tasks, especially in zero-shot settings. Interestingly, the performance gains remain even after fine-tuning on the task dataset (Tables 2  ###reference_###, 3  ###reference_###, 7  ###reference_###).\nThe performance gains are smaller for the more low-level tasks of action and object recognition (Tables 1  ###reference_###, 9  ###reference_###, and 6  ###reference_###), but much higher for the more high-level tasks of emotion understanding, sentiment analysis, persuasion strategy classification, and memorability simulation. This indicates that receiver behavior has richer signals for higher-level tasks. The gains are observed across both image and video benchmarks. We also observe that classification using a story generated by GPT-3.5 (following Bhattacharyya et al. [13  ###reference_b13###]) results in better performance than only using the video (Tables 1  ###reference_###, 9  ###reference_###, 2  ###reference_###).\nFigures 4  ###reference_###, 5  ###reference_###, 8  ###reference_###, and 3  ###reference_###, 6  ###reference_###, 7  ###reference_###, show several randomly sampled qualitative examples for dense captions generated by Behavior-LLaVA over images and videos respectively. It can be noticed that despite not being explicitly trained for this task, the model performs quite well, picking up various artistic, cognitive, and object and material properties. From Table 8  ###reference_###, while Behavior-LLaVA shows a decrease in correctness over LLaMA-Vid, it shows significant improvement in other aspects, including detail and quality. On these aspects, it even comes close to 2.5X larger models (LLaVA-1.6 (34B)).\nNext, we compare the signals from behavioral data of perception and action. For this, we compare Behavior-LLaVA trained on the perception data of Salicon using two instruction formats with Behavior-LLaVA trained on BLIFT. Further, within BLIFT, we compare the learnings from the task of predicting likes/views with the task of predicting only comments. Table 10  ###reference_0### contains the results. It can be noted from the table that training on just Salicon results in a performance decrease for the lower-level task of action recognition on MSRVTT-QA but improves performance on the higher-level task of Emotion recognition. However, the gains are smaller than those observed with training on BLIFT."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we explore the idea of learning behavior leading to learning content better. Humans produce behavior in response to content. Hence, logically, behavior should contain signals about content, which, if used as a training task, should help in learning content better. We follow this line of thought and show that training large vision and language models on user behavior data of comments and likes collected from Reddit and YouTube leads to performance improvements across a wide variety of tasks. The gains are higher on higher-level tasks such as emotion recognition, persuasion strategy classification, and question answering and smaller on lower-level tasks like action and object recognition. Further, the gains remain even after fine-tuning the VLMs on those benchmarks, thus demonstrating the importance of learning behavior in understanding content better."
        }
    ],
    "url": "http://arxiv.org/html/2405.00942v2",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.1.1",
            "2.1.2",
            "2.2"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "2.2",
            "3",
            "3.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.00942v2",
        "paper_title": "LLaVA Finds Free Lunch: Teaching Human Behavior Improves Content Understanding Abilities Of LLMs",
        "research_background": "### Paper's Motivation:\n\nThe paper is motivated by the premise that human behavior, which results from processing and reacting to stimuli, contains valuable signals that can enhance the understanding of content in NLP (Natural Language Processing) and CV (Computer Vision) tasks. Despite the potential of these behavioral signals, current large language models (LLMs) and large vision and language models (VLMs) often ignore this data, primarily because it is considered noisy and costly to collect in controlled settings. The paper aims to explore the untapped potential of readily available, large-scale behavioral data (such as likes, shares, and comments) collected in real-world scenarios.\n\n### Research Problem:\n\nThe key research problem addressed by this paper is how to effectively integrate large-scale, real-world human action data (collected from platforms like Reddit and YouTube) into the training of LLMs and VLMs to improve their content understanding capabilities. The study seeks to determine if and how this type of behavioral data can be used to enhance model performance across various NLP and CV tasks, thereby proposing a scalable method that minimizes cost and requires no architectural changes to the models.\n\n### Relevant Prior Work:\n\n1. **Foundational Communication Models**:\n   - Lasswell's model of communication, which outlines factors such as sender, message, channel, receiver, and behavior, is foundational to understanding the flow of communication and how behavior arises as a downstream effect ([68] ###reference_b68###, [46] ###reference_b46###, [47] ###reference_b47###).\n\n2. **Perceptual Signals in Human Behavior**:\n   - Previous studies have shown that perceptual signals like eye movements, saliency, keystrokes, and FMRI can improve performance on various NLP and CV tasks ([11] ###reference_b11###, [64] ###reference_b64###, [14] ###reference_b14###, [24] ###reference_b24###).\n   - Integration of perceptual signals has improved tasks such as visual and natural language question answering, text and image sentiment analysis, natural language inference, part-of-speech identification, named entity recognition, syntactic parsing, image captioning, and visual object detection ([60] ###reference_b60###, [42] ###reference_b42###, [71] ###reference_b71###, [10] ###reference_b10###, [26] ###reference_b26###, [8] ###reference_b8###, [9] ###reference_b9###, [32] ###reference_b32###, [31] ###reference_b31###, [63] ###reference_b63###, [21] ###reference_b21###, [74] ###reference_b74###, [44] ###reference_b44###).\n\n3. **Challenges in Using Perceptual Signals**:\n   - Collection of perceptual signals often requires specialized lab settings and equipment, making it both costly and limited in scale ([37] ###reference_b37###, [19] ###reference_b19###, [12] ###reference_b12###, [39] ###reference_b39###, [22] ###reference_b22###).\n\n4. **Action Data as an Alternative Source**:\n   - Unlike perceptual signals, action data such as likes, views, shares, comments, and purchase histories are abundantly available from digital platforms and are much less expensive to collect.\n   - Despite this abundance, action data has not been extensively explored for its potential to improve content understanding ([54] ###reference_b54###, [88] ###reference_b88###).\n\n5. **Existing Vision Language Models**:\n   - Vision language models like LLaMA-Vid and others typically use human annotations for training, which involves significant cost ([52] ###reference_b52###, [54] ###reference_b54###, [88] ###reference_b88###, [33] ###reference_b33###, [50] ###reference_b50###).\n   \nIn summary, this work is built on the insightful observation that human actions on digital platforms, if properly harnessed, can serve as a cost-effective, scalable means of improving the performance of content understanding models.",
        "methodology": "Methodology: In this section, we introduce our approach to train Behavior-LLaVA. Since no publicly available corpus consists of behavior together with image and video content, we first introduce our instruction fine-tuning dataset, \u201cBehavior-LLaVA Instruction Fine-Tuning dataset\u201d (BLIFT). \n\nNext, we introduce our methodology to train Behavior-LLaVA. Finally, we report the results of testing Behavior-LLaVA\u2019s capabilities on a hierarchy of tasks. The tasks cover low-level media understanding tasks like object and activity detection, high-level media understanding tasks like emotion, topic, and persuasion strategy classification.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets and Tasks:**\nThe Behavior-LLaVA model is evaluated on several tasks that assess diverse abilities such as:\n\n1. Generating detailed image and video descriptions.\n2. Emotion and sentiment analysis.\n3. Question answering.\n4. Video understanding tasks including scene and action detection.\n5. Memorability of a video, encompassing both short-term and long-term predictions.\n\nThe tasks test the model's content understanding and its ability to generalize across different dimensions of human behavior.\n\n**Baselines:**\nThe performance of the Behavior-LLaVA model is compared against the following baselines:\n\n1. **Video4096-GPT-3.5 generated story + Flan-t5-xxl** \n2. **Video4096-GPT-3.5 generated story + GPT-3.5 classifier**\n3. **LLaMA-Vid + GPT-3.5 Generated Story**\n4. **Ad-LLaVA**\n\nThese baselines represent previous efforts in video understanding and content generation using various combinations of models like GPT-3.5 and Flan-t5-xxl.\n\n**Evaluation Metrics:**\nThe performances are evaluated using metrics tailored to individual tasks. Key metrics provided in the results include:\n\n- Scene detection accuracy (scene)\n- Way of speaking detection (way_speaking)\n- Relationship detection (relationship)\n- Like ratio (like_ratio)\n- View count prediction (view_count)\n- Director detection (director)\n- Genre detection (genre)\n- Writer detection (writer)\n- Year detection (year)\n\n### Main Experimental Results\n\nThe results showcase the performance of the Behavior-LLaVA model compared to the baselines across different metrics:\n\n- **Behavior LLaVA + GPT-3.5 Generated Story:**\n  - Scene: 66.43\n  - Way_speaking: 41.03\n  - Relationship: 64.21\n  - Like_ratio: 0.17\n  - View_count: 5.12\n  - Director: 71.12\n  - Genre: 63.45\n  - Writer: 39.4\n  - Year: 79.3\n\n- **LLaMA-Vid + GPT-3.5 Generated Story:**\n  - Scene: 58.12\n  - Way_speaking: 35.5\n  - Relationship: 60.6\n  - Like_ratio: 0.314\n  - View_count: 10.34\n  - Director: 65.34\n  - Genre: 49.77\n  - Writer: 34.23\n  - Year: 72.12\n\nThe improvements of Behavior-LLaVA over LLaMA-Vid are also detailed:\n\n- Scene detection improved by 10.48%\n- Way of speaking detection improved by 15.58%\n- Relationship detection improved by 9.62%\n- Like ratio increased by 45.86%\n- View count prediction improved by 50.48%\n- Director detection improved by 8.85%\n- Genre detection improved by 27.49%\n- Writer detection improved by 15.1%\n- Year detection improved by 9.96%\n\nThese results indicate that the Behavior-LLaVA model significantly outperforms all the baseline models across various content understanding tasks, demonstrating its superior capabilities in understanding and predicting human behavior-related aspects in videos."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To determine the effect of training LLaMA-Vid on additional data samples without the behavior data and compare its performance with Behavior-LLaVA.",
            "experiment_process": "We first created Ad-LLaVA by training LLaMA-Vid on 730k samples using LLaMA-Vid's original tasks. This was done to separate out the effect of the additional data from the effect of behavior data. Ad-LLaVA was compared with Behavior-LLaVA and the original LLaMA-Vid across various tasks. The below models were used in the comparisons: Video4096-GPT-3.5 generated story + Flan-t5-xxl, Video4096-GPT-3.5 generated story + GPT-3.5 classifier, LLaMA-Vid + GPT-3.5 Generated Story, Ad-LLaVA, Behavior LLaVA + GPT-3.5 Generated Story.",
            "result_discussion": "Ad-LLaVA showed slight performance improvements over LLaMA-Vid; however, Behavior-LLaVA outperformed both Ad-LLaVA and LLaMA-Vid, underlining the importance of training on behavior data along with the additional samples. Behavior-LLaVA demonstrated improved performance across all tasks, with more significant gains in higher-level tasks such as emotion understanding, sentiment analysis, and memorability simulation.",
            "ablation_id": "2405.00942v2.No1"
        },
        {
            "research_objective": "To compare the effect of training Behavior-LLaVA on perception data versus action-level behavior data.",
            "experiment_process": "For perceptual signals, Behavior-LLaVA was trained on the Salicon dataset, which includes saliency information over objects and regions within images. Two tasks were formulated: estimating saliency over objects and regions in the images. For action-level behavior, the BLIFT dataset was used, which includes likes and comments for videos and images. The performance was measured across different tasks.",
            "result_discussion": "Training on Salicon showed performance improvements in higher-level tasks like emotion recognition but was less effective for lower-level tasks such as action recognition. However, training on BLIFT resulted in more significant overall performance improvements, particularly for higher-level tasks. The availability and scalability of action-level data seemed to be a crucial factor contributing to these gains.",
            "ablation_id": "2405.00942v2.No2"
        }
    ]
}