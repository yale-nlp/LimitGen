{
    "title": "MedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices",
    "abstract": "Large language models (LLMs) are revolutionizing various domains with their remarkable natural language processing (NLP) abilities. However, deploying LLMs in resource-constrained edge computing and embedded systems presents significant challenges. Another challenge lies in delivering medical assistance in remote areas with limited healthcare facilities and infrastructure. To address this, we introduce MedAide, an on-premise healthcare chatbot. It leverages tiny-LLMs integrated with LangChain, providing efficient edge-based preliminary medical diagnostics and support. MedAide employs model optimizations for minimal memory footprint and latency on embedded edge devices without server infrastructure. The training process is optimized using low-rank adaptation (LoRA). Additionally, the model is trained on diverse medical datasets, employing reinforcement learning from human feedback (RLHF) to enhance its domain-specific capabilities. The system is implemented on various consumer GPUs and Nvidia Jetson development board. MedAide achieves 77% accuracy in medical consultations and scores 56 in USMLE benchmark, enabling an energy-efficient healthcare assistance platform that alleviates privacy concerns due to edge-based deployment, thereby empowering the community.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "In recent years, large language models (LLMs) (Zhao2023,  ###reference_b1###) have emerged as a groundbreaking technology with the potential to revolutionize multiple domains, and are exceedingly gaining research interest. State-of-the-art LLMs, such as GPT-3 (Brown2020,  ###reference_b2###) and its successor GPT-4 (Bubeck2023,  ###reference_b3###) have revolutionized NLP tasks. Other LLMs are used for various generative tasks like robotics and smart homes/cities. These auto-regressive models learn the statistical patterns and semantic structures of human language. With their vast knowledge base, LLMs can generate coherent and contextually relevant text and understand complex queries. This prowess makes them a valuable resource for a wide range of applications including medicine and autonomous cognitive systems (10.3389/frai.2023.1169595,  ###reference_b4###)(jeblick2022chatgpt,  ###reference_b5###) and serve as a driving force behind our research.\nLanguage and interaction are fundamental elements in medicine, enabling healthcare professionals to gather comprehensive insights (Franz2018,  ###reference_b6###), developing a deeper understanding of the individual\u2019s health status. Navigating the landscape of medical applications, there is a compelling need for remote assistance as highlighted in Figure 1. A World Bank and WHO report (who,  ###reference_b7###) states that half of the world\u2019s population lacks essential health services, with healthcare costs pushing 100 million into extreme poverty annually. This underscores the urgent need for universal health coverage.\n###figure_1### Moreover, the deployment of LLMs on resource-constrained devices while minimizing latency presents a significant challenge, see Figure 2. State-of-the-art LLMs (Brown2020,  ###reference_b2###) (Bubeck2023,  ###reference_b3###) (yin2023survey,  ###reference_b9###) heavily rely on server-based deployment, limiting accessibility, and require significant power consumption and computational resources (Zhao2023,  ###reference_b1###).\n###figure_2### ###figure_3### The selection of LLM for edge deployment depends on network parameters, performance metrics, open-source availability, and hardware constraints. We utilized the open-LLM leader-board (Open-LLM-Leaderboard-Report-2023,  ###reference_b10###), which serves as a valuable platform for comprehensive analysis of LLMs. An analysis was conducted to identify suitable LLMs, and among them, LLaMa2-7B(touvron2023llama,  ###reference_b15###) emerged as the standout choice owing to its exceptional performance across benchmarks. Moreover, Bloom-560M model (BigScience,  ###reference_b16###) and OPT-125M (Zhang2022,  ###reference_b17###) models were selected based on fewer parameter facilitating deployment on resource-constrained embedded devices, open-source availability, and highest TruthfulQA score(lin2022truthfulqa,  ###reference_b11###) for their parameter range illustrated in Figure. 3.\nWe conducted additional analysis to benchmark our models against those with significantly larger parameter counts, which are specialized for medical datasets. Our analysis reveals that OPT-125M and Bloom-560M, with a small memory footprint, exhibit modest accuracy scores of 27.6%, 29.5%. These models are particularly attractive due to their relatively lower parameter counts, facilitating easier deployment, thereby achieving low-latency performance. Respectively, LLaMa2-7B achieves 51.9% accuracy on these tests, reflecting its efficacy in medical applications. Figure 4 showcasing their potential for deployment in medical applications.\n###figure_4### LangChain (langchain,  ###reference_b18###) plays a crucial role in our model deployment tool-chain through collection of medical databases, optimizing the results through Facebook AI Similarity Search (FAISS) (johnson2019billion,  ###reference_b19###). LangChain efficiently searches a database, to accelerate the retrieval of medical prescription. This streamlined process enhances the overall USMLE score by 2.5 %, facilitating more effective medical knowledge retrieval and utilization.\nTo enhance the domain-specific understanding of our models, we curated a custom dataset detailed in Table 1, from various sources including online medical forums, publicly available biomedical databases, and synthesized real-world clinical case studies enabling our models to grasp medical terminologies effectively. By training our models on this specialized dataset, we enrich their knowledge representation and enhance their performance in medical consultation scenarios. Moreover, we devise an experimental validation setup for our models primarily focusing on the doctor-patient interaction aspect which is lacking in most of the available datasets. In this evaluation, chatGPT4 reviews interactive dialogues for authenticity, response effectiveness, and overall performance, ranking MedAide (LLaMa-2) with a 77% accuracy in comparative analysis.\n###figure_5### To address the above-discussed research challenges, we present our novel MedAide system with the following contributions:\nMedAide system (overview in Figure 5) enables on-premise healthcare diagnosis by learning from domain specific dataset, by automatically collecting, refining and updating samples.\nWe investigated MedAide system with a backbone of three prominent LLM architectures that have demonstrated outstanding performance on the Open-LLM leader-board (Open-LLM-Leaderboard-Report-2023,  ###reference_b10###). Our analysis is centered around benchmarking their performance, with a particular emphasis on their effectiveness in the TruthfulQA task, by mitigating false answers learned from imitating human texts.\nOur proposed MedAide system leverages LLMs with optimizations, enabling seamless deployment on devices such as Nvidia Jetson or consumer-grade GPUs, by identifying hardware constrains through a rigorous model selection criteria for a specific edge device.\nTowards a practical solution, our system integrates LangChain (langchain,  ###reference_b18###) to construct toolchains for effective searching of medical databases for accurate prescription and consultation and medical recommendations.\n###figure_6### Paper Organization: Section 2 discusses state-of-the-art. Section 3 presents the MedAide system in further detail. Section 4 describes the evaluation framework, and results are presented in Section 5. We conclude in Section 6."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Background and Related Work",
            "text": "LLMs are typically based on transformer architectures (Vaswani2017,  ###reference_b20###) (Wolf2019TransformersSN,  ###reference_b21###), which have become the de facto standard for NLP tasks. Transformers leverage attention mechanisms to proficiently capture contextual and long-range dependencies within textual data (Peters2018DeepCW,  ###reference_b22###), while understanding the chain of discussion. LLMs have made significant contributions in various specialized domains. In healthcare, LLMs have been used for medical diagnosis, clinical decision support, and biomedical text mining. Based on a recent study by Dave et al. (2023) (10.3389/frai.2023.1169595,  ###reference_b4###), it is evident that ChatGPT and even GPT-4 demonstrate comparatively lower performance in vertical domains, particularly in the field of medicine. This can be attributed, in part, to the potential insufficiency of medical knowledge among annotators. Consequently, there are substantial prospects for further investigation and enhancement within this domain.\nEfforts to overcome this limitation and enhance performance in medical domains hold significant promise for advancing the capabilities of language models in healthcare. ChatDoctor (li2023chatdoctor,  ###reference_b23###), an advanced language model based on the LLaMa model (Touvron2023,  ###reference_b24###), is specifically designed for medical assistance. It simulates doctor-patient conversations, enabling patients to receive accurate diagnoses, personalized medical advice, and appropriate treatment options. Luo et al. (Luo2022,  ###reference_b25###) presented BioGPT, a domain-specific generative transformer language model trained on extensive biomedical literature offering generation capabilities tailored specifically for the biomedical domain and achieves superior performance across various biomedical datasets. DrBERT (labrak2023drbert,  ###reference_b26###), a specialized pre-trained language models (PLMs) for the French medical domain, trained on BERT (Devlin2018,  ###reference_b27###) introduced a novel approach by leveraging both left and right context during pre-training, resulting in state-of-the-art performance across various medical tasks. HuatuoGPT (huatuogpt-2023,  ###reference_b28###), a distilled language model for medical consultation, trained using ChatGPT synthesised dialogue and real-world data from doctors, employing RLAIF, achieves state-of-the-art performance in medical consultation. These model have excessive server dependence for deployment, due to high parameter count, constraining the availability on GPUs. Moreover, these LLMs often face a challenge known as \u201dhallucination,\u201d (hallucination,  ###reference_b29###) where they generate plausible but factually incorrect or nonsensical information. This occurs due to their reliance on patterns in the data they were trained on, without an intrinsic understanding of truth. LangChain integrated with LLMs addresses this issue by structuring interactions with the model in a way that mitigates hallucinations.\nDoctorGLM (xiong2023doctorglm,  ###reference_b30###), trained using ChatGLM-6B (du2022glm,  ###reference_b31###) sets a benchmark on Chinese datasets; however, our work outperforms it with a notable 21% increase in overall accuracy. Han et al. (Han2023,  ###reference_b32###) proposed MedAlpaca addressed the need for open-source models that prioritize patient privacy by developing specialized dataset for medical applications, highlighting the use-case in medical education. However, MedAide outperforms this work by a considerable margin, as demonstrated in our experimental evaluation section."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. MedAide: An LLM-based System for medical assistance",
            "text": "In this section we will describe the MedAide system in detail, along with the dataset generation, refinement and training workflow. We elaborate on LLM quantization and model selection along with LangChain integration (see an overview in Figure 6)."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. User Requirements",
            "text": "User requirements, pivotal in designing medical applications, dictate both the user interface design and model selection criteria, ensuring alignment with healthcare professionals\u2019 and patients\u2019 needs. Our analysis highlighted key demands: an intuitive user interface, accurate medical results, secure authentication, and adherence to data privacy laws like HIPAA. Accordingly, we crafted a user interface facilitating secure login and local data storage, overseen by authorized personnel. Additionally, our model selection caters to real-time processing and accuracy modes, tailoring the application to meet these critical user expectations effectively."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Hardware Constraints",
            "text": "There is a trade-off between ensuring high-quality performance and a set of hardware constraints for deployment in consumer-grade GPUs and embedded devices as highlighted in Figure 2. These constraints ensure that LLMs remain within the computational and memory capabilities of the target platforms. The metrics such as storage overhead, based on network parameters, and floating-point operations, quantifying the execution time of the LLMs on GPUs are employed. Given the resource constraints, the trade-off is considering model size for hardware specifications."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4. Domain Specific Training Methodology",
            "text": "Training LLMs such as LLaMa 2-7B (Touvron2023,  ###reference_b24###) and Bloom-560M (BigScience,  ###reference_b16###) on consumer devices is challenging due to resource limitations. We used Low Rank Adaptation (LoRA) (hu2022lora,  ###reference_b33###) to approximate high-dimensional datasets in lower-dimensional spaces, preserving key features. This technique reduced trainable parameters and GPU memory usage significantly. Bloom-560M, optimized for computational efficiency and accuracy, includes self-attention and feed-forward networks. Similarly, we trained OPT-125M, a model with 12 layers and attention heads, suitable for various hardware configurations used by end-users. Additionally, our approach incorporated Reinforcement Learning from Human Feedback (RLHF) to train a reward model. This model played a pivotal role in refining the training process by providing valuable, human-centric insights. We utilized this reward model to iteratively train a Proximal Policy Optimization (PPO) model, leveraging policy-based rewards. This integration of RLHF allowed for a more nuanced and effective training process, as the PPO model dynamically adjusted its learning strategy based on human feedback. This feedback loop ensured that the model\u2019s outputs not only adhered to technical accuracy but also aligned closely with practical, real-world applicability and user expectations. By embedding human feedback directly into the training cycle, we achieved a more robust and contextually aware LLM, capable of addressing the nuanced demands of medical applications with greater precision. Our implementation followed a structured three-step process as highlighted in Figure 7.\n###figure_7###"
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "3.5. Model Selection and Optimization",
            "text": "Our model selection and optimization process for the LLaMA 2-7B models includes quantization in Q4, Q8, and F16 formats to ensure efficient deployment. Q4 quantization reduces precision to 4 bits, offering smaller model sizes but potentially less accuracy, and is incompatible with Nvidia Jetson boards. Q8 quantization, with 8-bit precision, balances performance and accuracy, while F16 quantization uses 16-bit floating-point representation for complex tasks with reduced size and computational demands. After quantization, the models are calibrated using weight clipping, adjusting weight values to minimize precision loss and maintain performance.\nThe model selection criteria is designed to ensure efficient deployment on the target hardware. We begin by assessing the compatibility of models with the target hardware, particularly noting that NVIDIA boards do not support Q4 models. This ensures that the chosen model is viable for the intended device. We evaluate metrics such as memory usage, inference speed, and the balance between model accuracy and resource consumption. This evaluation is key to identifying the most efficient model within the hardware\u2019s operational constraints. The selection is further refined based on user-defined application requirements, such as the need for real-time processing or a focus on accuracy. This step ensures the model\u2019s alignment with the specific functional demands of the application. Finally, we conduct a comparative analysis of various LLMs, weighing the trade-offs each presents. This comprehensive analysis covers the model\u2019s feasibility on the device and its conformity to user requirements regarding processing speed and accuracy, ensuring a well-rounded selection process.\nIn our comprehensive dataset collection, we leveraged medical databases, e-books, and documents for LangChain (langchain,  ###reference_b18###) integration. To enhance processing efficiency, we segmented this extensive medical knowledge into 1000-character blocks with a 50-character overlap. Leveraging the robust capabilities of Hugging Face Instruct, we generated embeddings for each block, storing them in our local infrastructure with the support of the FAISS library (johnson2019billion,  ###reference_b19###). Notably, we optimized performance by utilizing the GPU-accelerated version of FAISS, resulting in a remarkable speed boost\u2014achieving search speeds 5-10 x faster than the CPU counterpart.\nIn response to a user query, we employ FAISS to identify the two closest neighbors using its similarity search function. These data chunks are then presented as contextual input to our prompt chain, in conjunction with the selected LLMs. We significantly alleviate the common issue of hallucination in LLMs. This approach ensures that the medical guidance provided is not only precise but also grounded in reliable and verified medical content, enhancing the overall safety and dependability of the advice offered."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Evaluation Setup",
            "text": "The evaluation setup employs a wide range of software and hardware resources for thorough testing and benchmarking. The software environment integrates various tools such as Python 3.10, Cuda 11.8, PyTorch, Lightning, and other essential libraries, along with FAISS for similarity search. Hardware-wise, the setup features diverse Nvidia GPUs, ranging from high-end models like RTX 6000 Ada to embedded solutions like Jetson AGX Xavier, facilitating a comprehensive evaluation of model performance across different computational environments, as indicated in Figure 8.\nThe methodology for evaluation involves deploying models on these hardware platforms to assess key metrics such as accuracy, runtime, and benchmark scores. The USMLE and ChatGPT-4 Scores serve as evaluation metrics described later in Section 5.2, providing a standardized measure of model performance.\n###figure_8###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Results",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1. Quantitative Analysis of Models",
            "text": "In our research, we performed a quantitative analysis of MedAide models using the USMLE QA dataset, a standard for evaluating LLMs in the medical domiain, depicted in Figure 9, also comparing models\u2019 performance and GPU memory consumption with SOTA LLMs, highlighting the efficiency. Figure 10 complements this by illustrating latency differences across a range of devices, crucial for practical healthcare applications.\n###figure_9### ###figure_10###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2. Qualitative Analysis of Models",
            "text": "This model comparison evaluates the performance of different models by examining their responses to 100 single-turn medical questions. Specifically, the comparison focuses on assessing the ratio of model performance for various categories. The evaluation was conducted using chatGPT4 for reviewing and analyzing the models\u2019 outputs, since it scores significantly higher on majority of the benchmarks, considering it as a baseline for performance. The aim of this comparison is to gain insights into how the models perform across various categories and to identify any variations or differences in their effectiveness in health care assessment. Figure 11 presents these findings, and our analysis reveals that our LLaMA 2-7B+FAISS model outperforms MedAlpaca and DoctorGLM by 21% and 13%, respectively. This significant margin underscores the efficacy of the LLaMA 2-7B+FAISS model in the evaluated scenarios.\n###figure_11###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6. Conclusion",
            "text": "This paper presented MedAide, an on-premise medical assistant powered by LLMs. MedAide utilizes specialized medical dataset to provide accurate and reliable healthcare support, including answering medical queries, offering personalized recommendations, and aiding in diagnostics. The results demonstrated the effectiveness of MedAide in various medical domains, showcasing its ability to comprehend complex medical queries, generate informative responses, and assist in clinical decision-making. The results showcase its potential to improve medical workflows, diagnostics, and patient care. Integrating MedAide into real-world healthcare settings holds great promise in enhancing the delivery of healthcare services."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1. </span>Dataset Collection from Different Sources</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.1\">Source</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.2.1\"># Samples</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.3.1\">URL</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.2.1.1\">HealthcareMagic</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.2.1.2\">112,641</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.2.1.3\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"www.healthcaremagic.com/\" title=\"\">www.healthcaremagic.com/</a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.3.2.1\">WebMD</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.3.2.2\">88,207</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.3.2.3\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"www.webmd.com/\" title=\"\">www.webmd.com/</a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.4.3.1\">AskDocs</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.4.3.2\">24,256</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.4.3.3\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.4.3.3.1\">www.askdocs.com</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.5.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.5.4.1\">iCliniq</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.5.4.2\">4,651</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.5.4.3\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"www.icliniq.com/\" title=\"\">www.icliniq.com/</a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.6.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.6.5.1\">HealthTap</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.6.5.2\">3,647</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.6.5.3\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"www.healthtap.com/\" title=\"\">www.healthtap.com/</a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.7.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.7.6.1\">ehealth</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.7.6.2\">1,710</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.7.6.3\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"medicalforums.omeka.net\" title=\"\">medicalforums.omeka.net</a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.8.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.8.7.1\">Huato-26M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.8.7.2\">85,000</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.8.7.3\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.8.7.3.1\">https://doi.org/10.48550</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.9.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.9.8.1\">MedQuAD</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.9.8.2\">47,457</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.9.8.3\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"github.com/abachaa/MedQuAD\" title=\"\">github.com/abachaa/MedQuAD</a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.10.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.10.9.1\">MedMCQa</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.10.9.2\">25,679</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.10.9.3\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.10.9.3.1\">https://doi.org/10.48550</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.11.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.11.10.1\">MedQSum</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.11.10.2\">1000</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.11.10.3\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.11.10.3.1\">http://doi.org/10.18653</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.12.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T1.1.12.11.1\">Medical Cases</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.12.11.2\">4363</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T1.1.12.11.3\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"www.github.com/adahealth\" title=\"\">www.github.com/adahealth</a></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 1. Dataset Collection from Different Sources"
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.00830v1_figure_1.png",
            "caption": "Figure 1. Mortality rate for 137 countries due to inadequate healthcare facilities (Kruk2018MortalityDT, ) where MedAide could be effective."
        },
        "2": {
            "figure_path": "2403.00830v1_figure_2.png",
            "caption": "Figure 2. The analysis of embedded devices from NVIDIA, Intel, and Google reveals a significant gap between the capabilities of embedded boards and the requirements of recent LLMs due to escalating memory demands and computational requirements."
        },
        "3": {
            "figure_path": "2403.00830v1_figure_3.png",
            "caption": "Figure 3. Open-LLM Leader-board benchmark (Open-LLM-Leaderboard-Report-2023, ) competing various state-of-the-art LLMs across diverse benchmarks, encompassing TruthfulQA (lin2022truthfulqa, ), MMLU (hendrycks2021measuring, ), ARC (clark2018think, ), and HellaSwag (zellers2019hellaswag, ) for a comprehensive evaluation."
        },
        "4": {
            "figure_path": "2403.00830v1_figure_4.png",
            "caption": "Figure 4. A comprehensive comparison between selected LLMs (OPT, LLaMa2, and Bloom) and state-of-the-art LLMs (Galactica, Gopher, Chinchilla, and Flan-Palm) to evaluate the performance of these models on different domains, shedding light on their feasibility for medical assistance."
        },
        "5": {
            "figure_path": "2403.00830v1_figure_5.png",
            "caption": "Figure 5. MedAide System overview with the input system requirements and system processes to generate the outputs."
        },
        "6": {
            "figure_path": "2403.00830v1_figure_6.png",
            "caption": "Figure 6. An Overview of Our MedAide: Our system integrates quality requirements and hardware constraints to guide its operation. We employ domain-specific training on a diverse medical dataset to fine-tune LLMs, followed by model quantization. We establish a model selection criteria, ensuring efficient deployment, by integrating LangChain empowered by FAISS."
        },
        "7": {
            "figure_path": "2403.00830v1_figure_7.png",
            "caption": "Figure 7. Model Training Workflow."
        },
        "8": {
            "figure_path": "2403.00830v1_figure_8.png",
            "caption": "Figure 8. Experimental evaluation setup deployed on Python 3.10 and tested on various testing platforms, and evaluation based on metrics and resources."
        },
        "9": {
            "figure_path": "2403.00830v1_figure_9.png",
            "caption": "Figure 9. Performance and Memory comparison for different LLM configurations and their respective USMLE scores"
        },
        "10": {
            "figure_path": "2403.00830v1_figure_10.png",
            "caption": "Figure 10. Latency comparison on GPUs architectures, providing a comprehensive overview of model performance across devices."
        },
        "11": {
            "figure_path": "2403.00830v1_figure_11.png",
            "caption": "Figure 11. Model perf. ratio for each category evaluated on 100 single-turn questions, reviewed by ChatGPT4."
        }
    },
    "references": [
        {
            "1": {
                "title": "A survey of large language models.",
                "author": "Wayne Xin Zhao et al.",
                "venue": "3 2023.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Language models are few-shot learners.",
                "author": "Tom B. Brown et al.",
                "venue": "Advances in Neural Information Processing Systems, 2020-December, 5 2020.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Sparks of artificial general intelligence: Early experiments with gpt-4.",
                "author": "S\u00e9bastien Bubeck et al.",
                "venue": "3 2023.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Chatgpt in medicine: an overview of its applications, advantages, limitations, future prospects, and ethical considerations.",
                "author": "Tirth et al. Dave.",
                "venue": "Frontiers in Artificial Intelligence, 6, 2023.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Chatgpt makes medicine easy to swallow: An exploratory case study on simplified radiology reports.",
                "author": "Katharina Jeblick et al.",
                "venue": "2022.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Reconsidering the role of language in medicine.",
                "author": "Berkeley Franz and John W. Murphy.",
                "venue": "Philosophy, Ethics, and Humanities in Medicine, 13:1\u20137, 6 2018.",
                "url": null
            }
        },
        {
            "7": {
                "title": "World bank and who: Half the world lacks access to essential health services, 100 million still pushed into extreme poverty because of health expenses, Nov 2017.",
                "author": "Mamiko Yoshizu.",
                "venue": null,
                "url": null
            }
        },
        {
            "8": {
                "title": "Mortality due to low-quality health systems in the universal health coverage era: a systematic analysis of amenable deaths in 137 countries.",
                "author": "Margaret E. Kruk et al.",
                "venue": "Lancet (London, England), 392:2203 \u2013 2212, 2018.",
                "url": null
            }
        },
        {
            "9": {
                "title": "A survey on multimodal large language models.",
                "author": "Shukang Yin et al.",
                "venue": "2023.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Open-llm-leaderboard-report, 2023.",
                "author": "Daniel Park.",
                "venue": null,
                "url": null
            }
        },
        {
            "11": {
                "title": "Truthfulqa: Measuring how models mimic human falsehoods, 2022.",
                "author": "Stephanie Lin et al.",
                "venue": null,
                "url": null
            }
        },
        {
            "12": {
                "title": "Measuring massive multitask language understanding, 2021.",
                "author": "Dan Hendrycks et al.",
                "venue": null,
                "url": null
            }
        },
        {
            "13": {
                "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018.",
                "author": "Peter Clark et al.",
                "venue": null,
                "url": null
            }
        },
        {
            "14": {
                "title": "Hellaswag: Can a machine really finish your sentence?, 2019.",
                "author": "Rowan Zellers et al.",
                "venue": null,
                "url": null
            }
        },
        {
            "15": {
                "title": "Llama 2: Open foundation and fine-tuned chat models, 2023.",
                "author": "Hugo Touvron et al.",
                "venue": null,
                "url": null
            }
        },
        {
            "16": {
                "title": "Bigscience language open-science open-access multilingual (bloom) language model.",
                "author": "Margaret Mitchell et al.",
                "venue": "May 2022.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Opt: Open pre-trained transformer language models.",
                "author": "Susan Zhang et al.",
                "venue": "5 2022.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Langchain retrieved from \u201dhttps://github.com/langchain-ai/langchain\u201d.",
                "author": "Harrison Chase.",
                "venue": null,
                "url": null
            }
        },
        {
            "19": {
                "title": "Billion-scale similarity search with GPUs.",
                "author": "Jeff et al. Johnson.",
                "venue": "IEEE Transactions on Big Data, 7(3):535\u2013547, 2019.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani et al.",
                "venue": "Advances in Neural Information Processing Systems, 2017-December:5999\u20136009, 6 2017.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Transformers: State-of-the-art natural language processing.",
                "author": "Thomas Wolf et al.",
                "venue": "In Conference on Empirical Methods in Natural Language Processing, 2019.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Deep contextualized word representations.",
                "author": "Matthew E. Peters et al.",
                "venue": "ArXiv, abs/1802.05365, 2018.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Cureus, 15(6), 2023.",
                "author": "Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge.",
                "venue": null,
                "url": null
            }
        },
        {
            "24": {
                "title": "Llama: Open and efficient foundation language models.",
                "author": "Hugo Touvron et al.",
                "venue": "2 2023.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Biogpt: generative pre-trained transformer for biomedical text generation and mining.",
                "author": "Renqian Luo et al.",
                "venue": "Briefings in Bioinformatics, 23, 11 2022.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Drbert: A robust pre-trained model in french for biomedical and clinical domains, 2023.",
                "author": "Yanis Labrak et al.",
                "venue": null,
                "url": null
            }
        },
        {
            "27": {
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding.",
                "author": "Jacob Devlin et al.",
                "venue": "1:4171\u20134186, 10 2018.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Huatuogpt, towards taming language models to be a doctor.",
                "author": "Hongbo Zhang et al.",
                "venue": "arXiv preprint arXiv:2305.15075, 2023.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Artificial hallucinations in chatgpt: Implications in scientific writing.",
                "author": "Hussam Alkaissi and Samy Mcfarlane.",
                "venue": "Cureus, 15, 02 2023.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Doctorglm: Fine-tuning your chinese doctor is not a herculean task, 2023.",
                "author": "Honglin Xiong et al.",
                "venue": null,
                "url": null
            }
        },
        {
            "31": {
                "title": "Glm: General language model pretraining with autoregressive blank infilling.",
                "author": "Zhengxiao et al. Du.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, pages 320\u2013335, 2022.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Medalpaca \u2013 an open-source collection of medical conversational ai models and training data.",
                "author": "Tianyu Han et al.",
                "venue": "4 2023.",
                "url": null
            }
        },
        {
            "33": {
                "title": "LoRA: Low-rank adaptation of large language models.",
                "author": "Edward J Hu et al.",
                "venue": "In International Conference on Learning Representations, 2022.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.00830v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5.1",
            "5.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "3.3",
            "3.4",
            "3.5"
        ]
    },
    "research_context": {
        "paper_id": "2403.00830v1",
        "paper_title": "MedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices",
        "research_background": "### Motivation\nThe rapid advancement and potential of large language models (LLMs) like GPT-3 and GPT-4 have demonstrated tremendous capabilities in natural language processing (NLP) and other generative tasks. These LLMs can generate coherent, context-aware text and respond to complex queries, making them a valuable resource across various domains, including medicine and autonomous cognitive systems. However, a significant portion of the global population still lacks essential healthcare services, necessitating innovative solutions to bridge this accessibility gap. Furthermore, deploying high-performing LLMs on resource-constrained edge devices while minimizing latency is a major challenge that restricts their widespread adoption. Therefore, there is a compelling need for an efficient, deployable LLM system that can operate effectively on edge devices for on-premise medical assistance.\n\n### Research Problem\nThe central research problem addressed in this study revolves around the deployment of large language models (LLMs) on resource-constrained edge devices to provide on-premise medical assistance. Key challenges include:\n1. Selecting suitable LLMs that balance performance with hardware constraints.\n2. Ensuring low-latency, efficient operation on devices like Nvidia Jetson or consumer-grade GPUs.\n3. Enhancing the LLMs\u2019 understanding and performance in medical consultation scenarios through specialized training on custom-curated datasets.\n4. Creating an integrated system that can effectively search medical databases for accurate prescriptions and medical recommendations.\n\n### Relevant Prior Work\n1. **State-of-the-Art LLMs**: The paper references breakthrough models such as GPT-3 and GPT-4, which have set benchmarks in NLP tasks (Brown2020###reference_b2###, Bubeck2023###reference_b3###). These models show the potential of LLMs in various domains, including their capability to understand and generate human-like text.\n   \n2. **Medical Applications of LLMs**: Prior work highlights the application of LLMs in medicine and autonomous cognitive systems (10.3389/frai.2023.1169595###reference_b4###, jeblick2022chatgpt###reference_b5###). These studies underscore the capacity of LLMs to enhance healthcare services through improved interaction and understanding.\n   \n3. **Challenges in LLM Deployment**: Existing LLMs primarily rely on server-based setups, which require significant computational resources and power (Zhao2023###reference_b1###). This poses a challenge for deploying these models on resource-constrained devices, limiting their accessibility and practical usage.\n   \n4. **Model Selection for Edge Deployment**: The selection of LLMs for edge devices takes into account network parameters, performance metrics, open-source availability, and hardware constraints. The paper discusses tools like the Open-LLM Leaderboard (Open-LLM-Leaderboard-Report-2023###reference_b10###) for evaluating these aspects.\n   \n5. **TruthfulQA Performance**: The paper mentions evaluating LLMs using benchmarks like the TruthfulQA task (lin2022truthfulqa###reference_b11###) to gauge their accuracy and efficacy in providing truthful, reliable responses.\n\n6. **Data Collection and Optimization**: Tools like LangChain (langchain###reference_b18###) and FAISS (johnson2019billion###reference_b19###) are noted for their roles in optimizing search and retrieval functions within the medical domain, enhancing the performance of deployed LLMs.\n\nBy addressing these research challenges, the proposed MedAide system aims to leverage LLMs effectively for providing on-premise medical assistance, thereby potentially improving healthcare accessibility and outcomes.",
        "methodology": "MedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices\n\n**Methodology:**\n\nIn this section, we will describe the MedAide system in detail, along with the dataset generation, refinement, and training workflow. We elaborate on LLM quantization and model selection along with LangChain integration.\n\n### System Overview\nMedAide is designed to provide medical assistance using large language models (LLMs) on edge devices, ensuring that the processing happens on-premise for privacy and efficiency. The key components and innovations in MedAide include:\n\n1. **Dataset Generation**: The procedure involves curating a comprehensive dataset pertinent to medical assistance. This dataset encompasses various medical records, consultations, and other relevant documents necessary for training the LLMs.\n\n2. **Dataset Refinement**: Post initial dataset gathering, a refinement process is employed to ensure data quality and relevance. This involves filtering out noise, handling missing values, and standardizing medical terminologies to maintain consistency across the dataset.\n\n3. **Training Workflow**: The refined dataset is then used to train the large language models. The training workflow encompasses several stages, from initial model training to continual fine-tuning based on feedback and additional data. This iterative approach helps in enhancing the model's performance over time.\n\n4. **LLM Quantization**: Given that edge devices have limited computational resources, quantization techniques are applied to the LLMs. Quantization helps in reducing the model size and computational complexity, enabling the deployment of these models on edge devices without significant loss in performance.\n\n5. **Model Selection**: It involves selecting the most appropriate large language models based on the specific requirements of medical assistance tasks. Criteria for selection include model accuracy, performance on specific tasks, and compatibility with quantization techniques.\n\n6. **LangChain Integration**: MedAide integrates LangChain, a framework that facilitates operations involving language models, particularly for chaining different LLMs for complex tasks. This integration is crucial for achieving flexible and efficient processing of medical queries and responses.\n\nThis holistic approach ensures that the MedAide system is robust, efficient, and capable of delivering high-quality medical assistance on edge devices.",
        "main_experiment_and_results": "### Main Experiment Setup\n\nThe experiment setup for evaluating the MedAide model uses a robust combination of software and hardware resources to thoroughly test and benchmark its performance. The software environment includes:\n\n- **Programming Language and Libraries**: Python 3.10, Cuda 11.8, PyTorch, Lightning, and other essential libraries.\n- **Similarity Search Tool**: FAISS.\n\nOn the hardware side, a range of Nvidia GPUs is utilized to capture performance across different computational scenarios:\n\n- **High-End GPUs**: RTX 6000 Ada.\n- **Embedded Solutions**: Jetson AGX Xavier.\n\n### Datasets and Evaluation Metrics\n\nThe evaluation makes use of two key metrics for measuring model performance:\n  \n- **USMLE Scores**: These scores are used to benchmark the model's ability to understand and process medical questions with the competence expected of a medical professional.\n- **ChatGPT-4 Scores**: These scores provide a comparative benchmark from an established large language model, serving as a point of reference for evaluating the MedAide model's performance.\n\n### Experimental Procedure\n\nThe models are deployed on the various hardware platforms mentioned to assess:\n\n- **Accuracy**: How well the model performs in terms of correctly answering medical queries.\n- **Runtime**: The time taken for processing inputs and generating responses.\n- **Benchmark Scores**: Comprehensive performance scores including the USMLE and ChatGPT-4 metrics.\n\n### Main Experimental Results\n\nThe results demonstrate:\n\n1. **Hardware Performance**: MedAide models were assessed across different Nvidia GPUs, emphasizing their adaptability and efficiency from high-end to embedded hardware environments.\n2. **Key Metrics**: The performance was quantified primarily through USMLE and ChatGPT-4 scores, providing standardized benchmarks to evaluate the efficacy of the system in real-world medical application scenarios."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Assess the performance of lightweight LLMs (LLaMa2-7B, Bloom-560M, and OPT-125M) in medical consultations when deployed on edge devices.",
            "experiment_process": "The models were selected based on the Open-LLM Leaderboard, focusing on their performance across benchmarks. The experiment compared these models against other large models tailored for medical datasets. LLaMa2-7B, Bloom-560M, and OPT-125M were tested for deployment efficiency on resource-constrained devices. The medical domain-specific custom dataset included data from online medical forums, databases, and clinical case studies. The models underwent evaluation on their accuracy scores in medical applications and the TruthfulQA task. LangChain and FAISS were used for optimizing search and retrieval processes from medical databases.",
            "result_discussion": "OPT-125M and Bloom-560M (27.6%, 29.5% accuracy) provided a lower but acceptable accuracy with minimal resource consumption, facilitating deployment on edge devices. LLaMa2-7B achieved higher accuracy (51.9%), making it suitable for more demanding medical applications. Integration with LangChain improved the USMLE score by 2.5%, enhancing knowledge retrieval. Overall, MedAide achieved around 77% accuracy in medical consultations.",
            "ablation_id": "2403.00830v1.No1"
        },
        {
            "research_objective": "Optimize fine-tuning procedures for LLMs (LLaMa2-7B and Bloom-560M) to balance accuracy and computational efficiency for edge device deployment.",
            "experiment_process": "Low Rank Adaptation (LoRA) was employed to reduce trainable parameters and GPU memory usage. This involved approximating high-dimensional datasets in lower-dimensional spaces while preserving key features. Models were trained, including OPT-125M with 12 layers and Bloom-560M, on consumer hardware with resource limitations. Reinforcement Learning from Human Feedback (RLHF) was integrated to refine the training process using a Proximal Policy Optimization (PPO) model, ensuring technical accuracy and practical applicability.",
            "result_discussion": "LoRA significantly reduced computational requirements without sacrificing key features, making models more efficient for constrained environments. RLHF integrated into the training cycles provided human-centric insights, refining the model outputs to meet real-world requirements better. This process resulted in robust, contextually aware LLMs capable of delivering precise medical recommendations in resource-limited scenarios.",
            "ablation_id": "2403.00830v1.No2"
        },
        {
            "research_objective": "Identify and calibrate appropriate quantization strategies for LLaMa2-7B models to optimize performance on edge hardware.",
            "experiment_process": "Quantization strategies including Q4, Q8, and F16 formats were analyzed to balance model sizes and computational efficiency. Following quantization, weight clipping was conducted to maintain precision. The selection criteria incorporated compatibility with edge hardware (Nvidia Jetson boards) and performance metrics including memory usage, inference speed, and accuracy. LangChain divided medical datasets into 1000-character blocks with 50-character overlap, generating embeddings and accelerating search with GPU-accelerated FAISS for data retrieval.",
            "result_discussion": "Q8 quantization provided a balance between model size and accuracy, being the most effective for edge devices like Nvidia Jetson. The use of GPU-accelerated FAISS significantly accelerated data retrieval speeds (5-10x faster than CPU counterpart), reducing model hallucination and ensuring reliable medical guidance. This optimized quantization and data retrieval strategy enhanced the feasibility of deploying complex medical LLMs on edge devices while maintaining high accuracy and efficiency.",
            "ablation_id": "2403.00830v1.No3"
        }
    ]
}