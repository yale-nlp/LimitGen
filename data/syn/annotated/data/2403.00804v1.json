{
    "title": "Uncovering Customer Issues through Topological Natural Language Analysis",
    "abstract": "E-commerce companies deal with a high volume of customer service requests daily. While a simple annotation system is often used to summarize the topics of customer contacts, thoroughly exploring each specific issue can be challenging. This presents a critical concern, especially during an emerging outbreak where companies must quickly identify and address specific issues. To tackle this challenge, we propose a novel machine learning algorithm that leverages natural language techniques and topological data analysis to monitor emerging and trending customer issues. Our approach involves an end-to-end deep learning framework that simultaneously tags the primary question sentence of each customer\u2019s transcript and generates sentence embedding vectors. We then whiten the embedding vectors and use them to construct an undirected graph. From there, we define trending and emerging issues based on the topological properties of each transcript. We have validated our results through various methods and found that they are highly consistent with news sources.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "E-commerce websites handle a vast number of online customer service requests daily. During a typical online customer service interaction, customers first interact with a chatbot which asks them questions to identify their intent. This intent is usually classified based on the product or service that the customer needs assistance with. For instance, an online consumer electronics retailer might use its chatbot to classify requests as relating to cell phones, computers, or home appliances, among others. The chatbot then routes the customer to an agent who specializes in the requested product or service to assist. While the actual business practices among companies may differ, the interaction process between customers and agents is generally similar. Agents usually begin with a greeting and ask for details about the customer\u2019s questions. They then engage in diagnosis and finally conclude with some closing remarks.\nGenerally, processing customer requests can take several minutes, making it one of the most time-consuming aspects of e-commerce business. Therefore, developing a standardized process to handle specific issues is critical to help customers save considerable time and optimize the available resources of agents. This is especially important during emerging events or sudden surges in customer inquiries. By anticipating common issues and developing standardized procedures for agents, businesses can improve their response times, reduce customer frustration, and ultimately enhance customer satisfaction.\nWe present a novel machine learning framework, as illustrated in Fig.1, that can detect emerging and trending issues without predefined lists. The terms \u201dtrending\u201d and \u201demerging\u201d refer to the topics that are most frequently discussed within the current time window and the topics that show the most rapid increase in discussion compared to the previous time window. Our approach comprises three distinct components. Firstly, a deep learning model is employed, which utilizes an attention mechanism to automatically tag the primary question sentence in each customer\u2019s transcript and generates sentence-level embedding vectors. To improve the performance of cosine similarity, we decouple the covariance matrix to whiten the embedding vectors, bringing the coordinates of the feature space close to an orthonormal basis. We then construct an undirected graph based on cosine similarity. Finally, we analyze the topology of the graph by calculating the centrality of each customer\u2019s question. This enables us to quantify both trending and emerging issues.\n###figure_1### Related Works Our work relates to several areas in the literature, including sentence-level attention, sentence tagging, and graph-based clustering. Works related to sentence attention include (Yang et al.,, 2016  ###reference_b16###), (Nallapati et al.,, 2016  ###reference_b6###), and (Lin et al.,, 2016  ###reference_b4###), which apply the idea to document classification, summarization, and text noise reduction, respectively. Regarding sentence tagging, (Collobert and Weston,, 2008  ###reference_b1###) and (Santos and Zadrozny,, 2014  ###reference_b11###) are works that perform sentence tagging from token-level representation, and have also influenced our approach. Additionally, we reference several works in graph-based clustering, including (Wang et al.,, 2019  ###reference_b15###) and (Nov\u00e1k et al.,, 2010  ###reference_b7###), which use this technique to handle repetitive sequences and multiview data. These works have inspired our research in the application of topological data analysis to natural language processing."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Question Tagging and Sentence Attention Model",
            "text": "We present a deep learning model that can automatically tag the primary question in a contact transcript. This model is a crucial component for detecting both trending and emerging issues, as the identified questions will be utilized in later sections."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "The Dataset",
            "text": "There has been a lack of publicly available datasets related to customer service transcripts. To address this gap, we partnered with customer service team to initiate this research, using a dataset from an online chat system that enables customers to communicate with customer service agents. We collected over 500,000 contact transcripts during 2022, recording conversations between customers and agents. Each contact also comes with a unique label of the product or service that the customer and agents discussed. It\u2019s worth noting that the dataset only contains customer text data, with all confidential information, such as names and account details, anonymized to protect privacy before being shared with researchers. Although the dataset is from a specific database, the methodology presented in this article can be applied to other use cases as well."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Sentence Embedding",
            "text": "Our goal is to identify the primary question sentence in a customer-agent contact transcript. We propose two hypotheses: (1) the primary question sentence typically appears in the first few sentences of the customer\u2019s interaction with the agent, and (2) it contains the most relevant information about the product or service being discussed. If these hypotheses hold, we can treat the problem as a machine learning task: identifying the customer sentences near the agent\u2019s initial response that are most useful in predicting the product or service of the contact for a machine learning classifier. To achieve this, we need information on attention weights at the sentence level.\nWe propose a deep learning model, as shown in Fig. 2, to achieve our goal. Unlike traditional text classification models that represent an article as a 2D tensor , where  is the number of tokens in the article and  is the dimension of the word embedding, our approach represents each article as a 3D tensor . Here,  is the number of sentences in the article, and  is the number of tokens per sentence. To accommodate varying numbers of sentences and tokens per sentence in each transcript, we use zero-padding to ensure a consistent tensor shape for subsequent processing.\nTo obtain sentence-level embeddings, we treat each sentence as a temporal slice and apply a time-distributed wrapper to a sequence model , such as BERT or LSTM. This ensures that the model receives only one sentence per time step, allowing us to embed each sentence. The resulting output tensor, , contains  sentence vectors, each with  dimensions.\nOur \u201dbag of sentences\u201d model currently does not consider sentence positions, but we have observed that customer questions tend to appear in early sentences during interactions with agents. This suggests that sentence positions can impact attention weights, so we incorporate sentence position information into the model.\nTo do this, we adapt the idea of position embedding used in many language models for tokens, but apply it to sentences. We assign each sentence an index, ranging from - to +, representing the number of sentences between the current sentence and the agent\u2019s first response. For instance, an index of -5 indicates that the sentence is five steps before the agent\u2019s first sentence, while +5 indicates five steps after. We shift the indices by , resulting in an allowed index range of 0 to +, with the sentence having an index of  being the agent\u2019s first sentence to avoid negative indices. For a sentence with index , the -th component of the position embedding vector  is given by  and :\nwhere  is the dimension of embedding vector. By adding  and , we get the final sentence embedding vector .\n###figure_2###"
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Sentention Attention",
            "text": "We define sentence level attention(Vaswani et al.,, 2017  ###reference_b14###):\n, where  is a query tensor (i.e. the sentence embedding),  is a key vector and  is a value tensor. We can generally define  without any impact on model performance. As a result, the attention value  is a vector and the sum of its elements must equal 1, due to the application of the softmax function.\nOur model\u2019s attention vector  has a simple interpretation. As shown in Fig. 2, each sentence in a transcript is represented as a vector  (the -th row of the 2D tensor ). The attention vector  is a linear combination of all sentences, computed as , subject to the constraint that . We then pass  through a fully connected layer with a softmax activation function to predict the product or service associated with the transcript. The attention weights  reflect the importance of each sentence in determining the product or service, with higher weights assigned to sentences containing more critical information."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Experiments",
            "text": "Our model was trained on 500,000 transcripts with 152 classes to predict the product or service discussed in each contact. We used DistilBERT (Sanh et al.,, 2020  ###reference_b10###) as the embedding model , with an output dimension of 768. To prepare the transcripts for training, we padded each one with zeros to create 64 sentences, each with 128 words.\nAfter training, we calculated attention weights  for each customer sentence in a transcript. To identify the primary question, we assumed that it is the sentence with the highest attention weight that is  steps before or after the agent\u2019s first sentence. In testing on 4,000 human-annotated transcripts, we found that  (i.e.,  steps) yielded the best results, correctly identifying the primary question sentence in 84.3% of the total transcripts. Fig. 1 in the Appendix illustrates an example of how our sentence attention model tags the primary question sentence.\nTo evaluate the impact of sentence position embedding on performance, we compared the model with and without this feature. Although the difference in accuracy was minimal (83.4%), we observed that sentence position embedding resulted in higher attention weights on the primary question sentence. Further investigation is needed to explore the effects of position embedding."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Topological Natural Language Analysis",
            "text": "Our objective is to identify both emerging and trending topics among the questions gathered by the model presented in Sec. 2. However, conventional clustering methods face several challenges in achieving this goal, including sensitivity to hyperparameters, scalability issues with large numbers of classes and samples, and lack of flexibility in defining distances.\nMoreover, identifying emerging topics involves changes in the volume of a topic between time windows, and conventional clustering methods cannot determine whether two clusters in different datasets are related to the same topics. It is even possible for an emerging topic to be present in the current time window without appearing in the previous time window. Additionally, conventional clustering methods may treat emerging topics as noise due to their much smaller volume than trending topics.\nTo overcome these challenges, we propose a topological-analysis-based method robust to hyperparameter selection and can quantitatively detect both emerging and trending topics between different time windows."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Undirected Representation Graph and Centrality",
            "text": "Let us assume that we have gathered primary customer questions and their corresponding sentence embeddings, , over two time periods,  and . Each sentence can be represented as a node in an undirected graph, with edges connecting nodes whose cosine similarity surpasses a specified threshold, . It is important to note that this graph is constructed using data from both time periods,  and . In this graph, a node with a higher number of neighbors indicates a greater number of similar questions.\nCentrality is a significant topological property of a node, which describes its importance within a graph. Various types of centrality exist, each applicable to different scenarios. In this case, we define two new types of centrality that modify the decay centrality. Assuming we have an undirected graph represented by an adjacency matrix , where  if nodes , the decay centrality of a node  in a graph  is defined as(van Steen,, 2010  ###reference_b13###):\nHere,  refers to the set of nodes in the graph, and  is the total number of nodes in the graph. This normalization factor ensures that the centrality  is independent of the graph\u2019s size. The attenuation factor  is typically selected such that . The topological distance  between nodes  and  is the graph distance, not the Euclidean distance . The numerator is given by an exponent , which results in  when nodes  and  are directly connected. A question with higher decay centrality means the graph has more similar questions.\nGiven that time is an additional property of each node, we modify the definition above and propose two new types of centrality:\nmatched decay centrality:\nmismatched decay centrality:\nIn these equations, the brackets  represent Iverson brackets, yielding 1 if the statement inside is true and 0 if false.  refers to the time window that node  belongs to.  considers contributions only when two nodes belong to the same time window, while  accounts for contributions from different windows, decaying exponentially as the distance increases. It is important to note that node  can reach node  through intermediate nodes in either the same or different time windows. Furthermore, we have . This definition splits the decay centrality into two terms based on the time window. We can effectively identify trending and emerging issues by utilizing matched and mismatched decay centrality.\n###figure_3###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Trending and emerging Issues",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Topological Perspective",
            "text": "We are only interested in the nodes in the current time window, so we define trending and emerging issues from a topological perspective as follows: 1). Trending issues are identified as the customer\u2019s primary questions with a large trending score:  2). Emerging issues are identified as the customer\u2019s primary questions with a large emerging score: .\nThe definitions for the trending and emerging scores are simple to understand. The trending score, denoted by , counts the number of similar questions in the graph that exist in the current time window. On the other hand, the emerging score, given by , represents the difference in centrality contributed from the previous time window and the current time window. It\u2019s important to note that . As a result, the emerging score lies between -1 and +1, where -1 and +1 correspond to the centrality entirely coming from the previous and current time window respectively, while 0 indicates equal contributions from both time windows. To avoid identifying small-size clusters that may not be significant for a business that receives a large volume of customer contacts, we introduce an additional filtering factor, , where  is a parameter that controls the strength of the filter. This factor applies a weight to the emerging score, such that  drops to 0 when , and saturates to 1 when ."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Experiment",
            "text": "We constructed a graph using the MessageUs transcript data by applying a cosine similarity threshold of , an attenuation factor of , and a filter factor of , where  is the total number of nodes. This filter factor was set to 10% of the maximum number of  values, and it helped to exclude small clusters that were unlikely to be significant. We used topological data analysis to identify the most prominent clusters, and we found that the topics of these clusters were highly consistent, even though the exact values of  and  were sensitive to the chosen hyperparameters. Varying the hyperparameters affected only the sizes of the clusters and did not significantly alter the topics we discovered, which demonstrates the robustness of the topology analysis approach.\nFig. 3 shows the distribution of , , and  for Tablet-related transcripts, using Jan 2023 and Feb 2023 as the previous and current time windows, respectively (around 100K data points). We compare the distributions for a cosine similarity threshold of  (panels (a)-(c)) and  (panels (d)-(f)). Decreasing  results in a more compact graph and a more Gaussian-like distribution of centrality. We observe that the emerging score distribution can be separated into two parts: a Gaussian-like distribution around zero due to the filter factor applied to nodes with , and a long-tail region due to nodes with . The filter factor helps to focus on the outliers, and the nodes in the long-tail region are generally similar, with high-score nodes unlikely to become low-score nodes due to changes in hyperparameters.\nIt is important to note that similar sentences tend to form clusters. As a result, the neighbors of a high centrality node also tend to have high centrality, as shown in Fig. 3(g). To avoid locating the same cluster multiple times, it is necessary to ensure that two centers are sufficiently far apart. To achieve this, we first designate the node with the largest  (for trending) or  (for emerging) as the cluster center and its surrounding neighbors with graph distance  as the cluster members. Next, we search for the node with the next-largest score at least a graph distance of 4 away from any known clusters as the next cluster center. We repeat this procedure until we have obtained the desired number of clusters.\nTable.1 of the appendix provides examples of sentences from the top-1 trending and emerging clusters of Tablet in February 2023. As one can see, all the customer\u2019s questions within each cluster are very similar, demonstrating the effectiveness of our approach in cosine similarity and topology-based topic detection."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Result and Discussion",
            "text": "We have successfully developed a machine learning model capable of extracting trending and emerging issues from extensive transcripts. However, directly validating the precision rate of our model presents a challenge. To address this challenge, we have proposed a human-annotation-based method for validation.\nOur data collection spanned from November 2022 to February 2023 and involved more than 10 different products, including Kindle, Echo, Music, eBook, and Prime Video. During this period, our focus was on identifying the top-3 trending and emerging issues. We took care to exclude issues that appeared unreasonable due to quality of data or instances of fraud or policy abuse attacks. As a result of this process, we successfully identified 84 trending issues and 64 emerging issues.\nTo assess trending issues, each annotator is asked to select three keywords that best represent each issue. We then count the number of transcripts containing these keywords simultaneously. Remarkably, we discovered that all 84 issues, representing 100% of the total, constitute a large portion, at least 10%, of the overall volume of each product line. Furthermore, the volume remains consistently stable from month to month. Although it\u2019s challenging to determine if these are the largest issues, it\u2019s noteworthy that a significant portion of them pertains to return or refund-related matters, aligning closely with our business experiences.\nRegarding emerging issues, we employ three methods to identify them. Firstly, we select the three most representative keywords for each issue and monitor changes in volume within transcripts containing these keywords. Additionally, we investigate whether the issue surfaces in the Amazon Digital and Device Forum for that month, where customers discuss Amazon\u2019s services, with a minimum of 10 replies ( not necessary in a single thread ). Lastly, we conduct a Google search to determine if these issues are covered in news media.\nOur surveys revealed some insightful findings: approximately 90% of emerging issues exhibit volume changes exceeding 30% between two months, around 60% of the issues are discussed in the Amazon Digital and Device Forum for that month, and roughly 15% of the issues are covered in news media, typically related to live events or new product launches. The surveys support our model indeed captures the emerging issues very well. In Table 2 of the appendix, we provide a few examples of emerging topics found in Feb 2023 and our validation results, and most of the topics align well with the news sources."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In summary, we have presented a unique machine learning framework for extracting customers\u2019 trending and emerging issues. Our work starts with an attention-based deep learning model that tags customers\u2019 primary questions and generates corresponding sentence embeddings simultaneously. We then transform the sentence embeddings into an isotropic coordinate system using whitening techniques to improve the cosine similarity performance. Finally, we apply topological natural language analysis methods to analyze the centrality of each question, enabling us to identify trending and emerging issues.\nOur work makes a significant contribution by demonstrating the application of a sentence-level attention mechanism in conversational transcripts, an area that has been understudied. We combine this mechanism with topological data analysis to extract useful information for a real-world problem."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Appendix: Example of Transcript",
            "text": "###figure_4###"
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Appendix: Trending & Emerging Issues on Fire Tablet",
            "text": "Trending Issues\nEmerging Issues\nI need to utilize the warranty for my kids\u2019 tablet with a cracked screen.\nOn the kids\u2019 tablet, whenever I try to access my child\u2019s profile, it consistently displays an error message saying \u201doops, something went wrong.\u201d However, when I switch to the parent profile, everything works fine.\nI need to check the warranty for my son\u2019s cracked tablet screen.\nAdditionally, the kids\u2019 profile on the tablet consistently displays an error message saying \u201doops, something went wrong,\u201d while the adult profiles work fine.\nMy son accidentally cracked the screen on our kid\u2019s tablet, and I\u2019m looking to get it replaced under the attached warranty.\nHowever, when I try to access my child\u2019s profile, I keep encountering an error message saying \u201doops, something went wrong.\u201d\nI would like to check if my tablet is still covered by warranty.\nFurthermore, the kids\u2019 profile on the tablet is not functioning properly. Despite attempting a factory reset, I am consistently greeted with an error message stating \u201doops, something went wrong\u201d whenever I try to load the kids\u2019 profile. However, the adult profile is functioning normally as usual.\nMy daughter accidentally broke the screen of her tablet for kids. I was wondering if it\u2019s possible to file a claim to have it repaired.\nAdditionally, I\u2019m experiencing difficulties accessing my child\u2019s page on the Tablet. While my information loads successfully, I keep encountering an error message saying \u201doops, something went wrong\u201d when Kids profile begins to load."
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C Appendix: Selected Emerging Issues on Various Products",
            "text": "Product\nCluster Center Sentence\nTopic Summarization\nValidation\nTablet\nIt keeps saying oops something went wrong when I try and get into my child\u2019s profile\nCustomers have reported encountering an error message when attempting to enter child\u2019s profile.\nDuring February, this issue was extensively discussed in Amazon\u2019s Device Forum and was actively being addressed by Amazon\u2019s engineering team.\nMusic\nHi I was getting an error code and I was unable to play any music from any play list\nCustomers have reported encountering an exception error with a specific code while playing music.\nWe compared the probability of finding the keywords \u201dexception/play/error\u201d together in a transcript and observed a 30% increase from the previous month.\nGame & Software\nHi My preorder for the Hogwarts Legacy is VERY late It s supposed to arrive today but won\u2019t be shipped until 23 This is really unacceptable.\nCustomers have reported that their preordered game, Hogwarts Legacy, has been delayed.\nWe confirmed that the official release date of Hogwarts Legacy was February 10, 2023, which explains why the related issue emerged in February.\nSport Video\nHello I was just wondering if I subscribe to MLB tv through Amazon Prime will i have access to any MLB Network shows Or just the games\nCustomers have raised questions about watching MLB games on Prime Video but have encountered technical issues.\nWe confirmed that the 2023 MLB season commenced in late February, which explains why the related issues emerged during that month.\nTV Stick\nThe screen says Amazon system recovery your Amazon fire tv will restart in a few minutes and should resume normal operation if it doesn\u2019t restart\nCustomers have reported that their Fire TV devices received updates but failed to restart properly.\nWe checked Amazon\u2019s Digital & Device Forum and confirmed that this issue has been widely discussed, and Amazon\u2019s engineering team has addressed it in the next update."
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A2.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A2.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" id=\"A2.T1.1.1.1.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A2.T1.1.1.1.1.1\">Trending Issues</p>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"A2.T1.1.1.1.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A2.T1.1.1.1.2.1\">Emerging Issues</p>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T1.1.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_tt\" id=\"A2.T1.1.2.1.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A2.T1.1.2.1.1.1\">I need to utilize the warranty for my kids\u2019 tablet with a cracked screen.</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_tt\" id=\"A2.T1.1.2.1.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A2.T1.1.2.1.2.1\">On the kids\u2019 tablet, whenever I try to access my child\u2019s profile, it consistently displays an error message saying \u201doops, something went wrong.\u201d However, when I switch to the parent profile, everything works fine.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T1.1.3.2\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" id=\"A2.T1.1.3.2.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A2.T1.1.3.2.1.1\">\u00a0I need to check the warranty for my son\u2019s cracked tablet screen.</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A2.T1.1.3.2.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A2.T1.1.3.2.2.1\">Additionally, the kids\u2019 profile on the tablet consistently displays an error message saying \u201doops, something went wrong,\u201d while the adult profiles work fine.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T1.1.4.3\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" id=\"A2.T1.1.4.3.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A2.T1.1.4.3.1.1\">\u00a0My son accidentally cracked the screen on our kid\u2019s tablet, and I\u2019m looking to get it replaced under the attached warranty.</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A2.T1.1.4.3.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A2.T1.1.4.3.2.1\">However, when I try to access my child\u2019s profile, I keep encountering an error message saying \u201doops, something went wrong.\u201d</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T1.1.5.4\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" id=\"A2.T1.1.5.4.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A2.T1.1.5.4.1.1\">\u00a0I would like to check if my tablet is still covered by warranty.</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A2.T1.1.5.4.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A2.T1.1.5.4.2.1\">Furthermore, the kids\u2019 profile on the tablet is not functioning properly. Despite attempting a factory reset, I am consistently greeted with an error message stating \u201doops, something went wrong\u201d whenever I try to load the kids\u2019 profile. However, the adult profile is functioning normally as usual.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T1.1.6.5\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_l ltx_border_r ltx_border_t\" id=\"A2.T1.1.6.5.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A2.T1.1.6.5.1.1\">\u00a0My daughter accidentally broke the screen of her tablet for kids. I was wondering if it\u2019s possible to file a claim to have it repaired.</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_r ltx_border_t\" id=\"A2.T1.1.6.5.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A2.T1.1.6.5.2.1\">Additionally, I\u2019m experiencing difficulties accessing my child\u2019s page on the Tablet. While my information loads successfully, I keep encountering an error message saying \u201doops, something went wrong\u201d when Kids profile begins to load.</p>\n</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>In our analysis of Amazon Fire Tablet\u2019s top-1 trending and emerging clusters in Feb 2023, we found that customers reported cracked screens as a trending issue and sought warranty replacements or repairs, while problems with the kids\u2019 profile emerged as an emerging issue. The similarity among sentences in the trending and emerging topics extracted from the same cluster suggests the effectiveness of our method.</figcaption>\n</figure>",
            "capture": "Table 1: In our analysis of Amazon Fire Tablet\u2019s top-1 trending and emerging clusters in Feb 2023, we found that customers reported cracked screens as a trending issue and sought warranty replacements or repairs, while problems with the kids\u2019 profile emerged as an emerging issue. The similarity among sentences in the trending and emerging topics extracted from the same cluster suggests the effectiveness of our method."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"A3.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A3.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A3.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t\" id=\"A3.T2.1.1.1.1\" style=\"width:43.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.1.1.1.1\">Product</p>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"A3.T2.1.1.1.2\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.1.1.2.1\">Cluster Center Sentence</p>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"A3.T2.1.1.1.3\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.1.1.3.1\">Topic Summarization</p>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"A3.T2.1.1.1.4\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.1.1.4.1\">Validation</p>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A3.T2.1.2.1\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt\" id=\"A3.T2.1.2.1.1\" style=\"width:43.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.2.1.1.1\">Tablet</p>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_tt\" id=\"A3.T2.1.2.1.2\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.2.1.2.1\">It keeps saying oops something went wrong when I try and get into my child\u2019s profile</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_tt\" id=\"A3.T2.1.2.1.3\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.2.1.3.1\">Customers have reported encountering an error message when attempting to enter child\u2019s profile.</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_tt\" id=\"A3.T2.1.2.1.4\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.2.1.4.1\">During February, this issue was extensively discussed in Amazon\u2019s Device Forum and was actively being addressed by Amazon\u2019s engineering team.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.1.3.2\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t\" id=\"A3.T2.1.3.2.1\" style=\"width:43.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.3.2.1.1\">Music</p>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A3.T2.1.3.2.2\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.3.2.2.1\">Hi I was getting an error code and I was unable to play any music from any play list</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A3.T2.1.3.2.3\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.3.2.3.1\">Customers have reported encountering an exception error with a specific code while playing music.</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A3.T2.1.3.2.4\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.3.2.4.1\">We compared the probability of finding the keywords \u201dexception/play/error\u201d together in a transcript and observed a 30% increase from the previous month.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.1.4.3\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t\" id=\"A3.T2.1.4.3.1\" style=\"width:43.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.4.3.1.1\">Game &amp; Software</p>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A3.T2.1.4.3.2\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.4.3.2.1\">Hi My preorder for the Hogwarts Legacy is VERY late It s supposed to arrive today but won\u2019t be shipped until 23 This is really unacceptable.</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A3.T2.1.4.3.3\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.4.3.3.1\">Customers have reported that their preordered game, Hogwarts Legacy, has been delayed.</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A3.T2.1.4.3.4\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.4.3.4.1\">We confirmed that the official release date of Hogwarts Legacy was February 10, 2023, which explains why the related issue emerged in February.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.1.5.4\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t\" id=\"A3.T2.1.5.4.1\" style=\"width:43.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.5.4.1.1\">Sport Video</p>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A3.T2.1.5.4.2\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.5.4.2.1\">Hello I was just wondering if I subscribe to MLB tv through Amazon Prime will i have access to any MLB Network shows Or just the games</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A3.T2.1.5.4.3\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.5.4.3.1\">Customers have raised questions about watching MLB games on Prime Video but have encountered technical issues.</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A3.T2.1.5.4.4\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.5.4.4.1\">We confirmed that the 2023 MLB season commenced in late February, which explains why the related issues emerged during that month.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.1.6.5\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_bb ltx_border_l ltx_border_rr ltx_border_t\" id=\"A3.T2.1.6.5.1\" style=\"width:43.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.6.5.1.1\">TV Stick</p>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_r ltx_border_t\" id=\"A3.T2.1.6.5.2\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.6.5.2.1\">The screen says Amazon system recovery your Amazon fire tv will restart in a few minutes and should resume normal operation if it doesn\u2019t restart</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_r ltx_border_t\" id=\"A3.T2.1.6.5.3\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.6.5.3.1\">Customers have reported that their Fire TV devices received updates but failed to restart properly.</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_r ltx_border_t\" id=\"A3.T2.1.6.5.4\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T2.1.6.5.4.1\">We checked Amazon\u2019s Digital &amp; Device Forum and confirmed that this issue has been widely discussed, and Amazon\u2019s engineering team has addressed it in the next update.</p>\n</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>The table presents a selection of emerging issues identified in February 2023, categorized by product line. The first column displays the corresponding question sentences that represent the cluster center node. In the second column, we summarize the topic of each cluster. The third column shows the evidence that confirms these issues as emerging. It is noteworthy that the majority of these emerging issues are consistent with various online news sources.</figcaption>\n</figure>",
            "capture": "Table 2: The table presents a selection of emerging issues identified in February 2023, categorized by product line. The first column displays the corresponding question sentences that represent the cluster center node. In the second column, we summarize the topic of each cluster. The third column shows the evidence that confirms these issues as emerging. It is noteworthy that the majority of these emerging issues are consistent with various online news sources."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.00804v1_figure_1.png",
            "caption": "Figure 1: Our proposed workflow involves several steps. Initially, the transcripts are passed to a sentence attention model to extract the primary questions asked by the customers and their corresponding sentence embeddings. The embeddings are then whitened to obtain representations in an isotropic coordinate system. These whitened vectors are then utilized to construct an undirected graph, and their topological properties are calculated to identify both trending and emerging issues."
        },
        "2": {
            "figure_path": "2403.00804v1_figure_2.png",
            "caption": "Figure 2: The Sentence Attention Model. The model consists of blue blocks, representing tensors, and green blocks representing operators. (a) The neural network is comprised of sentence tensors, Sisubscript\ud835\udc46\ud835\udc56S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and a sequence model, \u03a3\u03a3\\Sigmaroman_\u03a3, which outputs Q\u2032\u2062isuperscript\ud835\udc44\u2032\ud835\udc56Q^{{}^{\\prime}}{i}italic_Q start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT \u2032 end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT italic_i. The position embedding vector E\u2062i\ud835\udc38\ud835\udc56E{i}italic_E italic_i is combined with Q\u2032\u2062isuperscript\ud835\udc44\u2032\ud835\udc56Q^{{}^{\\prime}}{i}italic_Q start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT \u2032 end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT italic_i to create the sentence embeddings Q\u2062i\ud835\udc44\ud835\udc56Q{i}italic_Q italic_i. Finally, a linear classifier predicts the product/service. (b) The red block in (a) is described in detail. Tensor notation (S1;Tnsubscript\ud835\udc461subscript\ud835\udc47\ud835\udc5bS_{1};T_{n}italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT) refers to the n\ud835\udc5bnitalic_n-th token in sentence S1subscript\ud835\udc461S_{1}italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. The sequence model \u03a3\u03a3\\Sigmaroman_\u03a3 processes each word in a sentence using a time-distributed wrapper to handle multiple sentences. (c) The orange block in (a) is explained. A dense layer, K\ud835\udc3eKitalic_K, with a softmax activation function is applied to all sentence embedding vectors Qisubscript\ud835\udc44\ud835\udc56Q_{i}italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (via a time-distributed wrapper) to calculate attention scores \u03c3isubscript\ud835\udf0e\ud835\udc56\\sigma_{i}italic_\u03c3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Note that Qisubscript\ud835\udc44\ud835\udc56Q_{i}italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is equivalent to Visubscript\ud835\udc49\ud835\udc56V_{i}italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT."
        },
        "3": {
            "figure_path": "2403.00804v1_figure_3.png",
            "caption": "Figure 3: The centrality distribution of Fire Tablet in Feb 2023 is depicted in panels (a)-(f), where we compare the cosine similarity thresholds of \u03b1=0.8\ud835\udefc0.8\\alpha=0.8italic_\u03b1 = 0.8 and \u03b1=0.6\ud835\udefc0.6\\alpha=0.6italic_\u03b1 = 0.6. As for panel (g), the graph built using a few selected samples clearly demonstrates that similar sentences tend to cluster together, resulting in high centrality for nodes around the cluster centers."
        },
        "4": {
            "figure_path": "2403.00804v1_figure_4.png",
            "caption": "Figure 1: An illustration of the functioning of the question tagging model. The model calculates the attention score, \u03c3isubscript\ud835\udf0e\ud835\udc56\\sigma_{i}italic_\u03c3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, for each sentence in a transcript. The sentence preceding and following the agent\u2019s first sentence, with the highest score is predicted as the customer\u2019s primary question, i.e. the highlighted sentence."
        }
    },
    "references": [
        {
            "1": {
                "title": "A unified architecture for natural language processing: deep neural\nnetworks with multitask learning.",
                "author": "Collobert, R. and Weston, J. (2008).",
                "venue": "ICML \u201908: Proceedings of the 25th international conference on\nMachine learning, 81:160\u2013167.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Finding structure with randomness: Probabilistic algorithms for\nconstructing approximate matrix decompositions.",
                "author": "Halko, N., Martinsson, P.-G., and Tropp, J. A. (2009).",
                "venue": "arXiv, page 0909.4061.",
                "url": null
            }
        },
        {
            "3": {
                "title": "On the sentence embeddings from pre-trained language models.",
                "author": "Li, B., Zhou, H., Junxian He, M. W., Yang, Y., and Li, L. (2020).",
                "venue": "arXiv, page 2011.05864.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Neural relation extraction with selective attention over instances.",
                "author": "Lin, Y., Shen, S., Liu, Z., Luan, H., and Sun, M. (2016).",
                "venue": "Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics, 1:2124\u20132133.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Efficient estimation of word representations in vector space.",
                "author": "Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013).",
                "venue": "arXiv, page 1301.3781.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Abstractive text summarization using sequence-to-sequence rnns and\nbeyond.",
                "author": "Nallapati, R., Zhou, B., dos santos, C. N., and Caglar Gulcehre, B. X. (2016).",
                "venue": "arXiv, page 1602.06023.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Graph-based clustering and characterization of repetitive sequences\nin next-generation sequencing data.",
                "author": "Nov\u00e1k, P., Neumann, P., and Macas, J. (2010).",
                "venue": "BMC Bioinformatics, 11:378.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Glove: Global vectors for word representation.",
                "author": "Pennington, J., Socher, R., and Manning, C. (2014).",
                "venue": "Proceedings of the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), page 1532\u20131543.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Sentencebert: Sentence embeddings using siamese bertnetworks.",
                "author": "Reimers, N. and Gurevych, I. (2019).",
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on\nNatural Language Processing, page 3982\u20133992.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and\nlighter.",
                "author": "Sanh, V., Debut, L., and Julien Chaumond, T. W. (2020).",
                "venue": "arXiv, (1910):01108.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Learning character-level representations for part-of-speech tagging.",
                "author": "Santos, C. D. and Zadrozny, B. (2014).",
                "venue": "Proceedings of the 31st International Conference on Machine\nLearning, PMLR, 32:1818\u20131826.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Whitening sentence representations for better semantics and faster\nretrieval.",
                "author": "Su, J., Cao, J., Liu, W., and Ou, Y. (2021).",
                "venue": "arXiv, page 2103.15316.",
                "url": null
            }
        },
        {
            "13": {
                "title": "An introduction to graph theory and complex networks.",
                "author": "van Steen, M. (2010).",
                "venue": null,
                "url": null
            }
        },
        {
            "14": {
                "title": "Attention is all you need.",
                "author": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,\nKaiser, L., and Polosukhin, I. (2017).",
                "venue": "Advances in neural information processing systems, 30.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Gmc: Graph-based multi-view clustering.",
                "author": "Wang, H., Yang, Y., and Liu, B. (2019).",
                "venue": "IEEE Transactions on Knowledge and Data Engineering,\n32:1116\u20131129.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Hierarchical attention networks for document classification.",
                "author": "Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., and Hovy, E. (2016).",
                "venue": "Proceedings of NAACL-HLT 2016, pages 1480\u20131489.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.00804v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "2.4",
            "4",
            "4.1",
            "4.2",
            "5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2.4",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.00804v1",
        "paper_title": "Uncovering Customer Issues through Topological Natural Language Analysis",
        "research_background": "### Paper's Motivation\n\nThe paper is motivated by the need to enhance the efficiency of online customer service interactions on e-commerce websites. Given the large volume of customer service requests, the current manual handling process is time-consuming and inefficient. This problem is amplified during emerging events or sudden surges in customer inquiries. By anticipating common issues and standardizing procedures for agents, the paper aims to save significant time for customers and optimize agent resources, thereby improving response times, reducing customer frustration, and ultimately enhancing customer satisfaction.\n\n### Research Problem\n\nThe main research problem addressed by the paper is the development of a machine learning framework that can detect emerging and trending customer issues in e-commerce settings without relying on predefined lists. This involves identifying the primary question in each customer's transcript, generating sentence-level embedding vectors, constructing an undirected graph based on cosine similarity, and analyzing the graph to quantify trending and emerging issues.\n\n### Relevant Prior Work\n\n1. **Sentence-Level Attention**:\n   - Yang et al. (2016) applied sentence-level attention to document classification.\n   - Nallapati et al. (2016) utilized it for summarization tasks.\n   - Lin et al. (2016) incorporated sentence-level attention for text noise reduction.\n\n2. **Sentence Tagging**:\n   - Collobert and Weston (2008) performed sentence tagging from token-level representation.\n   - Santos and Zadrozny (2014) developed a similar approach from token-level representation that influenced the current work.\n\n3. **Graph-Based Clustering**:\n   - Wang et al. (2019) utilized graph-based clustering to handle repetitive sequences.\n   - Nov\u00e1k et al. (2010) applied it to multiview data.\n\nThese various prior works have informed and inspired the current research, particularly in the areas of sentence-level attention, sentence tagging, and graph-based clustering, which are crucial components of the proposed framework to analyze customer interaction data.",
        "methodology": "In this section, we describe the methodology employed to uncover customer issues by using topological natural language analysis. The proposed method revolves around a deep learning model designed to automatically identify and tag the primary question within a contact transcript. This tagging process is integral to the broader objective of detecting both trending and emerging issues in customer interactions. Once the primary questions are identified, they are utilized in subsequent analyses, forming the foundation of our approach. Key components of this methodology include:\n\n1. **Deep Learning Model:** The core of the methodology is a sophisticated deep learning model specifically trained to recognize and tag primary questions within contact transcripts.\n\n2. **Automatic Tagging:** The model automatically processes contact transcripts to identify the main question posed by the customer, thus eliminating the need for manual tagging.\n\n3. **Detection of Issues:** The tagged primary questions are key to detecting trending and emerging issues, enabling proactive responses to customer concerns.\n\nBy leveraging state-of-the-art machine learning techniques, this methodology enhances the ability to analyze customer interactions and identify pertinent issues efficiently.",
        "main_experiment_and_results": "**Experiment Setup:** - **Dataset:** The model was trained on 500,000 transcripts, with each transcript categorized into 152 classes to predict the product or service being discussed. - **Model:** DistilBERT (Sanh et al., 2020) was used as the embedding model, featuring an output dimension of 768. - **Preprocessing:** Each transcript was padded with zeros to create 64 sentences, each containing 128 words. - **Post-Training Calculation:** Attention weights were calculated for each customer sentence in a transcript. The primary question was assumed to be the sentence with the highest attention weight that is $ \\\\Delta $ steps from the agent's first sentence.\n\n**Evaluation Metrics:** - **Accuracy of Primary Question Identification:** The main metric used was the accuracy of correctly identifying the primary question sentence.\n\n**Results:** - On a test set of 4,000 human-annotated transcripts, the model achieved its best result with $ \\\\Delta = 4 $, correctly identifying the primary question sentence in 84.3% of the transcripts.\n\n**Visualization:** An example is provided in the Appendix of how the sentence attention model tags the primary question sentence.\n\nNote: Further investigations are warranted to explore the full effects of sentence position embedding."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the impact of sentence position embedding on model performance in identifying the primary question sentence in customer service transcripts.",
            "experiment_process": "The model was trained on 500,000 transcripts with 152 classes using DistilBERT as the embedding model. Each transcript was padded to create 64 sentences, each with 128 words. After training, attention weights were calculated for each sentence to identify the primary question. Testing was done on 4,000 human-annotated transcripts to evaluate the model's accuracy in detecting the primary question sentence. The model was compared with and without sentence position embedding, and accuracy metrics were used for evaluation.",
            "result_discussion": "The model correctly identified the primary question sentence in 84.3% of the transcripts. The inclusion of sentence position embedding resulted in a marginal increase in accuracy (83.4%) and higher attention weights on the primary question sentence, indicating that further investigation into position embedding's effects is required.",
            "ablation_id": "2403.00804v1.No1"
        },
        {
            "research_objective": "Analyze the robustness of topological data analysis in identifying prominent clusters from customer service transcripts using various cosine similarity thresholds and hyperparameters.",
            "experiment_process": "A graph was constructed using MessageUs transcript data with a cosine similarity threshold and specific attenuation and filter factors, set based on the total number of nodes. Topological data analysis was applied to identify clusters. The analysis was performed on Tablet-related transcripts from January and February 2023 (around 100K data points). Different cosine similarity thresholds and hyperparameters were tested to observe their effects on cluster sizes and distributions.",
            "result_discussion": "The topological data analysis proved robust, as varying hyperparameters affected only the cluster sizes, not the topics identified. The emerging score distribution showed a Gaussian-like distribution around zero and a long-tail region, indicating differentiated cluster nodes. The procedure ensured sufficient separation between cluster centers, enhancing the detection of unique trends and emerging issues. Overall, the clusters maintained high consistency in their topics despite changes in cosine similarity thresholds and hyperparameters.",
            "ablation_id": "2403.00804v1.No2"
        }
    ]
}