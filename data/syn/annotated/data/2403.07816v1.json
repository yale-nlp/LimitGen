{
    "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM",
    "abstract": "We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, Large Language Models (LLMs) have shown impressive performance in a wide-range of tasks\n(Brown et al., 2020  ###reference_b7###; Touvron et al., 2023  ###reference_b37###; Achiam et al., 2023  ###reference_b1###), including code generation (Li et al., 2022b  ###reference_b28###; Rozi\u00e8re et al., 2023  ###reference_b31###), solving math problems (Azerbayev et al., 2023  ###reference_b5###), multilinguality (Zhao et al., 2024  ###reference_b42###), etc.\nTraining such LLMs requires a large amount of compute and data, exceeding thousands of GPUs and trillions of tokens.\nThe training parallelization is typically done by maintaining multiple copies of the model on different GPUs and keeping them synchronized after each weight update.\nThe cost of this frequent communication is the main bottleneck in scaling the training to more GPUs.\nBesides this issue, synchronized training is more vulnerable to hardware failures as a single failed GPU can cause the whole training to halt (Zhang et al., 2022  ###reference_b41###; Gemini Team, 2023  ###reference_b14###).\nRecent work by Li et al. (2022a  ###reference_b27###) proposed the Branch-Train-Merge (BTM) method for embarrassingly parallel training of LLMs without any synchronization for improving the throughput of pretraining.\nIt starts by creating multiple copies of a seed LLM, then separately training each copy on different subsets of data.\nThis results in multiple independent LLMs that do not share any parameters and each LLM is an expert specializing in its own data distribution, such as knowledge domains, languages or even modalities.\nAt test time, an input prompt is classified into one or more of the domains, and then the final outputs are formed from the corresponding expert models which are combined to predict the next token.\nWhile this approach makes training more efficient, its main drawback is the lack of a unified single model making it impossible to do further supervised finetuning (SFT) or reinforcement learning from human feedback (RLHF) finetuning (Ouyang et al., 2022  ###reference_b29###), both of which can boost performance further, and are crucial steps in building aligned LLMs.\nA separate line of work for reducing the computational footprint of LLMs is the Mixture-of-Experts (MoE) approach (Jacobs et al., 1991  ###reference_b19###; Shazeer et al., 2017  ###reference_b36###), where only a subset of parameteters are active at any given time.\nIn particular, MoE is applied to the feedforward sublayer of Transformers (Fedus et al., 2022  ###reference_b13###; Roller et al., 2021  ###reference_b30###; Lewis et al., 2021  ###reference_b26###), allowing the total number of parameters to grow without additional computation.\nLLMs scaled in this way have shown impressive performance on downstream tasks (Jiang et al., 2024  ###reference_b21###; Xue et al., 2024  ###reference_b39###). Unlike Branch-Train-Merge, Mixture-of-Experts are often trained in a fully synchronized fashion, and the communication cost increases with the number of experts due to all-to-all communication.\nIn this paper, we aim for the best of both worlds, combining the advantages of Branch-Train-Merge and Mixture-of-Experts, while mitigating their disadvantages.\nWe achieve this by training multiple expert LLMs separately as in the Branch-Train-Merge method, but subsequently combine those experts into a single model using an MoE architecture.\nMore specifically, the feedforward sublayers from all the expert LLMs are brought together into a single MoE module at each layer, and a router network selects which feedforward expert to use at every token.\nWe merge other modules of the expert LLMs, including self-attention layers, by simply averaging their weights.\nThen the resulting model is MoE-finetuned\non all the combined data by continuing training, so that the router can learn to mix the expert feedforward (FF) modules.\nFigure 1  ###reference_### shows an overview of this method, which we call Branch-Train-MiX (BTX).\nThe main advantage of BTX compared to MoE is that expert training is embarrassingly parallel and asynchronous, reducing communication cost and increasing training throughput.\nCompared to Branch-Train-Merge, the final BTX model is a unified neural network that can be finetuned or used like any other standard LLM.\nThe final BTX model will not significantly increase inference FLOPs compared to the seed model since it is sparsely activated, despite having a much larger number of parameters.\nWe conduct our experiments using Llama-2 7B (Touvron et al., 2023  ###reference_b37###) as a seed model and train expert LLMs on different subsets of data corresponding to the domains of math, code and Wikipedia.\nWith the original Llama-2 7B weights added as a fourth expert, we finetune the combined MoE model for a relatively short period compared to the pretraining process.\nThe resulting BTX model brings significant improvements over the seed model on tasks across various domains, especially bridging the gap with specialized models on math and code related tasks, while retaining performance on the original capabilities where specialized models suffer from catastrophic forgetting. BTX outperforms BTM on all tasks demonstrating the benefits of learnt routing through MoE finetuning. Compared to purely MoE training such as sparse upcycling, BTX is more compute efficient with higher training throughput and more balanced performance across tasks in different domains.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Reducing communication between training workers for computational efficiency is a major topic of study for training deep learning systems.\nZhang et al. (2015  ###reference_b40###) introduced a method that allows model instances on different workers to diverge from each other, thus eliminating the constant need of synchronization. Instead, the workers are loosely synchronized to master weights using elastic averaging from time to time.\nA more recent work by Douillard et al. (2023  ###reference_b12###) showed that less frequent synchronization of diverged workers by averaging their weight changes and applying Nesterov momentum works well in practice for training LLMs.\nThe Branch-Train-Merge method (Li et al., 2022a  ###reference_b27###; Gururangan et al., 2023  ###reference_b16###) takes parallel training to the extreme by running multiple training processes completely independently.\nEach training process uses specific domain data, thus the corresponding model becomes an expert in that domain. Finally, the output distributions of those expert models are averaged to make a next token prediction.\nWhich experts to average is decided by classifying the input into one or more of the domains.\nWortsman et al. (2022  ###reference_b38###) showed simply averaging parameters of separately trained models improves performance, but the models only differed in their hyperparameters.\nMoE is used to scale deep networks in Shazeer et al. (2017  ###reference_b36###) using a simple Top-K routing scheme.\nSince the routing decisions are discrete and thus cannot be trained by gradient descent, various training methods have been explored for the Transformer architecture (Fedus et al., 2022  ###reference_b13###; Lewis et al., 2021  ###reference_b26###).\nSurprisingly Roller et al. (2021  ###reference_b30###) showed that even a fixed routing scheme without any learning works well, if the routing is done via a random mapping based on input tokens.\nIn larger scale experiments with recent LLMs, Jiang et al. (2024  ###reference_b21###) demonstrated that the MoE approach can match the performance of dense LLM counterparts using a much smaller number of active parameters.\nA study by Dai et al. (2024  ###reference_b11###) showed the advantage of more fine-grained experts, as well as having a shared expert that always stay active.\nMore similar to our work, Gururangan et al. (2021  ###reference_b15###) makes experts in feedforward layers specialize to specific domains using a domain-conditioned fixed routing, but it lacks the asynchronous training of our approach.\nOur method relates to continual learning (Awasthi and Sarawagi, 2019  ###reference_b4###) because domain experts are trained on datasets with different distributions from the initial data used for training the seed model, which is implemented by continued training after branching.\nSpecifically, our approach is related to parameter isolation methods (Lange et al., 2019  ###reference_b25###) as we have different parameters for different domains.\nAljundi et al. (2016  ###reference_b2###) also creates a new copy of a model to train on each domain.\nRusu et al. (2016  ###reference_b32###) adds a new model with a new domain, but connects it to the previous models so the previously learned features can be used.\nRozi\u00e8re et al. (2023  ###reference_b31###) showed continual training of a seed LLM on a specific domain of code can produce a strong domain expert model, and this converges much faster than starting from scratch.\nFor training a math expert, starting from a code expert rather than a general LLM was shown to be more beneficial (Shao et al., 2024  ###reference_b35###; Azerbayev et al., 2023  ###reference_b5###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Branch-Train-MiX",
            "text": "Given an existing LLM  which has been pretrained on a large corpora covering a wide variety of topics, we aim to improve its performance on  areas of expertise. This is achieved by continued pretraining with corresponding training datasets , each related to a specific knowledge domain such as math, code, etc.\nThe proposed method contains three stages: Branch, Train, and MiX.\nA common problem with MoE is the emergence of dead experts, which do not get activated by the router at all.\nCommon routing methods like Top-k are unlikely to escape from such a situation because a dead expert is never in the top-k selection, and therefore never receives a training signal.\nLoad balancing offers a simple solution by adding an extra loss term that encourages the experts to be utilized equally.\nWe use a loss term similar to (Fedus et al., 2022  ###reference_b13###):\nHere  is the current data batch, and  is a hyperparameter. This loss is computed in each layer and added to the NLL loss.\nBesides Top-k routing, we also experiment with other routing methods:\nSwitch: It is a Top-1 routing method proposed by Fedus et al. (2022  ###reference_b13###).\nSoft routing: We use softmax as the routing function , so all experts are activated both during training and inference. While it is likely to provide the best performance, it comes at the expense of increased compute.\nSample Top-1: We use the gumbel softmax (Jang et al., 2016  ###reference_b20###) for .\nAt training time, we generate a soft sample from the gumbel softmax, but zero out all its values except the largest one. Then we compute only one expert corresponding to this largest value, omitting the other expert computations. At inference time, we simply do hard sampling. We anneal the temperature to a sharp distribution at the end of training to gradually reduce the discrepancy between training and inference.\nThe number of modules in the MoE layer matches the number of domains we train on, since each module corresponds to one domain.\nHowever, we can increase the number of modules in a simple way by splitting each domain FF sublayer into multiple chunks.\nGiven  domains and an FF activation size of , we split each FF layer into  chunks with a dimension of .\nAs a result, the final MoE layer will have  modules.\nInstead of directly initializing MoE experts from domain experts in a one-to-one way, we also try including all domains in each MoE expert.\nThe motivation behind this is an observation that MoE experts trained in a standard way do not show domain specialization, but rather are activated uniformly across different domains (Jiang et al., 2024  ###reference_b21###).\nIn contrast, our domain experts are specialized to a specific domain through their training data.\nTo break this domain specialization, we split each domain expert\u2019s FF layers into  chunks and then merge the -th chunks from all domains to build the -th MoE expert.\nThis way, each MoE expert contains the same amount of parameters from all domains."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Branch & Train: Embarrassingly Parallel Expert Training",
            "text": "Initializing from the seed model , we train  expert LLMs , with each model  being trained on the corresponding dataset  in the same manner as during pretraining, using the usual language modeling objective.\nSince each expert model  can be trained in complete separation from the others, the whole training process becomes -way embarrassingly parallel. This training paradigm has several benefits in large-scale distributed training. It allows linear scaling of overall training throughput when scaling up the size of compute, while joint training often faces uncertain performance from increasing batch size. It has lower all-to-all communication cost. It is also more resilient, as a single training failure will only affect one of the  training processes instead of halting the entire training.\nAfter all the expert training is finished, we will end up with  different LLMs, with each specializing in a specific distribution.\nAt this point, the Branch-Train-Merge method (Li et al., 2022a  ###reference_b27###; Gururangan et al., 2023  ###reference_b16###) uses these domain experts as is, choosing which expert to use by determining which domain the input belongs to at inference time.\nUsually multiple experts are chosen, and their final output distributions are simply averaged to generate the next token.\nOur BTX approach, in contrast, merges these domain experts back into a single LLM that is finetuned further, as we will describe in the next section."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "MiX: Combining Separate Experts to be a Mixture-of-Experts",
            "text": "We employ a Mixture-of-Experts approach to combine the domain expert models .\nHowever, instead of using the classical procedure of mixing the final outputs from , we do a more fine-grained mixing by performing MoE within each layer of a Transformer.\nIn particular, we combine the different feedforward sublayers from the domain experts into a single MoE sublayer.\nIf  is the feedforward sublayer at the -th layer of the -th domain expert , then the combined MoE layer for input representation  at layer  will compute:\nHere  is a linear transformation and  is a routing function, which usually has sparse output and hence switches on only some experts.\nSince we can skip computing  if the corresponding router output is zero, the actual computation of  will be much more efficient than computing all domain experts.\nHowever, routing decisions can change from token to token, so one input sequence can employ all the domain expert FF layers if needed, even when only a few are accessed at any given token.\nIn our experiments, we use Top-k (k=2) routing where , unless otherwise stated.\nFor the self-attention sublayers, we combine the different domain experts by simply averaging their weights. The motivation behind this is the assumption that the self-attention layers are less domain specialized than the feedforward layers.\nWe do the same averaging for the remaining parameters (embeddings, etc.) as well.\nNote that the only new parameters we introduce are the router\u2019s transformation parameters , which are negligible in size compared to the rest of the network.\nNevertheless, those new parameters need to be finetuned, so the router can make optimal decisions in selecting which domain  to use.\nIn addition, funetuning is helpful because the self-attention weights are constructed by averaging, and are likely not optimal.\nOverall, the entire system has not been optimized for working together at all in the embarrassingly parallel training framework, but our hypothesis is that even a small amount of combined finetuning might make large improvements."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Variations",
            "text": "We also experimented with several variations of our method.\nA common problem with MoE is the emergence of dead experts, which do not get activated by the router at all.\nCommon routing methods like Top-k are unlikely to escape from such a situation because a dead expert is never in the top-k selection, and therefore never receives a training signal.\nLoad balancing offers a simple solution by adding an extra loss term that encourages the experts to be utilized equally.\nWe use a loss term similar to (Fedus et al., 2022  ###reference_b13###  ###reference_b13###):\nHere  is the current data batch, and  is a hyperparameter. This loss is computed in each layer and added to the NLL loss.\nBesides Top-k routing, we also experiment with other routing methods:\nSwitch: It is a Top-1 routing method proposed by Fedus et al. (2022  ###reference_b13###  ###reference_b13###).\nSoft routing: We use softmax as the routing function , so all experts are activated both during training and inference. While it is likely to provide the best performance, it comes at the expense of increased compute.\nSample Top-1: We use the gumbel softmax (Jang et al., 2016  ###reference_b20###  ###reference_b20###) for .\nAt training time, we generate a soft sample from the gumbel softmax, but zero out all its values except the largest one. Then we compute only one expert corresponding to this largest value, omitting the other expert computations. At inference time, we simply do hard sampling. We anneal the temperature to a sharp distribution at the end of training to gradually reduce the discrepancy between training and inference.\nThe number of modules in the MoE layer matches the number of domains we train on, since each module corresponds to one domain.\nHowever, we can increase the number of modules in a simple way by splitting each domain FF sublayer into multiple chunks.\nGiven  domains and an FF activation size of , we split each FF layer into  chunks with a dimension of .\nAs a result, the final MoE layer will have  modules.\nInstead of directly initializing MoE experts from domain experts in a one-to-one way, we also try including all domains in each MoE expert.\nThe motivation behind this is an observation that MoE experts trained in a standard way do not show domain specialization, but rather are activated uniformly across different domains (Jiang et al., 2024  ###reference_b21###  ###reference_b21###).\nIn contrast, our domain experts are specialized to a specific domain through their training data.\nTo break this domain specialization, we split each domain expert\u2019s FF layers into  chunks and then merge the -th chunks from all domains to build the -th MoE expert.\nThis way, each MoE expert contains the same amount of parameters from all domains."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We first analyze how\nexpert LLMs specialize to specific domains. Results are summarized in Table 1  ###reference_###.\nAs expected, individual expert LLMs achieve the best performance in their respective domain, where the math and code domains see especially large improvements.\nIn addition, there are several interesting observations.\nWe see that the math expert training improved its code performance as well, indicating a close relation of these domains.\nHowever, such single-domain continued training also suffers from catastrophic forgetting with significant performance drops on some tasks in other domains. For example, the math and code expert are much worse on TriviaQA than the seed model.\nTable 2  ###reference_### and footnote 1  ###reference_te1### (right) show aggregated performance across multiple domains. More detailed per-task results are reported in Table 8  ###reference_### in the Appendix.\nCompared to the seed model Llama-2 7B, BTX models (both Sample Top-1 and Top-2 corresponding to different number of active parameters) improve on all expert domains, such as math, coding and world knowledge without regressing on other tasks such as commonsense reasoning.\nBTX with Top-2 experts (our default) also approaches the best performance of the specialized models Llemma 7B and CodeLlama 7B in the math and coding domains, while drastically improving over those models on domains that are not their speciality such as world knowledge and commonsense reasoning.\nCompared to alternative data-matching (DM) methods for continued pretraining such as dense and sparse upcycling, BTX achieves better performance on average with small gaps in the math and coding domains. BTX outperforms BTM by a large margin on average,\nindicating that MoE finetuning to learn token-level routing is beneficial. Overall, the results demonstrate that BTX is a more compute efficient method for continued pretraining which is robust to task interference from multi-task learning.\nBTX also outperforms\nLlama-2 13B on all tasks except\nreasoning, even though Llama-2 13B\nuses significantly more training compute and has slightly more active parameters.\n###table_1### We further compare BTX with the sparse upcycling baseline in the compute-matching (CM) scenario. Both train on the same data mixture during the MoE stage, but differ in terms of the percent of compute spent on MoE training.\nWhile sparse cycling performs close behind BTX, the parallel training of experts increases the training throughput of BTX, as is shown in Table 3  ###reference_###. As a result, BTX can train with more than  the data than pure MoE given the same training compute budget, and achieves slightly higher average performance across all domains.\n###table_2### Despite that the MoE training stage uses a fraction of the total training budget in pretraining (for example, Llama-2 pretraining uses 2T tokens), BTX brings steep improvements on general capabilities compared to alternative continued pretraining approaches such as multi-task learning of the dense model and Branch-Train-Merge.\nAs a special case of BTX, sparse upcycling without expert training outperforms dense and BTM but not BTX, given the same or larger compute budget.\nThe compute efficiency gains of BTX are from the embarrassingly parallel training of experts before MoE finetuning.\nIn terms of the active number of parameters (shown as circle sizes in 1  ###reference_te1### (left)), the MoE models are similar to the Llama-2 13B model. BTX uses less than half of the additional training compute compared to Llama-2 13B, but demonstrates improved performance on expert domains (math, code, and knowledge) and achieves better overall performance. This indicates that BTX\u2019s training is more effective for the late stage of pretraining than using the same training protocol throughout the entire of pretraining."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "We base our experiments on the setup used for Llama-2 pretraining (Touvron et al., 2023  ###reference_b37###).\nIn particular, we use the Llama-2 7B model as our seed model."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 BTX Training",
            "text": "We use the pretrained Llama-2 (Touvron et al., 2023  ###reference_b37###) with 7B parameters as our seed model.\nAfter making three copies of the seed model Llama-2 7B, we continue training them on the following domain datasets to derive three domain experts:\nMath: The same data sources and mixture used in Llemma (Azerbayev et al., 2023  ###reference_b5###) model training. To be comparable to Llemma, we train on the same amount of data as well, i.e. 48k steps with 201B tokens in total.\nCode: The same data sources and mixture of code data used in CodeLlama pretraining (Rozi\u00e8re et al., 2023  ###reference_b31###).\nThe code expert LLM is trained for 50k steps with 210B tokens in total to be comparable with the math expert.\nWikipedia: Wikipedia documents extracted between June to August 2022. The data was preprocessed to remove hyperlinks, comments and other formatting boilerplate. Since this is a smaller dataset, we train a total of 42B tokens.\nWhile we can proceed with only these three domain experts, we also include the original seed LLM as a \u201cgeneralist\u201d expert so that its general knowledge is transferred to the final model.\nThus we mix these four expert models into a single MoE model as described in Section 3.2  ###reference_###.\nThen we finetune this MoE model on all the data sources used to train the four experts (including the original Llama-2 7B pretraining data for the generalist expert) and train for another 80B tokens.\nThe detailed sampling ratio across datasets in each domain as well as across the domains is described in Section 8  ###reference_###.\nFor BTX with default Top-2 routing, we use load balancing with , unless otherwise stated. For the Sample Top-1 routing, we use the temperature annealing schedule =max() from Jang et al. (2016  ###reference_b20###) with  where  is the number of training steps.\nFor the first layer only, we used soft-routing instead.\nSince the Sample Top-1 training is more efficient than Top-2, with the same compute budget it can train 160B tokens.\n###table_3###"
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Baselines",
            "text": "We compare to the following baselines:  \nLlama-2: We compare to the original Llama-2 7B that we use as a seed model, as well as Llama-2 13B.  \nDense: Instead of training separate LLMs on different domain datasets, the dense baseline continues to train the seed LLM with all the data.  \nSparse upcycling:  \nThis baseline (Komatsuzaki et al., 2022  ###reference_b23###) initializes a MoE model from the seed model by making 4 identical copies of the feedforward module as experts. We use the Top-2 router with randomly initialized parameters. In addition to training a data matching baseline with the same data as is used in BTX and the dense baseline, we also train a sparse upcycling baseline with the same amount of GPU-days, i.e. compute-matching (CM), using the MoE finetuning data mixture throughout training. This is equivalent to a special case of BTX which does not contain embarrassingly parallel expert training.  \nBranch-Train-Merge (BTM): This baseline (Li et al., 2022a  ###reference_b27###) uses the same expert LLMs as BTX (including the original seed model) but uses them directly without building a MoE model. For a given context (input), it selects Top-k expert LLMs based on the similarity between the context and experts\u2019 training data. Following the efficient inference method used in Gururangan et al. (2023  ###reference_b16###), both context and experts\u2019 training data are embedded via tf-idf. Top-k experts are selected based on cosine similarity to the mean tf-idf embedding of each expert.  \nCodeLlama 7B: A language model specializing in code (Rozi\u00e8re et al., 2023  ###reference_b31###) by continued training of the same seed model Llama-2 7B on code data. It also has other features such as long-context and infilling.  \nLlemma 7B: A language model specializing in mathematics (Azerbayev et al., 2023  ###reference_b5###) by continued training of CodeLlama 7B on math data.  \nWe use the same optimization hyperparameters for training of the baselines, expert models and MoE models. We use the AdamW optimizer with weight decay 0.1, and anneal the learning rate to the peak of with 100 steps of warmup, and decay to of the peak with a cosine schedule. We use a batch size of 4M tokens with a sequence length of 4096.  \n###table_4###"
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Evaluation",
            "text": "For evaluation, we use the zero- and few-shot performance on multiple benchmarks that test different skills:\nMath: we report the average performance on GSM8K (8 shot) (Cobbe et al., 2021  ###reference_b10###) and MATH (4 shot) (Hendrycks et al., 2021b  ###reference_b18###) for math reasoning.\nCode: we report the average performance of HumanEval (0 shot) (Chen et al., 2021  ###reference_b8###) and MBPP (3 shot) (Austin et al., 2021  ###reference_b3###) for code generation.\nWorld knowledge: we report the average performance of Natural Questions (5 shot)(Kwiatkowski et al., 2019  ###reference_b24###) and TriviaQA (5 shot) (Joshi et al., 2017  ###reference_b22###).\nReasoning: we report the average 0-shot performance of ARC-Easy and ARC-Challenge (Clark et al., 2018  ###reference_b9###), SIQA (Sap et al., 2019  ###reference_b34###), PIQA (Bisk et al., 2020  ###reference_b6###) and WinoGrande (Sakaguchi et al., 2021  ###reference_b33###).\nGeneral: we report performance on MMLU (5 shot) (Hendrycks et al., 2021a  ###reference_b17###) which covers multiple domains.\n###figure_2### ###figure_3###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Main Results",
            "text": "We first analyze how\nexpert LLMs specialize to specific domains. Results are summarized in Table 1  ###reference_###  ###reference_###.\nAs expected, individual expert LLMs achieve the best performance in their respective domain, where the math and code domains see especially large improvements.\nIn addition, there are several interesting observations.\nWe see that the math expert training improved its code performance as well, indicating a close relation of these domains.\nHowever, such single-domain continued training also suffers from catastrophic forgetting with significant performance drops on some tasks in other domains. For example, the math and code expert are much worse on TriviaQA than the seed model.\nTable 2  ###reference_###  ###reference_### and footnote 1  ###reference_te1###  ###reference_te1### (right) show aggregated performance across multiple domains. More detailed per-task results are reported in Table 8  ###reference_###  ###reference_### in the Appendix.\nCompared to the seed model Llama-2 7B, BTX models (both Sample Top-1 and Top-2 corresponding to different number of active parameters) improve on all expert domains, such as math, coding and world knowledge without regressing on other tasks such as commonsense reasoning.\nBTX with Top-2 experts (our default) also approaches the best performance of the specialized models Llemma 7B and CodeLlama 7B in the math and coding domains, while drastically improving over those models on domains that are not their speciality such as world knowledge and commonsense reasoning.\nCompared to alternative data-matching (DM) methods for continued pretraining such as dense and sparse upcycling, BTX achieves better performance on average with small gaps in the math and coding domains. BTX outperforms BTM by a large margin on average,\nindicating that MoE finetuning to learn token-level routing is beneficial. Overall, the results demonstrate that BTX is a more compute efficient method for continued pretraining which is robust to task interference from multi-task learning.\nBTX also outperforms\nLlama-2 13B on all tasks except\nreasoning, even though Llama-2 13B\nuses significantly more training compute and has slightly more active parameters.\n###table_5### We further compare BTX with the sparse upcycling baseline in the compute-matching (CM) scenario. Both train on the same data mixture during the MoE stage, but differ in terms of the percent of compute spent on MoE training.\nWhile sparse cycling performs close behind BTX, the parallel training of experts increases the training throughput of BTX, as is shown in Table 3  ###reference_###  ###reference_###. As a result, BTX can train with more than  the data than pure MoE given the same training compute budget, and achieves slightly higher average performance across all domains.\n###table_6### Despite that the MoE training stage uses a fraction of the total training budget in pretraining (for example, Llama-2 pretraining uses 2T tokens), BTX brings steep improvements on general capabilities compared to alternative continued pretraining approaches such as multi-task learning of the dense model and Branch-Train-Merge.\nAs a special case of BTX, sparse upcycling without expert training outperforms dense and BTM but not BTX, given the same or larger compute budget.\nThe compute efficiency gains of BTX are from the embarrassingly parallel training of experts before MoE finetuning.\nIn terms of the active number of parameters (shown as circle sizes in 1  ###reference_te1###  ###reference_te1### (left)), the MoE models are similar to the Llama-2 13B model. BTX uses less than half of the additional training compute compared to Llama-2 13B, but demonstrates improved performance on expert domains (math, code, and knowledge) and achieves better overall performance. This indicates that BTX\u2019s training is more effective for the late stage of pretraining than using the same training protocol throughout the entire of pretraining."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Overall Performance",
            "text": "We first analyze how\nexpert LLMs specialize to specific domains. Results are summarized in Table 1  ###reference_###  ###reference_###  ###reference_###.\nAs expected, individual expert LLMs achieve the best performance in their respective domain, where the math and code domains see especially large improvements.\nIn addition, there are several interesting observations.\nWe see that the math expert training improved its code performance as well, indicating a close relation of these domains.\nHowever, such single-domain continued training also suffers from catastrophic forgetting with significant performance drops on some tasks in other domains. For example, the math and code expert are much worse on TriviaQA than the seed model.\nTable 2  ###reference_###  ###reference_###  ###reference_### and footnote 1  ###reference_te1###  ###reference_te1###  ###reference_te1### (right) show aggregated performance across multiple domains. More detailed per-task results are reported in Table 8  ###reference_###  ###reference_###  ###reference_### in the Appendix.\nCompared to the seed model Llama-2 7B, BTX models (both Sample Top-1 and Top-2 corresponding to different number of active parameters) improve on all expert domains, such as math, coding and world knowledge without regressing on other tasks such as commonsense reasoning.\nBTX with Top-2 experts (our default) also approaches the best performance of the specialized models Llemma 7B and CodeLlama 7B in the math and coding domains, while drastically improving over those models on domains that are not their speciality such as world knowledge and commonsense reasoning.\nCompared to alternative data-matching (DM) methods for continued pretraining such as dense and sparse upcycling, BTX achieves better performance on average with small gaps in the math and coding domains. BTX outperforms BTM by a large margin on average,\nindicating that MoE finetuning to learn token-level routing is beneficial. Overall, the results demonstrate that BTX is a more compute efficient method for continued pretraining which is robust to task interference from multi-task learning.\nBTX also outperforms\nLlama-2 13B on all tasks except\nreasoning, even though Llama-2 13B\nuses significantly more training compute and has slightly more active parameters.\n###table_7### We further compare BTX with the sparse upcycling baseline in the compute-matching (CM) scenario. Both train on the same data mixture during the MoE stage, but differ in terms of the percent of compute spent on MoE training.\nWhile sparse cycling performs close behind BTX, the parallel training of experts increases the training throughput of BTX, as is shown in Table 3  ###reference_###  ###reference_###  ###reference_###. As a result, BTX can train with more than  the data than pure MoE given the same training compute budget, and achieves slightly higher average performance across all domains.\n###table_8###"
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Better compute-performance tradeoff",
            "text": "We compare BTX with baselines in terms of compute efficiency in footnote 1  ###reference_te1### (left). The X-axis shows the total training compute starting from the seed model measured in GPU days, which includes the domain expert training and finetuning of the MoE model. The Y-axis measures the overall performance reported in Table 2  ###reference_###.\nDespite that the MoE training stage uses a fraction of the total training budget in pretraining (for example, Llama-2 pretraining uses 2T tokens), BTX brings steep improvements on general capabilities compared to alternative continued pretraining approaches such as multi-task learning of the dense model and Branch-Train-Merge.\nAs a special case of BTX, sparse upcycling without expert training outperforms dense and BTM but not BTX, given the same or larger compute budget.\nThe compute efficiency gains of BTX are from the embarrassingly parallel training of experts before MoE finetuning.\nIn terms of the active number of parameters (shown as circle sizes in 1  ###reference_te1###  ###reference_te1###  ###reference_te1### (left)), the MoE models are similar to the Llama-2 13B model. BTX uses less than half of the additional training compute compared to Llama-2 13B, but demonstrates improved performance on expert domains (math, code, and knowledge) and achieves better overall performance. This indicates that BTX\u2019s training is more effective for the late stage of pretraining than using the same training protocol throughout the entire of pretraining."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Ablations & Analysis",
            "text": ""
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Ablations of BTX training",
            "text": "First, we compare the different routing methods with varying amount of active parameters for different amounts of finetuning. For fair comparison, load balancing is not used in any of them.\nResults are shown in Table 4  ###reference_###.\nFor Switch routing, we set its capacity factor to 1.5 (a hard limit after which routed tokens will be dropped). We found the Switch router to be subpar in average performance.\nThe soft routing performs the best, but that is expected since it lacks sparsity and has the highest number of active parameters.\nOverall, the Top-2 routing gives us a good balance between performance and efficiency.\nWe also ablate additional design choices of BTX, with results summarized in Table 5  ###reference_###.\nWe found that MoE training without load balancing performs worse on the coding task (HumanEval), but has higher math (GSM8k) accuracy.\nThe routing analysis in the next section will give more insight into this trade-off.\nNext, freezing the feedforward modules initialized from each expert, and only training the rest of the MoE model has little impact on performance across all tasks.\nThis suggests that individual experts already gained sufficient domain knowledge during the branch-train stage, while the mix (MoE finetuning) stage mainly trains the other parameters such as averaged weights in the self-attention and the router transformations .\nWe also test our blending and splitting techniques described in Section 3.3  ###reference_###.\nThe performance across all tasks dropped when experts are mixed, suggesting that domain FF layers cannot be mixed in this way.\nSplitting each domain FF into  chunks to obtain 8 modules in the MoE layer also does not improve performance, even if Top-4 routing is used to match the active number of parameters.\n###figure_4### ###figure_5###"
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Routing Analysis",
            "text": "To gain an in-depth understanding of the performance of BTX, we run model evaluations on downstream tasks and examine the routing decisions among the experts. The results are summarized in Figure 3  ###reference_###, and we also report detailed ablation results for different BTX setups in Section 10  ###reference_###. Compared to other routing methods, Top-2 routing with load balancing ensures a more uniform distribution of the load between experts. Analyzing the token probability distributions, we observe a shift towards low probability scores across all experts with load balancing, especially closer to the final layers of the model, which contributes to the fair routing. Interestingly, all models without load balance heavily rely on the Math expert, with a low overall contribution from other experts, especially the Code expert. A dead Code expert comes \u201cback to life\u201d with load balancing introduced in training. In fact, it not only becomes visible, but becomes the dominant expert in the math and code domains.\nExamples of the routing decisions for Top-2 with load balancing can be found in the Table 6  ###reference_###. Overall across math domain tasks, tokens are often routed to the Code and Llama-2 7B experts. If we look at a more detailed token distribution (Section 10  ###reference_###, Figure 6  ###reference_###), we find that the GSM8K task prefers Code and Llama-2 experts, while the MATH task relies more on the in-domain Math expert. We hypothesise that this happens because the GSM8K dataset consists of grade school math problems that require common sense knowledge and basic arithmetic operations.\nBoth the Code and World knowledge tasks mostly route to the in-domain Code and Wikipedia experts respectively.\nAs observed earlier in Section 4.3.1  ###reference_.SSS1###, when load balancing is introduced, there are improvements in coding tasks but degradation in math tasks, which can be explained with these changes in domain expert routing. The reasoning tasks in contrast exhibit similar behaviour, and rely equally on Math and generalist LLM\u2019s expertise."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduced Branch-Train-MiX (BTX), a simple continued pretraining method to improve an LLM\u2019s capabilities. It trains multiple copies of a seed LLM to specialize in multiple domains in an asynchronous and parallel fashion and later merges them back into a single Mixture-of-Experts (MoE) model via finetuning.\nWhile the initial parallel training stage brings higher training throughput and scalability, the second MoE finetuning stage makes the final LLM more performant.\nOur experiments suggest that a generalist LLM\u2019s performance can be boosted by continued training on datasets with specialized knowledge and skills using our method. We find that the BTX approach is more compute efficient than training a larger generalist LLM or several separately specialized LLMs. These insights can inform how to allocate compute in late pretraining to achieve a strong generalist model."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations & Future Work",
            "text": "Although our experimental results on BTX are promising, we have not fully explored its potential in this paper.\nDue to compute limitations, we only experimented with three domains and four experts in this paper.\nTraining on more domains such as using unsupervised domain discovery (Gururangan et al., 2023  ###reference_b16###) should amplify the benefit of the parallelization of experts training.\nHaving more experts will also make the final MoE model more efficient because the number of active experts can remain the same while its overall capacity increases.\nIn our experiments, we used a simple implementation of MoE and did not optimize it using more complex techniques such as placing different experts on different GPUs to run them in parallel.\nSuch an efficient MoE implementation could shorten the training time of BTX, and the sparse upcycling baseline as well.\nCompared to BTM, BTX provides an approach to finetune the combined experts, which can be directly applied in instruction finetuning or RLHF procedures.\nHowever, we leave that for future work as we focused on the pretraining stage in this paper.\nThe question of whether experts in MoE are better off specializing in specific domains or not is an interesting one that is worth further investigation.\nOur approach explicitly tied experts to certain domains, but such specialization does not seem to emerge naturally during MoE training (Jiang et al., 2024  ###reference_b21###).\nWe observed that some experts are used more in their corresponding domain tasks, showing that their domain specialization partially remains even after the MoE finetuning.\nWe only compared BTX to two of its special variants, i.e. BTM with 100% compute allocated to expert training and 0% on MoE finetuning, and sparse upcycling with 0% compute allocated to expert training and 100% on MoE finetuning. Future work could perform a thorough sweep of the compute allocation ratio between expert training and MoE training. Also, we did not perform experiments with different data mixtures for MoE finetuning other than uniform sampling."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Acknowledgements",
            "text": "We thank Margaret Li, Kushal Tirumala, Luke Zettlemoyer, Artidoro Pagnoni, Suchin Gururangan, Mike Lewis and Emily Dinan for their discussion and feedback, and Andrew Cohen and Arun Babu for their help with the training implementation."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Data mixture",
            "text": "Table 7  ###reference_### shows the exact data mixture ratios used in training each domain expert.\nFor finetuning the MoE model, we sample datasets that used to train math expert, code expert, wikipedia expert and the original Llama-2 7B with probabilities 30.16%, 40.31%, 10.30% and 19.23%.\n###table_9###"
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Evaluation",
            "text": "We use the same evaluation metrics as is used in Touvron et al. (2023  ###reference_b37###) and Rozi\u00e8re et al. (2023  ###reference_b31###): for code tasks (HumanEval and MBPP) we report pass@1, for math tasks (GSM8k and MATH) and knowledge tasks (Natural Questions and TriviaQA) we report exact match, we report accuracy for MMLU and ARC. We use greedy decoding for all generations. Detailed results on all tasks are reported in Table 8  ###reference_###.\n###table_10###"
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T1.2\">\n<tr class=\"ltx_tr\" id=\"S4.T1.2.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T1.2.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T1.2.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.1.2.1\" style=\"font-size:90%;\">Math</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T1.2.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.1.3.1\" style=\"font-size:90%;\">Code</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S4.T1.2.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.1.4.1\" style=\"font-size:90%;\">General knowledge</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2\">\n<td class=\"ltx_td\" id=\"S4.T1.2.2.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.2.1\" style=\"font-size:90%;\">GSM8K</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.3.1\" style=\"font-size:90%;\">MATH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.4.1\" style=\"font-size:90%;\">Human</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.5.1\" style=\"font-size:90%;\">MBPP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.6.1\" style=\"font-size:90%;\">Natural</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.7.1\" style=\"font-size:90%;\">Trivia</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.2.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.8.1\" style=\"font-size:90%;\">MMLU</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.3\">\n<td class=\"ltx_td\" id=\"S4.T1.2.3.1\"></td>\n<td class=\"ltx_td\" id=\"S4.T1.2.3.2\"></td>\n<td class=\"ltx_td\" id=\"S4.T1.2.3.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.2.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.3.4.1\" style=\"font-size:90%;\">Eval</span></td>\n<td class=\"ltx_td\" id=\"S4.T1.2.3.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.2.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.3.6.1\" style=\"font-size:90%;\">Questions</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.2.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.3.7.1\" style=\"font-size:90%;\">QA</span></td>\n<td class=\"ltx_td\" id=\"S4.T1.2.3.8\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.2.4.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T1.2.4.1.1\" style=\"font-size:90%;\">Llama-2 7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.4.2\"><span class=\"ltx_text\" id=\"S4.T1.2.4.2.1\" style=\"font-size:90%;\">14.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.4.3\"><span class=\"ltx_text\" id=\"S4.T1.2.4.3.1\" style=\"font-size:90%;\">2.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.4.4\"><span class=\"ltx_text\" id=\"S4.T1.2.4.4.1\" style=\"font-size:90%;\">12.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.4.5\"><span class=\"ltx_text\" id=\"S4.T1.2.4.5.1\" style=\"font-size:90%;\">20.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.4.6\"><span class=\"ltx_text\" id=\"S4.T1.2.4.6.1\" style=\"font-size:90%;\">16.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.4.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.4.7.1\" style=\"font-size:90%;\">58.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.4.8\"><span class=\"ltx_text\" id=\"S4.T1.2.4.8.1\" style=\"font-size:90%;\">46.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.2.5.1\"><span class=\"ltx_text\" id=\"S4.T1.2.5.1.1\" style=\"font-size:90%;\">Math expert</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.2.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.5.2.1\" style=\"font-size:90%;\">39.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.2.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.5.3.1\" style=\"font-size:90%;\">18.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.2.5.4\"><span class=\"ltx_text\" id=\"S4.T1.2.5.4.1\" style=\"font-size:90%;\">25.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.2.5.5\"><span class=\"ltx_text\" id=\"S4.T1.2.5.5.1\" style=\"font-size:90%;\">33.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.2.5.6\"><span class=\"ltx_text\" id=\"S4.T1.2.5.6.1\" style=\"font-size:90%;\">14.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.2.5.7\"><span class=\"ltx_text\" id=\"S4.T1.2.5.7.1\" style=\"font-size:90%;\">37.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.2.5.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.5.8.1\" style=\"font-size:90%;\">52.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.2.6.1\"><span class=\"ltx_text\" id=\"S4.T1.2.6.1.1\" style=\"font-size:90%;\">Code expert</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.2.6.2\"><span class=\"ltx_text\" id=\"S4.T1.2.6.2.1\" style=\"font-size:90%;\">12.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.2.6.3\"><span class=\"ltx_text\" id=\"S4.T1.2.6.3.1\" style=\"font-size:90%;\">4.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.2.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.6.4.1\" style=\"font-size:90%;\">31.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.2.6.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.6.5.1\" style=\"font-size:90%;\">40.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.2.6.6\"><span class=\"ltx_text\" id=\"S4.T1.2.6.6.1\" style=\"font-size:90%;\">11.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.2.6.7\"><span class=\"ltx_text\" id=\"S4.T1.2.6.7.1\" style=\"font-size:90%;\">29.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.2.6.8\"><span class=\"ltx_text\" id=\"S4.T1.2.6.8.1\" style=\"font-size:90%;\">39.6</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T1.2.7.1\"><span class=\"ltx_text\" id=\"S4.T1.2.7.1.1\" style=\"font-size:90%;\">Wikipedia expert</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.2.7.2\"><span class=\"ltx_text\" id=\"S4.T1.2.7.2.1\" style=\"font-size:90%;\">11.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.2.7.3\"><span class=\"ltx_text\" id=\"S4.T1.2.7.3.1\" style=\"font-size:90%;\">3.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.2.7.4\"><span class=\"ltx_text\" id=\"S4.T1.2.7.4.1\" style=\"font-size:90%;\">11.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.2.7.5\"><span class=\"ltx_text\" id=\"S4.T1.2.7.5.1\" style=\"font-size:90%;\">15.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.2.7.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.7.6.1\" style=\"font-size:90%;\">21.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.2.7.7\"><span class=\"ltx_text\" id=\"S4.T1.2.7.7.1\" style=\"font-size:90%;\">57.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.2.7.8\"><span class=\"ltx_text\" id=\"S4.T1.2.7.8.1\" style=\"font-size:90%;\">43.1</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Individual domain expert LLM performance on representative tasks, compared to the seed model <span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T1.11.1\">Llama-2 7B</span>. As expected, the code and math experts excel at their corresponding domain tasks.\nThe Wikipedia expert performs better on Natural Questions, but the math expert has the best score on MMLU.\nThis could be because MMLU contains many math subjects and math training is shown to help on this task <cite class=\"ltx_cite ltx_citemacro_citep\">(Shao et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07816v1#bib.bib35\" title=\"\">2024</a>)</cite>.</figcaption>\n</figure>",
            "capture": "Table 1: Individual domain expert LLM performance on representative tasks, compared to the seed model Llama-2 7B. As expected, the code and math experts excel at their corresponding domain tasks.\nThe Wikipedia expert performs better on Natural Questions, but the math expert has the best score on MMLU.\nThis could be because MMLU contains many math subjects and math training is shown to help on this task (Shao et\u00a0al., 2024)."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T2.2\">\n<tr class=\"ltx_tr\" id=\"S4.T2.2.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T2.2.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.2.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.1.2.1\" style=\"font-size:90%;\">Math</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.2.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.1.3.1\" style=\"font-size:90%;\">Code</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.2.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.1.4.1\" style=\"font-size:90%;\">Knowledge</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.2.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.1.5.1\" style=\"font-size:90%;\">Reasoning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.2.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.1.6.1\" style=\"font-size:90%;\">MMLU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.2.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.1.7.1\" style=\"font-size:90%;\">Average</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T2.2.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S4.T2.2.2.1.1\" style=\"font-size:80%;\">Specialized LLMs</span></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T2.2.2.2\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T2.2.2.3\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T2.2.2.4\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T2.2.2.5\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T2.2.2.6\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T2.2.2.7\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.3.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T2.2.3.1.1\" style=\"font-size:90%;\">CodeLlama 7B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.3.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T2.2.3.2.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T2.2.3.2.2\" style=\"font-size:90%;\">8.1</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.3.3\"><span class=\"ltx_text\" id=\"S4.T2.2.3.3.1\" style=\"font-size:90%;\">36.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.3.4\"><span class=\"ltx_text\" id=\"S4.T2.2.3.4.1\" style=\"font-size:90%;\">22.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.3.5\"><span class=\"ltx_text\" id=\"S4.T2.2.3.5.1\" style=\"font-size:90%;\">56.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.3.6\"><span class=\"ltx_text\" id=\"S4.T2.2.3.6.1\" style=\"font-size:90%;\">38.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.3.7\"><span class=\"ltx_text\" id=\"S4.T2.2.3.7.1\" style=\"font-size:90%;\">37.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.4.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T2.2.4.1.1\" style=\"font-size:90%;\">Llemma 7B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.4.2\"><span class=\"ltx_text\" id=\"S4.T2.2.4.2.1\" style=\"font-size:90%;\">28.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.4.3\"><span class=\"ltx_text\" id=\"S4.T2.2.4.3.1\" style=\"font-size:90%;\">33.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.4.4\"><span class=\"ltx_text\" id=\"S4.T2.2.4.4.1\" style=\"font-size:90%;\">17.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.4.5\"><span class=\"ltx_text\" id=\"S4.T2.2.4.5.1\" style=\"font-size:90%;\">38.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.4.6\"><span class=\"ltx_text\" id=\"S4.T2.2.4.6.1\" style=\"font-size:90%;\">33.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.4.7\"><span class=\"ltx_text\" id=\"S4.T2.2.4.7.1\" style=\"font-size:90%;\">32.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.2.5.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S4.T2.2.5.1.1\" style=\"font-size:80%;\">Generalist LLMs</span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.2.5.2\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.2.5.3\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.2.5.4\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.2.5.5\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.2.5.6\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.2.5.7\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.6.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T2.2.6.1.1\" style=\"font-size:90%;\">Llama-2 7B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.6.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T2.2.6.2.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T2.2.6.2.2\" style=\"font-size:90%;\">8.6</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.6.3\"><span class=\"ltx_text\" id=\"S4.T2.2.6.3.1\" style=\"font-size:90%;\">16.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.6.4\"><span class=\"ltx_text\" id=\"S4.T2.2.6.4.1\" style=\"font-size:90%;\">37.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.6.5\"><span class=\"ltx_text\" id=\"S4.T2.2.6.5.1\" style=\"font-size:90%;\">63.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.6.6\"><span class=\"ltx_text\" id=\"S4.T2.2.6.6.1\" style=\"font-size:90%;\">46.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.6.7\"><span class=\"ltx_text\" id=\"S4.T2.2.6.7.1\" style=\"font-size:90%;\">40.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.7.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T2.2.7.1.1\" style=\"font-size:90%;\">Llama-2 13B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.7.2\"><span class=\"ltx_text\" id=\"S4.T2.2.7.2.1\" style=\"font-size:90%;\">16.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.7.3\"><span class=\"ltx_text\" id=\"S4.T2.2.7.3.1\" style=\"font-size:90%;\">24.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.7.4\"><span class=\"ltx_text\" id=\"S4.T2.2.7.4.1\" style=\"font-size:90%;\">40.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.7.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.7.5.1\" style=\"font-size:90%;\">66.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.7.6\"><span class=\"ltx_text\" id=\"S4.T2.2.7.6.1\" style=\"font-size:90%;\">52.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.7.7\"><span class=\"ltx_text\" id=\"S4.T2.2.7.7.1\" style=\"font-size:90%;\">45.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.8.1\"><span class=\"ltx_text\" id=\"S4.T2.2.8.1.1\" style=\"font-size:90%;\">Dense (DM)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.8.2\"><span class=\"ltx_text\" id=\"S4.T2.2.8.2.1\" style=\"font-size:90%;\">18.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.8.3\"><span class=\"ltx_text\" id=\"S4.T2.2.8.3.1\" style=\"font-size:90%;\">25.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.8.4\"><span class=\"ltx_text\" id=\"S4.T2.2.8.4.1\" style=\"font-size:90%;\">39.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.8.5\"><span class=\"ltx_text\" id=\"S4.T2.2.8.5.1\" style=\"font-size:90%;\">63.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.8.6\"><span class=\"ltx_text\" id=\"S4.T2.2.8.6.1\" style=\"font-size:90%;\">49.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.8.7\"><span class=\"ltx_text\" id=\"S4.T2.2.8.7.1\" style=\"font-size:90%;\">44.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.9.1\"><span class=\"ltx_text\" id=\"S4.T2.2.9.1.1\" style=\"font-size:90%;\">Sparse upcycling (DM), Top-2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.9.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.9.2.1\" style=\"font-size:90%;\">28.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.9.3\"><span class=\"ltx_text\" id=\"S4.T2.2.9.3.1\" style=\"font-size:90%;\">34.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.9.4\"><span class=\"ltx_text\" id=\"S4.T2.2.9.4.1\" style=\"font-size:90%;\">34.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.9.5\"><span class=\"ltx_text\" id=\"S4.T2.2.9.5.1\" style=\"font-size:90%;\">62.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.9.6\"><span class=\"ltx_text\" id=\"S4.T2.2.9.6.1\" style=\"font-size:90%;\">51.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.9.7\"><span class=\"ltx_text\" id=\"S4.T2.2.9.7.1\" style=\"font-size:90%;\">46.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.10.1\"><span class=\"ltx_text\" id=\"S4.T2.2.10.1.1\" style=\"font-size:90%;\">BTM, Top-1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.10.2\"><span class=\"ltx_text\" id=\"S4.T2.2.10.2.1\" style=\"font-size:90%;\">21.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.10.3\"><span class=\"ltx_text\" id=\"S4.T2.2.10.3.1\" style=\"font-size:90%;\">36.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.10.4\"><span class=\"ltx_text\" id=\"S4.T2.2.10.4.1\" style=\"font-size:90%;\">26.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.10.5\"><span class=\"ltx_text\" id=\"S4.T2.2.10.5.1\" style=\"font-size:90%;\">61.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.10.6\"><span class=\"ltx_text\" id=\"S4.T2.2.10.6.1\" style=\"font-size:90%;\">44.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.10.7\"><span class=\"ltx_text\" id=\"S4.T2.2.10.7.1\" style=\"font-size:90%;\">43.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.11.1\"><span class=\"ltx_text\" id=\"S4.T2.2.11.1.1\" style=\"font-size:90%;\">\nBTM, Top-2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.11.2\"><span class=\"ltx_text\" id=\"S4.T2.2.11.2.1\" style=\"font-size:90%;\">21.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.11.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.11.3.1\" style=\"font-size:90%;\">36.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.11.4\"><span class=\"ltx_text\" id=\"S4.T2.2.11.4.1\" style=\"font-size:90%;\">26.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.11.5\"><span class=\"ltx_text\" id=\"S4.T2.2.11.5.1\" style=\"font-size:90%;\">61.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.11.6\"><span class=\"ltx_text\" id=\"S4.T2.2.11.6.1\" style=\"font-size:90%;\">44.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.11.7\"><span class=\"ltx_text\" id=\"S4.T2.2.11.7.1\" style=\"font-size:90%;\">43.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.12\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.12.1\"><span class=\"ltx_text\" id=\"S4.T2.2.12.1.1\" style=\"font-size:90%;\">BTX, Sample Top-1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.12.2\"><span class=\"ltx_text\" id=\"S4.T2.2.12.2.1\" style=\"font-size:90%;\">26.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.12.3\"><span class=\"ltx_text\" id=\"S4.T2.2.12.3.1\" style=\"font-size:90%;\">31.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.12.4\"><span class=\"ltx_text\" id=\"S4.T2.2.12.4.1\" style=\"font-size:90%;\">40.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.12.5\"><span class=\"ltx_text\" id=\"S4.T2.2.12.5.1\" style=\"font-size:90%;\">63.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.12.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.12.6.1\" style=\"font-size:90%;\">53.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.12.7\"><span class=\"ltx_text\" id=\"S4.T2.2.12.7.1\" style=\"font-size:90%;\">47.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T2.2.13.1\"><span class=\"ltx_text\" id=\"S4.T2.2.13.1.1\" style=\"font-size:90%;\">BTX, Top-2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.2.13.2\"><span class=\"ltx_text\" id=\"S4.T2.2.13.2.1\" style=\"font-size:90%;\">27.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.2.13.3\"><span class=\"ltx_text\" id=\"S4.T2.2.13.3.1\" style=\"font-size:90%;\">34.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.2.13.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.13.4.1\" style=\"font-size:90%;\">41.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.2.13.5\"><span class=\"ltx_text\" id=\"S4.T2.2.13.5.1\" style=\"font-size:90%;\">63.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.2.13.6\"><span class=\"ltx_text\" id=\"S4.T2.2.13.6.1\" style=\"font-size:90%;\">52.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.2.13.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.13.7.1\" style=\"font-size:90%;\">47.9</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Aggregated performance of BTX compared against various baselines, including both generalist and specialized pretrained models, tested on various capabilities aggregated across popular benchmarks. Dense, sparse upcycling, BTM and BTX are trained on exactly the same amount and mixture of data with the exception that BTM does not have the finetuning stage.\n</figcaption>\n</figure>",
            "capture": "Table 2: Aggregated performance of BTX compared against various baselines, including both generalist and specialized pretrained models, tested on various capabilities aggregated across popular benchmarks. Dense, sparse upcycling, BTM and BTX are trained on exactly the same amount and mixture of data with the exception that BTM does not have the finetuning stage.\n"
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T3.2\">\n<tr class=\"ltx_tr\" id=\"S4.T3.2.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T3.2.1.1\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.2.1.2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.1.2.1\" style=\"font-size:80%;\">MoE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.2.1.3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.1.3.1\" style=\"font-size:80%;\">Training</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.2.1.4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.1.4.1\" style=\"font-size:80%;\">Total compute</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T3.2.1.5\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.1.5.1\" style=\"font-size:80%;\">#tokens</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.2.1.6\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.1.6.1\" style=\"font-size:80%;\">Math</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.2.1.7\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.1.7.1\" style=\"font-size:80%;\">Code</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.2.1.8\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.1.8.1\" style=\"font-size:80%;\">Knowledge</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.2.1.9\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.1.9.1\" style=\"font-size:80%;\">Reasoning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.2.1.10\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.1.10.1\" style=\"font-size:80%;\">MMLU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.2.1.11\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.1.11.1\" style=\"font-size:80%;\">Average</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.2.2\">\n<td class=\"ltx_td\" id=\"S4.T3.2.2.1\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.2.2.2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.2.2.1\" style=\"font-size:80%;\">compute</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.2.2.3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.2.3.1\" style=\"font-size:80%;\">time (days)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.2.2.4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.2.4.1\" style=\"font-size:80%;\">(GPU-days)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.2.2.5\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.2.5.1\" style=\"font-size:80%;\">(B)</span></td>\n<td class=\"ltx_td\" id=\"S4.T3.2.2.6\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.2.2.7\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.2.2.8\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.2.2.9\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.2.2.10\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.2.2.11\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.2.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.2.3.1\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.3.1.1\" style=\"font-size:80%;\">BTX</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.2.3.2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T3.2.3.2.1\" style=\"font-size:80%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T3.2.3.2.2\" style=\"font-size:80%;\">23%</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.2.3.3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.3.3.1\" style=\"font-size:80%;\">7.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.2.3.4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T3.2.3.4.1\" style=\"font-size:80%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T3.2.3.4.2\" style=\"font-size:80%;\">926.1</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.2.3.5\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.3.5.1\" style=\"font-size:80%;\">533</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.2.3.6\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.3.6.1\" style=\"font-size:80%;\">27.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.2.3.7\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.3.7.1\" style=\"font-size:80%;\">34.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.2.3.8\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.3.8.1\" style=\"font-size:80%;\">41.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.2.3.9\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.3.9.1\" style=\"font-size:80%;\">63.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.2.3.10\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.3.10.1\" style=\"font-size:80%;\">52.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.2.3.11\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.3.11.1\" style=\"font-size:80%;\">47.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.2.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T3.2.4.1\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.4.1.1\" style=\"font-size:80%;\">Sparse upcycling (CM)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.2.4.2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.4.2.1\" style=\"font-size:80%;\">100%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.2.4.3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.4.3.1\" style=\"font-size:80%;\">7.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.2.4.4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.4.4.1\" style=\"font-size:80%;\">1007.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.2.4.5\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.4.5.1\" style=\"font-size:80%;\">252</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.2.4.6\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.4.6.1\" style=\"font-size:80%;\">28.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.2.4.7\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.4.7.1\" style=\"font-size:80%;\">30.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.2.4.8\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.4.8.1\" style=\"font-size:80%;\">41.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.2.4.9\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.4.9.1\" style=\"font-size:80%;\">62.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.2.4.10\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.4.10.1\" style=\"font-size:80%;\">52.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.2.4.11\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.2.4.11.1\" style=\"font-size:80%;\">47.3</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T3.5.1.1\" style=\"font-size:113%;\">Table 3</span>: </span><span class=\"ltx_text\" id=\"S4.T3.6.2\" style=\"font-size:113%;\">Comparison between BTX and Sparse upcycling with compute-matching (CM), which is a special case of BTX without the expert training stage as is shown by the first column that 100% of compute is spent on MoE training. We also report total training time, compute and number of training tokens. Comparing both performance on individual domains as well as the average, we can see that BTX has more balanced performance, in addition to higher throughput.</span></figcaption>\n</figure>",
            "capture": "Table 3: Comparison between BTX and Sparse upcycling with compute-matching (CM), which is a special case of BTX without the expert training stage as is shown by the first column that 100% of compute is spent on MoE training. We also report total training time, compute and number of training tokens. Comparing both performance on individual domains as well as the average, we can see that BTX has more balanced performance, in addition to higher throughput."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T4.2\">\n<tr class=\"ltx_tr\" id=\"S4.T4.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T4.2.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.1.1.1\" style=\"font-size:90%;\">Routing method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T4.2.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.1.2.1\" style=\"font-size:90%;\">Active parameters (B)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.2.1.3\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" id=\"S4.T4.2.1.3.1\" style=\"font-size:90%;\">MoE Finetune tokens (B)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.2.1.4\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" id=\"S4.T4.2.1.4.1\" style=\"font-size:90%;\">Average score</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.1.1\" style=\"font-size:90%;\">Training</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.2.1\" style=\"font-size:90%;\">Inference</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T4.2.3.1\"><span class=\"ltx_text\" id=\"S4.T4.2.3.1.1\" style=\"font-size:90%;\">Switch Top-1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.2.3.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T4.2.3.2.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T4.2.3.2.2\" style=\"font-size:90%;\">6.7</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.2.3.3\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T4.2.3.3.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T4.2.3.3.2\" style=\"font-size:90%;\">6.7</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.2.3.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T4.2.3.4.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T4.2.3.4.2\" style=\"font-size:90%;\">10</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.2.3.5\"><span class=\"ltx_text\" id=\"S4.T4.2.3.5.1\" style=\"font-size:90%;\">24.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.2.4.1\"><span class=\"ltx_text\" id=\"S4.T4.2.4.1.1\" style=\"font-size:90%;\">Sample Top-1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.4.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T4.2.4.2.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T4.2.4.2.2\" style=\"font-size:90%;\">6.7</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.4.3\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T4.2.4.3.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T4.2.4.3.2\" style=\"font-size:90%;\">6.7</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.4.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T4.2.4.4.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T4.2.4.4.2\" style=\"font-size:90%;\">10</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.4.5\"><span class=\"ltx_text\" id=\"S4.T4.2.4.5.1\" style=\"font-size:90%;\">33.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.2.5.1\"><span class=\"ltx_text\" id=\"S4.T4.2.5.1.1\" style=\"font-size:90%;\">Top-2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.5.2\"><span class=\"ltx_text\" id=\"S4.T4.2.5.2.1\" style=\"font-size:90%;\">11.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.5.3\"><span class=\"ltx_text\" id=\"S4.T4.2.5.3.1\" style=\"font-size:90%;\">11.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.5.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T4.2.5.4.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T4.2.5.4.2\" style=\"font-size:90%;\">10</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.5.5\"><span class=\"ltx_text\" id=\"S4.T4.2.5.5.1\" style=\"font-size:90%;\">34.6</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.2.6.1\"><span class=\"ltx_text\" id=\"S4.T4.2.6.1.1\" style=\"font-size:90%;\">Soft routing</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.6.2\"><span class=\"ltx_text\" id=\"S4.T4.2.6.2.1\" style=\"font-size:90%;\">19.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.6.3\"><span class=\"ltx_text\" id=\"S4.T4.2.6.3.1\" style=\"font-size:90%;\">19.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.6.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T4.2.6.4.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T4.2.6.4.2\" style=\"font-size:90%;\">10</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.6.5\"><span class=\"ltx_text\" id=\"S4.T4.2.6.5.1\" style=\"font-size:90%;\">35.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.2.7.1\"><span class=\"ltx_text\" id=\"S4.T4.2.7.1.1\" style=\"font-size:90%;\">Sample Top-1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.7.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T4.2.7.2.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T4.2.7.2.2\" style=\"font-size:90%;\">6.7</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.7.3\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T4.2.7.3.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T4.2.7.3.2\" style=\"font-size:90%;\">6.7</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.7.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T4.2.7.4.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T4.2.7.4.2\" style=\"font-size:90%;\">40</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.7.5\"><span class=\"ltx_text\" id=\"S4.T4.2.7.5.1\" style=\"font-size:90%;\">35.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.2.8.1\"><span class=\"ltx_text\" id=\"S4.T4.2.8.1.1\" style=\"font-size:90%;\">Top-2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.8.2\"><span class=\"ltx_text\" id=\"S4.T4.2.8.2.1\" style=\"font-size:90%;\">11.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.8.3\"><span class=\"ltx_text\" id=\"S4.T4.2.8.3.1\" style=\"font-size:90%;\">11.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.8.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T4.2.8.4.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T4.2.8.4.2\" style=\"font-size:90%;\">40</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.8.5\"><span class=\"ltx_text\" id=\"S4.T4.2.8.5.1\" style=\"font-size:90%;\">35.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.2.9.1\"><span class=\"ltx_text\" id=\"S4.T4.2.9.1.1\" style=\"font-size:90%;\">Soft routing</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.9.2\"><span class=\"ltx_text\" id=\"S4.T4.2.9.2.1\" style=\"font-size:90%;\">19.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.9.3\"><span class=\"ltx_text\" id=\"S4.T4.2.9.3.1\" style=\"font-size:90%;\">19.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.9.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T4.2.9.4.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T4.2.9.4.2\" style=\"font-size:90%;\">40</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.9.5\"><span class=\"ltx_text\" id=\"S4.T4.2.9.5.1\" style=\"font-size:90%;\">37.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.2.10.1\"><span class=\"ltx_text\" id=\"S4.T4.2.10.1.1\" style=\"font-size:90%;\">Sample Top-1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.10.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T4.2.10.2.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T4.2.10.2.2\" style=\"font-size:90%;\">6.7</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.10.3\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T4.2.10.3.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T4.2.10.3.2\" style=\"font-size:90%;\">6.7</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.10.4\"><span class=\"ltx_text\" id=\"S4.T4.2.10.4.1\" style=\"font-size:90%;\">160</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.10.5\"><span class=\"ltx_text\" id=\"S4.T4.2.10.5.1\" style=\"font-size:90%;\">36.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T4.2.11.1\"><span class=\"ltx_text\" id=\"S4.T4.2.11.1.1\" style=\"font-size:90%;\">Top-2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.2.11.2\"><span class=\"ltx_text\" id=\"S4.T4.2.11.2.1\" style=\"font-size:90%;\">11.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.2.11.3\"><span class=\"ltx_text\" id=\"S4.T4.2.11.3.1\" style=\"font-size:90%;\">11.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.2.11.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T4.2.11.4.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T4.2.11.4.2\" style=\"font-size:90%;\">80</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.2.11.5\"><span class=\"ltx_text\" id=\"S4.T4.2.11.5.1\" style=\"font-size:90%;\">37.3</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Ablations on different routing methods during BTX training. Average score is based on performance on representative tasks including GSM8K, HumanEval, Natural Questions, ARC Challenge and MMLU.</figcaption>\n</figure>",
            "capture": "Table 4: Ablations on different routing methods during BTX training. Average score is based on performance on representative tasks including GSM8K, HumanEval, Natural Questions, ARC Challenge and MMLU."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T5.2\">\n<tr class=\"ltx_tr\" id=\"S4.T5.2.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T5.2.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.2.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.2.1.2.1\" style=\"font-size:90%;\">GSM8K</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.2.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.2.1.3.1\" style=\"font-size:90%;\">Human</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.2.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.2.1.4.1\" style=\"font-size:90%;\">Natural</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.2.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.2.1.5.1\" style=\"font-size:90%;\">ARC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.2.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.2.1.6.1\" style=\"font-size:90%;\">MMLU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.2.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.2.1.7.1\" style=\"font-size:90%;\">Average</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.2.2\">\n<td class=\"ltx_td\" id=\"S4.T5.2.2.1\"></td>\n<td class=\"ltx_td\" id=\"S4.T5.2.2.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.2.2.3.1\" style=\"font-size:90%;\">Eval</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.2.2.4.1\" style=\"font-size:90%;\">Questions</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.2.2.5.1\" style=\"font-size:90%;\">Challenge</span></td>\n<td class=\"ltx_td\" id=\"S4.T5.2.2.6\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.2.2.7.1\" style=\"font-size:90%;\">Score</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.2.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T5.2.3.1\"><span class=\"ltx_text\" id=\"S4.T5.2.3.1.1\" style=\"font-size:90%;\">BTX</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.2.3.2\"><span class=\"ltx_text\" id=\"S4.T5.2.3.2.1\" style=\"font-size:90%;\">29.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.2.3.3\"><span class=\"ltx_text\" id=\"S4.T5.2.3.3.1\" style=\"font-size:90%;\">27.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.2.3.4\"><span class=\"ltx_text\" id=\"S4.T5.2.3.4.1\" style=\"font-size:90%;\">23.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.2.3.5\"><span class=\"ltx_text\" id=\"S4.T5.2.3.5.1\" style=\"font-size:90%;\">43.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.2.3.6\"><span class=\"ltx_text\" id=\"S4.T5.2.3.6.1\" style=\"font-size:90%;\">50.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.2.3.7\"><span class=\"ltx_text\" id=\"S4.T5.2.3.7.1\" style=\"font-size:90%;\">34.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.2.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.2.4.1\"><span class=\"ltx_text\" id=\"S4.T5.2.4.1.1\" style=\"font-size:90%;\">no load-balancing (LB)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.2.4.2\"><span class=\"ltx_text\" id=\"S4.T5.2.4.2.1\" style=\"font-size:90%;\">34.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.2.4.3\"><span class=\"ltx_text\" id=\"S4.T5.2.4.3.1\" style=\"font-size:90%;\">19.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.2.4.4\"><span class=\"ltx_text\" id=\"S4.T5.2.4.4.1\" style=\"font-size:90%;\">23.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.2.4.5\"><span class=\"ltx_text\" id=\"S4.T5.2.4.5.1\" style=\"font-size:90%;\">44.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.2.4.6\"><span class=\"ltx_text\" id=\"S4.T5.2.4.6.1\" style=\"font-size:90%;\">51.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.2.4.7\"><span class=\"ltx_text\" id=\"S4.T5.2.4.7.1\" style=\"font-size:90%;\">34.6</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.2.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.2.5.1\"><span class=\"ltx_text\" id=\"S4.T5.2.5.1.1\" style=\"font-size:90%;\">no LB &amp; freeze experts</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.5.2\"><span class=\"ltx_text\" id=\"S4.T5.2.5.2.1\" style=\"font-size:90%;\">34.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.5.3\"><span class=\"ltx_text\" id=\"S4.T5.2.5.3.1\" style=\"font-size:90%;\">18.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.5.4\"><span class=\"ltx_text\" id=\"S4.T5.2.5.4.1\" style=\"font-size:90%;\">24.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.5.5\"><span class=\"ltx_text\" id=\"S4.T5.2.5.5.1\" style=\"font-size:90%;\">44.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.5.6\"><span class=\"ltx_text\" id=\"S4.T5.2.5.6.1\" style=\"font-size:90%;\">51.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.5.7\"><span class=\"ltx_text\" id=\"S4.T5.2.5.7.1\" style=\"font-size:90%;\">34.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.2.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.2.6.1\"><span class=\"ltx_text\" id=\"S4.T5.2.6.1.1\" style=\"font-size:90%;\">blending experts</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.6.2\"><span class=\"ltx_text\" id=\"S4.T5.2.6.2.1\" style=\"font-size:90%;\">13.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.6.3\"><span class=\"ltx_text\" id=\"S4.T5.2.6.3.1\" style=\"font-size:90%;\">17.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.6.4\"><span class=\"ltx_text\" id=\"S4.T5.2.6.4.1\" style=\"font-size:90%;\">9.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.6.5\"><span class=\"ltx_text\" id=\"S4.T5.2.6.5.1\" style=\"font-size:90%;\">34.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.6.6\"><span class=\"ltx_text\" id=\"S4.T5.2.6.6.1\" style=\"font-size:90%;\">36.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.6.7\"><span class=\"ltx_text\" id=\"S4.T5.2.6.7.1\" style=\"font-size:90%;\">22.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.2.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.2.7.1\"><span class=\"ltx_text\" id=\"S4.T5.2.7.1.1\" style=\"font-size:90%;\">split experts, top-2 of 8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.7.2\"><span class=\"ltx_text\" id=\"S4.T5.2.7.2.1\" style=\"font-size:90%;\">22.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.7.3\"><span class=\"ltx_text\" id=\"S4.T5.2.7.3.1\" style=\"font-size:90%;\">20.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.7.4\"><span class=\"ltx_text\" id=\"S4.T5.2.7.4.1\" style=\"font-size:90%;\">16.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.7.5\"><span class=\"ltx_text\" id=\"S4.T5.2.7.5.1\" style=\"font-size:90%;\">39.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.7.6\"><span class=\"ltx_text\" id=\"S4.T5.2.7.6.1\" style=\"font-size:90%;\">41.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.2.7.7\"><span class=\"ltx_text\" id=\"S4.T5.2.7.7.1\" style=\"font-size:90%;\">28.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.2.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T5.2.8.1\"><span class=\"ltx_text\" id=\"S4.T5.2.8.1.1\" style=\"font-size:90%;\">split experts, top-4 of 8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.2.8.2\"><span class=\"ltx_text\" id=\"S4.T5.2.8.2.1\" style=\"font-size:90%;\">29.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.2.8.3\"><span class=\"ltx_text\" id=\"S4.T5.2.8.3.1\" style=\"font-size:90%;\">26.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.2.8.4\"><span class=\"ltx_text\" id=\"S4.T5.2.8.4.1\" style=\"font-size:90%;\">22.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.2.8.5\"><span class=\"ltx_text\" id=\"S4.T5.2.8.5.1\" style=\"font-size:90%;\">44.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.2.8.6\"><span class=\"ltx_text\" id=\"S4.T5.2.8.6.1\" style=\"font-size:90%;\">49.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.2.8.7\"><span class=\"ltx_text\" id=\"S4.T5.2.8.7.1\" style=\"font-size:90%;\">34.5</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Ablations on different BTX training strategies. All variants are initialized from the same experts and trained for a total of 10B tokens during MoE finetuning.</figcaption>\n</figure>",
            "capture": "Table 5: Ablations on different BTX training strategies. All variants are initialized from the same experts and trained for a total of 10B tokens during MoE finetuning."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T6\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T6.2\" style=\"width:412.4pt;height:366.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-51.6pt,45.8pt) scale(0.8,0.8) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T6.2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T6.2.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T6.2.1.1.1\">Task</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T6.2.1.1.2\">Question and generation</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.2.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T6.2.1.2.1\">GSM8K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T6.2.1.2.2\">\n<span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.1\"></span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T6.2.1.2.2.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.2.2.2.1.1\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.2.2.2.1.1.1\">Q: <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.1.1.1\" style=\"color:#0000FF;\">Jan<span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.1.1.1.1\" style=\"color:#FF00FF;\">et</span>\u2019</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.1.1.2\" style=\"color:#FF8000;\">s <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.1.1.2.1\" style=\"color:#FF00FF;\">ducks lay</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.1.1.2.2\" style=\"color:#FF00FF;\"> </span>1<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.1.1.2.3\" style=\"color:#FF00FF;\">6 eggs <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.1.1.2.3.1\" style=\"color:#0000FF;\">per </span>day</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.1.1.2.4\" style=\"color:#00FFFF;\">. <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.1.1.2.4.1\" style=\"color:#0000FF;\">She e</span></span>ats <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.1.1.2.5\" style=\"color:#FF00FF;\">three for breakfast</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.1.1.2.6\" style=\"color:#FF00FF;\"> <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.1.1.2.6.1\" style=\"color:#00FFFF;\">every <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.1.1.2.6.1.1\" style=\"color:#FF00FF;\">morning</span></span> <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.1.1.2.6.2\" style=\"color:#00FFFF;\">and <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.1.1.2.6.2.1\" style=\"color:#FF00FF;\">bakes muff</span></span></span>ins <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.1.1.2.7\" style=\"color:#0000FF;\">for</span></span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.2.2.2.1.2\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.2.2.2.1.2.1\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.2.1.1\" style=\"color:#0000FF;\">her <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.2.1.1.1\" style=\"color:#FF00FF;\">friends </span>every <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.2.1.1.2\" style=\"color:#FF00FF;\">day</span></span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.2.1.2\" style=\"color:#FF00FF;\"> <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.2.1.2.1\" style=\"color:#00FFFF;\">with <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.2.1.2.1.1\" style=\"color:#FF00FF;\">four. <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.2.1.2.1.1.1\" style=\"color:#0000FF;\">She s</span>ells the remainder <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.2.1.2.1.1.2\" style=\"color:#0000FF;\">at </span>the farmers\u2019 market</span></span> <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.2.1.2.2\" style=\"color:#00FFFF;\">daily for <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.2.1.2.2.1\" style=\"color:#FF00FF;\">$</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.2.1.2.2.2\" style=\"color:#FF8000;\">2 <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.2.1.2.2.2.1\" style=\"color:#FF00FF;\">per fresh</span></span></span></span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.2.2.2.1.3\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.2.2.2.1.3.1\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.3.1.1\" style=\"color:#FF00FF;\">du</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.3.1.2\" style=\"color:#FF8000;\">ck <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.3.1.2.1\" style=\"color:#FF00FF;\">egg</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.3.1.2.2\" style=\"color:#FF0000;\">. <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.3.1.2.2.1\" style=\"color:#00FFFF;\">How</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.3.1.2.2.2\" style=\"color:#00FFFF;\"> </span></span>much <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.3.1.2.3\" style=\"color:#FF00FF;\">in dollars <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.3.1.2.3.1\" style=\"color:#0000FF;\">does she <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.3.1.2.3.1.1\" style=\"color:#008080;\">make </span>every</span></span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.3.1.2.4\" style=\"color:#0000FF;\"> </span>day <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.3.1.2.5\" style=\"color:#0000FF;\">at the <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.3.1.2.5.1\" style=\"color:#FF00FF;\">far</span></span>mers\u2019 market<span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.3.1.2.6\" style=\"color:#00FFFF;\">?</span></span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.2.2.2.1.4\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.2.2.2.1.4.1\">A: <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.4.1.1\" style=\"color:#0000FF;\">Jan</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.4.1.2\" style=\"color:#FF8000;\">et<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.4.1.2.1\" style=\"color:#FF00FF;\">\u2019</span>s <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.4.1.2.2\" style=\"color:#FF00FF;\">du</span>cks <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.4.1.2.3\" style=\"color:#FF00FF;\">lay</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.4.1.2.4\" style=\"color:#FF00FF;\"> </span>16 eggs <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.4.1.2.5\" style=\"color:#FF0000;\">per </span>day. <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.4.1.2.6\" style=\"color:#0000FF;\">She e</span>ats <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.4.1.2.7\" style=\"color:#FF00FF;\">three</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.4.1.2.8\" style=\"color:#FF00FF;\"> <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.4.1.2.8.1\" style=\"color:#FF0000;\">for </span></span>breakfast <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.4.1.2.9\" style=\"color:#FF0000;\">every </span>morning<span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.4.1.2.10\" style=\"color:#00FFFF;\">. <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.4.1.2.10.1\" style=\"color:#0000FF;\">So she has</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.4.1.2.10.2\" style=\"color:#0000FF;\"> </span></span>1<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.4.1.2.11\" style=\"color:#FF00FF;\">6 <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.4.1.2.11.1\" style=\"color:#008080;\">-</span></span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.4.1.2.12\" style=\"color:#008080;\"> </span>3 <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.4.1.2.13\" style=\"color:#FF00FF;\">=</span></span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.2.2.2.1.5\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.2.2.2.1.5.1\"><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.5.1.1\" style=\"color:#FF8000;\">13 <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.1\" style=\"color:#FF00FF;\">eggs <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.1.1\" style=\"color:#0000FF;\">left. She </span>b</span>akes <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.2\" style=\"color:#FF0000;\">m</span>uffins <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.3\" style=\"color:#0000FF;\">for</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.4\" style=\"color:#0000FF;\"> <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.4.1\" style=\"color:#FF0000;\">her <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.4.1.1\" style=\"color:#FF00FF;\">friends</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.4.1.2\" style=\"color:#FF00FF;\"> </span>every </span></span>day <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.5\" style=\"color:#008080;\">with </span>4<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.6\" style=\"color:#FF00FF;\">.</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.7\" style=\"color:#FF00FF;\"> <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.7.1\" style=\"color:#FF0000;\">So she <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.7.1.1\" style=\"color:#008080;\">has </span></span></span>13 <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.8\" style=\"color:#008080;\">-</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.9\" style=\"color:#008080;\"> </span>4 = 9 <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.10\" style=\"color:#FF00FF;\">eggs</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.11\" style=\"color:#FF00FF;\"> <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.11.1\" style=\"color:#FF0000;\">left. <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.5.1.1.11.1.1\" style=\"color:#0000FF;\">She</span></span></span></span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.2.2.2.1.6\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.2.2.2.1.6.1\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.6.1.1\" style=\"color:#008080;\">s</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.6.1.2\" style=\"color:#FF8000;\">ells <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.6.1.2.1\" style=\"color:#0000FF;\">the</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.6.1.2.2\" style=\"color:#0000FF;\"> </span>remainder <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.6.1.2.3\" style=\"color:#FF0000;\">at the <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.6.1.2.3.1\" style=\"color:#FF00FF;\">far</span></span>mers\u2019 <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.6.1.2.4\" style=\"color:#008080;\">market</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.6.1.2.5\" style=\"color:#008080;\"> <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.6.1.2.5.1\" style=\"color:#FF0000;\">daily <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.6.1.2.5.1.1\" style=\"color:#00FFFF;\">for <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.6.1.2.5.1.1.1\" style=\"color:#008080;\">$</span></span></span></span>2 per fresh duck egg<span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.6.1.2.6\" style=\"color:#FF0000;\">. <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.6.1.2.6.1\" style=\"color:#0000FF;\">So she has</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.6.1.2.6.2\" style=\"color:#0000FF;\"> </span></span>9 <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.6.1.2.7\" style=\"color:#008080;\">*</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.6.1.2.8\" style=\"color:#008080;\"> </span>2 <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.6.1.2.9\" style=\"color:#FF00FF;\">=</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.6.1.2.10\" style=\"color:#FF00FF;\"> </span>18 <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.6.1.2.11\" style=\"color:#FF00FF;\">dollars<span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.6.1.2.11.1\" style=\"color:#0000FF;\">.</span></span></span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.2.2.2.1.7\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.2.2.2.1.7.1\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.2.2.2.1.7.1.1\" style=\"color:#0000FF;\">The</span><span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.7.1.2\" style=\"color:#0000FF;\"> <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.7.1.2.1\" style=\"color:#FF8000;\">answer <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.2.1.7.1.2.1.1\" style=\"color:#00FFFF;\">is </span>18.</span></span></span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T6.2.1.2.2.3\"></span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.2.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T6.2.1.3.1\">Human Eval</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T6.2.1.3.2\">\n<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.1\"></span><span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T6.2.1.3.2.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.3.2.2.1.1\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.3.2.2.1.1.1\">Q: <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.1.1.1\" style=\"color:#FF0000;\">from typing import <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.1.1.1.1\" style=\"color:#0000FF;\">List</span></span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.3.2.2.1.2\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.3.2.2.1.2.1\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.2.1.1\" style=\"color:#008080;\">def has_close<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.2.1.1.1\" style=\"color:#FF8000;\">_</span>elements(numbers: List[float<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.2.1.1.2\" style=\"color:#FF8000;\">], </span>threshold: <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.2.1.1.3\" style=\"color:#00FFFF;\">float<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.2.1.1.3.1\" style=\"color:#FF8000;\">)</span></span>-&gt; bool<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.2.1.1.4\" style=\"color:#FF8000;\">:</span></span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.3.2.2.1.3\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.3.2.2.1.3.1\">\u2003<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.3.1.1\" style=\"color:#008080;\">\"\"\" Check if in given list</span><span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.3.1.2\" style=\"color:#008080;\"> <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.3.1.2.1\" style=\"color:#0000FF;\">of <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.3.1.2.1.1\" style=\"color:#008080;\">numbers</span>, <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.3.1.2.1.2\" style=\"color:#008080;\">are any</span></span> <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.3.1.2.2\" style=\"color:#FF00FF;\">two <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.3.1.2.2.1\" style=\"color:#008080;\">numbers closer</span></span> <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.3.1.2.3\" style=\"color:#0000FF;\">to <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.3.1.2.3.1\" style=\"color:#FF00FF;\">each <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.3.1.2.3.1.1\" style=\"color:#FF0000;\">other </span></span>than</span></span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.3.2.2.1.4\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.3.2.2.1.4.1\">\u2003<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.4.1.1\" style=\"color:#008080;\">given threshold.</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.3.2.2.1.5\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.3.2.2.1.5.1\">\u2003<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.5.1.1\" style=\"color:#008080;\">&gt; &gt; &gt; has</span><span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.5.1.2\" style=\"color:#FF0000;\">_close_<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.5.1.2.1\" style=\"color:#0000FF;\">elements<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.5.1.2.1.1\" style=\"color:#008080;\">([1.0<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.5.1.2.1.1.1\" style=\"color:#FF8000;\">, </span>2<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.5.1.2.1.1.2\" style=\"color:#FF8000;\">.0, 3.0], </span>0<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.5.1.2.1.1.3\" style=\"color:#FF8000;\">.</span>5)</span></span></span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.3.2.2.1.6\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.3.2.2.1.6.1\">\u2003<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.6.1.1\" style=\"color:#008080;\">False</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.3.2.2.1.7\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.3.2.2.1.7.1\">\u2003<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.7.1.1\" style=\"color:#008080;\">&gt; &gt; &gt; has<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.7.1.1.1\" style=\"color:#00FFFF;\">_close</span></span><span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.7.1.2\" style=\"color:#FF0000;\">_<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.7.1.2.1\" style=\"color:#00FFFF;\">elements<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.7.1.2.1.1\" style=\"color:#FF8000;\">([<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.7.1.2.1.1.1\" style=\"color:#008080;\">1</span>.0, <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.7.1.2.1.1.2\" style=\"color:#008080;\">2</span>.8<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.7.1.2.1.1.3\" style=\"color:#008080;\">, 3</span>.<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.7.1.2.1.1.4\" style=\"color:#008080;\">0</span>, <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.7.1.2.1.1.5\" style=\"color:#008080;\">4</span>.<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.7.1.2.1.1.6\" style=\"color:#008080;\">0, </span>5.0<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.7.1.2.1.1.7\" style=\"color:#008080;\">, </span>2<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.7.1.2.1.1.8\" style=\"color:#008080;\">.</span>0<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.7.1.2.1.1.9\" style=\"color:#008080;\">],</span></span></span><span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.7.1.2.2\" style=\"color:#008080;\"> </span>0<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.7.1.2.3\" style=\"color:#008080;\">.<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.7.1.2.3.1\" style=\"color:#FF8000;\">3)</span></span></span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.3.2.2.1.8\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.3.2.2.1.8.1\">\u2003<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.8.1.1\" style=\"color:#FF8000;\">True</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.3.2.2.1.9\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.3.2.2.1.9.1\">\u2003<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.9.1.1\" style=\"color:#FF8000;\">\"\"\"</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.3.2.2.1.10\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.3.2.2.1.10.1\">A: <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.10.1.1\" style=\"color:#008080;\">for i in</span><span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.10.1.2\" style=\"color:#008080;\"> <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.10.1.2.1\" style=\"color:#FF0000;\">range<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.10.1.2.1.1\" style=\"color:#FF8000;\">(<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.10.1.2.1.1.1\" style=\"color:#008080;\">len</span>(</span>numbers<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.10.1.2.1.2\" style=\"color:#008080;\">) - <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.10.1.2.1.2.1\" style=\"color:#FF8000;\">1):</span></span></span></span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.3.2.2.1.11\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.3.2.2.1.11.1\">\u2003<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.11.1.1\" style=\"color:#008080;\">for j <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.11.1.1.1\" style=\"color:#FF8000;\">in <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.11.1.1.1.1\" style=\"color:#0000FF;\">range</span>(</span>i +</span><span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.11.1.2\" style=\"color:#008080;\"> <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.11.1.2.1\" style=\"color:#FF0000;\">1<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.11.1.2.1.1\" style=\"color:#FF8000;\">, <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.11.1.2.1.1.1\" style=\"color:#008080;\">len</span>(</span><span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.11.1.2.1.2\" style=\"color:#0000FF;\">numbers<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.11.1.2.1.2.1\" style=\"color:#008080;\">)):</span></span></span></span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.3.2.2.1.12\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.3.2.2.1.12.1\">\u2003\u2003<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.12.1.1\" style=\"color:#008080;\">if abs(numbers<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.12.1.1.1\" style=\"color:#FF8000;\">[i] - numbers<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.12.1.1.1.1\" style=\"color:#00FFFF;\">[j]</span>) </span>&lt; threshold<span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.12.1.1.2\" style=\"color:#FF8000;\">:</span></span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.3.2.2.1.13\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.3.2.2.1.13.1\">\u2003\u2003\u2003<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.13.1.1\" style=\"color:#008080;\">return <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.13.1.1.1\" style=\"color:#00FFFF;\">True</span></span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.3.2.2.1.14\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.3.2.2.1.14.1\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.3.2.2.1.14.1.1\" style=\"color:#008080;\">return <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.2.1.14.1.1.1\" style=\"color:#00FFFF;\">False</span></span></span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T6.2.1.3.2.3\"></span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.2.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T6.2.1.4.1\">\n<span class=\"ltx_text\" id=\"S4.T6.2.1.4.1.1\"></span><span class=\"ltx_text\" id=\"S4.T6.2.1.4.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T6.2.1.4.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.4.1.2.1.1\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.4.1.2.1.1.1\">Natural</span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.4.1.2.1.2\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.4.1.2.1.2.1\">Questions</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T6.2.1.4.1.3\"></span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T6.2.1.4.2\">\n<span class=\"ltx_text\" id=\"S4.T6.2.1.4.2.1\"></span><span class=\"ltx_text\" id=\"S4.T6.2.1.4.2.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T6.2.1.4.2.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.4.2.2.1.1\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.4.2.2.1.1.1\">Q: <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.4.2.2.1.1.1.1\" style=\"color:#00FFFF;\">who <span class=\"ltx_text\" id=\"S4.T6.2.1.4.2.2.1.1.1.1.1\" style=\"color:#0000FF;\">got <span class=\"ltx_text\" id=\"S4.T6.2.1.4.2.2.1.1.1.1.1.1\" style=\"color:#FF0000;\">the first no</span></span></span><span class=\"ltx_text\" id=\"S4.T6.2.1.4.2.2.1.1.1.2\" style=\"color:#FF00FF;\">bel prize <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.4.2.2.1.1.1.2.1\" style=\"color:#0000FF;\">in</span><span class=\"ltx_text\" id=\"S4.T6.2.1.4.2.2.1.1.1.2.2\" style=\"color:#0000FF;\"> </span>physics</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T6.2.1.4.2.2.1.2\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T6.2.1.4.2.2.1.2.1\">A: <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.2.1.4.2.2.1.2.1.1\" style=\"color:#FF0000;\">Max</span><span class=\"ltx_text\" id=\"S4.T6.2.1.4.2.2.1.2.1.2\" style=\"color:#FF0000;\"> <span class=\"ltx_text\" id=\"S4.T6.2.1.4.2.2.1.2.1.2.1\" style=\"color:#FF00FF;\">Plan<span class=\"ltx_text\" id=\"S4.T6.2.1.4.2.2.1.2.1.2.1.1\" style=\"color:#FF8000;\">ck</span></span></span></span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T6.2.1.4.2.3\"></span>\n</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T6.4.1.1\" style=\"font-size:90%;\">Table 6</span>: </span><span class=\"ltx_text\" id=\"S4.T6.5.2\" style=\"font-size:90%;\">Examples of the token routing decisions for the Top-2 routing with load balancing in the math (GSM8K), code (Human Eval), and knowledge (Natural Questions) domains. Tokens highlighted are routed to the following experts: <span class=\"ltx_text\" id=\"S4.T6.5.2.1\" style=\"color:#FF0000;\">Wikipedia and <span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T6.5.2.1.1\">LLaMa-2 7B</span>, <span class=\"ltx_text\" id=\"S4.T6.5.2.1.2\" style=\"color:#FF00FF;\">Math and <span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T6.5.2.1.2.1\">LLaMa-2 7B</span>, <span class=\"ltx_text\" id=\"S4.T6.5.2.1.2.2\" style=\"color:#FF8000;\">Code and <span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T6.5.2.1.2.2.1\">LLaMa-2 7B</span>, <span class=\"ltx_text\" id=\"S4.T6.5.2.1.2.2.2\" style=\"color:#008080;\">Math and Code, <span class=\"ltx_text\" id=\"S4.T6.5.2.1.2.2.2.1\" style=\"color:#0000FF;\">Wikipedia and Math, <span class=\"ltx_text\" id=\"S4.T6.5.2.1.2.2.2.1.1\" style=\"color:#00FFFF;\">Wikipedia and Code. <span class=\"ltx_text\" id=\"S4.T6.5.2.1.2.2.2.1.1.1\" style=\"color:#000000;\">Tokens that were routed to the in-domain expert are underlined.</span></span></span></span></span></span></span></span></figcaption>\n</figure>",
            "capture": "Table 6: Examples of the token routing decisions for the Top-2 routing with load balancing in the math (GSM8K), code (Human Eval), and knowledge (Natural Questions) domains. Tokens highlighted are routed to the following experts: Wikipedia and LLaMa-2 7B, Math and LLaMa-2 7B, Code and LLaMa-2 7B, Math and Code, Wikipedia and Math, Wikipedia and Code. Tokens that were routed to the in-domain expert are underlined."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S8.T7\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S8.T7.2\">\n<tr class=\"ltx_tr\" id=\"S8.T7.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S8.T7.2.1.1\"><span class=\"ltx_text\" id=\"S8.T7.2.1.1.1\" style=\"font-size:90%;\">Domain</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S8.T7.2.1.2\"><span class=\"ltx_text\" id=\"S8.T7.2.1.2.1\" style=\"font-size:90%;\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S8.T7.2.1.3\"><span class=\"ltx_text\" id=\"S8.T7.2.1.3.1\" style=\"font-size:90%;\">Sampling ratio (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S8.T7.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S8.T7.2.2.1\" rowspan=\"5\"><span class=\"ltx_text\" id=\"S8.T7.2.2.1.1\" style=\"font-size:90%;\">Math</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S8.T7.2.2.2\"><span class=\"ltx_text\" id=\"S8.T7.2.2.2.1\" style=\"font-size:90%;\">AlgebraicStack</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S8.T7.2.2.3\"><span class=\"ltx_text\" id=\"S8.T7.2.2.3.1\" style=\"font-size:90%;\">13.57</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S8.T7.2.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S8.T7.2.3.1\"><span class=\"ltx_text\" id=\"S8.T7.2.3.1.1\" style=\"font-size:90%;\">OpenWebMath</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S8.T7.2.3.2\"><span class=\"ltx_text\" id=\"S8.T7.2.3.2.1\" style=\"font-size:90%;\">54.27</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S8.T7.2.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S8.T7.2.4.1\"><span class=\"ltx_text\" id=\"S8.T7.2.4.1.1\" style=\"font-size:90%;\">Arxiv</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S8.T7.2.4.2\"><span class=\"ltx_text\" id=\"S8.T7.2.4.2.1\" style=\"font-size:90%;\">27.14</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S8.T7.2.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S8.T7.2.5.1\"><span class=\"ltx_text\" id=\"S8.T7.2.5.1.1\" style=\"font-size:90%;\">Github</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S8.T7.2.5.2\"><span class=\"ltx_text\" id=\"S8.T7.2.5.2.1\" style=\"font-size:90%;\">2.99</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S8.T7.2.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S8.T7.2.6.1\"><span class=\"ltx_text\" id=\"S8.T7.2.6.1.1\" style=\"font-size:90%;\">Commoncrawl</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S8.T7.2.6.2\"><span class=\"ltx_text\" id=\"S8.T7.2.6.2.1\" style=\"font-size:90%;\">5.01</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S8.T7.2.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S8.T7.2.7.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S8.T7.2.7.1.1\" style=\"font-size:90%;\">Code</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S8.T7.2.7.2\"><span class=\"ltx_text\" id=\"S8.T7.2.7.2.1\" style=\"font-size:90%;\">Code</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S8.T7.2.7.3\"><span class=\"ltx_text\" id=\"S8.T7.2.7.3.1\" style=\"font-size:90%;\">82.18</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S8.T7.2.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S8.T7.2.8.1\"><span class=\"ltx_text\" id=\"S8.T7.2.8.1.1\" style=\"font-size:90%;\">Natural language related to code</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S8.T7.2.8.2\"><span class=\"ltx_text\" id=\"S8.T7.2.8.2.1\" style=\"font-size:90%;\">9.90</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S8.T7.2.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S8.T7.2.9.1\"><span class=\"ltx_text\" id=\"S8.T7.2.9.1.1\" style=\"font-size:90%;\">Natural language</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S8.T7.2.9.2\"><span class=\"ltx_text\" id=\"S8.T7.2.9.2.1\" style=\"font-size:90%;\">6.93</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S8.T7.2.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S8.T7.2.10.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S8.T7.2.10.1.1\" style=\"font-size:90%;\">Wikipedia</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S8.T7.2.10.2\"><span class=\"ltx_text\" id=\"S8.T7.2.10.2.1\" style=\"font-size:90%;\">Wikipedia</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S8.T7.2.10.3\"><span class=\"ltx_text\" id=\"S8.T7.2.10.3.1\" style=\"font-size:90%;\">90.91</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S8.T7.2.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S8.T7.2.11.1\"><span class=\"ltx_text\" id=\"S8.T7.2.11.1.1\" style=\"font-size:90%;\">Commoncrawl</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S8.T7.2.11.2\"><span class=\"ltx_text\" id=\"S8.T7.2.11.2.1\" style=\"font-size:90%;\">9.09</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>Data sources and weights for domain experts.</figcaption>\n</figure>",
            "capture": "Table 7: Data sources and weights for domain experts."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"S9.T8\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S9.T8.2\">\n<tr class=\"ltx_tr\" id=\"S9.T8.2.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S9.T8.2.1.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S9.T8.2.1.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.1.2.1\" style=\"font-size:80%;\">GSM8K</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S9.T8.2.1.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.1.3.1\" style=\"font-size:80%;\">MATH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S9.T8.2.1.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.1.4.1\" style=\"font-size:80%;\">Human</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S9.T8.2.1.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.1.5.1\" style=\"font-size:80%;\">MBPP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S9.T8.2.1.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.1.6.1\" style=\"font-size:80%;\">Natural</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S9.T8.2.1.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.1.7.1\" style=\"font-size:80%;\">Trivia</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S9.T8.2.1.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.1.8.1\" style=\"font-size:80%;\">ARC-e</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S9.T8.2.1.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.1.9.1\" style=\"font-size:80%;\">ARC-c</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S9.T8.2.1.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.1.10.1\" style=\"font-size:80%;\">Wino</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S9.T8.2.1.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.1.11.1\" style=\"font-size:80%;\">SIQA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S9.T8.2.1.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.1.12.1\" style=\"font-size:80%;\">PIQA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S9.T8.2.1.13\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.1.13.1\" style=\"font-size:80%;\">MMLU</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S9.T8.2.2\">\n<td class=\"ltx_td\" id=\"S9.T8.2.2.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td\" id=\"S9.T8.2.2.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td\" id=\"S9.T8.2.2.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.2.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.2.4.1\" style=\"font-size:80%;\">Eval</span></td>\n<td class=\"ltx_td\" id=\"S9.T8.2.2.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.2.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.2.6.1\" style=\"font-size:80%;\">Questions</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.2.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.2.7.1\" style=\"font-size:80%;\">QA</span></td>\n<td class=\"ltx_td\" id=\"S9.T8.2.2.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td\" id=\"S9.T8.2.2.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td\" id=\"S9.T8.2.2.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td\" id=\"S9.T8.2.2.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td\" id=\"S9.T8.2.2.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td\" id=\"S9.T8.2.2.13\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S9.T8.2.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S9.T8.2.3.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S9.T8.2.3.1.1\" style=\"font-size:80%;\">Specialized LLMs</span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.3.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.3.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.3.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.3.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.3.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.3.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.3.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.3.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.3.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.3.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.3.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.3.13\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S9.T8.2.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S9.T8.2.4.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S9.T8.2.4.1.1\" style=\"font-size:80%;\">CodeLlama 7B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.4.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.4.2.1\" style=\"font-size:80%;\">13.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.4.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.4.3.1\" style=\"font-size:80%;\">3.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.4.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.4.4.1\" style=\"font-size:80%;\">31.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.4.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.4.5.1\" style=\"font-size:80%;\">41.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.4.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.4.6.1\" style=\"font-size:80%;\">11.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.4.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.4.7.1\" style=\"font-size:80%;\">32.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.4.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.4.8.1\" style=\"font-size:80%;\">67.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.4.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.4.9.1\" style=\"font-size:80%;\">34.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.4.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.4.10.1\" style=\"font-size:80%;\">62.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.4.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.4.11.1\" style=\"font-size:80%;\">46.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.4.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.4.12.1\" style=\"font-size:80%;\">72.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.4.13\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.4.13.1\" style=\"font-size:80%;\">38.6</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S9.T8.2.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S9.T8.2.5.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S9.T8.2.5.1.1\" style=\"font-size:80%;\">Llemma 7B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.5.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.5.2.1\" style=\"font-size:80%;\">39.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.5.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.5.3.1\" style=\"font-size:80%;\">16.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.5.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.5.4.1\" style=\"font-size:80%;\">25.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.5.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.5.5.1\" style=\"font-size:80%;\">41.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.5.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.5.6.1\" style=\"font-size:80%;\">9.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.5.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.5.7.1\" style=\"font-size:80%;\">24.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.5.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.5.8.1\" style=\"font-size:80%;\">28.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.5.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.5.9.1\" style=\"font-size:80%;\">26.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.5.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.5.10.1\" style=\"font-size:80%;\">50.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.5.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.5.11.1\" style=\"font-size:80%;\">37.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.5.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.5.12.1\" style=\"font-size:80%;\">51.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.5.13\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.5.13.1\" style=\"font-size:80%;\">33.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S9.T8.2.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S9.T8.2.6.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S9.T8.2.6.1.1\" style=\"font-size:80%;\">Generalist LLMs</span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.6.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.6.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.6.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.6.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.6.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.6.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.6.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.6.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.6.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.6.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.6.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S9.T8.2.6.13\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S9.T8.2.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S9.T8.2.7.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S9.T8.2.7.1.1\" style=\"font-size:80%;\">Llama-2 7B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.7.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.7.2.1\" style=\"font-size:80%;\">14.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.7.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.7.3.1\" style=\"font-size:80%;\">2.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.7.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.7.4.1\" style=\"font-size:80%;\">12.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.7.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.7.5.1\" style=\"font-size:80%;\">20.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.7.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.7.6.1\" style=\"font-size:80%;\">16.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.7.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.7.7.1\" style=\"font-size:80%;\">58.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.7.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.7.8.1\" style=\"font-size:80%;\">76.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.7.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.7.9.1\" style=\"font-size:80%;\">43.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.7.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.7.10.1\" style=\"font-size:80%;\">69.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.7.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.7.11.1\" style=\"font-size:80%;\">48.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.7.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.7.12.1\" style=\"font-size:80%;\">78.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.7.13\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.7.13.1\" style=\"font-size:80%;\">46.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S9.T8.2.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S9.T8.2.8.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S9.T8.2.8.1.1\" style=\"font-size:80%;\">Llama-2 13B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.8.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.8.2.1\" style=\"font-size:80%;\">28.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.8.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.8.3.1\" style=\"font-size:80%;\">3.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.8.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.8.4.1\" style=\"font-size:80%;\">18.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.8.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.8.5.1\" style=\"font-size:80%;\">30.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.8.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.8.6.1\" style=\"font-size:80%;\">16.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.8.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.8.7.1\" style=\"font-size:80%;\">63.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.8.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.8.8.1\" style=\"font-size:80%;\">77.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.8.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.8.9.1\" style=\"font-size:80%;\">49.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.8.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.8.10.1\" style=\"font-size:80%;\">73.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.8.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.8.11.1\" style=\"font-size:80%;\">50.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.8.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.8.12.1\" style=\"font-size:80%;\">80.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.8.13\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.8.13.1\" style=\"font-size:80%;\">52.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S9.T8.2.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S9.T8.2.9.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.9.1.1\" style=\"font-size:80%;\">Dense (DM)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.9.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.9.2.1\" style=\"font-size:80%;\">26.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.9.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.9.3.1\" style=\"font-size:80%;\">9.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.9.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.9.4.1\" style=\"font-size:80%;\">20.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.9.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.9.5.1\" style=\"font-size:80%;\">30.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.9.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.9.6.1\" style=\"font-size:80%;\">24.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.9.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.9.7.1\" style=\"font-size:80%;\">55.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.9.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.9.8.1\" style=\"font-size:80%;\">76.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.9.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.9.9.1\" style=\"font-size:80%;\">44.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.9.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.9.10.1\" style=\"font-size:80%;\">68.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.9.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.9.11.1\" style=\"font-size:80%;\">48.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.9.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.9.12.1\" style=\"font-size:80%;\">78.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.9.13\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.9.13.1\" style=\"font-size:80%;\">49.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S9.T8.2.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S9.T8.2.10.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.10.1.1\" style=\"font-size:80%;\">Sparse upcycling (DM), Top-2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.10.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.10.2.1\" style=\"font-size:80%;\">37.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.10.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.10.3.1\" style=\"font-size:80%;\">18.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.10.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.10.4.1\" style=\"font-size:80%;\">29.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.10.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.10.5.1\" style=\"font-size:80%;\">40.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.10.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.10.6.1\" style=\"font-size:80%;\">18.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.10.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.10.7.1\" style=\"font-size:80%;\">49.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.10.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.10.8.1\" style=\"font-size:80%;\">76.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.10.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.10.9.1\" style=\"font-size:80%;\">43.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.10.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.10.10.1\" style=\"font-size:80%;\">66.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.10.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.10.11.1\" style=\"font-size:80%;\">47.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.10.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.10.12.1\" style=\"font-size:80%;\">77.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.10.13\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.10.13.1\" style=\"font-size:80%;\">51.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S9.T8.2.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"S9.T8.2.11.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.11.1.1\" style=\"font-size:80%;\">Sparse upcycling (CM), Top-2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.11.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.11.2.1\" style=\"font-size:80%;\">40.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.11.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.11.3.1\" style=\"font-size:80%;\">16.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.11.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.11.4.1\" style=\"font-size:80%;\">26.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.11.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.11.5.1\" style=\"font-size:80%;\">35.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.11.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.11.6.1\" style=\"font-size:80%;\">24.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.11.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.11.7.1\" style=\"font-size:80%;\">58.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.11.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.11.8.1\" style=\"font-size:80%;\">75.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.11.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.11.9.1\" style=\"font-size:80%;\">44.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.11.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.11.10.1\" style=\"font-size:80%;\">69.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.11.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.11.11.1\" style=\"font-size:80%;\">47.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.11.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.11.12.1\" style=\"font-size:80%;\">78.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.11.13\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.11.13.1\" style=\"font-size:80%;\">52.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S9.T8.2.12\">\n<td class=\"ltx_td ltx_align_left\" id=\"S9.T8.2.12.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.12.1.1\" style=\"font-size:80%;\">BTM, Top-1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.12.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.12.2.1\" style=\"font-size:80%;\">27.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.12.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.12.3.1\" style=\"font-size:80%;\">15.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.12.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.12.4.1\" style=\"font-size:80%;\">30.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.12.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.12.5.1\" style=\"font-size:80%;\">41.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.12.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.12.6.1\" style=\"font-size:80%;\">15.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.12.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.12.7.1\" style=\"font-size:80%;\">38.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.12.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.12.8.1\" style=\"font-size:80%;\">72.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.12.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.12.9.1\" style=\"font-size:80%;\">38.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.12.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.12.10.1\" style=\"font-size:80%;\">68.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.12.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.12.11.1\" style=\"font-size:80%;\">47.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.12.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.12.12.1\" style=\"font-size:80%;\">77.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.12.13\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.12.13.1\" style=\"font-size:80%;\">44.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S9.T8.2.13\">\n<td class=\"ltx_td ltx_align_left\" id=\"S9.T8.2.13.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.13.1.1\" style=\"font-size:80%;\">BTM, Top-2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.13.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.13.2.1\" style=\"font-size:80%;\">27.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.13.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.13.3.1\" style=\"font-size:80%;\">15.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.13.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.13.4.1\" style=\"font-size:80%;\">30.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.13.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.13.5.1\" style=\"font-size:80%;\">42.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.13.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.13.6.1\" style=\"font-size:80%;\">15.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.13.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.13.7.1\" style=\"font-size:80%;\">38.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.13.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.13.8.1\" style=\"font-size:80%;\">73.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.13.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.13.9.1\" style=\"font-size:80%;\">38.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.13.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.13.10.1\" style=\"font-size:80%;\">68.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.13.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.13.11.1\" style=\"font-size:80%;\">48.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.13.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.13.12.1\" style=\"font-size:80%;\">78.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.13.13\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.13.13.1\" style=\"font-size:80%;\">44.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S9.T8.2.14\">\n<td class=\"ltx_td ltx_align_left\" id=\"S9.T8.2.14.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.14.1.1\" style=\"font-size:80%;\">BTX, sample Top-1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.14.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.14.2.1\" style=\"font-size:80%;\">36.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.14.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.14.3.1\" style=\"font-size:80%;\">15.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.14.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.14.4.1\" style=\"font-size:80%;\">25.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.14.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.14.5.1\" style=\"font-size:80%;\">37.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.14.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.14.6.1\" style=\"font-size:80%;\">23.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.14.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.14.7.1\" style=\"font-size:80%;\">56.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.14.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.14.8.1\" style=\"font-size:80%;\">76.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.14.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.14.9.1\" style=\"font-size:80%;\">45.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.14.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.14.10.1\" style=\"font-size:80%;\">70.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.14.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.14.11.1\" style=\"font-size:80%;\">48.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.14.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.14.12.1\" style=\"font-size:80%;\">78.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S9.T8.2.14.13\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.14.13.1\" style=\"font-size:80%;\">53.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S9.T8.2.15\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S9.T8.2.15.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.15.1.1\" style=\"font-size:80%;\">BTX, Top-2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S9.T8.2.15.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.15.2.1\" style=\"font-size:80%;\">37.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S9.T8.2.15.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.15.3.1\" style=\"font-size:80%;\">17.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S9.T8.2.15.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.15.4.1\" style=\"font-size:80%;\">28.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S9.T8.2.15.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.15.5.1\" style=\"font-size:80%;\">39.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S9.T8.2.15.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.15.6.1\" style=\"font-size:80%;\">24.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S9.T8.2.15.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.15.7.1\" style=\"font-size:80%;\">57.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S9.T8.2.15.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.15.8.1\" style=\"font-size:80%;\">76.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S9.T8.2.15.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.15.9.1\" style=\"font-size:80%;\">45.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S9.T8.2.15.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.15.10.1\" style=\"font-size:80%;\">67.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S9.T8.2.15.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.15.11.1\" style=\"font-size:80%;\">48.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S9.T8.2.15.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.15.12.1\" style=\"font-size:80%;\">78.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S9.T8.2.15.13\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S9.T8.2.15.13.1\" style=\"font-size:80%;\">52.5</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S9.T8.5.1.1\" style=\"font-size:113%;\">Table 8</span>: </span><span class=\"ltx_text\" id=\"S9.T8.6.2\" style=\"font-size:113%;\">Individual task performance of BTX and baselines.\n</span></figcaption>\n</figure>",
            "capture": "Table 8: Individual task performance of BTX and baselines.\n"
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.07816v1_figure_1.png",
            "caption": "Figure 1: The Branch-Train-MiX (BTX) method has three steps: 1) branch from a pretrained seed LLM by making multiple copies of it; 2) train those copies separately on different subsets of data to obtain expert LLMs; 3) mix those expert LLMs by combining them into a single LLM using mixture-of-experts feedforward (FF) layers, and finetuning the overall unified model."
        },
        "2": {
            "figure_path": "2403.07816v1_figure_2.png",
            "caption": "Figure 2: \nLeft: The average performance vs training budget of BTX compared to various baselines, with different active parameters at inference time indicated by circle size. All the models except Llama-2 13B are trained starting from Llama-2 7B using the datasets described in Section 4.1.1. The X-axis shows the total training compute starting from the seed model measured in GPU days111The GPU days of Llama-2 13B is an approximate measurement, calculated by doubling the training compute of a 7B model trained with the same amount of pretraining data (according to Touvron et al. (2023) Table 2). Since Llama-2 13B is not trained from the seed model, we simply report their difference in GPU days.\n, and the Y-axis is the average score over all the tasks (as computed in Table 2). The BTX models outperform the baselines that started from the same seed model, as well as Llama-2 13B.\nRight: The normalized performance over different domains where the scores are divided by the highest one. We see large improvements for BTX in code (which matches the specialized model) and math tasks compared to the seed model Llama-2 7B, even outperforming the Llama-2 13B model."
        },
        "3": {
            "figure_path": "2403.07816v1_figure_3.png",
            "caption": "Figure 2: \nLeft: The average performance vs training budget of BTX compared to various baselines, with different active parameters at inference time indicated by circle size. All the models except Llama-2 13B are trained starting from Llama-2 7B using the datasets described in Section 4.1.1. The X-axis shows the total training compute starting from the seed model measured in GPU days111The GPU days of Llama-2 13B is an approximate measurement, calculated by doubling the training compute of a 7B model trained with the same amount of pretraining data (according to Touvron et al. (2023) Table 2). Since Llama-2 13B is not trained from the seed model, we simply report their difference in GPU days.\n, and the Y-axis is the average score over all the tasks (as computed in Table 2). The BTX models outperform the baselines that started from the same seed model, as well as Llama-2 13B.\nRight: The normalized performance over different domains where the scores are divided by the highest one. We see large improvements for BTX in code (which matches the specialized model) and math tasks compared to the seed model Llama-2 7B, even outperforming the Llama-2 13B model."
        },
        "4": {
            "figure_path": "2403.07816v1_figure_4.png",
            "caption": "Figure 3: BTX routing decisions of the tokens at various layers to different experts (Wiki, Math, Code, LLaMa-2 7B) for different downstream tasks. The tasks are aggregated by domain: Code (Human Eval, MBPP), Math (GSM8K, MATH), World knowledge (Natural Questions, TriviaQA), and Reasoning (ARC-Easy, ARC-Challenge, SIQA, PIQA, and WinoGrande). We observe that Top-2 routing with load balancing (top) ensures a more uniform distribution of the load between experts compared to Top-2 without load balancing (bottom)."
        },
        "5": {
            "figure_path": "2403.07816v1_figure_5.png",
            "caption": "Figure 3: BTX routing decisions of the tokens at various layers to different experts (Wiki, Math, Code, LLaMa-2 7B) for different downstream tasks. The tasks are aggregated by domain: Code (Human Eval, MBPP), Math (GSM8K, MATH), World knowledge (Natural Questions, TriviaQA), and Reasoning (ARC-Easy, ARC-Challenge, SIQA, PIQA, and WinoGrande). We observe that Top-2 routing with load balancing (top) ensures a more uniform distribution of the load between experts compared to Top-2 without load balancing (bottom)."
        },
        "6": {
            "figure_path": "2403.07816v1_figure_6.png",
            "caption": "Figure 4: BTX routing decisions of the tokens at various layers to different experts (Wiki, Math, Code, LLaMa-2 7B) for different downstream tasks. The tasks are aggregated by domain: Code (Human Eval, MBPP), Math (GSM8K, MATH), World knowledge (Natural Questions, TriviaQA), and Reasoning (ARC-Easy, ARC-Challenge, SIQA, PIQA, and WinoGrande). We observe that top-2 routing with load balancing ensures more uniform distribution of the load between experts compared to the other routing methods across all layers."
        },
        "7": {
            "figure_path": "2403.07816v1_figure_7.png",
            "caption": "Figure 4: BTX routing decisions of the tokens at various layers to different experts (Wiki, Math, Code, LLaMa-2 7B) for different downstream tasks. The tasks are aggregated by domain: Code (Human Eval, MBPP), Math (GSM8K, MATH), World knowledge (Natural Questions, TriviaQA), and Reasoning (ARC-Easy, ARC-Challenge, SIQA, PIQA, and WinoGrande). We observe that top-2 routing with load balancing ensures more uniform distribution of the load between experts compared to the other routing methods across all layers."
        },
        "8": {
            "figure_path": "2403.07816v1_figure_8.png",
            "caption": "Figure 4: BTX routing decisions of the tokens at various layers to different experts (Wiki, Math, Code, LLaMa-2 7B) for different downstream tasks. The tasks are aggregated by domain: Code (Human Eval, MBPP), Math (GSM8K, MATH), World knowledge (Natural Questions, TriviaQA), and Reasoning (ARC-Easy, ARC-Challenge, SIQA, PIQA, and WinoGrande). We observe that top-2 routing with load balancing ensures more uniform distribution of the load between experts compared to the other routing methods across all layers."
        },
        "9": {
            "figure_path": "2403.07816v1_figure_9.png",
            "caption": "Figure 4: BTX routing decisions of the tokens at various layers to different experts (Wiki, Math, Code, LLaMa-2 7B) for different downstream tasks. The tasks are aggregated by domain: Code (Human Eval, MBPP), Math (GSM8K, MATH), World knowledge (Natural Questions, TriviaQA), and Reasoning (ARC-Easy, ARC-Challenge, SIQA, PIQA, and WinoGrande). We observe that top-2 routing with load balancing ensures more uniform distribution of the load between experts compared to the other routing methods across all layers."
        },
        "10": {
            "figure_path": "2403.07816v1_figure_10.png",
            "caption": "Figure 5: Routing probabilities per expert across different layers for Human Eval task. We compare top-2 routing with (left) and without load balancing (right)."
        },
        "11": {
            "figure_path": "2403.07816v1_figure_11.png",
            "caption": "Figure 5: Routing probabilities per expert across different layers for Human Eval task. We compare top-2 routing with (left) and without load balancing (right)."
        },
        "12": {
            "figure_path": "2403.07816v1_figure_12.png",
            "caption": "Figure 5: Routing probabilities per expert across different layers for Human Eval task. We compare top-2 routing with (left) and without load balancing (right)."
        },
        "13": {
            "figure_path": "2403.07816v1_figure_13.png",
            "caption": "Figure 5: Routing probabilities per expert across different layers for Human Eval task. We compare top-2 routing with (left) and without load balancing (right)."
        },
        "14": {
            "figure_path": "2403.07816v1_figure_14.png",
            "caption": "Figure 5: Routing probabilities per expert across different layers for Human Eval task. We compare top-2 routing with (left) and without load balancing (right)."
        },
        "15": {
            "figure_path": "2403.07816v1_figure_15.png",
            "caption": "Figure 5: Routing probabilities per expert across different layers for Human Eval task. We compare top-2 routing with (left) and without load balancing (right)."
        },
        "16": {
            "figure_path": "2403.07816v1_figure_16.png",
            "caption": "Figure 6: Routing decision of the tokens in Math and Reasoning domains. We observe that GSM8K task prefers Code and Llama-2 experts, while MATH task relies more on in-domain expert. In the Reasoning domain, the load is distributed between Math and LLaMa-2 7B experts."
        },
        "17": {
            "figure_path": "2403.07816v1_figure_17.png",
            "caption": "Figure 6: Routing decision of the tokens in Math and Reasoning domains. We observe that GSM8K task prefers Code and Llama-2 experts, while MATH task relies more on in-domain expert. In the Reasoning domain, the load is distributed between Math and LLaMa-2 7B experts."
        }
    },
    "references": [
        {
            "1": {
                "title": "Gpt-4 technical report.",
                "author": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.",
                "venue": "arXiv preprint arXiv:2303.08774, 2023.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Expert gate: Lifelong learning with a network of experts.",
                "author": "Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars.",
                "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 7120\u20137129, 2016.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Program synthesis with large language models.",
                "author": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton.",
                "venue": "ArXiv, abs/2108.07732, 2021.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Continual learning with neural networks: A review.",
                "author": "Abhijeet Awasthi and Sunita Sarawagi.",
                "venue": "In Proceedings of the ACM India Joint International Conference on Data Science and Management of Data, pages 362\u2013365, 2019.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Llemma: An open language model for mathematics.",
                "author": "Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck.",
                "venue": "ArXiv, abs/2310.10631, 2023.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Piqa: Reasoning about physical commonsense in natural language.",
                "author": "Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.",
                "venue": "In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432\u20137439, 2020.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Language models are few-shot learners.",
                "author": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.",
                "venue": "ArXiv, abs/2005.14165, 2020.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Evaluating large language models trained on code.",
                "author": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.",
                "venue": "ArXiv, abs/2107.03374, 2021.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Think you have solved question answering? Try ARC, the AI2 reasoning challenge.",
                "author": "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.",
                "venue": "arXiv preprint arXiv:1803.05457, 2018.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Training verifiers to solve math word problems.",
                "author": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.",
                "venue": "arXiv preprint arXiv:2110.14168, 2021.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models.",
                "author": "Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang.",
                "venue": "ArXiv, abs/2401.06066, 2024.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Diloco: Distributed low-communication training of language models.",
                "author": "Arthur Douillard, Qixuang Feng, Andrei A. Rusu, Rachita Chhaparia, Yani Donchev, Adhiguna Kuncoro, Marc\u2019Aurelio Ranzato, Arthur Szlam, and Jiajun Shen.",
                "venue": "ArXiv, abs/2311.08105, 2023.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.",
                "author": "William Fedus, Barret Zoph, and Noam Shazeer.",
                "venue": "The Journal of Machine Learning Research, 23(1):5232\u20135270, 2022.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Gemini: a family of highly capable multimodal models.",
                "author": "Gemini Team.",
                "venue": "arXiv preprint arXiv:2312.11805, 2023.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Demix layers: Disentangling domains for modular language modeling.",
                "author": "Suchin Gururangan, Michael Lewis, Ari Holtzman, Noah A. Smith, and Luke Zettlemoyer.",
                "venue": "In North American Chapter of the Association for Computational Linguistics, 2021.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Scaling expert language models with unsupervised domain discovery.",
                "author": "Suchin Gururangan, Margaret Li, Mike Lewis, Weijia Shi, Tim Althoff, Noah A Smith, and Luke Zettlemoyer.",
                "venue": "arXiv preprint arXiv:2303.14177, 2023.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Measuring massive multitask language understanding.",
                "author": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.",
                "venue": "In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Measuring mathematical problem solving with the math dataset.",
                "author": "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Xiaodong Song, and Jacob Steinhardt.",
                "venue": "ArXiv, abs/2103.03874, 2021b.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Adaptive mixtures of local experts.",
                "author": "Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton.",
                "venue": "Neural Computation, 3:79\u201387, 1991.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Categorical reparameterization with gumbel-softmax.",
                "author": "Eric Jang, Shixiang Gu, and Ben Poole.",
                "venue": "arXiv preprint arXiv:1611.01144, 2016.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Mixtral of experts.",
                "author": "Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u2019elio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed.",
                "venue": "ArXiv, abs/2401.04088, 2024.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.",
                "author": "Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer.",
                "venue": "ArXiv, abs/1705.03551, 2017.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Sparse upcycling: Training mixture-of-experts from dense checkpoints.",
                "author": "Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby.",
                "venue": "ArXiv, abs/2212.05055, 2022.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Natural questions: a benchmark for question answering research.",
                "author": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.",
                "venue": "Transactions of the Association of Computational Linguistics, 2019.",
                "url": null
            }
        },
        {
            "25": {
                "title": "A continual learning survey: Defying forgetting in classification tasks.",
                "author": "Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale\u0161 Leonardis, Gregory G. Slabaugh, and Tinne Tuytelaars.",
                "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 44:3366\u20133385, 2019.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Base layers: Simplifying training of large, sparse models.",
                "author": "Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer.",
                "venue": "In International Conference on Machine Learning, 2021.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Branch-train-merge: Embarrassingly parallel training of expert language models.",
                "author": "Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A. Smith, and Luke Zettlemoyer.",
                "venue": "ArXiv, abs/2208.03306, 2022a.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Competition-level code generation with alphacode.",
                "author": "Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom, Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de, Masson d\u2019Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey, Cherepanov, James Molloy, Daniel Jaymin Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de, Freitas, Koray Kavukcuoglu, and Oriol Vinyals.",
                "venue": "Science, 378:1092 \u2013 1097, 2022b.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Training language models to follow instructions with human feedback.",
                "author": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe.",
                "venue": "ArXiv, abs/2203.02155, 2022.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Hash layers for large sparse models.",
                "author": "Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston.",
                "venue": "In Neural Information Processing Systems, 2021.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Code llama: Open foundation models for code.",
                "author": "Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, I. Evtimov, Joanna Bitton, Manish P Bhatt, Cristian Cant\u00f3n Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u2019efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve.",
                "venue": "ArXiv, abs/2308.12950, 2023.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Progressive neural networks.",
                "author": "Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.",
                "venue": "ArXiv, abs/1606.04671, 2016.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Winogrande: An adversarial winograd schema challenge at scale.",
                "author": "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.",
                "venue": "Communications of the ACM, 64(9):99\u2013106, 2021.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Socialiqa: Commonsense reasoning about social interactions.",
                "author": "Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.",
                "venue": "arXiv preprint arXiv:1904.09728, 2019.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models.",
                "author": "Zhihong Shao, Peiyi Wang, Qihao Zhu, R. X. Xu, Jun-Mei Song, Mingchuan Zhang, Y. K. Li, Yu Wu, and Daya Guo.",
                "venue": "ArXiv, abs/2402.03300, 2024.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.",
                "author": "Noam M. Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean.",
                "venue": "ArXiv, abs/1701.06538, 2017.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Llama 2: Open foundation and fine-tuned chat models, 2023.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom.",
                "venue": null,
                "url": null
            }
        },
        {
            "38": {
                "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.",
                "author": "Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt.",
                "venue": "ArXiv, abs/2203.05482, 2022.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Openmoe: An early effort on open mixture-of-experts language models.",
                "author": "Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You.",
                "venue": "arXiv preprint arXiv:2402.01739, 2024.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Deep learning with elastic averaging sgd.",
                "author": "Sixin Zhang, Anna E Choromanska, and Yann LeCun.",
                "venue": "In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Opt: Open pre-trained transformer language models.",
                "author": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.",
                "venue": "ArXiv, abs/2205.01068, 2022.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Llama beyond english: An empirical study on language capability transfer.",
                "author": "Jun Zhao, Zhihao Zhang, Qi Zhang, Tao Gui, and Xuanjing Huang.",
                "venue": "arXiv preprint arXiv:2401.01055, 2024.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.07816v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.3",
            "4.3.1",
            "4.3.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.3",
            "4.3.1",
            "4.3.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.07816v1",
        "paper_title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM",
        "research_background": "### Paper's Motivation\n\nThe paper is motivated by the impressive performance of Large Language Models (LLMs) across various tasks, such as code generation, solving math problems, and multilinguality. However, training these models requires significant computational resources and data, with a major bottleneck being the cost of synchronization across multiple GPUs. Additionally, the current methods like Branch-Train-Merge (BTM), which improve training efficiency by creating independent expert models, lack a unified model that can undergo fine-tuning, a crucial step for performance enhancement and alignment. The motivation is to combine the best aspects of BTM and the Mixture-of-Experts (MoE) approach, mitigating their respective disadvantages to achieve a more efficient and effective training and inference process.\n\n### Research Problem\n\nThe research problem addressed in the paper is how to efficiently and effectively train LLMs by combining the embarrassingly parallel training from BTM and the parameter efficiency of MoE, while ensuring the final model is unified and can be fine-tuned or used as any other standard LLM. The goal is to reduce communication costs and increase training throughput without sacrificing the ability to perform supervised fine-tuning (SFT) or reinforcement learning from human feedback (RLHF), thereby improving the overall performance across multiple domains.\n\n### Relevant Prior Work\n\n1. **Large Language Models (LLMs)**: The paper builds on the substantial progress made by LLMs in various tasks (Brown et al., 2020; Touvron et al., 2023; Achiam et al., 2023), including code generation (Li et al., 2022b; Rozi\u00e8re et al., 2023), math problem-solving (Azerbayev et al., 2023), and multilinguality (Zhao et al., 2024).\n\n2. **Branch-Train-Merge (BTM)**: Proposed by Li et al., (2022a), BTM allows embarrassingly parallel training of LLMs without synchronization, leading to domain-specific expert models. However, it results in multiple independent models that cannot be fine-tuned further as a unified system (Ouyang et al., 2022).\n\n3. **Mixture-of-Experts (MoE)**: Initially introduced by Jacobs et al. (1991) and later adapted to LLMs (Shazeer et al., 2017; Fedus et al., 2022; Roller et al., 2021; Lewis et al., 2021), MoE activates only a subset of parameters at any given time, improving computational efficiency. However, MoE training requires synchronization of model parameters, adding communication overhead.\n\nThe paper proposes a novel solution called Branch-Train-MiX (BTX), which integrates the advantages of BTM and MoE by asynchronously training multiple expert LLMs and subsequently merging them into a unified MoE model. This approach aims to leverage the efficient training aspects of BTM and the parameter efficiency of MoE, thereby enhancing overall model performance and reducing computational costs.",
        "methodology": "### Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM\n\n#### Methodology:\nThe aim is to enhance an existing large language model (LLM), pretrained on a wide variety of topics, with improved performance in specific areas of expertise such as math or code. The process involves continued pretraining on domain-specific datasets. The methodology unfolds in three stages: Branch, Train, and Mix.\n\n##### Branch:\nThis stage deals with dividing the model into domain experts, each fine-tuned on a specific knowledge domain, to form branches. This pretraining focuses the experts on particular domains to build specialized capabilities.\n\n##### Train:\nDuring training, the model faces a common issue in Mixture-of-Experts (MoE) models known as \"dead experts,\" which are experts that the routing algorithm does not activate, resulting in these experts not receiving any training signal. Standard routing methods like Top-k are inadequate as they never select dead experts, ensuring these experts see no use.\n\nTo counteract this, load balancing is implemented by incorporating a loss term, akin to Fedus et al. (2022), which promotes equal utilization of experts. This loss term, computed in each layer and added to the Negative Log-Likelihood (NLL) loss, is designed as follows:\n- \\[L_{\\text{load_balancing}} = \\alpha \\sum{\\mathbf{1}} (E,\\mathbb{E}_i)\\]\n  Here, \\(\\mathbf{1}\\) represents the current data batch, and \\(\\alpha\\) is a hyperparameter.\n\n#### Routing Methods:\nBesides the traditional Top-k routing, several other methods are explored:\n- **Switch Routing**: This Top-1 routing method selects the expert with the highest score for each token, as proposed by Fedus et al. (2022).\n- **Soft Routing**: Here, softmax functions as the routing mechanism, activating all experts during both training and inference. While this method could yield the best performance, it requires more compute resources.\n- **Sample Top-1**: Utilizes gumbel softmax (Jang et al., 2016). During training, a soft sample from the gumbel softmax is generated, and all values are zeroed out except the largest. Only the corresponding expert is computed. At inference time, hard sampling is used, with the temperature annealed throughout training to reduce training-inference discrepancy.\n\n#### MiX:\nThe MoE layer's number of modules matches the number of domains, as each module represents one domain. However, the model can be expanded by splitting each domain's feed-forward (FF) sublayer into multiple chunks. Assuming \\(N\\) domains and an FF activation size of \\(D\\), each FF layer is split into \\(M\\) chunks with a dimension \\(D/M\\). Thus, the final MoE layer has \\(N \\times M\\) modules.\n\nA key innovation is the initialization of MoE experts:\n- Instead of one-to-one initialization from domain experts, a mixing approach is used: each domain expert\u2019s FF layers are split into chunks, and corresponding chunks from all domains are merged to build the MoE expert layers. Therefore, each MoE expert has parameters from all domains.\n  \nThis nuanced initialization method prevents the MoE experts from becoming uniformly activated across different domains, fostering domain specialization naturally instead of uniform activation.\n\n### Key Components and Innovations:\n1. **Branching into Domain Experts**: Splitting the pretrained model into specialized domain experts.\n2. **Load Balancing Techniques**: Using additional loss terms to ensure all experts get activated and trained evenly.\n3. **Innovative Routing Strategies**: Experimenting with Switch, Softmax, and Sample Top-1 routing methods to optimize the expert activation.\n4. **Enhanced MoE Layer Construction**:\n    - Matching modules to domains, with potential for expansion by splitting FF layers.\n    - Mixing domain parameters within each MoE expert to prevent rigid domain specialization.\n\nThis multi-faceted approach is designed to refine the performance of large LLMs across specific domains while overcoming common pitfalls in MoE models.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### Experiment Setup:\n\n1. **Datasets**:\n   - The study involves evaluating expert LLMs in diverse domains, including math, code, world knowledge, and commonsense reasoning.\n\n2. **Baselines**:\n   - **Seed Model**: Llama-2 7B \n   - **Expert Models**: Llemma 7B (math-focused) and CodeLlama 7B (code-focused)\n   - **Continued Pretraining Methods**: Dense upcycling (DM), sparse upcycling (DM), Branch-Train-Merge (BTM)\n   - **Larger Model**: Llama-2 13B\n\n3. **Evaluation Metrics**:\n   - Performance metrics across multiple tasks in various domains.\n   - Comparison includes individual domain performance, aggregated performance across domains, and efficiency in terms of training compute.\n\n#### Main Experimental Results:\n\n- **Specialization and Catastrophic Forgetting**:\n  - Expert LLMs perform best in their specialized domains but suffer from catastrophic forgetting in other domains. For instance, the math and code experts had worse performance on TriviaQA compared to the seed model.\n\n- **BTX Model Performance**:\n  - BTX models (both Sample Top-1 and Top-2) consistently improve performance on expert domains (math, coding, world knowledge) without regressing on tasks like commonsense reasoning.\n  - BTX (Top-2 experts) approaches the specialized models' performance in math and coding while significantly improving in non-specialized domains.\n\n- **Compute Efficiency**:\n  - BTX is more compute efficient than using the same training protocol throughout pretraining, achieving similar performance with less than half the additional training compute compared to Llama-2 13B.\n  - Training throughput is increased due to parallel expert training during the MoE stage.\n\n- **General Capabilities**:\n  - The MoE training stage, despite using a fraction of the total pretraining budget, brings substantial improvements in general capabilities.\n\nOverall, the results indicate that BTX offers a robust and compute-efficient method for continued pretraining, demonstrating substantial improvements in domain-specialized performance without compromising general capabilities."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate the effectiveness of different routing methods and design choices in the training process of the Branch-Train-MiX (BTX) model to optimize performance and efficiency.",
            "experiment_process": "Different routing methods (Switch, soft, and Top-2) were tested with varying amounts of active parameters without using load balancing. Various experimental settings were ablative, such as MoE training without load balancing, freezing of feedforward modules, and using blending and splitting techniques described in Section 3.3. The tests were conducted on tasks like coding (HumanEval) and math (GSM8K) to compare performance implications.",
            "result_discussion": "The Switch router exhibited subpar average performance, whereas soft routing, despite lacking sparsity, performed the best due to the highest number of active parameters. Top-2 routing provided an optimal balance between performance and efficiency. MoE training without load balancing was worse for coding but improved math accuracy. Freezing feedforward modules indicated that experts had acquired sufficient domain knowledge during branch training. Blending and splitting techniques did not improve performance, suggesting domain-specific FF layers cannot be mixed.",
            "ablation_id": "2403.07816v1.No1"
        },
        {
            "research_objective": "Evaluate the routing decisions among experts in various BTX model setups to understand load distribution and performance across different tasks.",
            "experiment_process": "Model evaluations were conducted on downstream tasks, examining token routing among experts with and without load balancing. Specific routing methods like Top-2 with load balancing were closely analyzed for token probability distributions and their impact on load distribution and task performance. The analysis compared the effects of including or excluding load balancing concerning task-specific expert contributions.",
            "result_discussion": "Top-2 routing with load balancing resulted in more uniform load distribution among experts. Without load balancing, models heavily relied on the Math expert, with minimal contribution from the Code expert. Load balancing revived the Code expert's contributions, making it dominant in math and code domains. The detailed token routing analysis showed domain-specific expert reliance, explaining performance trade-offs observed in Section 4.3.1, such as coding task improvements and math task performance degradation.",
            "ablation_id": "2403.07816v1.No2"
        }
    ]
}