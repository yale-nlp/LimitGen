{
    "title": "To Err Is Human, but Llamas Can Learn It Too",
    "abstract": "This study explores enhancing grammatical error correction (GEC) through artificial error generation (AEG) using language models (LMs). Specifically, we fine-tune Llama 2-based LMs for error generation and find that this approach yields synthetic errors akin to human errors. Next, we train GEC Llama models with the help of these artificial errors and outperform previous state-of-the-art error correction models, with gains ranging between 0.8 and 6 F points across all tested languages (German, Ukrainian, and Estonian). Moreover, we demonstrate that generating errors by fine-tuning smaller sequence-to-sequence models and prompting large commercial LMs (GPT-3.5 and GPT-4) also results in synthetic errors beneficially affecting error generation models.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The grammatical error correction (GEC) task aims to correct spelling and grammatical errors in the input text. The best-performing approaches to this task currently use neural networks (Junczys-Dowmunt et al., 2018  ###reference_b13###; Omelianchuk et al., 2020  ###reference_b25###; Rothe et al., 2021  ###reference_b30###, and several others), which are known to be data-hungry. At the same time, openly available human error correction data is severely limited even for high-resource languages like German, Arabic, and Czech Bryant et al. (2023  ###reference_b4###).\nThe lack of error correction data is commonly addressed through the creation of synthetic data, where errors are automatically added into correct sentences \u2013 also called artificial error generation (AEG). The most common approach to AEG is applying random probabilistic perturbation (deletion, insertion, replacement) of words and/or characters in the correct sentence Zhao et al. (2019  ###reference_b50###); Grundkiewicz et al. (2019  ###reference_b10###); Rothe et al. (2021  ###reference_b30###), alternatives include usage of intricate hand-crafted rules and confusion sets Rozovskaya and Roth (2010  ###reference_b31###); Xu et al. (2019  ###reference_b47###); Kara et al. (2023  ###reference_b14###); Bondarenko et al. (2023  ###reference_b1###) and automatically learning to generate errors Xie et al. (2018  ###reference_b46###); Kiyono et al. (2019  ###reference_b16###); Stahlberg and Kumar (2021  ###reference_b36###) \u2013 also referred to as back-translation (BT)***by analogy with the machine translation technique Sennrich et al. (2016  ###reference_b34###). However, to the best of our knowledge, none of the related work on AEG makes use of pre-trained foundation models.\nThis gap is precisely the focus of the present work: using pre-trained language models for synthetic error generation. We approach the task by fine-tuning open language models (LMs) that are based on Llama 2 Touvron et al. (2023  ###reference_b42###) and show that this can result in successful AEG results even when very limited amounts of human error data are available. Our analysis shows that the resulting errors are much more similar to natural human errors.\nWe also compare the approach to prompting commercial LMs (GPT-3.5 and GPT-4: OpenAI, 2023  ###reference_b26###) to perform AEG, as well as include other open models commonly employed for GEC and tune them for AEG: mT5 Rothe et al. (2021  ###reference_b30###); Palma Gomez et al. (2023  ###reference_b28###) and NLLB Luhtaru et al. (2024  ###reference_b20###). The details of our proposed methodology are given in Section 3  ###reference_###.\nOur final goal and evaluation setting is improving grammatical error correction for low-resource languages. In particular, we focus on German, Ukrainian, and Estonian GEC. For error correction, we also fine-tuned Llama 2 and compared it to the prompting of variants of GPT-4. Our experimental results show that Llama-based language models with fewer learned parameters can sometimes beat state-of-the-art results achieved with a bigger model. When pre-trained on our LM-generated synthetic errors, the resulting GEC models achieve the best current results on the included benchmarks in all three evaluated cases, including previous state-of-the-art and 4-shot GPT-4.\nIn summary, our contributions are as follows:\nWe show that pre-trained language models can be fine-tuned to generate high-quality synthetic errors.\nWe compare the influence of different models applied to AEG (LLama/GPT/mT5/NLLB) on subsequent GEC models.\nWe achieve new state-of-the-art GEC results across all tested languages with Llama 2-based models outperforming related work as well as GPT-4.\nThe paper is structured as follows. We outline related work in Section 2  ###reference_###, methodology experimental settings in Section 3  ###reference_###, and results in Section 4  ###reference_###. Additional questions on the same topic are discussed in Section 5  ###reference_### and the paper is concluded in Section 6  ###reference_###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "The use of synthetic data is a common concept in GEC. The first effective neural method proposed by Junczys-Dowmunt et al. (2018  ###reference_b13###) approaches GEC as low-resource Machine Translation (MT), making it a relatively resource-heavy method encouraging synthetic data generation. Over the years, there have been different approaches to deliberately introducing errors into monolingual text, like rule-based and probabilistic methods, methods based on confusion sets and error patterns, models trained for error generation and using round-trip translation Bryant et al. (2023  ###reference_b4###).\nOne widely adopted approach to generating synthetic data involves the probabilistic addition of errors to monolingual corpora. This technique encompasses inserting, deleting, substituting, or moving characters or words without considering the context, as described by Grundkiewicz et al. (2019  ###reference_b10###), Zhao et al. (2019  ###reference_b50###), and Rothe et al. (2021  ###reference_b30###). Additionally, Grundkiewicz et al. (2019  ###reference_b10###) introduced a \"reverse speller\"approach that suggests word replacements from confusion sets based on the speller\u2019s corrections. This method has been applied to several languages such as German, Czech, Russian, Ukrainian, Icelandic and Estonian N\u00e1plava and Straka (2019  ###reference_b21###); Trinh and Rozovskaya (2021  ###reference_b43###); N\u00e1plava et al. (2022  ###reference_b22###); Palma Gomez et al. (2023  ###reference_b28###); Ing\u00f3lfsd\u00f3ttir et al. (2023  ###reference_b12###); Luhtaru et al. (2024  ###reference_b20###).\nAs we show later, errors generated with the context-free probabilistic method differ from human errors and thus cover a much smaller number of error types, shown by significantly lower GEC recall.\nLearned methods of error generation typically require more resources. Before the widespread adoption of transformers and MT, various studies explored alternative approaches for training models for error generation. For instance, Felice and Yuan (2014  ###reference_b9###) and Rei et al. (2017  ###reference_b29###) utilized statistical machine translation to generate errors, while Xie et al. (2018  ###reference_b46###) and Yuan et al. (2019  ###reference_b49###) experimented with convolutional neural networks (CNNs) for this purpose. Additionally, Kasewa et al. (2018  ###reference_b15###) investigated using RNN-based sequence-to-sequence models with attention mechanisms.\nMoving towards more modern MT architectures, Htut and Tetreault (2019  ###reference_b11###) tested various model frameworks, including transformers, and Kiyono et al. (2019  ###reference_b16###) specifically employed transformer models. Both of the latter studies trained models from scratch, utilizing datasets ranging from approximately 500,000 to over a million error correction examples to train the artificial error generation system. In contrast, our work generates up to 1 million sentences with synthetic error while using between 9k and 33k human error sentences to fine-tune the base models.\nDuring the last few years, there has been no one error-generation method that has proved its superiority. It depends on language and available resources. For English Stahlberg and Kumar (2021  ###reference_b36###) train Seq2Edit models Stahlberg and Kumar (2020  ###reference_b35###) from scratch for learning to create diverse sets of errors. As mentioned in the beginning, synthetic probabilistic errors have found wide use for different languages. For instance, Ing\u00f3lfsd\u00f3ttir et al. (2023  ###reference_b12###) combine probabilistic character/word permutations with a rule-based approach for Icelandic and Kara et al. (2023  ###reference_b14###) curate special rules for generating Turkish data.\nIn addition, Oda (2023  ###reference_b24###) shows that generating new targets for synthetic datasets can be beneficial. Fang et al. (2023  ###reference_b8###) argue that translationese can be closer in domain to language learner\u2019s text than traditional monolingual corpora, which can cause domain mismatch problems.\nNext, we present the key methodological details of our work."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology and Experiments",
            "text": "The primary target of our work is to apply generative language models to artificial error generation (AEG) via fine-tuning. Additionally, we experiment with prompting large language models to perform the same task and include two seq2seq models that are fine-tuned to do the same.\nThe efficiency of proposed AEG solutions is evaluated using them to improve grammatical error correction (GEC). Thus, we also fine-tune generative LMs to perform the GEC task and compare the results to prompting-based GEC results and related work.\nThe general pipeline of our approach is straight-forward:\nFine-tune an LM to generate errors using human error data, with correct sentences as input and sentences with errors as output.\nApply that AEG LM to correct sentences in order to add a synthetically erroneous counterpart\nFine-tune an LM on that synthetic dataset to correct grammatical errors. Equivalent to Step 1, with the sentence pair direction reversed.\nContinue fine-tuning GEC LM on the (typically smaller) dataset with human errors.\nApply the models to the erroneous sentences of the benchmark test sets and evaluate the results\nNext, we describe the technical details of our implementation and the experimental setup."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Data",
            "text": "We use two distinct types of data in our work. Firstly, we rely on datasets containing examples of grammatical error corrections to train our error generation systems and correction models. Secondly, we incorporate monolingual data to create synthetic datasets by introducing errors. See an overview of used data in Table 1  ###reference_###.\nWe use the language learners\u2019 corpus from the University of Tartu (UT-L2 GEC) Rummo and Praakli (2017  ###reference_b32###) for gold data in Estonian. In Ukrainian, we use the UA-GEC corpus Syvokon et al. (2023  ###reference_b37###) used in the UNLP 2023 Shared Task on Grammatical Error Correction for Ukrainian Syvokon and Romanyshyn (2023  ###reference_b38###), using the GEC+Fluency data for training. For German, we rely on the widely used Falko-Merlin (FM) corpus Boyd (2018  ###reference_b2###).\nFor monolingual Estonian data, we employ the Estonian National Corpus 2021 Koppel and Kallas (2022  ###reference_b18###). We randomly sample equal sets from the latest Wikipedia, Web, and Fiction subsets and shuffle these together. For Ukrainian and German, we use the CC-100 dataset Conneau et al. (2020  ###reference_b5###); Wenzek et al. (2020  ###reference_b44###). Depending on the experiments, we sample the required number of sentences from the larger corpora (i.e., one million or 100 thousand sentences or a set equal to gold corpora sizes)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Models and Training",
            "text": "Llama-2-based models. We fine-tune models that have been enhanced with bilingual capabilities using continued pre-training from Llama-2-7B (Touvron et al., 2023  ###reference_b42###). For Estonian, we use Llammas-base\u2020\u2020\u2020huggingface.co/tartuNLP/Llammas-base  ###reference_se###, and for German, LeoLM\u2021\u2021\u2021huggingface.co/LeoLM/leo-hessianai-7b  ###reference_-7b###. For Ukrainian, we apply continued pre-training to replicate the conditions of Estonian LM by training with 5B tokens from CulturaX (Nguyen et al., 2023  ###reference_b23###) with 25% of the documents being in English and the rest in Ukrainian. For GEC and AEG fine-tuning, we formatted the training data with a prompt (see Table 8  ###reference_### and 9  ###reference_###) loosely based on Alpaca (Taori et al., 2023  ###reference_b40###). During fine-tuning, the loss is calculated on the tokens of the correct sentence.\nFine-tuning details (including hyperparameters) are discussed in Appendix B.1  ###reference_###.\nOther models we use are NLLB Team et al. (2022  ###reference_b41###) and mT5 Xue et al. (2021  ###reference_b48###). Specifically, we use the NLLB-200-1.3B-Distilled and mt5-large (1.2B parameter) models for our experiments and train NLLB models using Fairseq Ott et al. (2019  ###reference_b27###) and mT5 with HuggingFace Transformers Wolf et al. (2020  ###reference_b45###). When training in two stages, first with synthetic data and later with human errors, we keep the state of the learning rate scheduler, following the fine-tuning approach rather than retraining as defined by Grundkiewicz et al. (2019  ###reference_b10###). See Appendices B.2  ###reference_### and B.3  ###reference_### for further details."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Generation",
            "text": "Prompt engineering. We perform iterative prompt engineering, analyzing intermediate qualitative results and updating the prompt. For instance, we initially started with a simple 2-shot prompt (temperature = 0.1) asking GPT-3.5 to add grammatical and spelling mistakes into the input text but noticed that some error types were missing. We then improved the prompt by specifying the missing error types, adding two more examples, and upping the temperature. Our final prompt uses four examples and a model temperature of 1.0. See Appendix A for the prompts. We randomly pick the examples from each language\u2019s train set for few-shot prompting. When comparing the prompting between GPT-4-Turbo and GPT-3.5-Turbo, we use an identical random set of examples to ensure comparability.\n\nFinally, we converged on using GPT-3.5-turbo for more massive error generation (100,000 sentence pairs per language). The motivation for that is partially financial (as GPT-4/GPT-4-turbo are several times more expensive) as well as performance-driven (see Figure 1 and description for details).\n\nWe apply simple post-processing to the resulting set because, in some cases, parts from the prompt are duplicated in the output. If the model didn\u2019t generate a response due to safety model activation or the response was too short or too long compared to the input sentence, we replaced the output with the source text (equivalent to adding no errors).\n\nThe precise model versions we prompt are gpt-4-1106-preview for GPT-4-Turbo (using the OpenAI API) and gpt-3.5-turbo (GPT-3.5-Turbo) and gpt-4 (GPT-4) (using Azure OpenAI API, version 0613 for both)."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Evaluation",
            "text": "We evaluate the performance of our GEC models using test sets and evaluation metrics consistent with those employed in previous works (see datasets in Table 1  ###reference_###).\nFor Estonian, we evaluate our models using the Estonian learner language corpus (EstGEC-L2)\u00b6\u00b6\u00b6github.com/tlu-dt-nlp/EstGEC-L2-Corpus/  ###reference_rpus/###, alongside a modified version of the MaxMatch scorer\u2225\u2225\u2225github.com/TartuNLP/estgec/tree/main/\nM2_scorer_est  ###reference_in/M2_scorer_est###, following Luhtaru et al. (2024  ###reference_b20###). The Estonian scorer also outputs recall per error category, accounting for both other errors within the word order error scope and not accounting for these. We report the ones that do consider other errors separately. For Ukrainian, our evaluation methodology aligns with that of the UNLP 2023 Shared Task Syvokon and Romanyshyn (2023  ###reference_b38###), utilizing the CodaLab platform for submissions to a closed test set that uses the ERRANT scorer for evaluationBryant et al. (2017  ###reference_b3###). We follow the GEC+Fluency track setting since it encompasses a wider range of challenging errors. For German, we use the test set from the Falko-Merlin (FM) corpus Boyd (2018  ###reference_b2###) that several works have reported their scores on and the original MaxMatch scorer Dahlmeier and Ng (2012  ###reference_b7###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "In this section, we evaluate the performance of Llama-based models for GEC and AEG tasks. We then compare the AEG effectiveness between NLLB and mT5 models against Llama-based models to see if smaller, more efficient models can generate quality data. Separately, we assess AEG through prompting with GPT-3.5-turbo versus Llama models with trained error generation. Finally, we examine the quality of generated errors against human data and probabilistic reverse-speller errors."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Artificial Error Generation and Correction with Llama",
            "text": "We compare LLama-based language model (LM) fine-tuning error corrections across three configurations: (1) the baseline approach of training exclusively on human error GEC data, (2) the established related work approach of training on probabilistic reverse-speller AEG data and then continuing training with human error GEC data, and (3) our approach of training on back-translation style AEG data produced by fine-tuned Llama-based models first, followed by fine-tuning on human data.\nThe resulting scores are compared in Table 2  ###reference_###, along with previous state-of-the-art (SOTA) scores and results of GEC via 4-shot prompting of GPT-4/GPT-4-turbo. Results show that llama-based models, further enhanced through continued pre-training, exhibit strong correction capabilities across our study languages. Even without synthetic data, these models outperform current state-of-the-art (SOTA) methods in Estonian and Ukrainian error correction, and are not too far behind in German, trailing the best score by less than two points. However, it\u2019s important to note the discrepancy in model sizes for a fair comparison; our 7B Llama model significantly exceeds the NLLB-200-1.3B-Distilled model Team et al. (2022  ###reference_b41###) used for Estonian Luhtaru et al. (2024  ###reference_b20###) and the mBART model Tang et al. (2021  ###reference_b39###) for Ukrainian Bondarenko et al. (2023  ###reference_b1###) in size. At the same time, it is smaller than the 13B mT5-xxl model used for German Rothe et al. (2021  ###reference_b30###).\nIncorporating synthetic data as a preliminary step to fine-tuning significantly enhances performance across all languages and synthetic data types. Notably, our back-translation style synthetic data consistently delivers superior precision and recall compared to the probabilistic reverse-speller (or probabilistic) method. This approach results in a 2-2.4 point increase in the F score relative to solely using gold data for fine-tuning. Conversely, the gains from using probabilistic reverse-speller data are more modest, ranging from 0.6 to 1.5 points, highlighting the enhanced utility of our learned AEG errors.\nOur systems consistently outperform GPT-4 models regarding precision across all languages studied. However, GPT-4 models exhibit higher recall rates for Estonian and German. This discrepancy indicates that while our systems are more accurate in identifying correct instances, GPT-4 models better retrieve a broader range of relevant errors in these languages. On the other hand, the performance of GPT-4 models on the Ukrainian test set is notably lower compared to other methods and languages."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Artificial Error Generation with Smaller Models",
            "text": "Since error generation with 7B Llama-based models can be costly and time-consuming and many other architectures have proved useful for correction, we also explore smaller models for AEG: the 1.3B NLLB model and 1.2B mT5-large. The goal here is to see if these can also produce useful errors.\nTable 3  ###reference_### shows the results of the analysis. Both models can learn valuable information that improves performance beyond what is achieved with fine-tuning on gold data alone. Notably, errors generated by the NLLB model are particularly effective, delivering results close to those achieved by LM-generated errors in Estonian and German, almost matching the performance of LLama-based models. However, for Ukrainian, NLLB-generated errors fall behind probabilistic reverse-speller errors. The Ukrainian NLLB zero-shot GEC performance is also significantly lower than for Estonian or German (see more in Appendix C  ###reference_###) or English that Luhtaru et al. (2024  ###reference_b20###) also tested.\nThe mT5 models, in contrast, appear less adept at error generation. The errors produced by mT5 lag behind those from probabilistic reverse speller for Ukrainian and German and offer only a minimal improvement for Estonian.\nWe can also see that the scores before gold fine-tuning highlight that Ukrainian scores are notably low across all methods. However, these scores recover well after fine-tuning, suggesting the synthetic data may not align well with the text domain or error types specific to the Ukrainian language. Estonian and German models show higher scores for models trained with just AEG data and improve less drastically with fine-tuning."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Artificial Error Generation with Prompting",
            "text": "To assess the capability of generating errors without additional LM training, we utilize advanced commercial models, specifically exploring the efficiency of error generation through prompting GPT-3.5-turbo with datasets comprising 100,000 sentences. We later also explore the effectiveness of GPT-4-turbo in a more limited setting (see Section 4.4  ###reference_###).\nThe generation cost depends on the sum of input and completion tokens. Ukrainian, our most expensive language, had the highest number of tokens per 100,000 sentences: 98 million input and 12 million completion tokens. The cost for input tokens with GPT-3.5-Turbo in USD is $147, and for completion tokens, it is $25 \u2013 in total, $172 for generating 100,000 Ukrainian sentences. In comparison, the costs with GPT-4-Turbo would have been $983 and $370, respectively******openai.com/pricing  ###reference_openai.com/pricing###.\nTable 4  ###reference_### shows the results of continued pre-training Llama-based models on the same amount of sentences (100,000) with synthetic errors from prompting or fine-tuning. In terms of error correction quality after gold fine-tuning, employing GPT-3.5-turbo for prompting and fine-tuning Llama-2-based models are both viable strategies for artificial error generation, as they lead to very close F scores in all three languages (with a slight difference in favor of fine-tuning errors for German: 75.58 vs 76.25).\nAnalyzing the performance before gold fine-tuning reveals distinct differences between the two methods. For Estonian and German, recall rates are significantly higher with fine-tuning than prompting, though precision is slightly compromised. Conversely, Ukrainian exhibits the reverse pattern. However, it\u2019s important to note that any disparities observed before gold fine-tuning are greatly diminished after training on actual error correction examples. The most considerable remaining difference is under 0.7 points for German, with smaller discrepancies for Estonian and Ukrainian.\nWhen comparing LLama model scores for 100k to the ones with only gold tuning (see Table 2  ###reference_###), we can see that although scores increase more modestly, only 100k examples of synthetic data increase the scores more for German (almost 2 F-score points), a bit for Estonian (around 0.4 points) and stay the same for Ukrainian with higher precision and lower recall. This shows a possible text domain mismatch between the human error train/test data and our choice of monolingual sentences. This negative effect is alleviated with higher numbers of pretraining AEG data in the 1M sentence experiments."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Error Generation Quality",
            "text": "Finally, we run a direct comparison between human errors and artificial ones. To do so we train models using the same number of sentences as the respective human error set sizes: 19k sentence pairs for German, 33k for Ukrainian, and 9k sentence pairs for Estonian. We include comparing these models to ones based on one million probabilistic sentences.\nOur findings indicate that the precision of all synthetic data closely matches that of high-quality (gold) data in both Estonian and German, as illustrated in Figure 1  ###reference_###. A notable distinction, however, is observed in recall rates. For Estonian and German, the recall for errors generated by LMs is more comparable to human-generated (gold) data than errors produced through probabilistic methods.\nUkrainian scores with synthetic data are substantially worse than gold data, regardless of the AEG method. Still, recall for LM-generated errors is significantly higher than for simple probabilistic errors. This might be due to a larger mismatch in the text domain or error frequency. Ukrainian UA-GEC data predominantly contains punctuation errors (43%) and has a two times smaller error rate than German (8.2 vs 16.8) Syvokon et al. (2023  ###reference_b37###).\nComparing GPT-3.5-turbo with GPT-4-turbo, we find similar performance overall. However, for Estonian, GPT-4-turbo exhibits higher recall but lower precision. For German, GPT-4-turbo shows reductions in both precision and recall. Performance is nearly identical for Ukrainian between the two models. Overall, the F scores of GPT-4-turbo are slightly lower for Estonian and German and marginally higher for Ukrainian compared to GPT-3.5.\nWhen analyzing the recall for various error categories in Estonian, it is evident that our models trained with AEG data particularly face challenges in inserting missing punctuation marks and correcting errors related to word order, as depicted in Figure 2  ###reference_###. Errors generated probabilistically excel in identifying spelling mistakes and can correct certain errors in noun and verb forms. However, they generally perform poorly in addressing issues beyond spelling errors. This comparison suggests that our learned and prompted synthetic errors are much more similar to naturally made human errors."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "We investigated contemporary methods for generating artificial errors (AEG) for Estonian, German, and Ukrainian languages with relatively scarce resources. These languages have approximately 10k, 20k, and 30k error correction examples derived from corpora with varied error type distributions. The Estonian and Ukrainian corpora notably include language learner texts, characterized by a high frequency of errors, whereas the Ukrainian corpus contains more native-speaker texts with less edits.\nAcross these languages, our primary approach (AEG using Llama-based models and grammar error correction with Llama-based language models) demonstrated consistent efficacy after fine-tuning with error correction examples. This success underscores the value of the learned error generation method over the probabilistic reverse-speller approach, as evidenced by improved precision and recall based on reference metrics. The other methods \u2013 prompting and smaller models \u2013 also consistently prove useful.\nHowever, before fine-tuning with gold-standard GEC examples, we observed divergent language behaviors, raising questions about potential overfitting to these test sets and the generalizability of methods trained on specific datasets. For instance, our Ukrainian test set presented challenges for all methods lacking specific training data, including those involving GPT-4 models. It remains unclear whether methods that tend towards paraphrasing and fluency edits, including GPT models Coyne et al. (2023  ###reference_b6###), fail to align with the precise edits needed, overcorrect, or generate incorrect corrections. Critiques of current GEC metrics, which are argued to poorly correlate with human judgments Sakaguchi et al. (2016  ###reference_b33###); \u00d6stling et al. (2023  ###reference_b51###), suggest that true quality assessment may require human evaluation \u2014 a step beyond the scope of our study.\nThe observed performance with generated errors may also relate to mismatches between the chosen monolingual sentences and the original GEC human data. While our study utilizes web texts, resembling the essay-like texts typically employed in GEC and differing from native speaker constructions, these sentences might be simpler than those the model is accustomed to handling."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In conclusion, our research demonstrates the significant potential of Llama-based LMs in addressing the challenges of GEC for low-resource languages. We have successfully developed state-of-the-art systems for Estonian, Ukrainian, and German by leveraging these models as both correctors and synthetic data generators. We also explore other methods for AEG and show that prompting stronger commercial LMs is another way of generating high-quality data, and fine-tuning smaller models also has potential when the resources are more limited.\nPotential directions for future work include tuning LMs to perform AEG and GEC multilingually (like  Rothe et al., 2021  ###reference_b30###; Luhtaru et al., 2024  ###reference_b20###), applying our proposed AEG methods to monolingual data of a more similar text domain to the benchmarks Oda (2023  ###reference_b24###). An interesting direction would be to test these methods with high-resource GEC languages (English, Chinese)."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Our work focuses on three languages, recognizing that numerous other languages with grammar error correction (GEC) datasets exist outside our study\u2019s scope. We selected languages based on recent relevant research activities: Ukrainian due to its recent Shared Task; Estonian, a newly emerging language in GEC research; and German for comparison with a robust 13B model. To comprehensively validate our method, further exploration across additional languages is necessary.\nOur objective was not to devise the optimal system exhaustively. Therefore, several avenues remain unexplored, such as varying generation methods, testing different temperatures, and adjusting parameters. Moreover, we capped the generation of synthetic sentences at one million, below the volume utilized in many (though not all) synthetic data studies. Questions about the ideal amount of data needed its dependency on the quality of synthetic and gold examples, remain unanswered.\nFurthermore, our study lacks human evaluation, a component for more reliably assessing the real-world efficacy of GEC systems."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "\u0414\u043e\u0434\u0430\u0442\u043e\u043a A Prompts",
            "text": "We present the prompts used to generate 1) 100,000 sets with GPT-3.5-Turbo and 2) preliminary sets with GPT-4-Turbo in Tables 5  ###reference_###, 6  ###reference_###, 7  ###reference_### for Estonian, German, and Ukrainian respectively.\n###table_1### Muuda sisendteksti, genereerides sinna vigu, mida v\u00f5ib teha eesti keele \u00f5ppija. V\u00e4ljundtekstina tagasta sisendtekst, kuhu oled genereerinud vead. Sisendteksti genereeri \u00f5igekirja-, grammatika-, s\u00f5navaliku-, s\u00f5naj\u00e4rje-, kirjavahem\u00e4rgi- ning stiilivigu. Kui sisendtekstis on vigu, siis \u00e4ra neid paranda, vaid genereeri vigu juurde. \u00dclesande kohta on neli n\u00e4idet:\nSisendtekst: {correct}\nV\u00e4ljundtekst: {incorrect}\nSisendtekst: {correct}\nV\u00e4ljundtekst: {incorrect}\nSisendtekst: {correct}\nV\u00e4ljundtekst: {incorrect}\nSisendtekst: {correct}\nV\u00e4ljundtekst: {incorrect}\nSisendtekst: {input}\nV\u00e4ljundtekst:\n###table_2### Erzeugen Sie im Eingabetext Fehler, wie sie jemand, der Deutsch lernt, machen k\u00f6nnte. Geben Sie als Ausgabetext den Eingabetext zur\u00fcck, in den Sie Fehler eingef\u00fcgt haben. Erzeugen Sie Rechtschreib-, Grammatik-, Wortwahl-, Wortreihenfolge-, Zeichensetzungs- und Stilfehler im Eingabetext. Sollten im Eingabetext bereits Fehler vorhanden sein, korrigieren Sie diese nicht, sondern erzeugen Sie zus\u00e4tzliche Fehler. Es gibt vier Beispiele f\u00fcr die Aufgabe:\nEingabetext: {correct}\nAusgabetext: {incorrect}\nEingabetext: {correct}\nAusgabetext: {incorrect}\nEingabetext: {correct}\nAusgabetext: {incorrect}\nEingabetext: {correct}\nAusgabetext: {incorrect}\nEingabetext: {input}\nAusgabetext:\n###table_3### \u0417\u043ci\u043di\u0442\u044c \u0432\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442 \u0448\u043b\u044f\u0445\u043e\u043c \u0433\u0435\u043d\u0435\u0440\u0430\u0446i\u0457 \u0432 \u043d\u044c\u043e\u043c\u0443 \u043f\u043e\u043c\u0438\u043b\u043e\u043a, \u044f\u043ai \u043ci\u0433 \u0431\u0438 \u0437\u0440\u043e\u0431\u0438\u0442\u0438 \u0443\u0447\u0435\u043d\u044c, \u0449\u043e \u0432\u0438\u0432\u0447\u0430\u0454 \u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0443 \u043c\u043e\u0432\u0443. \u041d\u0430 \u0432\u0438\u0445\u043e\u0434i \u043f\u043e\u0432\u0435\u0440\u0442\u0430\u0439\u0442\u0435 \u0432\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442, \u0443 \u044f\u043a\u0438\u0439 \u0432\u0438 \u0432\u043d\u0435\u0441\u043b\u0438 \u043f\u043e\u043c\u0438\u043b\u043a\u0438. \u0423 \u0432\u0445i\u0434\u043d\u043e\u043c\u0443 \u0442\u0435\u043a\u0441\u0442i \u0433\u0435\u043d\u0435\u0440\u0443\u0439\u0442\u0435 \u043f\u043e\u043c\u0438\u043b\u043a\u0438 \u043f\u0440\u0430\u0432\u043e\u043f\u0438\u0441\u0443, \u0433\u0440\u0430\u043c\u0430\u0442\u0438\u043a\u0438, \u0432\u0438\u0431\u043e\u0440\u0443 \u0441\u043bi\u0432, \u043f\u043e\u0440\u044f\u0434\u043a\u0443 \u0441\u043bi\u0432, \u0440\u043e\u0437\u0434i\u043b\u043e\u0432\u0438\u0445 \u0437\u043d\u0430\u043ai\u0432 \u0442\u0430 \u0441\u0442\u0438\u043b\u044e. \u042f\u043a\u0449\u043e \u0443 \u0432\u0445i\u0434\u043d\u043e\u043c\u0443 \u0442\u0435\u043a\u0441\u0442i \u0454 \u043f\u043e\u043c\u0438\u043b\u043a\u0438, \u0442\u043e \u043d\u0435 \u0432\u0438\u043f\u0440\u0430\u0432\u043b\u044f\u0439\u0442\u0435 \u0457\u0445, \u0430 \u0433\u0435\u043d\u0435\u0440\u0443\u0439\u0442\u0435 \u0434\u043e\u0434\u0430\u0442\u043a\u043e\u0432i \u043f\u043e\u043c\u0438\u043b\u043a\u0438. \u0414\u0430\u043bi \u043d\u0430\u0432\u0435\u0434\u0435\u043di \u0447\u043e\u0442\u0438\u0440\u0438 \u043f\u0440\u0438\u043a\u043b\u0430\u0434\u0438 \u0434\u043e \u0446i\u0454\u0457 \u0437\u0430\u0434\u0430\u0447i\n\u0412\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: {correct}\n\u0412\u0438\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: {incorrect}\n\u0412\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: {correct}\n\u0412\u0438\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: {incorrect}\n\u0412\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: {correct}\n\u0412\u0438\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: {incorrect}\n\u0412\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: {correct}\n\u0412\u0438\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: {incorrect}\n\u0412\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: {input}\n\u0412\u0438\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442:\n###table_4### ### Instruction:\nReply with a corrected version of the input sentence in {language} with all grammatical and spelling errors fixed. If there are no errors, reply with a copy of the original sentence.\n### Input:\n### Response:\n###table_5### ### Instruction:\nReply with a grammatically incorrect version of the {language} input sentence.\n### Input:\n### Response:"
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "\u0414\u043e\u0434\u0430\u0442\u043e\u043a B Training details",
            "text": ""
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "\u0414\u043e\u0434\u0430\u0442\u043e\u043a C NLLB correction",
            "text": "The GEC performance of the NLLB model without any synthetic data is in Table 12  ###reference_###. The zero-shot results for Estonian and German are significantly higher than for Ukrainian. We notice that the Ukrainian dataset contains characters not present in NLLB vocabulary, like special quotation marks, which the normalization script unifies but appear as errors while testing. In addition, the Ukrainian test set contains far fewer edits, which, especially in a zero-shot scenario, means worse scores because NLLB paraphrases more rigorously Luhtaru et al. (2024  ###reference_b20###)."
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" id=\"S3.T1.1.1.1.1\">Corpus</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.1.1.1.2\">Language</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.1.1.1.3\">Train</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.1.1.1.4\">Test</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S3.T1.1.2.1.1\">UT-L2 GEC</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.1.2.1.2\">ET</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.1.2.1.3\">8,935</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.1.2.1.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.3.2.1\">EstGEC-L2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.3.2.2\">ET</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.3.2.3\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.3.2.4\">2,029</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.4.3.1\">UA-GEC</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.4.3.2\">UK</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.4.3.3\">31,038</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.4.3.4\">1,271</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.5.4.1\">FM</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.5.4.2\">DE</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.5.4.3\">19,237</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.5.4.4\">2,337</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.6.5.1\">ENC 2021</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.6.5.2\">ET</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.6.5.3\">1M/100k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.6.5.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T1.1.7.6.1\">CC-100</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.7.6.2\">UK/DE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.7.6.3\">1M/100k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.7.6.4\">-</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">\u0422\u0430\u0431\u043b. 1: </span>Data used for training and testing.</figcaption>\n</figure>",
            "capture": "\u0422\u0430\u0431\u043b. 1: Data used for training and testing."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.3\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.3.4.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S3.T2.3.4.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T2.3.4.1.1.1\">Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S3.T2.3.4.1.2\">Estonian</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S3.T2.3.4.1.3\">Ukrainian</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S3.T2.3.4.1.4\">German</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.3.4\">P</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.3.5\">R</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.1\">F\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.3.6\">P</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.3.7\">R</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.2.2.2\">F\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.3.8\">P</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.3.9\">R</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.3.3\">F\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.5.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.3.5.2.1\">GPT-4-turbo (4-shot)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.5.2.2\">70.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.5.2.3\">57.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.5.2.4\">67.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.5.2.5\">39.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.5.2.6\">42.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.5.2.7\">40.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.5.2.8\">64.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.5.2.9\">69.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.5.2.10\">65.12</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.6.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.3.6.3.1\">GPT-4 (4-shot)</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.6.3.2\">70.04</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.6.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.6.3.3.1\">59.03</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.6.3.4\">67.52</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.6.3.5\">36.25</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.6.3.6\">37.77</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.6.3.7\">36.54</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.6.3.8\">65.22</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.6.3.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.6.3.9.1\">69.75</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.6.3.10\">66.08</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.7.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.3.7.4.1\">Old SOTA (rel. work)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.7.4.2\">71.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.7.4.3\">55.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.7.4.4\">67.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.7.4.5\">79.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.7.4.6\">43.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.7.4.7\">68.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.7.4.8\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.7.4.9\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.7.4.10\">75.96</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.8.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.3.8.5.1\">Llama + gold</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.8.5.2\">71.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.8.5.3\">55.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.8.5.4\">67.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.8.5.5\">79.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.8.5.6\">51.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.8.5.7\">72.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.8.5.8\">76.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.8.5.9\">65.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.8.5.10\">74.31</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.9.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.3.9.6.1\">Llama + 1M prob + gold</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.9.6.2\">72.59</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.9.6.3\">54.72</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.9.6.4\">68.14</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.9.6.5\">80.37</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.9.6.6\">53.19</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.9.6.7\">72.92</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.9.6.8\">78.22</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.9.6.9\">67.65</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.9.6.10\">75.85</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.10.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T2.3.10.7.1\">Llama + 1M BT + gold</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.3.10.7.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.10.7.2.1\">73.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.3.10.7.3\">57.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.3.10.7.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.10.7.4.1\">69.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.3.10.7.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.10.7.5.1\">82.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.3.10.7.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.10.7.6.1\">53.41</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.3.10.7.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.10.7.7.1\">74.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.3.10.7.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.10.7.8.1\">79.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.3.10.7.9\">68.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.3.10.7.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.10.7.10.1\">76.75</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">\u0422\u0430\u0431\u043b. 2: </span>Comparison of Llama 2-based models (denoted as Llama) after extended pre-training and GEC fine-tuning: Models without synthetic data (LLM + gold) versus models with synthetic data generated with a probabilistic reverse-speller method (LLM + 1M prob + gold) and back-translation style learned synthetic data (LLM + 1M BT + gold). State-of-the-art benchmarks include <cite class=\"ltx_cite ltx_citemacro_citet\">Luhtaru et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.05493v1#bib.bib20\" title=\"\">2024</a>)</cite> for Estonian (NLLB-200-1.3B-Distilled with mixed synthetic and translation data training), <cite class=\"ltx_cite ltx_citemacro_citet\">Bondarenko et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.05493v1#bib.bib1\" title=\"\">2023</a>)</cite> for Ukrainian (mBART-based model with synthetic data), and <cite class=\"ltx_cite ltx_citemacro_citet\">Rothe et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.05493v1#bib.bib30\" title=\"\">2021</a>)</cite> for German (mT5 xxl with multilingual synthetic data and GEC fine-tuning).</figcaption>\n</figure>",
            "capture": "\u0422\u0430\u0431\u043b. 2: Comparison of Llama 2-based models (denoted as Llama) after extended pre-training and GEC fine-tuning: Models without synthetic data (LLM + gold) versus models with synthetic data generated with a probabilistic reverse-speller method (LLM + 1M prob + gold) and back-translation style learned synthetic data (LLM + 1M BT + gold). State-of-the-art benchmarks include Luhtaru et\u00a0al. (2024) for Estonian (NLLB-200-1.3B-Distilled with mixed synthetic and translation data training), Bondarenko et\u00a0al. (2023) for Ukrainian (mBART-based model with synthetic data), and Rothe et\u00a0al. (2021) for German (mT5 xxl with multilingual synthetic data and GEC fine-tuning)."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T3.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T3.3.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T3.3.1.1.1\">Lang/Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.3.1.1.2\">Llama</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.3.1.1.3\">NLLB</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.3.1.1.4\">mT5</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T3.3.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T3.3.2.1.1\">ET (AEG only)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.3.2.1.2\">65.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.3.2.1.3\">65.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.3.2.1.4\">59.40</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.3.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T3.3.3.2.1\">ET (AEG + gold)</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.3.2.2\">69.97</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.3.2.3\">69.73</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.3.2.4\">68.57</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.3.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T3.3.4.3.1\">UK (AEG only)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.3.4.3.2\">28.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.3.4.3.3\">27.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.3.4.3.4\">16.79</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.3.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T3.3.5.4.1\">UK (AEG + gold)</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.5.4.2\">74.09</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.5.4.3\">72.30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.5.4.4\">72.51</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.3.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T3.3.6.5.1\">DE (AEG only)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.3.6.5.2\">71.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.3.6.5.3\">69.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.3.6.5.4\">54.96</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.3.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T3.3.7.6.1\">DE (AEG + gold)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.3.7.6.2\">76.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.3.7.6.3\">76.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.3.7.6.4\">74.77</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">\u0422\u0430\u0431\u043b. 3: </span>F-scores for Llama-based models fine-tuned with 1M sentences generated with different AEG models and then further fine-tuned with gold GEC data. The errors are generated with 7B Llama-2-based models, 1.3B NLLB model and 1.2B mT5 model.</figcaption>\n</figure>",
            "capture": "\u0422\u0430\u0431\u043b. 3: F-scores for Llama-based models fine-tuned with 1M sentences generated with different AEG models and then further fine-tuned with gold GEC data. The errors are generated with 7B Llama-2-based models, 1.3B NLLB model and 1.2B mT5 model."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.2.3.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"S4.T4.2.3.1.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S4.T4.2.3.1.2\">Prompting</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S4.T4.2.3.1.3\">Fine-tuning</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T4.2.4.2.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T4.2.4.2.1.1\">Lang/Model</span></th>\n<td class=\"ltx_td ltx_align_center\" colspan=\"3\" id=\"S4.T4.2.4.2.2\">GPT-3.5-turbo (100k)</td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"3\" id=\"S4.T4.2.4.2.3\">Llama (100k)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.2.3\">P</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.2.4\">R</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.1.1\">F\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.2.5\">P</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.2.6\">R</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.2.2\">F\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T4.2.5.3.1\">ET (AEG only)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.5.3.2\">71.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.5.3.3\">44.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.5.3.4\">63.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.5.3.5\">67.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.5.3.6\">50.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.5.3.7\">63.41</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T4.2.6.4.1\">ET (AEG + gold)</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.6.4.2\">71.11</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.6.4.3\">56.56</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.6.4.4\">67.63</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.6.4.5\">71.51</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.6.4.6\">56.51</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.6.4.7\">67.91</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T4.2.7.5.1\">UK (AEG only)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.7.5.2\">28.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.7.5.3\">22.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.7.5.4\">27.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.7.5.5\">40.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.7.5.6\">19.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.7.5.7\">33.26</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T4.2.8.6.1\">UK (AEG + gold)</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.8.6.2\">80.82</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.8.6.3\">51.33</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.8.6.4\">72.49</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.8.6.5\">80.89</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.8.6.6\">50.31</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.8.6.7\">72.12</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T4.2.9.7.1\">DE (AEG only)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.9.7.2\">70.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.9.7.3\">49.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.9.7.4\">65.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.9.7.5\">70.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.9.7.6\">59.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.9.7.7\">67.56</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.10.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T4.2.10.8.1\">DE (AEG + gold)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.2.10.8.2\">78.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.2.10.8.3\">67.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.2.10.8.4\">75.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.2.10.8.5\">78.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.2.10.8.6\">67.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.2.10.8.7\">76.25</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">\u0422\u0430\u0431\u043b. 4: </span>Scores of Llama-based models fine-tuned with 100k sentences generated by Llama-based model fine-tuned for error generation and GPT-3.5-model prompted to add errors.</figcaption>\n</figure>",
            "capture": "\u0422\u0430\u0431\u043b. 4: Scores of Llama-based models fine-tuned with 100k sentences generated by Llama-based model fine-tuned for error generation and GPT-3.5-model prompted to add errors."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A1.T5.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T5.1.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"A1.T5.1.1.1.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T5.1.1.1.1.1\">Muuda sisendteksti, genereerides sinna vigu, mida v\u00f5ib teha eesti keele \u00f5ppija. V\u00e4ljundtekstina tagasta sisendtekst, kuhu oled genereerinud vead. Sisendteksti genereeri \u00f5igekirja-, grammatika-, s\u00f5navaliku-, s\u00f5naj\u00e4rje-, kirjavahem\u00e4rgi- ning stiilivigu. Kui sisendtekstis on vigu, siis \u00e4ra neid paranda, vaid genereeri vigu juurde. \u00dclesande kohta on neli n\u00e4idet:</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.2.2\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.1.2.2.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T5.1.2.2.1.1\">Sisendtekst: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T5.1.2.2.1.1.1\">{correct}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.3.3\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.1.3.3.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T5.1.3.3.1.1\">V\u00e4ljundtekst: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T5.1.3.3.1.1.1\">{incorrect}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.4.4\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.1.4.4.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T5.1.4.4.1.1\">Sisendtekst: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T5.1.4.4.1.1.1\">{correct}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.5.5\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.1.5.5.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T5.1.5.5.1.1\">V\u00e4ljundtekst: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T5.1.5.5.1.1.1\">{incorrect}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.6.6\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.1.6.6.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T5.1.6.6.1.1\">Sisendtekst: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T5.1.6.6.1.1.1\">{correct}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.7.7\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.1.7.7.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T5.1.7.7.1.1\">V\u00e4ljundtekst: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T5.1.7.7.1.1.1\">{incorrect}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.8.8\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.1.8.8.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T5.1.8.8.1.1\">Sisendtekst: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T5.1.8.8.1.1.1\">{correct}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.9.9\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.1.9.9.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T5.1.9.9.1.1\">V\u00e4ljundtekst: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T5.1.9.9.1.1.1\">{incorrect}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.10.10\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T5.1.10.10.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T5.1.10.10.1.1\">Sisendtekst: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T5.1.10.10.1.1.1\">{input}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.11.11\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"A1.T5.1.11.11.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T5.1.11.11.1.1\">V\u00e4ljundtekst:</p>\n</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">\u0422\u0430\u0431\u043b. 5: </span>GPT prompt - Estonian</figcaption>\n</figure>",
            "capture": "\u0422\u0430\u0431\u043b. 5: GPT prompt - Estonian"
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T6\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A1.T6.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T6.1.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"A1.T6.1.1.1.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T6.1.1.1.1.1\">Erzeugen Sie im Eingabetext Fehler, wie sie jemand, der Deutsch lernt, machen k\u00f6nnte. Geben Sie als Ausgabetext den Eingabetext zur\u00fcck, in den Sie Fehler eingef\u00fcgt haben. Erzeugen Sie Rechtschreib-, Grammatik-, Wortwahl-, Wortreihenfolge-, Zeichensetzungs- und Stilfehler im Eingabetext. Sollten im Eingabetext bereits Fehler vorhanden sein, korrigieren Sie diese nicht, sondern erzeugen Sie zus\u00e4tzliche Fehler. Es gibt vier Beispiele f\u00fcr die Aufgabe:</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.2.2\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T6.1.2.2.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T6.1.2.2.1.1\">Eingabetext: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T6.1.2.2.1.1.1\">{correct}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.3.3\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T6.1.3.3.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T6.1.3.3.1.1\">Ausgabetext: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T6.1.3.3.1.1.1\">{incorrect}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.4.4\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T6.1.4.4.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T6.1.4.4.1.1\">Eingabetext: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T6.1.4.4.1.1.1\">{correct}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.5.5\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T6.1.5.5.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T6.1.5.5.1.1\">Ausgabetext: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T6.1.5.5.1.1.1\">{incorrect}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.6.6\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T6.1.6.6.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T6.1.6.6.1.1\">Eingabetext: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T6.1.6.6.1.1.1\">{correct}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.7.7\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T6.1.7.7.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T6.1.7.7.1.1\">Ausgabetext: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T6.1.7.7.1.1.1\">{incorrect}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.8.8\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T6.1.8.8.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T6.1.8.8.1.1\">Eingabetext: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T6.1.8.8.1.1.1\">{correct}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.9.9\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T6.1.9.9.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T6.1.9.9.1.1\">Ausgabetext: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T6.1.9.9.1.1.1\">{incorrect}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.10.10\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T6.1.10.10.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T6.1.10.10.1.1\">Eingabetext: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T6.1.10.10.1.1.1\">{input}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.11.11\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"A1.T6.1.11.11.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T6.1.11.11.1.1\">Ausgabetext:</p>\n</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">\u0422\u0430\u0431\u043b. 6: </span>GPT prompt - German</figcaption>\n</figure>",
            "capture": "\u0422\u0430\u0431\u043b. 6: GPT prompt - German"
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T7\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A1.T7.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T7.1.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"A1.T7.1.1.1.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T7.1.1.1.1.1\">\u0417\u043ci\u043di\u0442\u044c \u0432\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442 \u0448\u043b\u044f\u0445\u043e\u043c \u0433\u0435\u043d\u0435\u0440\u0430\u0446i\u0457 \u0432 \u043d\u044c\u043e\u043c\u0443 \u043f\u043e\u043c\u0438\u043b\u043e\u043a, \u044f\u043ai \u043ci\u0433 \u0431\u0438 \u0437\u0440\u043e\u0431\u0438\u0442\u0438 \u0443\u0447\u0435\u043d\u044c, \u0449\u043e \u0432\u0438\u0432\u0447\u0430\u0454 \u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0443 \u043c\u043e\u0432\u0443. \u041d\u0430 \u0432\u0438\u0445\u043e\u0434i \u043f\u043e\u0432\u0435\u0440\u0442\u0430\u0439\u0442\u0435 \u0432\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442, \u0443 \u044f\u043a\u0438\u0439 \u0432\u0438 \u0432\u043d\u0435\u0441\u043b\u0438 \u043f\u043e\u043c\u0438\u043b\u043a\u0438. \u0423 \u0432\u0445i\u0434\u043d\u043e\u043c\u0443 \u0442\u0435\u043a\u0441\u0442i \u0433\u0435\u043d\u0435\u0440\u0443\u0439\u0442\u0435 \u043f\u043e\u043c\u0438\u043b\u043a\u0438 \u043f\u0440\u0430\u0432\u043e\u043f\u0438\u0441\u0443, \u0433\u0440\u0430\u043c\u0430\u0442\u0438\u043a\u0438, \u0432\u0438\u0431\u043e\u0440\u0443 \u0441\u043bi\u0432, \u043f\u043e\u0440\u044f\u0434\u043a\u0443 \u0441\u043bi\u0432, \u0440\u043e\u0437\u0434i\u043b\u043e\u0432\u0438\u0445 \u0437\u043d\u0430\u043ai\u0432 \u0442\u0430 \u0441\u0442\u0438\u043b\u044e. \u042f\u043a\u0449\u043e \u0443 \u0432\u0445i\u0434\u043d\u043e\u043c\u0443 \u0442\u0435\u043a\u0441\u0442i \u0454 \u043f\u043e\u043c\u0438\u043b\u043a\u0438, \u0442\u043e \u043d\u0435 \u0432\u0438\u043f\u0440\u0430\u0432\u043b\u044f\u0439\u0442\u0435 \u0457\u0445, \u0430 \u0433\u0435\u043d\u0435\u0440\u0443\u0439\u0442\u0435 \u0434\u043e\u0434\u0430\u0442\u043a\u043e\u0432i \u043f\u043e\u043c\u0438\u043b\u043a\u0438. \u0414\u0430\u043bi \u043d\u0430\u0432\u0435\u0434\u0435\u043di \u0447\u043e\u0442\u0438\u0440\u0438 \u043f\u0440\u0438\u043a\u043b\u0430\u0434\u0438 \u0434\u043e \u0446i\u0454\u0457 \u0437\u0430\u0434\u0430\u0447i</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T7.1.2.2\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T7.1.2.2.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T7.1.2.2.1.1\">\u0412\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T7.1.2.2.1.1.1\">{correct}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T7.1.3.3\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T7.1.3.3.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T7.1.3.3.1.1\">\u0412\u0438\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T7.1.3.3.1.1.1\">{incorrect}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T7.1.4.4\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T7.1.4.4.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T7.1.4.4.1.1\">\u0412\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T7.1.4.4.1.1.1\">{correct}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T7.1.5.5\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T7.1.5.5.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T7.1.5.5.1.1\">\u0412\u0438\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T7.1.5.5.1.1.1\">{incorrect}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T7.1.6.6\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T7.1.6.6.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T7.1.6.6.1.1\">\u0412\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T7.1.6.6.1.1.1\">{correct}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T7.1.7.7\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T7.1.7.7.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T7.1.7.7.1.1\">\u0412\u0438\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T7.1.7.7.1.1.1\">{incorrect}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T7.1.8.8\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T7.1.8.8.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T7.1.8.8.1.1\">\u0412\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T7.1.8.8.1.1.1\">{correct}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T7.1.9.9\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T7.1.9.9.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T7.1.9.9.1.1\">\u0412\u0438\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T7.1.9.9.1.1.1\">{incorrect}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T7.1.10.10\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T7.1.10.10.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T7.1.10.10.1.1\">\u0412\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T7.1.10.10.1.1.1\">{input}</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T7.1.11.11\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"A1.T7.1.11.11.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T7.1.11.11.1.1\">\u0412\u0438\u0445i\u0434\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442:</p>\n</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">\u0422\u0430\u0431\u043b. 7: </span>GPT prompt - Ukrainian </figcaption>\n</figure>",
            "capture": "\u0422\u0430\u0431\u043b. 7: GPT prompt - Ukrainian "
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T8\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A1.T8.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T8.1.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"A1.T8.1.1.1.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.1.1.1.1.1\">### Instruction:</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.1.2.2\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T8.1.2.2.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.1.2.2.1.1\">Reply with a corrected version of the input sentence in <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T8.1.2.2.1.1.1\">{language}</span> with all grammatical and spelling errors fixed. If there are no errors, reply with a copy of the original sentence.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.1.3.3\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T8.1.3.3.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.1.3.3.1.1\">### Input:</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.1.4.4\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T8.1.4.4.1\" style=\"width:433.6pt;\"><span class=\"ltx_text ltx_font_typewriter ltx_align_top\" id=\"A1.T8.1.4.4.1.1\">{input}</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.1.5.5\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T8.1.5.5.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.1.5.5.1.1\">### Response:</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.1.6.6\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"A1.T8.1.6.6.1\" style=\"width:433.6pt;\"><span class=\"ltx_text ltx_font_typewriter ltx_align_top\" id=\"A1.T8.1.6.6.1.1\">{correction}</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">\u0422\u0430\u0431\u043b. 8: </span>Llama-based model GEC instruction format loosely based on Alpaca <cite class=\"ltx_cite ltx_citemacro_citep\">(Taori et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.05493v1#bib.bib40\" title=\"\">2023</a>)</cite>. The instruction is based on <cite class=\"ltx_cite ltx_citemacro_citet\">Coyne et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.05493v1#bib.bib6\" title=\"\">2023</a>)</cite>.</figcaption>\n</figure>",
            "capture": "\u0422\u0430\u0431\u043b. 8: Llama-based model GEC instruction format loosely based on Alpaca (Taori et\u00a0al., 2023). The instruction is based on Coyne et\u00a0al. (2023)."
        },
        "9": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T9\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A1.T9.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T9.1.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"A1.T9.1.1.1.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T9.1.1.1.1.1\">### Instruction:</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T9.1.2.2\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T9.1.2.2.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T9.1.2.2.1.1\">Reply with a grammatically incorrect version of the <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T9.1.2.2.1.1.1\">{language}</span> input sentence.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T9.1.3.3\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T9.1.3.3.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T9.1.3.3.1.1\">### Input:</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T9.1.4.4\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T9.1.4.4.1\" style=\"width:433.6pt;\"><span class=\"ltx_text ltx_font_typewriter ltx_align_top\" id=\"A1.T9.1.4.4.1.1\">{input}</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T9.1.5.5\">\n<td class=\"ltx_td ltx_align_justify\" id=\"A1.T9.1.5.5.1\" style=\"width:433.6pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T9.1.5.5.1.1\">### Response:</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T9.1.6.6\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"A1.T9.1.6.6.1\" style=\"width:433.6pt;\"><span class=\"ltx_text ltx_font_typewriter ltx_align_top\" id=\"A1.T9.1.6.6.1.1\">{correction}</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">\u0422\u0430\u0431\u043b. 9: </span>Llama-based model AEG instruction format loosely based on Alpaca <cite class=\"ltx_cite ltx_citemacro_citep\">(Taori et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.05493v1#bib.bib40\" title=\"\">2023</a>)</cite>.</figcaption>\n</figure>",
            "capture": "\u0422\u0430\u0431\u043b. 9: Llama-based model AEG instruction format loosely based on Alpaca (Taori et\u00a0al., 2023)."
        },
        "10": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T10\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A2.T10.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T10.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"A2.T10.1.1.1.1\">Parameter</th>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"A2.T10.1.1.1.2\">Value</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T10.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A2.T10.1.2.2.1\">LR</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T10.1.2.2.2\">5e-6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T10.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T10.1.3.3.1\">LR<sub class=\"ltx_sub\" id=\"A2.T10.1.3.3.1.1\">final</sub>\n</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A2.T10.1.3.3.2\">5e-7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T10.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T10.1.4.4.1\">LR-schedule</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A2.T10.1.4.4.2\">linear</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T10.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T10.1.5.5.1\">Epochs</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A2.T10.1.5.5.2\">3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T10.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T10.1.6.6.1\">Max sequence length</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A2.T10.1.6.6.2\">1024</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T10.1.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T10.1.7.7.1\">Batch size (total)</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A2.T10.1.7.7.2\">128</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T10.1.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T10.1.8.8.1\">Gradient clipping</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A2.T10.1.8.8.2\">1.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T10.1.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T10.1.9.9.1\">Weight decay</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A2.T10.1.9.9.2\">0.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T10.1.10.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T10.1.10.10.1\">Optimizer</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A2.T10.1.10.10.2\">AdamW</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T10.1.11.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T10.1.11.11.1\">Precision</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A2.T10.1.11.11.2\">bf16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T10.1.12.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A2.T10.1.12.12.1\">DeepSpeed</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A2.T10.1.12.12.2\">Zero Stage 2</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">\u0422\u0430\u0431\u043b. 10: </span>Llama-based GEC model fine-tuning parameters.</figcaption>\n</figure>",
            "capture": "\u0422\u0430\u0431\u043b. 10: Llama-based GEC model fine-tuning parameters."
        },
        "11": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T11\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A2.T11.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T11.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"A2.T11.1.1.1.1\">Parameter</th>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"A2.T11.1.1.1.2\">Value</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T11.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A2.T11.1.2.2.1\">LR</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T11.1.2.2.2\">2e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T11.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T11.1.3.3.1\">LR<sub class=\"ltx_sub\" id=\"A2.T11.1.3.3.1.1\">final</sub>\n</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A2.T11.1.3.3.2\">2e-6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T11.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T11.1.4.4.1\">LR-schedule</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A2.T11.1.4.4.2\">linear</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T11.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T11.1.5.5.1\">Updates</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A2.T11.1.5.5.2\">19080</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T11.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T11.1.6.6.1\">Max sequence length</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A2.T11.1.6.6.2\">1024</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T11.1.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T11.1.7.7.1\">Batch size (total)</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A2.T11.1.7.7.2\">256</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T11.1.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T11.1.8.8.1\">Gradient clipping</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A2.T11.1.8.8.2\">1.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T11.1.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T11.1.9.9.1\">Weight decay</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A2.T11.1.9.9.2\">0.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T11.1.10.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T11.1.10.10.1\">Optimizer</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A2.T11.1.10.10.2\">AdamW</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T11.1.11.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T11.1.11.11.1\">Precision</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A2.T11.1.11.11.2\">bf16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T11.1.12.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A2.T11.1.12.12.1\">DeepSpeed</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A2.T11.1.12.12.2\">Zero Stage 2</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">\u0422\u0430\u0431\u043b. 11: </span>Llama continued pre-training parameters.</figcaption>\n</figure>",
            "capture": "\u0422\u0430\u0431\u043b. 11: Llama continued pre-training parameters."
        },
        "12": {
            "table_html": "<figure class=\"ltx_table\" id=\"A3.T12\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A3.T12.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A3.T12.2.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"A3.T12.2.3.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"A3.T12.2.3.1.1.1\">Lang</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"A3.T12.2.3.1.2\">Zero-shot</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"A3.T12.2.3.1.3\">Gold fine-tuning</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T12.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A3.T12.2.2.3\">P</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A3.T12.2.2.4\">R</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A3.T12.1.1.1\">F\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A3.T12.2.2.5\">P</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A3.T12.2.2.6\">R</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A3.T12.2.2.2\">F\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A3.T12.2.4.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A3.T12.2.4.1.1\">Estonian</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T12.2.4.1.2\">43.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T12.2.4.1.3\">45.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T12.2.4.1.4\">44.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T12.2.4.1.5\">61.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T12.2.4.1.6\">49.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T12.2.4.1.7\">58.39</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T12.2.5.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A3.T12.2.5.2.1\">Ukrainian</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T12.2.5.2.2\">8.24</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T12.2.5.2.3\">31.57</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T12.2.5.2.4\">9.67</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T12.2.5.2.5\">35.62</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T12.2.5.2.6\">34.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T12.2.5.2.7\">35.31</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T12.2.6.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A3.T12.2.6.3.1\">German</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T12.2.6.3.2\">43.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T12.2.6.3.3\">41.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T12.2.6.3.4\">43.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T12.2.6.3.5\">73.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T12.2.6.3.6\">67.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T12.2.6.3.7\">72.44</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">\u0422\u0430\u0431\u043b. 12: </span>Zero-shot and gold fine-tuning scores of NLLB-200-1.3B-Distilled models on Ukrainian UA-GEC gec+fluency test set. </figcaption>\n</figure>",
            "capture": "\u0422\u0430\u0431\u043b. 12: Zero-shot and gold fine-tuning scores of NLLB-200-1.3B-Distilled models on Ukrainian UA-GEC gec+fluency test set. "
        }
    },
    "image_paths": {},
    "references": [
        {
            "1": {
                "title": "Comparative study of models trained on synthetic data for Ukrainian grammatical error correction.",
                "author": "Maksym Bondarenko, Artem Yushko, Andrii Shportko, and Andrii Fedorych. 2023.",
                "venue": "In Proceedings of the Second Ukrainian Natural Language Processing Workshop (UNLP), pages 103\u2013113, Dubrovnik, Croatia.",
                "url": "https://aclanthology.org/2023.unlp-1.13"
            }
        },
        {
            "2": {
                "title": "Using Wikipedia edits in low resource grammatical error correction.",
                "author": "Adriane Boyd. 2018.",
                "venue": "In Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text, pages 79\u201384, Brussels, Belgium.",
                "url": "https://aclanthology.org/W18-6111"
            }
        },
        {
            "3": {
                "title": "Automatic annotation and evaluation of error types for grammatical error correction.",
                "author": "Christopher Bryant, Mariano Felice, and Ted Briscoe. 2017.",
                "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 793\u2013805, Vancouver, Canada.",
                "url": "https://aclanthology.org/P17-1074"
            }
        },
        {
            "4": {
                "title": "Grammatical error correction: A survey of the state of the art.",
                "author": "Christopher Bryant, Zheng Yuan, Muhammad Reza Qorib, Hannan Cao, Hwee Tou Ng, and Ted Briscoe. 2023.",
                "venue": "Computational Linguistics, pages 643\u2013701.",
                "url": "https://aclanthology.org/2023.cl-3.4"
            }
        },
        {
            "5": {
                "title": "Unsupervised cross-lingual representation learning at scale.",
                "author": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online.",
                "url": "https://aclanthology.org/2020.acl-main.747"
            }
        },
        {
            "6": {
                "title": "Analyzing the performance of gpt-3.5 and gpt-4 in grammatical error correction.",
                "author": "Steven Coyne, Keisuke Sakaguchi, Diana Galvan-Sosa, Michael Zock, and Kentaro Inui. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2303.14342"
            }
        },
        {
            "7": {
                "title": "Better evaluation for grammatical error correction.",
                "author": "Daniel Dahlmeier and Hwee Tou Ng. 2012.",
                "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 568\u2013572, Montr\u00e9al, Canada.",
                "url": "https://aclanthology.org/N12-1067"
            }
        },
        {
            "8": {
                "title": "TransGEC: Improving grammatical error correction with translationese.",
                "author": "Tao Fang, Xuebo Liu, Derek F. Wong, Runzhe Zhan, Liang Ding, Lidia S. Chao, Dacheng Tao, and Min Zhang. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL 2023, pages 3614\u20133633, Toronto, Canada.",
                "url": "https://aclanthology.org/2023.findings-acl.223"
            }
        },
        {
            "9": {
                "title": "Generating artificial errors for grammatical error correction.",
                "author": "Mariano Felice and Zheng Yuan. 2014.",
                "venue": "In Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 116\u2013126, Gothenburg, Sweden.",
                "url": "https://aclanthology.org/E14-3013"
            }
        },
        {
            "10": {
                "title": "Neural grammatical error correction systems with unsupervised pre-training on synthetic data.",
                "author": "Roman Grundkiewicz, Marcin Junczys-Dowmunt, and Kenneth Heafield. 2019.",
                "venue": "In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 252\u2013263, Florence, Italy.",
                "url": "https://aclanthology.org/W19-4427"
            }
        },
        {
            "11": {
                "title": "The unbearable weight of generating artificial errors for grammatical error correction.",
                "author": "Phu Mon Htut and Joel Tetreault. 2019.",
                "venue": "In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 478\u2013483, Florence, Italy.",
                "url": "https://aclanthology.org/W19-4449"
            }
        },
        {
            "12": {
                "title": "Byte-level grammatical error correction using synthetic and curated corpora.",
                "author": "Svanhv\u00edt Lilja Ing\u00f3lfsd\u00f3ttir, Petur Ragnarsson, Haukur J\u00f3nsson, Haukur Simonarson, Vilhjalmur Thorsteinsson, and V\u00e9steinn Sn\u00e6bjarnarson. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7299\u20137316, Toronto, Canada.",
                "url": "https://aclanthology.org/2023.acl-long.402"
            }
        },
        {
            "13": {
                "title": "Approaching neural grammatical error correction as a low-resource machine translation task.",
                "author": "Marcin Junczys-Dowmunt, Roman Grundkiewicz, Shubha Guha, and Kenneth Heafield. 2018.",
                "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 595\u2013606, New Orleans, Louisiana.",
                "url": "https://aclanthology.org/N18-1055"
            }
        },
        {
            "14": {
                "title": "GECTurk: Grammatical error correction and detection dataset for Turkish.",
                "author": "Atakan Kara, Farrin Marouf Sofian, Andrew Bond, and G\u00f6zde \u015eahin. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: IJCNLP-AACL 2023 (Findings), pages 278\u2013290, Nusa Dua, Bali. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2023.findings-ijcnlp.26"
            }
        },
        {
            "15": {
                "title": "Wronging a right: Generating better errors to improve grammatical error detection.",
                "author": "Sudhanshu Kasewa, Pontus Stenetorp, and Sebastian Riedel. 2018.",
                "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4977\u20134983, Brussels, Belgium.",
                "url": "https://aclanthology.org/D18-1541"
            }
        },
        {
            "16": {
                "title": "An empirical study of incorporating pseudo data into grammatical error correction.",
                "author": "Shun Kiyono, Jun Suzuki, Masato Mita, Tomoya Mizumoto, and Kentaro Inui. 2019.",
                "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1236\u20131242, Hong Kong, China.",
                "url": "https://aclanthology.org/D19-1119"
            }
        },
        {
            "17": {
                "title": "Moses: Open source toolkit for statistical machine translation.",
                "author": "Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond\u0159ej Bojar, Alexandra Constantin, and Evan Herbst. 2007.",
                "venue": "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177\u2013180, Prague, Czech Republic.",
                "url": "https://aclanthology.org/P07-2045"
            }
        },
        {
            "18": {
                "title": "Eesti keele \u00fchendkorpuste sari 2013\u20132021: mahukaim eestikeelsete digitekstide kogu.",
                "author": "Kristina Koppel and Jelena Kallas. 2022.",
                "venue": "Eesti Rakenduslingvistika \u00dchingu aastaraamat Estonian Papers in Applied Linguistics, 18:207\u2013228.",
                "url": "https://doi.org/10.5128/ERYa18.12"
            }
        },
        {
            "19": {
                "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.",
                "author": "Taku Kudo and John Richardson. 2018.",
                "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66\u201371, Brussels, Belgium.",
                "url": "https://aclanthology.org/D18-2012"
            }
        },
        {
            "20": {
                "title": "No Error Left Behind: Multilingual Grammatical Error Correction with Pre-trained Translation Models.",
                "author": "Agnes Luhtaru, Elizaveta Korotkova, and Mark Fishel. 2024.",
                "venue": "In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2024).",
                "url": "https://todo.ai"
            }
        },
        {
            "21": {
                "title": "Grammatical error correction in low-resource scenarios.",
                "author": "Jakub N\u00e1plava and Milan Straka. 2019.",
                "venue": "In Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019), pages 346\u2013356, Hong Kong, China.",
                "url": "https://aclanthology.org/D19-5545"
            }
        },
        {
            "22": {
                "title": "Czech grammar error correction with a large and diverse corpus.",
                "author": "Jakub N\u00e1plava, Milan Straka, Jana Strakov\u00e1, and Alexandr Rosen. 2022.",
                "venue": "Transactions of the Association for Computational Linguistics, 10:452\u2013467.",
                "url": "https://aclanthology.org/2022.tacl-1.26"
            }
        },
        {
            "23": {
                "title": "Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages.",
                "author": "Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2309.09400"
            }
        },
        {
            "24": {
                "title": "Training for grammatical error correction without human-annotated L2 learners\u2019 corpora.",
                "author": "Mikio Oda. 2023.",
                "venue": "In Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), pages 455\u2013465, Toronto, Canada.",
                "url": "https://aclanthology.org/2023.bea-1.38"
            }
        },
        {
            "25": {
                "title": "GECToR \u2013 grammatical error correction: Tag, not rewrite.",
                "author": "Kostiantyn Omelianchuk, Vitaliy Atrasevych, Artem Chernodub, and Oleksandr Skurzhanskyi. 2020.",
                "venue": "In Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 163\u2013170, Seattle, WA, USA \u2192 Online.",
                "url": "https://aclanthology.org/2020.bea-1.16"
            }
        },
        {
            "26": {
                "title": "Gpt-4 technical report.",
                "author": "OpenAI. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2303.08774"
            }
        },
        {
            "27": {
                "title": "fairseq: A fast, extensible toolkit for sequence modeling.",
                "author": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019.",
                "venue": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48\u201353, Minneapolis, Minnesota.",
                "url": "https://aclanthology.org/N19-4009"
            }
        },
        {
            "28": {
                "title": "A low-resource approach to the grammatical error correction of Ukrainian.",
                "author": "Frank Palma Gomez, Alla Rozovskaya, and Dan Roth. 2023.",
                "venue": "In Proceedings of the Second Ukrainian Natural Language Processing Workshop (UNLP), pages 114\u2013120, Dubrovnik, Croatia.",
                "url": "https://aclanthology.org/2023.unlp-1.14"
            }
        },
        {
            "29": {
                "title": "Artificial error generation with machine translation and syntactic patterns.",
                "author": "Marek Rei, Mariano Felice, Zheng Yuan, and Ted Briscoe. 2017.",
                "venue": "In Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 287\u2013292, Copenhagen, Denmark.",
                "url": "https://aclanthology.org/W17-5032"
            }
        },
        {
            "30": {
                "title": "A simple recipe for multilingual grammatical error correction.",
                "author": "Sascha Rothe, Jonathan Mallinson, Eric Malmi, Sebastian Krause, and Aliaksei Severyn. 2021.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 702\u2013707, Online.",
                "url": "https://aclanthology.org/2021.acl-short.89"
            }
        },
        {
            "31": {
                "title": "Generating confusion sets for context-sensitive error correction.",
                "author": "Alla Rozovskaya and Dan Roth. 2010.",
                "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961\u2013970, Cambridge, MA.",
                "url": "https://aclanthology.org/D10-1094"
            }
        },
        {
            "32": {
                "title": "T\u00dc eesti keele (v\u00f5\u00f5rkeelena) osakonna \u00f5ppijakeele tekstikorpus [the language learner\u2019s corpus of the department of estonian language of the university of tartu].",
                "author": "Ingrid Rummo and Kristiina Praakli. 2017.",
                "venue": "In EAAL 2017: 16th annual conference Language as an ecosystem, 20-21 April 2017, Tallinn, Estonia: abstracts, 2017, p. 12-13.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Reassessing the Goals of Grammatical Error Correction: Fluency Instead of Grammaticality.",
                "author": "Keisuke Sakaguchi, Courtney Napoles, Matt Post, and Joel Tetreault. 2016.",
                "venue": "Transactions of the Association for Computational Linguistics, 4:169\u2013182.",
                "url": "https://doi.org/10.1162/tacl_a_00091"
            }
        },
        {
            "34": {
                "title": "Improving neural machine translation models with monolingual data.",
                "author": "Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.",
                "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86\u201396, Berlin, Germany.",
                "url": "https://aclanthology.org/P16-1009"
            }
        },
        {
            "35": {
                "title": "Seq2Edits: Sequence transduction using span-level edit operations.",
                "author": "Felix Stahlberg and Shankar Kumar. 2020.",
                "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5147\u20135159, Online.",
                "url": "https://aclanthology.org/2020.emnlp-main.418"
            }
        },
        {
            "36": {
                "title": "Synthetic data generation for grammatical error correction with tagged corruption models.",
                "author": "Felix Stahlberg and Shankar Kumar. 2021.",
                "venue": "In Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications, pages 37\u201347, Online.",
                "url": "https://aclanthology.org/2021.bea-1.4"
            }
        },
        {
            "37": {
                "title": "UA-GEC: Grammatical error correction and fluency corpus for the Ukrainian language.",
                "author": "Oleksiy Syvokon, Olena Nahorna, Pavlo Kuchmiichuk, and Nastasiia Osidach. 2023.",
                "venue": "In Proceedings of the Second Ukrainian Natural Language Processing Workshop (UNLP), pages 96\u2013102, Dubrovnik, Croatia.",
                "url": "https://aclanthology.org/2023.unlp-1.12"
            }
        },
        {
            "38": {
                "title": "The UNLP 2023 shared task on grammatical error correction for Ukrainian.",
                "author": "Oleksiy Syvokon and Mariana Romanyshyn. 2023.",
                "venue": "In Proceedings of the Second Ukrainian Natural Language Processing Workshop (UNLP), pages 132\u2013137, Dubrovnik, Croatia.",
                "url": "https://aclanthology.org/2023.unlp-1.16"
            }
        },
        {
            "39": {
                "title": "Multilingual translation from denoising pre-training.",
                "author": "Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2021.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3450\u20133466, Online.",
                "url": "https://aclanthology.org/2021.findings-acl.304"
            }
        },
        {
            "40": {
                "title": "Stanford alpaca: An instruction-following llama model.",
                "author": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023.",
                "venue": "https://github.com/tatsu-lab/stanford_alpaca.",
                "url": null
            }
        },
        {
            "41": {
                "title": "No language left behind: Scaling human-centered machine translation.",
                "author": "NLLB Team, Marta R. Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm\u00e1n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022.",
                "venue": null,
                "url": "http://arxiv.org/abs/2207.04672"
            }
        },
        {
            "42": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023.",
                "venue": "arXiv preprint arXiv:2307.09288.",
                "url": "http://arxiv.org/abs/2307.09288"
            }
        },
        {
            "43": {
                "title": "New dataset and strong baselines for the grammatical error correction of Russian.",
                "author": "Viet Anh Trinh and Alla Rozovskaya. 2021.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4103\u20134111, Online.",
                "url": "https://aclanthology.org/2021.findings-acl.359"
            }
        },
        {
            "44": {
                "title": "CCNet: Extracting high quality monolingual datasets from web crawl data.",
                "author": "Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Armand Joulin, and Edouard Grave. 2020.",
                "venue": "In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4003\u20134012, Marseille, France.",
                "url": "https://aclanthology.org/2020.lrec-1.494"
            }
        },
        {
            "45": {
                "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing.",
                "author": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020.",
                "venue": null,
                "url": "http://arxiv.org/abs/1910.03771"
            }
        },
        {
            "46": {
                "title": "Noising and denoising natural language: Diverse backtranslation for grammar correction.",
                "author": "Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew Ng, and Dan Jurafsky. 2018.",
                "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 619\u2013628, New Orleans, Louisiana.",
                "url": "https://aclanthology.org/N18-1057"
            }
        },
        {
            "47": {
                "title": "Erroneous data generation for grammatical error correction.",
                "author": "Shuyao Xu, Jiehao Zhang, Jin Chen, and Long Qin. 2019.",
                "venue": "In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 149\u2013158, Florence, Italy.",
                "url": "https://aclanthology.org/W19-4415"
            }
        },
        {
            "48": {
                "title": "mT5: A massively multilingual pre-trained text-to-text transformer.",
                "author": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021.",
                "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498, Online.",
                "url": "https://aclanthology.org/2021.naacl-main.41"
            }
        },
        {
            "49": {
                "title": "Neural and FST-based approaches to grammatical error correction.",
                "author": "Zheng Yuan, Felix Stahlberg, Marek Rei, Bill Byrne, and Helen Yannakoudakis. 2019.",
                "venue": "In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 228\u2013239, Florence, Italy.",
                "url": "https://aclanthology.org/W19-4424"
            }
        },
        {
            "50": {
                "title": "Improving grammatical error correction via pre-training a copy-augmented architecture with unlabeled data.",
                "author": "Wei Zhao, Liang Wang, Kewei Shen, Ruoyu Jia, and Jingming Liu. 2019.",
                "venue": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 156\u2013165, Minneapolis, Minnesota.",
                "url": "https://aclanthology.org/N19-1014"
            }
        },
        {
            "51": {
                "title": "Evaluation of really good grammatical error correction.",
                "author": "Robert \u00d6stling, Katarina Gillholm, Murathan Kurfal\u0131, Marie Mattson, and Mats Wir\u00e9n. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2308.08982"
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.05493v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2403.05493v1",
        "paper_title": "To Err Is Human, but Llamas Can Learn It Too",
        "research_background": "**Motivation**:\nThe motivation behind the paper lies in the limitations of current approaches to grammatical error correction (GEC), specifically the data dependency of neural network-based models and the scarcity of authentic error correction datasets even for high-resource languages. The authors seek to address this issue by investigating whether pre-trained language models can generate high-quality synthetic errors that closely resemble natural human errors. By leveraging these synthetic errors, the ultimate aim is to enhance GEC performance, particularly for low-resource languages.\n\n**Research Problem**:\nThe primary research problem is the insufficiency of human error correction data and the quality of synthetic error generation (AEG). Existing methods for AEG often rely on simplistic random perturbations or rule-based approaches that might not generate errors reflective of real human errors. The study identifies a gap in the utilization of pre-trained foundation models like Llama 2 for AEG and aims to explore their effectiveness in generating data that can improve GEC outcomes.\n\n**Relevant Prior Work**:\n1. **Neural Networks for GEC**: Best-performing models previously utilized neural networks for grammatical error correction, as discussed in works by Junczys-Dowmunt et al., 2018; Omelianchuk et al., 2020; Rothe et al., 2021.\n2. **Artificial Error Generation Methods**:\n   - **Random Perturbation**: Techniques applying probabilistic alterations to text, cited by Zhao et al., 2019; Grundkiewicz et al., 2019; Rothe et al., 2021.\n   - **Hand-Crafted Rules and Confusion Sets**: Methods relying on detailed predefined rules, endorsed by Rozovskaya and Roth, 2010; Xu et al., 2019; Kara et al., 2023; Bondarenko et al., 2023.\n   - **Back-Translation**: Generating errors by translating between languages to introduce mistakes, as noted by Xie et al., 2018; Kiyono et al., 2019; Stahlberg and Kumar, 2021.\n3. **Machine Translation Analogy**: The concept of back-translation, likened to a technique used in machine translation discussed by Sennrich et al., 2016.\n4. **Technological Frameworks**: The study references new language models like Llama 2 by Touvron et al., 2023, and commercially available models such as GPT-3.5 and GPT-4 by OpenAI, 2023, along with other open models like mT5 by Rothe et al., 2021, and NLLB by Luhtaru et al., 2024.\n\nThe prior works primarily focus on traditional methods for GEC and AEG but did not explore the potential of pre-trained foundation models for generating high-fidelity synthetic errors, which is the novel direction this paper pursues.",
        "methodology": "To Err Is Human, but Llamas Can Learn It Too\n**Methodology:**\n\n**Primary Target and Approach:**\nThe core objective of this study is to leverage generative language models (LMs) to enhance artificial error generation (AEG) by means of fine-tuning. In parallel, we explore prompting large language models (LLMs) to accomplish this task. Additionally, we incorporate two sequence-to-sequence (seq2seq) models, fine-tuning them for the same purpose.\n\n**Evaluation of AEG Solutions:**\nThe performance and efficiency of the proposed AEG solutions are primarily evaluated through their application in grammatical error correction (GEC). As part of this, we also fine-tune generative LMs for the GEC task, comparing these results with both prompting-based GEC results and existing related work.\n\n**General Pipeline:**\n1. **Fine-tuning for Error Generation:**\n   * Fine-tune a language model (LM) to produce errors using human error data. The correct sentences are provided as input, with the desired output being sentences containing errors.\n2. **Synthetic Error Addition:**\n   * Apply the AEG LM to correct sentences, thereby generating a synthetically erroneous counterpart.\n3. **Fine-tuning for Error Correction:**\n   * Fine-tune an LM on the synthetically generated dataset to correct grammatical errors. This is analogous to Step 1 but with the direction of the sentence pair reversed.\n4. **Continued Fine-tuning:**\n   * Further fine-tune the GEC LM using a typically smaller dataset containing human errors.\n5. **Benchmark Testing:**\n   * Apply the models to the erroneous sentences of benchmark test sets and evaluate the results.\n\n**Technical Implementation and Setup:**\nTo ensure clarity and replicability, subsequent sections of the paper will delve into the technical details of our implementation and the specifics of the experimental setup.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Experiment Setup:**\n1. **Objective**: Evaluate Llama-based models' performance on Grammatical Error Correction (GEC) and Artificial Error Generation (AEG) tasks.\n2. **Comparison**: \n   - Compare AEG effectiveness between NLLB and mT5 models against Llama-based models.\n   - Assess AEG through prompting with GPT-3.5-turbo versus Llama models with trained error generation.\n   \n**Datasets:**\n- Specific datasets are not mentioned, but the context suggests using standard benchmarks for GEC and datasets enhanced for AEG.\n\n**Baselines:**\n- **NLLB Model**: Small, efficient model for comparison in AEG.\n- **mT5 Model**: Another baseline for AEG comparison.\n- **GPT-3.5-turbo**: Used for AEG through prompting to compare against Llama models.\n- **Probabilistic Reverse-Speller Errors**: For assessing generated errors against human data.\n\n**Evaluation Metrics:**\n- Effectiveness in AEG and GEC tasks.\n- Quality of generated errors compared to human data.\n  \n**Main Experimental Results:**\n- The paper presents comparative performance details showing how Llama-based models perform relative to the mentioned baselines. \n- It highlights the efficiency and quality of errors generated by specific models (Llama versus GPT-3.5-turbo, and Llama versus NLLB and mT5).\n- The results include a qualitative analysis of errors against human data and probabilistic reverse-speller errors, demonstrating the practical applicability of error generation by different models.\n\nThis description provides a clear distinction of the main experiment setup and results, focusing on the comparative analysis without delving into specific ablation studies."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The research aims to compare the effectiveness of different methods of artificial error generation (AEG) using Llama-based models for grammatical error correction (GEC). The objective is to determine whether back-translation style AEG data produced by fine-tuned Llama-based models can outperform existing methods based on probabilistic reverse-speller data.",
            "experiment_process": "The study compares three GEC approaches: (1) training exclusively on human error data, (2) using probabilistic reverse-speller AEG data followed by human error data, and (3) our back-translation style AEG data produced by fine-tuned Llama-based models followed by human data. Performance scores are compared across Estonian, Ukrainian, and German languages using F-scores and other metrics, with evaluations against current state-of-the-art models and GPT-4 prompting for error correction.",
            "result_discussion": "Llama-based models exhibit strong correction capabilities and surpass the state-of-the-art methods in Estonian and Ukrainian, only slightly lagging in German. Training with back-translation style synthetic data enhances performance significantly, improving F scores by 2-2.4 points over gold data alone. In contrast, gains from probabilistic reverse-speller data are smaller (0.6 to 1.5 points). While Llama-based systems outperform GPT-4 in precision, GPT-4 has higher recall rates for Estonian and German, indicating broader error retrieval ability.",
            "ablation_id": "2403.05493v1.No1"
        },
        {
            "research_objective": "The study explores whether smaller models, specifically the 1.3B NLLB and 1.2B mT5-large, can produce valuable artificial errors for grammatical error correction comparably to Llama-based models.",
            "experiment_process": "The NLLB and mT5 models are fine-tuned to generate synthetic errors for Estonian, Ukrainian, and German languages. The generated errors are then used to train GEC models, whose performances are compared to those trained solely on gold data. Scores are evaluated via metrics such as F-scores before and after fine-tuning on actual error correction examples.",
            "result_discussion": "NLLB model-generated errors particularly excel for Estonian and German, nearly matching Llama-based model performance, but underperform for Ukrainian. mT5-generated errors generally lag behind probabilistic reverse speller data for Ukrainian and German and offer minimal improvement for Estonian. Synthetic data aligns less well with the text domain or error types for Ukrainian, indicated by lower initial scores that improve post-fine-tuning.",
            "ablation_id": "2403.05493v1.No2"
        },
        {
            "research_objective": "The research investigates whether prompting advanced commercial models like GPT-3.5-turbo and GPT-4-turbo can effectively generate artificial errors for grammatical error correction without additional language model training.",
            "experiment_process": "Errors are generated through prompting GPT-3.5-turbo and, in a limited setting, GPT-4-turbo with datasets of 100,000 sentences. The study compares costs and evaluates error correction quality by fine-tuning Llama-based models on these synthetic errors and comparing the F-scores, precision, and recall rates against those achieved through fine-tuning models with back-translation errors.",
            "result_discussion": "GPT-3.5-turbo and fine-tuning Llama-based models yield close F scores in error correction post-gold fine-tuning. Before gold fine-tuning, recall rates are significantly higher for fine-tuning than prompting for Estonian and German, whereas the reverse is true for Ukrainian. The final differences in scores post-gold fine-tuning are minimal. This suggests some domain mismatches that are mitigated with increased pretraining data in larger datasets.",
            "ablation_id": "2403.05493v1.No3"
        },
        {
            "research_objective": "This study aims to compare the quality of artificially generated errors and human errors in grammatical error correction.",
            "experiment_process": "Models are trained using the same number of sentences as respective human error sets (19k for German, 33k for Ukrainian, and 9k for Estonian) with both synthetic and human errors. Performance is compared in terms of precision, recall rates, and F-scores, with evaluations done across various error categories for detailed analysis.",
            "result_discussion": "Synthetic data precision for Estonian and German is on par with human errors, but LM-generated errors achieve higher recall for these languages compared to probabilistic methods. Ukrainian synthetic data scores are worse than human data, yet LM-generated errors show better recall than probabilistic. Synthetic errors closely resemble naturally occurring human errors, particularly for punctuation and word order issues, indicating the efficacy of the synthetic approach.",
            "ablation_id": "2403.05493v1.No4"
        }
    ]
}