{
    "title": "InternLM2 Technical Report",
    "abstract": "The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k \u201cNeedle-in-a-Haystack\u201d test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model\u2019s evolution.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Since the introduction of ChatGPT and GPT-4 (OpenAI, 2023  ###reference_b64###), Large Language Models (LLMs) have surged in popularity across the academic and industrial spheres. Models trained on billions of tokens have demonstrated profound empathy and problem-solving capabilities, leading to widespread speculation that the era of Artificial General Intelligence (AGI) may soon be upon us. Despite this enthusiasm, the path to developing models with capabilities comparable to those of ChatGPT or GPT-4 remains elusive. The open-source community has been working diligently to bridge the gap between proprietary LLMs and their open-source counterparts. In the past year, several notable open-source LLMs, such as LLaMA (Touvron et al., 2023a  ###reference_b84###; b  ###reference_b85###), Qwen (Bai et al., 2023a  ###reference_b10###), Mistral (Jiang et al., 2023  ###reference_b44###), and Deepseek (Bi et al., 2024  ###reference_b14###), have made significant strides. In this paper, we introduce InternLM2, a new Large Language Model that outperforms the previously mentioned models.\nThe development of Large Language Models (LLMs) encompasses several main phases: pre-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022  ###reference_b65###). Pre-training is chiefly based on leveraging a vast corpus of natural text, amassing trillions of tokens. This phase is aimed at equipping LLMs with a broad repository of knowledge and fundamental skills. The quality of data is considered the most crucial factor during pre-training. However, technical reports on LLMs (Touvron et al., 2023a  ###reference_b84###; b  ###reference_b85###; Bai et al., 2023a  ###reference_b10###; Bi et al., 2024  ###reference_b14###) in the past have seldom addressed the processing of pre-training data. InternLM2 extensively details how it prepares text, code, and long-context data for pre-training.\nHow to effectively extend the context length of LLMs is currently a hot research topic, since many downstream applications, such as Retrieval-Augmented Generation (RAG) (Gao et al., 2023  ###reference_b36###) and agents (Xi et al., 2023  ###reference_b90###), rely on long contexts.\nInternLM2 first employs Group Query Attention (GQA) to enable a smaller memory footprint when inferring long sequences. In the pre-training phase, we initially train InternLM2 with 4k context texts, then transit the training corpus to high-quality 32k texts for further training. Upon completion, through positional encoding extrapolation (LocalLLaMA, 2023  ###reference_b58###), InternLM2 achieves commendable performance in the \u201cNeedle-in-a-Haystack\u201d test within 200k contexts.\nFollowing long-context pre-training, we utilize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) to ensure the model adheres well to human instructions and aligns with human values. Notably, we also construct corresponding 32k data during these processes to further improve the long-context processing capability of InternLM2. Besides, we introduce COnditional OnLine RLHF (COOL RLHF), which adopts a conditional reward model to reconcile diverse but potentially conflicting preferences and executes Proximal Policy Optimization (PPO) over multiple rounds to mitigate emerging reward hacking in each phase. To elucidate the impact of RLHF within the community, we also release models in their pre- and post-RLHF stages, named InternLM2-Chat-{size}-SFT and InternLM2-Chat-{size}, respectively.\nOur contributions are twofold, highlighting not only the model\u2019s exceptional performance across diverse benchmarks but also our comprehensive approach to its development in different stages. Key points include\nOpen-Sourcing InternLM2 with Exceptional Performance: We have open-sourced models of various sizes, including 1.8B, 7B, and 20B, all of which perform well in both subjective and objective evaluations. Additionally, we have released models from different stages to facilitate community analysis of changes post-SFT and RLHF training.\nDesigned with a 200k Context Window: InternLM2 exhibits impressive long-context performance, nearly perfectly identifying all \u201cneedles\u201d in the \u201cNeedle-in-a-Haystack\u201d experiment with a 200k context. Furthermore, we provide experience of training long-context LLMs across all stages, including pretraining, SFT, and RLHF.\nComprehensive Data Preparation Guidance: We have elaborated on the preparation of data for LLMs, including pre-training data, domain-specific enhancement data, SFT data, and RLHF data. These details will benefit the community in better training of LLMs.\nInnovative RLHF Training Techniques: We introduced Conditional Online RLHF (COOL RLHF) to harmonize various preferences, significantly enhancing the performance of InternLM2 in various subjective dialogue evaluations. We have also conducted a preliminary analysis and comparison of RLHF\u2019s subjective and objective results to offer the community insights into RLHF."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Infrastructure",
            "text": "In this part, we introduce the training framework InternEvo which was used during pretraining, SFT and RLHF.\nA trade-off exists between memory utilization and communication cost in distributed LLM training. Initially, the communication cost can be effectively reduced by diminishing the communication scale. This involves limiting communications to a smaller group of GPUs, potentially within the same node, which mitigates the overall communication cost. Building upon this principle, InternEvo addresses communication challenges by implementing a suite of adaptive sharding techniques to achieve strong scaling performance (Chen et al., 2024b  ###reference_b21###). These include Full-Replica, Full-Sharding, and Partial-Sharding, which allow each component of the model states\u2014parameters, gradients, and optimizer states\u2014to independently select the most appropriate sharding approach and device mesh configuration. This flexibility facilitates a more nuanced distribution of model states across the GPU infrastructure. InternEvo also introduces an optimization framework designed to identify the most efficient sharding factors. This aims to minimize communication expenses while adhering to the memory constraints of the GPU.\nFurther reducing communication overhead, InternEvo strategically coordinates communication and computation to optimize overall system performance. When employing parameter sharding, the model\u2019s entire parameters are distributed across multiple GPUs to conserve GPU memory. During each forward and backward pass of every micro-batch in a training step, InternEvo efficiently pre-fetches the complete parameter set for upcoming layers through AllGather, while concurrently computing the current layer. The generated gradients undergo synchronization within the parameter sharding group through ReduceScatter and are subsequently synchronized across parameter sharding groups using AllReduce. These communication processes are skillfully overlapped with the backward computation, maximizing efficiency in the training pipeline. In the case of optimizer state sharding, where the GPU broadcasts updated parameters within the sharding group through Broadcast, InternEvo employs a strategic overlap with the forward computation of the next training step. These innovative overlap approaches effectively balance communication overhead and computation execution time, resulting in a significant enhancement in overall system performance.\nOne of the primary challenges in long-sequence training is the trade-off between computation speed and communication overhead. InternEvo breaks down GPU memory management into a hierarchical space with four parallel dimensions\u2014data, tensor, sequence, and pipeline\u2014and three sharding dimensions\u2014parameter, gradient, and optimizer state (Chen et al., 2024a  ###reference_b20###). We conduct a thorough analysis of memory and communication costs for each dimension, utilizing an execution simulator to identify and implement the optimal parallelization strategy. The optimal execution plan can be automatically searched based on the training scale, sequence length, model size, and batch size. With this execution plan, InternEvo exhibits the capability to handle long contexts (up to 1 million tokens) during training. InternEvo also implements memory management techniques to reduce GPU memory fragmentation, a common issue in long-sequence training scenarios. It uses a memory pool for unified memory management and introduces a defragmentation technique that proactively consolidates small memory chunks to prevent out-of-memory errors.\nWe have also addressed the challenges of efficiently training LLMs in GPU datacenters, which often face issues such as frequent hardware failures, complex parallelization strategies, and imbalanced resource utilization. To tackle these issues, we conducted an in-depth characterization study of a six-month LLM development workload trace from our GPU datacenter (Hu et al., 2024  ###reference_b41###). This study identified discrepancies between LLMs and previous deep learning workloads and explored resource utilization patterns and job failure impacts. Based on our analysis, we introduced two system efforts: a fault-tolerant pretraining system that enhances fault tolerance through LLM-involved failure diagnosis and automatic recovery, and a decoupled scheduling system for evaluation tasks that provides timely model performance feedback.\nIn our implementation, we have incorporated an asynchronous saving mechanism that regularly archives model weights and optimizer states to distributed file and object storage at predefined intervals. Throughout the training process, each GPU first saves its model states in local storage and subsequently asynchronously uploads this data to the remote distributed storage system. This dual-step process ensures that, in the event of occasional hardware or network failures automatically detected by our system, only a minimal amount of training progress is lost. To optimize storage space, we systematically transfer these temporary model checkpoints from hot storage to cost-effective cold storage. Furthermore, our system is designed to seamlessly resume model training even when the parallelization configuration is altered, providing flexibility and continuity in the training pipeline.\nThe efficiency of InternEvo has also been successfully demonstrated in the Reinforcement Learning from Human Feedback (RLHF) stage, where multiple LLMs are deployed for interactive training. For instance, in the Proximal Policy Optimization (PPO) process, we utilize four equally-sized models and train two of them; InternEvo enables each model to be executed at its optimal configuration. To enhance the coordination of multiple models, we have developed an innovative RLHF framework that builds upon InternEvo and Ray. This framework is characterized by its flexibility and scalability, allowing it to perform effectively at scale. It is capable of integrating with various LLM execution engines and supporting diverse algorithmic designs. For a comprehensive description of the \u201dAlignment\u201d concept, please refer to Section 4  ###reference_###."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "InternEvo",
            "text": "We utilize InternEvo, an efficient and lightweight pretraining framework, for model training. This framework enables us to scale model training across thousands of GPUs. This is achieved through a combination of data, tensor (Shoeybi et al., 2019  ###reference_b79###), sequence (Korthikanti et al., 2023  ###reference_b47###), and pipeline (Huang et al., 2019  ###reference_b42###) parallelism. To further enhance GPU memory efficiency, InternEvo incorporates various Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al., 2020  ###reference_b71###) strategies, significantly reducing the memory footprint required for training. In addition, to enhance hardware utilization, we incorporate the FlashAttention technique (Dao, 2023  ###reference_b32###) and mixed-precision training (Narang et al., 2017  ###reference_b63###) with BF16.\nInternEvo demonstrates strong scaling performance when training InternLM across thousands of GPUs. As shown in Figure 1  ###reference_###, when training InternLM-7B on 8 GPUs with a global batch size of 4 million tokens, InternEvo achieves 64% Model FLOPs Utilization (MFU). Scaling up the training to 1024 GPUs, InternEvo maintains an impressive 53% MFU with the same global batch size. This level of scaling performance is particularly challenging to attain, as the batch size remains constant and the computation-to-communication ratio decreases with an increased number of GPUs. In contrast, DeepSpeed (Rasley et al., 2020  ###reference_b72###) only manages to achieve around 36% MFU when training the InternLM-7B on 1024 GPUs using ZeRO-1 (Rajbhandari et al., 2020  ###reference_b71###) and MiCS (Zhang et al., 2022  ###reference_b99###).\nInternEvo also exhibits strong scaling of sequence length, supporting  tokens when training LLMs of varying sizes. For instance, when training the InternLM-7B model with a sequence length of  tokens, InternEvo can achieve nearly 88% MFU. In comparison, DeepSpeed-Ulysses and Megatron-LM only reach about 65% MFU. This improvement in training performance is also observed with LLMs of larger sizes, such as models with 30 billion or 70 billion parameters.\n###figure_1### A trade-off exists between memory utilization and communication cost in distributed LLM training. Initially, the communication cost can be effectively reduced by diminishing the communication scale. This involves limiting communications to a smaller group of GPUs, potentially within the same node, which mitigates the overall communication cost. Building upon this principle, InternEvo addresses communication challenges by implementing a suite of adaptive sharding techniques to achieve strong scaling performance (Chen et al., 2024b  ###reference_b21###  ###reference_b21###). These include Full-Replica, Full-Sharding, and Partial-Sharding, which allow each component of the model states\u2014parameters, gradients, and optimizer states\u2014to independently select the most appropriate sharding approach and device mesh configuration. This flexibility facilitates a more nuanced distribution of model states across the GPU infrastructure. InternEvo also introduces an optimization framework designed to identify the most efficient sharding factors. This aims to minimize communication expenses while adhering to the memory constraints of the GPU.\nFurther reducing communication overhead, InternEvo strategically coordinates communication and computation to optimize overall system performance. When employing parameter sharding, the model\u2019s entire parameters are distributed across multiple GPUs to conserve GPU memory. During each forward and backward pass of every micro-batch in a training step, InternEvo efficiently pre-fetches the complete parameter set for upcoming layers through AllGather, while concurrently computing the current layer. The generated gradients undergo synchronization within the parameter sharding group through ReduceScatter and are subsequently synchronized across parameter sharding groups using AllReduce. These communication processes are skillfully overlapped with the backward computation, maximizing efficiency in the training pipeline. In the case of optimizer state sharding, where the GPU broadcasts updated parameters within the sharding group through Broadcast, InternEvo employs a strategic overlap with the forward computation of the next training step. These innovative overlap approaches effectively balance communication overhead and computation execution time, resulting in a significant enhancement in overall system performance.\nOne of the primary challenges in long-sequence training is the trade-off between computation speed and communication overhead. InternEvo breaks down GPU memory management into a hierarchical space with four parallel dimensions\u2014data, tensor, sequence, and pipeline\u2014and three sharding dimensions\u2014parameter, gradient, and optimizer state (Chen et al., 2024a  ###reference_b20###  ###reference_b20###). We conduct a thorough analysis of memory and communication costs for each dimension, utilizing an execution simulator to identify and implement the optimal parallelization strategy. The optimal execution plan can be automatically searched based on the training scale, sequence length, model size, and batch size. With this execution plan, InternEvo exhibits the capability to handle long contexts (up to 1 million tokens) during training. InternEvo also implements memory management techniques to reduce GPU memory fragmentation, a common issue in long-sequence training scenarios. It uses a memory pool for unified memory management and introduces a defragmentation technique that proactively consolidates small memory chunks to prevent out-of-memory errors.\nWe have also addressed the challenges of efficiently training LLMs in GPU datacenters, which often face issues such as frequent hardware failures, complex parallelization strategies, and imbalanced resource utilization. To tackle these issues, we conducted an in-depth characterization study of a six-month LLM development workload trace from our GPU datacenter (Hu et al., 2024  ###reference_b41###  ###reference_b41###). This study identified discrepancies between LLMs and previous deep learning workloads and explored resource utilization patterns and job failure impacts. Based on our analysis, we introduced two system efforts: a fault-tolerant pretraining system that enhances fault tolerance through LLM-involved failure diagnosis and automatic recovery, and a decoupled scheduling system for evaluation tasks that provides timely model performance feedback.\nIn our implementation, we have incorporated an asynchronous saving mechanism that regularly archives model weights and optimizer states to distributed file and object storage at predefined intervals. Throughout the training process, each GPU first saves its model states in local storage and subsequently asynchronously uploads this data to the remote distributed storage system. This dual-step process ensures that, in the event of occasional hardware or network failures automatically detected by our system, only a minimal amount of training progress is lost. To optimize storage space, we systematically transfer these temporary model checkpoints from hot storage to cost-effective cold storage. Furthermore, our system is designed to seamlessly resume model training even when the parallelization configuration is altered, providing flexibility and continuity in the training pipeline.\nThe efficiency of InternEvo has also been successfully demonstrated in the Reinforcement Learning from Human Feedback (RLHF) stage, where multiple LLMs are deployed for interactive training. For instance, in the Proximal Policy Optimization (PPO) process, we utilize four equally-sized models and train two of them; InternEvo enables each model to be executed at its optimal configuration. To enhance the coordination of multiple models, we have developed an innovative RLHF framework that builds upon InternEvo and Ray. This framework is characterized by its flexibility and scalability, allowing it to perform effectively at scale. It is capable of integrating with various LLM execution engines and supporting diverse algorithmic designs. For a comprehensive description of the \u201dAlignment\u201d concept, please refer to Section 4  ###reference_###  ###reference_###."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Model Structure",
            "text": "Transformer (Vaswani et al., 2017  ###reference_b86###) has been predominantly used as the backbone for past Large Language Models (LLMs) due to its excellent parallelization capabilities, which fully leverage the power of GPUs (Brown et al., 2020  ###reference_b17###; Chowdhery et al., 2023  ###reference_b25###; Zeng et al., 2023  ###reference_b96###). LLaMA (Touvron et al., 2023a  ###reference_b84###) builds on Transformer architecture by replacing LayerNorm (Ba et al., 2016  ###reference_b9###) with RMSNorm (Zhang & Sennrich, 2019  ###reference_b97###) and setting the activation function to SwiGLU (Shazeer, 2020  ###reference_b78###), resulting in improved training efficiency and performance.\nSince the unveiling of LLaMA (Touvron et al., 2023a  ###reference_b84###), the community has vigorously engaged in augmenting the ecosystem built around the LLaMA architecture. This includes advancements in high-efficiency inference (lla, 2023  ###reference_b2###) and operator optimization (Dao, 2023  ###reference_b32###), among others. To ensure our model, InternLM2, aligns seamlessly with this well-established ecosystem, alongside other renowned LLMs, such as Falcon (Almazrouei et al., 2023  ###reference_b4###), Qwen (Bai et al., 2023a  ###reference_b10###), Baichuan (Yang et al., 2023  ###reference_b92###), Mistral (Jiang et al., 2023  ###reference_b44###), we opt to adhere to the structural design principles of LLaMA. In pursuit of enhancing efficiency, we consolidate the , , and  matrices, which has resulted in a training acceleration of over 5% during the pre-training phase. Moreover, to better support diverse tensor parallelism (tp) transformations, we have reconfigured the matrix layout. Rather than stacking the , , and  matrices in a straightforward manner, we adopt an interleaving approach for each head\u2019s , , and , as depicted in Figure 2  ###reference_###. This design modification facilitates the adjustment of the tensor parallel size through either splitting or concatenating the matrices along their last dimension, thereby enhancing the model\u2019s flexibility in varying distributed computing environments. InternLM2 aims to infer beyond 32K context, therefore, InternLM2 series models all choose Grouped-Query Attention (GQA) (Ainslie et al., 2023  ###reference_b3###), so that it can infer both in high speed and low GPU memory with very long contexts.\n###figure_2###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Pre-train",
            "text": "We introduce pre-training data, pre-training settings, and three pre-training phases in this part.\nWe have statistically analyzed the number of documents, data storage size, and data Bytes proportion in the pre-training dataset according to their sources, as shown in Table 1  ###reference_###. Among them, the Chinese and English data from web pages account for 86.46% of the total, making it the primary source. Although the data volume from other sources is relatively small, such as books and technical literature(abbreviated as techlit), the average document length is longer and the content quality is relatively higher, making them equally important.\nThe data processing pipeline used in this work is shown in Figure 3  ###reference_###. The entire data processing pipeline first standardizes data from different sources to obtain Format data. Then, heuristic statistical rules are used for data filtering to get Clean data. Next, the Locality-Sensitive Hashing (LSH) method is used for data deduplication to obtain Dedup data. We then apply a composite safety strategy to filter the data, resulting in Safe data. We have adopted different quality filtering strategies for data from various sources, ultimately obtaining High-quality pre-training data.\n###figure_3### We will detail the data processing pipeline using web page data as an example. Our web page data mainly comes from Common Crawl111https://commoncrawl.org/. Firstly, we need to decompress the original Warc format files and use Trafilatura (Barbaresi, 2021  ###reference_b13###) for HTML parsing and main text extraction. Then, we use the pycld2222https://pypi.org/project/pycld2/ library for language detection and classification of the main text. Finally, we assign a unique identifier to the data and store it in jsonl (JSON lines) format and obtained Format data.\nWeb page data randomly extracted from the internet often contains a large amount of low-quality data, such as parsing errors, formatting errors, and non-natural language text. A common practice is to design rule-based regularization and filtering methods to modify and filter the data, as seen in Gopher (Rae et al., 2021  ###reference_b70###), C4 (Dodge et al., 2021  ###reference_b33###), and RefinedWeb (Penedo et al., 2023  ###reference_b67###). Based on our observations of the data, we have designed a series of heuristic filtering rules that focus on anomalies in separation and line breaks, frequency of abnormal characters, and distribution of punctuation marks. By applying these filters, we obtained Clean data.\nA large amount of duplicate texts exist on the Internet, which can negatively impact model training. Therefore, we employed a method based on Locality-Sensitive Hashing (LSH) to perform fuzzy deduplication on the data. More specifically, we used the MinHash method (Broder, 1997  ###reference_b16###), establishing signatures with 128 hash functions on the 5-gram of the documents, and using 0.7 as the threshold for deduplication. We aimed to retain the most recent data, that is, prioritizing data with larger CC dumps numbers. We obtained the Dedup data after LSH deduplication.\nThe internet is rife with toxic and pornographic content, the use of which for model training can negatively impact performance and increase the likelihood of unsafe content generation. Therefore, we employed a comprehensive safety strategy combining \u201cdomain blocking\u201d, \u201cword blocking\u201d, \u201cpornography classifier\u201d, and \u201ctoxicity classifier\u201d to filter the data. Specifically, we constructed a block domain list comprising approximately 13M unsafe domains and a block word list containing 36,289 unsafe words for preliminary data filtering. Given that word blocking might inadvertently exclude a large amount of data, we opted for a cautious approach in compiling the block word list.\nTo further improve the detection rate of unsafe content, we fine-tuned the BERT model using the \u201cToxic Comment Classification Challenge\u201d dataset from Kaggle, resulting in a toxicity classifier. We sampled some data from Dedup data and annotated it using the Perspective API333https://perspectiveapi.com/ to create a pornography classification dataset. We then fine-tuned the BERT model with this dataset, yielding a pornography classifier. Finally, we used these two classifiers for secondary filtering of the data, filtering out data with scores below the threshold, resulting in Safe data.\nCompared to sources such as books, papers, and patents, internet-sourced data contains a significant amount of low-quality content. Based on our observations, the primary causes for this low-quality content are twofold: 1. The internet is rife with marketing advertisements, which tend to be repetitive and carry little information. 2. Many web pages consist of lists of article abstracts or product descriptions, resulting in extracted text that is difficult to read and lacks logical coherence.\nTo filter out these low-quality contents, we first organized manual data annotation. For the advertisements classification task, annotators were asked to identify whether a piece of data contains advertising content (both overall and partial advertisements are marked as low quality). For the fluency classification task, annotators were asked to rate the data on four dimensions: consistency, noise, information content, and grammar, resulting in a comprehensive fluency score. We then fine-tuned the BERT model using the manually annotated data, obtaining an advertisements classifier and a fluency classifier. Finally, we used these two classifiers for secondary filtering of the data, filtering out data with scores below the threshold, resulting in High-quality pre-train data.\n###figure_4### We collect data from various sources, including direct crawling from GitHub, public datasets, and online resources related to coding and programming, like Q&A forums, tutorial sites, and API documentation, the statistics is shown in Figure 4  ###reference_###.\nTable 2  ###reference_### reflects the data quality assessment based on a scoring model we trained. High-quality data will have a higher sampling weight and can undergo multiple training iterations in the pre-training phase. Moderate-quality data has a normal sampling weight and is typically trained once. Low-quality data are excluded, as our empirical findings affirm that removing them is vital for optimizing model performance and ensuring training stability despite their proportion being relatively small.\nAll data is converted to a unified markdown format. Nevertheless, a very small fraction of the data still exhibited corrupted HTML or XML formats. We applied a set of heuristic rules to minimize these occurrences, though we did not invest too much effort in format purification. Markdown was selected for its simplicity\u2014minimizing the token overhead for formatting\u2014and its compatibility with interleaving code and natural language. The real format used for the pre-training is more complex, involving the concatenation of multiple code files based on their dependencies. The main idea is to utilize the interleaving data, which is pivotal for teaching the model about programming. This point is also mentioned in recent studies (Guo et al., 2024  ###reference_b38###).\nDeduplicating code data is similar to processing natural language except for tokenization, which impacts hyperparameter selection. For instance, Python examples use two spaces, four spaces, or a tab character to signify indentation. A conventional whitespace tokenizer, or one tailored for natural language, might mistakenly assess these samples as different data, which is inaccurate. Our insight is that an effective tokenizer is essential for applying a universal deduplication strategy. Although recent studies have explored fine-grained deduplication at the paragraph or line level, our approach remains at the file level to preserve context integrity.\nQuality is a pivotal yet nebulous aspect of pre-training in LLM research, primarily due to the difficulty in quantifying its impact on model performance regarding the scale. We adopted a hybrid, multi-stage filtering process including rule- and model-based scorers. Rule-based scorers are heuristic and varied, though we discovered that code style is not a reliable quality metric and can misclassify too many codes as low-quality. For the model-based scoring, we evaluated several backbone models, training them with roughly 50,000 labeled samples. However, we observed that the correlation between scorer assessments and human judgments varied across languages, and enlarging the training set did not substantially enhance scorer accuracy. Consequently, we only employed model-based scoring for languages where the model predictions align well with human evaluations on a human-annotated validated set.\n###figure_5### In order to obtain reliable annotations of our model-based scorer, we introduce an iterative annotation process (illustrated in Figure 5  ###reference_###) to address the challenge that the definition of code quality data is vague. Identifying code that would be helpful for teaching an LLM is also non-trivial for human experts, for instance, a widely recognized code repository might be overly complex for a beginner. The proposed iterative workflow allows annotators to verify model predictions and refine the guidelines accordingly. To improve the annotation efficiency, we only ask the annotator to check the samples labeled by the scorer as high-quality and low-quality with high confidence. Besides, there is an automatic validation process in each iteration to ensure the previously annotated samples are correctly classified by the scorer, which is shown as yellow dot lines in the figure. In practice, we took three iterations to finalize our scoring model.\nThe training context window of InternLM2 has been expanded to 32,000 tokens, allowing the utilization of the entire context of a code repository. The structure of the repository may have been broken in previous data processing steps, such as the filtering of code files by their extensions and deduplication. So we first regroup code files originating from the same repository and perform dependency sorting to establish a sequence for concatenating these files. Therefore, a code repository will be viewed as a single long markdown file with code blocks, which allows the model to learn dependencies across files.\nWe employ regular expressions to detect the \u201cimport\u201d relations across various programming languages, and we use topological sorting to determine the concatenation order of files. In practice, the arrangement of files may break folder boundaries, resulting in an interleaving sequence of files from multiple sub-folders. Non-code files, such as markdowns and other documentation, are positioned preceding the first code file located in the same sub-folder.\nFor the corner cases like multiple paths between an ascendant and descendant and loops within the \u201cimport\u201d relation graph, we take the shorter path for the former and use the alphabet order to decide the start point for the latter. A trick in finding \u201cimport\u201d relations is to resolve the batched import, such as \u201c__init__.py\u201d or \u201c#include xx.h\u201d. Those files may import a bunch of unused dependencies, so we apply heuristic rules to refine our detection of \u201cimport\u201d relationships, ensuring that we accurately identify and process these relations at a finer level.\nOur data processing pipeline is designed to filter out low-quality long text data.\nIt comprises three stages: a) Length selection, a rule-based filter that selects data samples exceeding 32K bytes; b) Statistical filters, which leverage statistical features to identify and remove anomalous data; c) Perplexity filters, which utilize the difference in perplexity to assess the coherence between text segments, filtering out samples with distracting context. Note that all data chosen for the long context training is a subset of the standard pre-training corpus, meaning that the long context data will be learned at least twice during the pre-training.\nWe employ a variety of lexical and linguistic features to construct our statistical filters. Data samples failing to comply with established rules are excluded from the pre-training corpus. The full list of these filters can be found in Lv et al. (2024  ###reference_b60###). A remarkable filter is the presence of conjunctions and other words that imply discourse structure, such as \u201cEspecially\u201d, \u201cFormally\u201d, etc. The overall guidance of designing such filters is to filter out the meaningless data rather than selecting the most high-quality data. Statistical filters are particularly effective with long text data, as the statistical features are far more consistent than those in short text data. For instance, a 20-token text may not yield reliable statistics, but a 32K-token text will have a much clearer statistical feature distribution.\nPerplexity is often treated as an estimator for the probability of a text sequence, , and we slightly change its usage to estimate the conditional probability between two text segments , where  is preceding of . When the  and  are strongly correlated, the conditional probability should be higher than estimating the probability of  alone, which also means a negative perplexity difference. Conversely, if the probability changed in the reverse direction, meaning that the  is a distracting context, it should be removed from the pre-training corpus. Ideally, adding more context should not negatively impact the predictability of subsequent text. However, we\u2019ve observed exceptions in cases of improperly joined texts, such as failed HTML parsing, random social media snippets, and other instances stemming from recognition errors in sources with complex layouts. Note that we only filter the data based on the perplexity difference rather than the perplexity itself, and this could largely mitigate the bias introduced by the estimator itself (using which model to compute the perplexity). The bias of the perplexity estimator has been discussed in Wettig et al. (2024  ###reference_b88###); Sachdeva et al. (2024  ###reference_b74###).\nSelecting appropriate thresholds is a challenging yet essential part of the data filtering process, and this challenge becomes more severe because we have built many filters. We have two lessons on setting thresholds:\nTailoring thresholds to each domain rather than seeking a universal solution. For example, the statistical filter for conjunction words would not apply to long code data, which typically does not have any conjunction. Similarly, textbooks, research papers, novels, and patents each exhibit unique characteristics. A universal threshold would likely misclassify a significant amount of data. The same logic applies to setting thresholds across different languages; thus, we adjust thresholds for each domain individually.\nEmploying a validation set to streamline the process, focusing only on borderline cases. Unlike learning-based feature extractors or scorers, our statistical and perplexity filters yield smooth results within the same domain. It allows us to focus on samples lying near the threshold, simplifying the adjustment of thresholds since we only need to determine whether to lower or raise them. Lv et al. (2024  ###reference_b60###) illustrates the data clusters alongside scores from specific filters, demonstrating the interpretability of our proposed filters.\nFigure 6  ###reference_### shows the data distribution before and after applying all proposed filters. The entire filtering process eliminates a large proportion of Web page data (Common Crawl) and Patent, while most Book and Paper data are persevered.\n###figure_6### ###figure_7###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Pre-training data",
            "text": "The pre-training of LLMs is critically shaped by data, which presents a multifaceted challenge. It encompasses handling sensitive data, covering comprehensive knowledge, and balancing efficiency and quality. In this section, we will depict our data processing pipeline for preparing general domain textual data, programming language-related data, and long textual data.\nWe have statistically analyzed the number of documents, data storage size, and data Bytes proportion in the pre-training dataset according to their sources, as shown in Table 1  ###reference_###  ###reference_###. Among them, the Chinese and English data from web pages account for 86.46% of the total, making it the primary source. Although the data volume from other sources is relatively small, such as books and technical literature(abbreviated as techlit), the average document length is longer and the content quality is relatively higher, making them equally important.\nThe data processing pipeline used in this work is shown in Figure 3  ###reference_###  ###reference_###. The entire data processing pipeline first standardizes data from different sources to obtain Format data. Then, heuristic statistical rules are used for data filtering to get Clean data. Next, the Locality-Sensitive Hashing (LSH) method is used for data deduplication to obtain Dedup data. We then apply a composite safety strategy to filter the data, resulting in Safe data. We have adopted different quality filtering strategies for data from various sources, ultimately obtaining High-quality pre-training data.\n###figure_8### We will detail the data processing pipeline using web page data as an example. Our web page data mainly comes from Common Crawl111https://commoncrawl.org/. Firstly, we need to decompress the original Warc format files and use Trafilatura (Barbaresi, 2021  ###reference_b13###  ###reference_b13###) for HTML parsing and main text extraction. Then, we use the pycld2222https://pypi.org/project/pycld2/ library for language detection and classification of the main text. Finally, we assign a unique identifier to the data and store it in jsonl (JSON lines) format and obtained Format data.\nWeb page data randomly extracted from the internet often contains a large amount of low-quality data, such as parsing errors, formatting errors, and non-natural language text. A common practice is to design rule-based regularization and filtering methods to modify and filter the data, as seen in Gopher (Rae et al., 2021  ###reference_b70###  ###reference_b70###), C4 (Dodge et al., 2021  ###reference_b33###  ###reference_b33###), and RefinedWeb (Penedo et al., 2023  ###reference_b67###  ###reference_b67###). Based on our observations of the data, we have designed a series of heuristic filtering rules that focus on anomalies in separation and line breaks, frequency of abnormal characters, and distribution of punctuation marks. By applying these filters, we obtained Clean data.\nA large amount of duplicate texts exist on the Internet, which can negatively impact model training. Therefore, we employed a method based on Locality-Sensitive Hashing (LSH) to perform fuzzy deduplication on the data. More specifically, we used the MinHash method (Broder, 1997  ###reference_b16###  ###reference_b16###), establishing signatures with 128 hash functions on the 5-gram of the documents, and using 0.7 as the threshold for deduplication. We aimed to retain the most recent data, that is, prioritizing data with larger CC dumps numbers. We obtained the Dedup data after LSH deduplication.\nThe internet is rife with toxic and pornographic content, the use of which for model training can negatively impact performance and increase the likelihood of unsafe content generation. Therefore, we employed a comprehensive safety strategy combining \u201cdomain blocking\u201d, \u201cword blocking\u201d, \u201cpornography classifier\u201d, and \u201ctoxicity classifier\u201d to filter the data. Specifically, we constructed a block domain list comprising approximately 13M unsafe domains and a block word list containing 36,289 unsafe words for preliminary data filtering. Given that word blocking might inadvertently exclude a large amount of data, we opted for a cautious approach in compiling the block word list.\nTo further improve the detection rate of unsafe content, we fine-tuned the BERT model using the \u201cToxic Comment Classification Challenge\u201d dataset from Kaggle, resulting in a toxicity classifier. We sampled some data from Dedup data and annotated it using the Perspective API333https://perspectiveapi.com/ to create a pornography classification dataset. We then fine-tuned the BERT model with this dataset, yielding a pornography classifier. Finally, we used these two classifiers for secondary filtering of the data, filtering out data with scores below the threshold, resulting in Safe data.\nCompared to sources such as books, papers, and patents, internet-sourced data contains a significant amount of low-quality content. Based on our observations, the primary causes for this low-quality content are twofold: 1. The internet is rife with marketing advertisements, which tend to be repetitive and carry little information. 2. Many web pages consist of lists of article abstracts or product descriptions, resulting in extracted text that is difficult to read and lacks logical coherence.\nTo filter out these low-quality contents, we first organized manual data annotation. For the advertisements classification task, annotators were asked to identify whether a piece of data contains advertising content (both overall and partial advertisements are marked as low quality). For the fluency classification task, annotators were asked to rate the data on four dimensions: consistency, noise, information content, and grammar, resulting in a comprehensive fluency score. We then fine-tuned the BERT model using the manually annotated data, obtaining an advertisements classifier and a fluency classifier. Finally, we used these two classifiers for secondary filtering of the data, filtering out data with scores below the threshold, resulting in High-quality pre-train data.\n###figure_9### We collect data from various sources, including direct crawling from GitHub, public datasets, and online resources related to coding and programming, like Q&A forums, tutorial sites, and API documentation, the statistics is shown in Figure 4  ###reference_###  ###reference_###.\nTable 2  ###reference_###  ###reference_### reflects the data quality assessment based on a scoring model we trained. High-quality data will have a higher sampling weight and can undergo multiple training iterations in the pre-training phase. Moderate-quality data has a normal sampling weight and is typically trained once. Low-quality data are excluded, as our empirical findings affirm that removing them is vital for optimizing model performance and ensuring training stability despite their proportion being relatively small.\nAll data is converted to a unified markdown format. Nevertheless, a very small fraction of the data still exhibited corrupted HTML or XML formats. We applied a set of heuristic rules to minimize these occurrences, though we did not invest too much effort in format purification. Markdown was selected for its simplicity\u2014minimizing the token overhead for formatting\u2014and its compatibility with interleaving code and natural language. The real format used for the pre-training is more complex, involving the concatenation of multiple code files based on their dependencies. The main idea is to utilize the interleaving data, which is pivotal for teaching the model about programming. This point is also mentioned in recent studies (Guo et al., 2024  ###reference_b38###  ###reference_b38###).\nDeduplicating code data is similar to processing natural language except for tokenization, which impacts hyperparameter selection. For instance, Python examples use two spaces, four spaces, or a tab character to signify indentation. A conventional whitespace tokenizer, or one tailored for natural language, might mistakenly assess these samples as different data, which is inaccurate. Our insight is that an effective tokenizer is essential for applying a universal deduplication strategy. Although recent studies have explored fine-grained deduplication at the paragraph or line level, our approach remains at the file level to preserve context integrity.\nQuality is a pivotal yet nebulous aspect of pre-training in LLM research, primarily due to the difficulty in quantifying its impact on model performance regarding the scale. We adopted a hybrid, multi-stage filtering process including rule- and model-based scorers. Rule-based scorers are heuristic and varied, though we discovered that code style is not a reliable quality metric and can misclassify too many codes as low-quality. For the model-based scoring, we evaluated several backbone models, training them with roughly 50,000 labeled samples. However, we observed that the correlation between scorer assessments and human judgments varied across languages, and enlarging the training set did not substantially enhance scorer accuracy. Consequently, we only employed model-based scoring for languages where the model predictions align well with human evaluations on a human-annotated validated set.\n###figure_10### In order to obtain reliable annotations of our model-based scorer, we introduce an iterative annotation process (illustrated in Figure 5  ###reference_###  ###reference_###) to address the challenge that the definition of code quality data is vague. Identifying code that would be helpful for teaching an LLM is also non-trivial for human experts, for instance, a widely recognized code repository might be overly complex for a beginner. The proposed iterative workflow allows annotators to verify model predictions and refine the guidelines accordingly. To improve the annotation efficiency, we only ask the annotator to check the samples labeled by the scorer as high-quality and low-quality with high confidence. Besides, there is an automatic validation process in each iteration to ensure the previously annotated samples are correctly classified by the scorer, which is shown as yellow dot lines in the figure. In practice, we took three iterations to finalize our scoring model.\nThe training context window of InternLM2 has been expanded to 32,000 tokens, allowing the utilization of the entire context of a code repository. The structure of the repository may have been broken in previous data processing steps, such as the filtering of code files by their extensions and deduplication. So we first regroup code files originating from the same repository and perform dependency sorting to establish a sequence for concatenating these files. Therefore, a code repository will be viewed as a single long markdown file with code blocks, which allows the model to learn dependencies across files.\nWe employ regular expressions to detect the \u201cimport\u201d relations across various programming languages, and we use topological sorting to determine the concatenation order of files. In practice, the arrangement of files may break folder boundaries, resulting in an interleaving sequence of files from multiple sub-folders. Non-code files, such as markdowns and other documentation, are positioned preceding the first code file located in the same sub-folder.\nFor the corner cases like multiple paths between an ascendant and descendant and loops within the \u201cimport\u201d relation graph, we take the shorter path for the former and use the alphabet order to decide the start point for the latter. A trick in finding \u201cimport\u201d relations is to resolve the batched import, such as \u201c__init__.py\u201d or \u201c#include xx.h\u201d. Those files may import a bunch of unused dependencies, so we apply heuristic rules to refine our detection of \u201cimport\u201d relationships, ensuring that we accurately identify and process these relations at a finer level.\nOur data processing pipeline is designed to filter out low-quality long text data.\nIt comprises three stages: a) Length selection, a rule-based filter that selects data samples exceeding 32K bytes; b) Statistical filters, which leverage statistical features to identify and remove anomalous data; c) Perplexity filters, which utilize the difference in perplexity to assess the coherence between text segments, filtering out samples with distracting context. Note that all data chosen for the long context training is a subset of the standard pre-training corpus, meaning that the long context data will be learned at least twice during the pre-training.\nWe employ a variety of lexical and linguistic features to construct our statistical filters. Data samples failing to comply with established rules are excluded from the pre-training corpus. The full list of these filters can be found in Lv et al. (2024  ###reference_b60###  ###reference_b60###). A remarkable filter is the presence of conjunctions and other words that imply discourse structure, such as \u201cEspecially\u201d, \u201cFormally\u201d, etc. The overall guidance of designing such filters is to filter out the meaningless data rather than selecting the most high-quality data. Statistical filters are particularly effective with long text data, as the statistical features are far more consistent than those in short text data. For instance, a 20-token text may not yield reliable statistics, but a 32K-token text will have a much clearer statistical feature distribution.\nPerplexity is often treated as an estimator for the probability of a text sequence, , and we slightly change its usage to estimate the conditional probability between two text segments , where  is preceding of . When the  and  are strongly correlated, the conditional probability should be higher than estimating the probability of  alone, which also means a negative perplexity difference. Conversely, if the probability changed in the reverse direction, meaning that the  is a distracting context, it should be removed from the pre-training corpus. Ideally, adding more context should not negatively impact the predictability of subsequent text. However, we\u2019ve observed exceptions in cases of improperly joined texts, such as failed HTML parsing, random social media snippets, and other instances stemming from recognition errors in sources with complex layouts. Note that we only filter the data based on the perplexity difference rather than the perplexity itself, and this could largely mitigate the bias introduced by the estimator itself (using which model to compute the perplexity). The bias of the perplexity estimator has been discussed in Wettig et al. (2024  ###reference_b88###  ###reference_b88###); Sachdeva et al. (2024  ###reference_b74###  ###reference_b74###).\nSelecting appropriate thresholds is a challenging yet essential part of the data filtering process, and this challenge becomes more severe because we have built many filters. We have two lessons on setting thresholds:\nTailoring thresholds to each domain rather than seeking a universal solution. For example, the statistical filter for conjunction words would not apply to long code data, which typically does not have any conjunction. Similarly, textbooks, research papers, novels, and patents each exhibit unique characteristics. A universal threshold would likely misclassify a significant amount of data. The same logic applies to setting thresholds across different languages; thus, we adjust thresholds for each domain individually.\nEmploying a validation set to streamline the process, focusing only on borderline cases. Unlike learning-based feature extractors or scorers, our statistical and perplexity filters yield smooth results within the same domain. It allows us to focus on samples lying near the threshold, simplifying the adjustment of thresholds since we only need to determine whether to lower or raise them. Lv et al. (2024  ###reference_b60###  ###reference_b60###) illustrates the data clusters alongside scores from specific filters, demonstrating the interpretability of our proposed filters.\nFigure 6  ###reference_###  ###reference_### shows the data distribution before and after applying all proposed filters. The entire filtering process eliminates a large proportion of Web page data (Common Crawl) and Patent, while most Book and Paper data are persevered.\n###figure_11### ###figure_12###"
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Text Data",
            "text": "The text data in our pre-training dataset can be categorized by source into web pages, papers, patents, and books. To transform these sources into a pre-training dataset, we first standardize all data into a specified format, categorize them by type and language, and store them in JSON Lines (jsonl) format. Then, for all data, we apply several processing steps including rule-based filtering, data deduplication, safety filtering, and quality filtering. This results in a rich, safe, and high-quality text dataset.\nWe have statistically analyzed the number of documents, data storage size, and data Bytes proportion in the pre-training dataset according to their sources, as shown in Table 1  ###reference_###  ###reference_###  ###reference_###. Among them, the Chinese and English data from web pages account for 86.46% of the total, making it the primary source. Although the data volume from other sources is relatively small, such as books and technical literature(abbreviated as techlit), the average document length is longer and the content quality is relatively higher, making them equally important.\nThe data processing pipeline used in this work is shown in Figure 3  ###reference_###  ###reference_###  ###reference_###. The entire data processing pipeline first standardizes data from different sources to obtain Format data. Then, heuristic statistical rules are used for data filtering to get Clean data. Next, the Locality-Sensitive Hashing (LSH) method is used for data deduplication to obtain Dedup data. We then apply a composite safety strategy to filter the data, resulting in Safe data. We have adopted different quality filtering strategies for data from various sources, ultimately obtaining High-quality pre-training data.\n###figure_13### We will detail the data processing pipeline using web page data as an example. Our web page data mainly comes from Common Crawl111https://commoncrawl.org/. Firstly, we need to decompress the original Warc format files and use Trafilatura (Barbaresi, 2021  ###reference_b13###  ###reference_b13###  ###reference_b13###) for HTML parsing and main text extraction. Then, we use the pycld2222https://pypi.org/project/pycld2/ library for language detection and classification of the main text. Finally, we assign a unique identifier to the data and store it in jsonl (JSON lines) format and obtained Format data.\nWeb page data randomly extracted from the internet often contains a large amount of low-quality data, such as parsing errors, formatting errors, and non-natural language text. A common practice is to design rule-based regularization and filtering methods to modify and filter the data, as seen in Gopher (Rae et al., 2021  ###reference_b70###  ###reference_b70###  ###reference_b70###), C4 (Dodge et al., 2021  ###reference_b33###  ###reference_b33###  ###reference_b33###), and RefinedWeb (Penedo et al., 2023  ###reference_b67###  ###reference_b67###  ###reference_b67###). Based on our observations of the data, we have designed a series of heuristic filtering rules that focus on anomalies in separation and line breaks, frequency of abnormal characters, and distribution of punctuation marks. By applying these filters, we obtained Clean data.\nA large amount of duplicate texts exist on the Internet, which can negatively impact model training. Therefore, we employed a method based on Locality-Sensitive Hashing (LSH) to perform fuzzy deduplication on the data. More specifically, we used the MinHash method (Broder, 1997  ###reference_b16###  ###reference_b16###  ###reference_b16###), establishing signatures with 128 hash functions on the 5-gram of the documents, and using 0.7 as the threshold for deduplication. We aimed to retain the most recent data, that is, prioritizing data with larger CC dumps numbers. We obtained the Dedup data after LSH deduplication.\nThe internet is rife with toxic and pornographic content, the use of which for model training can negatively impact performance and increase the likelihood of unsafe content generation. Therefore, we employed a comprehensive safety strategy combining \u201cdomain blocking\u201d, \u201cword blocking\u201d, \u201cpornography classifier\u201d, and \u201ctoxicity classifier\u201d to filter the data. Specifically, we constructed a block domain list comprising approximately 13M unsafe domains and a block word list containing 36,289 unsafe words for preliminary data filtering. Given that word blocking might inadvertently exclude a large amount of data, we opted for a cautious approach in compiling the block word list.\nTo further improve the detection rate of unsafe content, we fine-tuned the BERT model using the \u201cToxic Comment Classification Challenge\u201d dataset from Kaggle, resulting in a toxicity classifier. We sampled some data from Dedup data and annotated it using the Perspective API333https://perspectiveapi.com/ to create a pornography classification dataset. We then fine-tuned the BERT model with this dataset, yielding a pornography classifier. Finally, we used these two classifiers for secondary filtering of the data, filtering out data with scores below the threshold, resulting in Safe data.\nCompared to sources such as books, papers, and patents, internet-sourced data contains a significant amount of low-quality content. Based on our observations, the primary causes for this low-quality content are twofold: 1. The internet is rife with marketing advertisements, which tend to be repetitive and carry little information. 2. Many web pages consist of lists of article abstracts or product descriptions, resulting in extracted text that is difficult to read and lacks logical coherence.\nTo filter out these low-quality contents, we first organized manual data annotation. For the advertisements classification task, annotators were asked to identify whether a piece of data contains advertising content (both overall and partial advertisements are marked as low quality). For the fluency classification task, annotators were asked to rate the data on four dimensions: consistency, noise, information content, and grammar, resulting in a comprehensive fluency score. We then fine-tuned the BERT model using the manually annotated data, obtaining an advertisements classifier and a fluency classifier. Finally, we used these two classifiers for secondary filtering of the data, filtering out data with scores below the threshold, resulting in High-quality pre-train data."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Code Data",
            "text": "Programming is a crucial skill for a LLM, offering support for a variety of downstream applications, such as coding assistance, software development, and building tool-use agents. Moreover, Groeneveld et al. (2024  ###reference_b37###) indicate the possibility of enhancing reasoning capabilities by training on code data, as code is generally well-structured, rigorous, and predictable than natural language.\n###figure_14### We collect data from various sources, including direct crawling from GitHub, public datasets, and online resources related to coding and programming, like Q&A forums, tutorial sites, and API documentation, the statistics is shown in Figure 4  ###reference_###  ###reference_###  ###reference_###.\nTable 2  ###reference_###  ###reference_###  ###reference_### reflects the data quality assessment based on a scoring model we trained. High-quality data will have a higher sampling weight and can undergo multiple training iterations in the pre-training phase. Moderate-quality data has a normal sampling weight and is typically trained once. Low-quality data are excluded, as our empirical findings affirm that removing them is vital for optimizing model performance and ensuring training stability despite their proportion being relatively small.\nAll data is converted to a unified markdown format. Nevertheless, a very small fraction of the data still exhibited corrupted HTML or XML formats. We applied a set of heuristic rules to minimize these occurrences, though we did not invest too much effort in format purification. Markdown was selected for its simplicity\u2014minimizing the token overhead for formatting\u2014and its compatibility with interleaving code and natural language. The real format used for the pre-training is more complex, involving the concatenation of multiple code files based on their dependencies. The main idea is to utilize the interleaving data, which is pivotal for teaching the model about programming. This point is also mentioned in recent studies (Guo et al., 2024  ###reference_b38###  ###reference_b38###  ###reference_b38###).\nDeduplicating code data is similar to processing natural language except for tokenization, which impacts hyperparameter selection. For instance, Python examples use two spaces, four spaces, or a tab character to signify indentation. A conventional whitespace tokenizer, or one tailored for natural language, might mistakenly assess these samples as different data, which is inaccurate. Our insight is that an effective tokenizer is essential for applying a universal deduplication strategy. Although recent studies have explored fine-grained deduplication at the paragraph or line level, our approach remains at the file level to preserve context integrity.\nQuality is a pivotal yet nebulous aspect of pre-training in LLM research, primarily due to the difficulty in quantifying its impact on model performance regarding the scale. We adopted a hybrid, multi-stage filtering process including rule- and model-based scorers. Rule-based scorers are heuristic and varied, though we discovered that code style is not a reliable quality metric and can misclassify too many codes as low-quality. For the model-based scoring, we evaluated several backbone models, training them with roughly 50,000 labeled samples. However, we observed that the correlation between scorer assessments and human judgments varied across languages, and enlarging the training set did not substantially enhance scorer accuracy. Consequently, we only employed model-based scoring for languages where the model predictions align well with human evaluations on a human-annotated validated set.\n###figure_15### In order to obtain reliable annotations of our model-based scorer, we introduce an iterative annotation process (illustrated in Figure 5  ###reference_###  ###reference_###  ###reference_###) to address the challenge that the definition of code quality data is vague. Identifying code that would be helpful for teaching an LLM is also non-trivial for human experts, for instance, a widely recognized code repository might be overly complex for a beginner. The proposed iterative workflow allows annotators to verify model predictions and refine the guidelines accordingly. To improve the annotation efficiency, we only ask the annotator to check the samples labeled by the scorer as high-quality and low-quality with high confidence. Besides, there is an automatic validation process in each iteration to ensure the previously annotated samples are correctly classified by the scorer, which is shown as yellow dot lines in the figure. In practice, we took three iterations to finalize our scoring model.\nThe training context window of InternLM2 has been expanded to 32,000 tokens, allowing the utilization of the entire context of a code repository. The structure of the repository may have been broken in previous data processing steps, such as the filtering of code files by their extensions and deduplication. So we first regroup code files originating from the same repository and perform dependency sorting to establish a sequence for concatenating these files. Therefore, a code repository will be viewed as a single long markdown file with code blocks, which allows the model to learn dependencies across files.\nWe employ regular expressions to detect the \u201cimport\u201d relations across various programming languages, and we use topological sorting to determine the concatenation order of files. In practice, the arrangement of files may break folder boundaries, resulting in an interleaving sequence of files from multiple sub-folders. Non-code files, such as markdowns and other documentation, are positioned preceding the first code file located in the same sub-folder.\nFor the corner cases like multiple paths between an ascendant and descendant and loops within the \u201cimport\u201d relation graph, we take the shorter path for the former and use the alphabet order to decide the start point for the latter. A trick in finding \u201cimport\u201d relations is to resolve the batched import, such as \u201c__init__.py\u201d or \u201c#include xx.h\u201d. Those files may import a bunch of unused dependencies, so we apply heuristic rules to refine our detection of \u201cimport\u201d relationships, ensuring that we accurately identify and process these relations at a finer level."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Pre-training Settings",
            "text": "In this part, we present tokenization and pre-training hyper-parameters."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Tokenization",
            "text": "We have chosen to utilize the tokenization approach of GPT-4 due to its exceptional efficiency in compressing a wide array of textual content. Our primary reference is the cl100k vocabulary 444https://github.com/openai/tiktoken  ###reference_###, which mainly encompasses English and programming language tokens, totaling 100,256 entries, with a minor inclusion of fewer than 3,000 Chinese tokens. To optimize compression rates for InternLM when processing Chinese text, while maintaining the overall vocabulary size under 100,000, we carefully selected the top 60,004 tokens from the cl100k vocabulary and integrated them with 32,397 Chinese tokens. Additionally, we included 147 spare tokens to round out the selection, culminating in a vocabulary size that aligns with a multiple of 256, thereby facilitating efficient training."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Pre-training Hyper-parameters",
            "text": "The basic hyper-parameters are listed in Table 3  ###reference_###. During training, AdamW (Loshchilov & Hutter, 2019  ###reference_b59###) with  and  is used to optimize the model. We use the cosine learning rate decay and the learning rate decays to 10% of its maximum."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Pre-training Phases",
            "text": "The total number of tokens used for pre-training the 1.8B, 7B, and 20B models ranges from 2.0T to 2.6T, and the pre-training process consists of three distinct phases. In the first phase, we used pre-training corpora with lengths not exceeding 4k. In the second phase, we included 50% of pre-training corpora with lengths not exceeding 32k. In the third phase, we utilized capability-specific enhancement data. During each phase, we mixed data in English, Chinese, and code."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 4k Context Training",
            "text": "For approximately 90% of the training steps, we trained using data with lengths of up to 4096 tokens. If the length of the data exceeded 4096, we would forcibly truncate it and use the remaining part for training as well."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Long Context Training",
            "text": "The utilization of extended context windows significantly enhances the performance of Large Language Models (LLMs) in a variety of applications, such as retrieval-augmented generation (Gao et al., 2023  ###reference_b36###) and intelligent agents (Xi et al., 2023  ###reference_b90###). Motivated by cutting-edge advancements in training for long context (Rozi\u00e8re et al., 2023  ###reference_b73###; Xiong et al., 2023  ###reference_b91###; Liu et al., 2023b  ###reference_b57###), our training process for InternLM2 begins with a 4K context corpus and subsequently transitions to a corpus with 32K context. Instead of only using 32k corpus, 50% of the data is still shorter than 4096 tokens. This long context training phase accounts for about 9% of the total steps.\nWhen accommodated to these longer sequences, we adjusted the Rotary Positional Embedding (RoPE) base from 50,000 to 1,000,000, ensuring more effective positional encoding for long contexts (Liu et al., 2023b  ###reference_b57###). Owing to the good scalability of InternEvo (Chen et al., 2024a  ###reference_b20###) and flash attention (Dao, 2023  ###reference_b32###), the training speed only decrease 40% when changing context window from 4K to 32K."
        },
        {
            "section_id": "3.3.3",
            "parent_section_id": "3.3",
            "section_name": "3.3.3 Capability Specific Enhancement Training",
            "text": "Capabilities such as reasoning, mathematical problem-solving, and knowledge memorizing are key abilities expected from large language models.\nHowever, in the pre-training process, high-quality capability-related data is sparsely distributed in the entire corpus, which makes it hard for models to be proficient at these mentioned capabilities. Previous works, such as Qwen (Bai et al., 2023a  ###reference_b10###), GLM-130B (Zeng et al., 2023  ###reference_b96###), Nemotron-4 (Parmar et al., 2024  ###reference_b66###), have tried to incorporate instruction-based or high quality data during the pre-training stage to enhance these abilities.\nIn InternLM2, we collect an enriched dataset with a meticulously curated mix of high-quality retrieved data and various types of open-source data from the huggingface\ndatasets platform 555https://huggingface.co/datasets  ###reference_huggingface.co/datasets###.\nIn total, we collect 24 Billion tokens in this dataset, and details of this corpus are shown in Table 4  ###reference_###.\nWe filter out test set related data and run a contamination test as illustrated in Section 5.4  ###reference_###. To make the model fit these data well, we employed a smaller learning rate and batch size.\nAfter this enhancement training phase, the InternLM2 model exhibited substantial performance improvements in coding, reasoning, question answering, and examinations, with detailed evaluations results shown in the following evaluation section.\nTo aid the research community, we have released checkpoints before and after the enhancement training, naming them InternLM2-{size}-Base and InternLM2-{size}, respectively."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Alignment",
            "text": "The pre-training stage empowers LLMs with the foundation abilities and knowledge that are necessary for solving various tasks.\nWe further fine-tune the LLMs to fully elicit their capabilities and guide LLMs to serve as helpful and harmless AI assistants.\nThis stage, also commonly referred to as \u2018Alignment\u2019, typically contains two phases: supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF).\nDuring SFT, we fine-tune the model to follow diverse human instructions by high-quality instruction data (Sec.4.1  ###reference_###).\nThen we propose COnditionalOnLine RLHF, which applies a novel conditional reward model that can reconcile different kinds of human preferences (e.g., multi-step reasoning accuracy, helpfulness, harmlessness), and conducts three-round online RLHF to reduce reward hacking (Sec. 4.2  ###reference_###.\nIn the alignment stage, we keep the long-context capability of LLMs by utilizing long-context pre-training data during SFT and RLHF 4.3  ###reference_###.\nWe also introduce our practices of improving the tool utilization capability of LLMs 4.4  ###reference_###.\nThe training process of the Conditional Reward Model involves an extensive dataset, encompassing various fields such as dialogue, article writing, poetry, summarization, coding, mathematics, and formatted output, with up to 2.4 million binarized preference pairs. This comprehensive dataset ensures the model\u2019s broad adaptability and enhances its capability to undertake reinforcement learning in a broader and more complex array of situations. Thus, by employing the conditional system prompt method, the reward model can respond to complex human requirements, providing a more nuanced control over the reward scores during the PPO phase.\nFurthermore, to reduce the influence of the imbalance between easy and difficult samples in the dataset, we modify the original ranking loss function (Burges et al., 2005  ###reference_b18###) inspired by Focal Loss (Lin et al., 2017  ###reference_b55###).\nWe add a difficulty decay coefficient to the ranking loss, making the loss value larger for difficult samples and smaller for easy ones, preventing overfitting on a large number of easy samples. The focal ranking loss is formulated as\nwhere  represents the probability that  is greater than . The difficulty decay coefficient only takes effect when the model correctly predicts the preference of the training sample, i.e., , otherwise it equals to 1. The term  represents a hyper-parameter that is instrumental in modulating the difficulty decay ratio. Here we set it to 2 by default. Concurrently, to ensure the stability and consistency of the output scores from the reward model across different training, we introduce a logarithmic barrier penalty to the reward scores to confine the score distribution within a range of -5 to 5, defined as\nThis constraint is critical as it obviates the need to modify additional reward-related hyper-parameters in the PPO stage, which could potentially arise due to variations of the reward score distribution in different reward models.\nOverall, the loss function of the reward model is\nThe parameter  is a weighting coefficient that balances the contribution of  and . We set it to a default value of 0.02 based on our observation in the preliminary experimental results. These enhancements improve the robustness and consistency of the reward model, particularly in the context of datasets characterized by an imbalance between easy and difficult samples.\nIn our experiment, we align the sizes of the reward models with those of the actor models used in PPO. Following the methods described in InstructGPT(Ouyang et al., 2022  ###reference_b65###), we initialize the reward models using the SFT model weights, modifying the output layer to a one-dimensional linear mapping layer, which was randomly initialized. Our batch construction strategy focuses on fixing the total length of preference data at 16384 tokens per batch, rather than limiting the number of preference pairs, to avoid training inefficiencies due to data padding. The maximum context length is set at 8192. A special token is appended to each sequence\u2019s end, with its output value used as the reward score. We adapt AdamW as the optimizer. The learning rate follows a cosine annealing schedule, decreasing from 1e-5 to 5e-6 and weight decay is set to 0.01. To prevent overfitting, the models are trained for one epoch.\nThe fast path in Online RLHF focuses on quickly identifying and rectifying reward hacking incidents through targeted patches to improve the reliability of the reward model. As the PPO training goes by, the LLMs are encouraged to gravitate towards high-reward regions, which usually expose more reward hacking scenarios that can be easily detected.\nAfter identifying the hacking pattern after each round of RLHF, we construct preference pairs that highlight these patterns by comparing responses generated by early and late-stage PPO models in the current round. Incorporating 20 to 100 such preference pairs into the training process is sufficient to prevent the reward model from the corresponding hacking pattern significantly. This process allows for a swift fix of the reward model to the emerging hacking behaviors, enhancing the reward model\u2019s reliability and adherence to desired outcomes.\nIn comparison to the fast path that focuses on fixing reward hacking, the slow path aims at a general improvement of the reward model\u2019s upper bound, especially the reliability and robustness of the reward model at the high-reward regions, by covering the LLMs responses from the most recent and capable models, following previous work (Bai et al., 2022  ###reference_b11###). To achieve this, responses generated by models at various stages of training (including the SFT model, early-stage PPO model, and late-stage PPO model) are used to form pairwise comparisons. These pairs are then presented to professional human annotators to label their preferences.\nSuch a process provides a more nuanced and thorough refinement of the reward model but requires extensive human annotation time. To improve the efficiency of online RLHF, we only use the accumulated human preferences of all previous models at the launch time of our experiments (i.e., which may not include the preferences of responses produced by models at the current round due to the time cost of human labeling). By continuously updating the model based on human feedback, the Slow Path ensures that the reward model evolves in tandem with the complexities and subtleties of human preferences.\nIn our implementation of Online RLHF, we conducted three rounds of refinement. In these cycles, we gathered thousands of preference patches and online preference data in the Fast Path to update the reward model, and use all the existing human preference data of responses from previous models.\nEach round of Online RLHF provided valuable insights, allowing us to dynamically adjust and refine the reward model, thereby enhancing the overall performance and reliability of language models trained with human feedback.\n###figure_18### As is common practice, we initialize the reference model and the actor model from the SFT model weights. The critic model is initialized from the reward model (excluding the linear head) and undergoes a 50-iteration pre-training phase, during which the actor model is frozen. This phase is critical for stabilizing the value estimation in early training, thus preventing the potentially detrimental effects of unstable values. We conducted ablation studies comparing the initialization of the critic model from the reward model versus the SFT model, as show in Figure 9  ###reference_###. Our results show that the critic model initialized from the reward model experiences larger losses during the first few iterations of PPO training, but after approximately 20 iterations, it consistently exhibits lower losses and leads to higher rewards for the actor model. We hypothesize that the higher loss observed during the initial stage may reveal fundamental differences between the tasks of reward modeling and critic modeling. The subsequent lower loss could be attributed to a more consistent internal understanding of the world knowledge and a better grasp of the assessment principles.\n###figure_19### As discussed earlier, our reward models are trained to adapt to various conditions. Consequently, for queries from different domains, we prepend appropriate conditional system prompts to every sampled response before calculating reward scores, as illustrated in Figure 10  ###reference_###. This practice ensures that the model\u2019s responses are contextually aligned with the varied demands of different domains.\nTo mitigate the risk of catastrophic forgetting during the PPO phase, we incorporate a pre-train loss, following the InstructGPT methodology. The coefficient for the pre-train loss is set at 0.5, and the volume of the pre-train data is approximately 50% of that for PPO training. This addition helps to preserve the knowledge acquired during the initial training stages, ensuring that the model retains its foundational abilities and knowledge base while adapting to new feedback and learning through PPO.\nWe set the KL divergence coefficient to 0.01. The learning rates for the actor model and the critic model are set to 1e-6 and 5e-6, respectively. We found that a larger  value for PPO leads to higher rewards in our case, so we set it to 0.99. We adopted a slightly conservative sampling strategy, with , to strike a balance between sampling diversity and convergence speed. Unlike some conventional approaches, we do not apply value loss clipping or advantage normalization. Despite extensive RL tricks, the training remains remarkably stable, partially due to our meticulous Online RLHF efforts.\nWe adopt a modified version of the ChatML format to enable general tool calling by introducing the \u201cenvironment\u201d role. Such a modification shares the same format in chat scenarios but provides more clear signal to the model when adopting agents. Additionally, we define two specific keywords to support the diverse purpose of AI agents, i.e., code interpreter (<|interpreter|>) and external plugins (<|plugin|>). This enables us to adopt a unified streaming format that can handle various types of plugin extensions and AI environments while being compatible with general chat. Figure 17  ###reference_### shows a concrete example of streaming chat format. To fully elicit the agent ability of InternLM2, we align the agent corpus to the chat domain and disentangle it along the basic capabilities of the language model for fine-grained training, as described in Agent-FLAN (Chen et al., 2024c  ###reference_b24###).\nWe also enhance the ability of InternLM2-Chat to solve the math problems by code interpreters, by treating the Python code interpreter as a special tool using the same schema described in Tool Learning. We adopt the reasoning interleaved with coding (RICO) strategy and construct the data in an iterative hard example mining manner, described in InternLM-Math (Ying et al., 2024  ###reference_b94###)."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Supervised Fine-Tuning",
            "text": "In the supervised fine-tuning (SFT) stage, we use a dataset of 10 million instruction data instances, which have been screened to ensure their helpfulness and harmlessness. The dataset encompasses a diverse range of topics, including general conversation, NLP tasks, mathematical problems, code generation and function calls, etc. Figure 7  ###reference_### shows the detailed distribution of SFT data topics. To facilitate a versatile representation of such various tasks, we transform the data samples into the ChatML (Cha,  ###reference_b1###) format. Both the 7B and 20B models undergo training for one epoch using the AdamW optimizer with an initial learning rate of 4e-5.\n###figure_20###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "COOL Reinforcement Learning from Human Feedback",
            "text": "Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017  ###reference_b26###; Ouyang et al., 2022  ###reference_b65###) is an innovative approach within the realm of large language models. By incorporating human feedback, RLHF creates reward models that serve as proxies for human preferences, thereby providing reward signals for LLMs to learn through the use of Proximal Policy Optimization (PPO) (Schulman et al., 2017  ###reference_b77###). This methodology enables models to better understand and execute tasks that are difficult to define through traditional methods.\nDespite the achievements of RLHF, there are still some issues in its practical application. The first is the preference conflicts. For example, in developing a dialogue system, we expect it to provide useful information (helpful) while not producing harmful or inappropriate content (harmless). However, these two preferences often cannot be satisfied simultaneously in practice, as providing useful information might involve sensitive or high-risk content in some cases. Existing RLHF methods (Touvron et al., 2023b  ###reference_b85###; Dai et al., 2023  ###reference_b31###; Wu et al., 2023  ###reference_b89###) usually rely on multiple preference models for scoring, which also introduces more models in the training pipeline thus increases computational cost and slows down training speed.\nSecond, RLHF faces the issue of reward hacking, especially when the policy becomes more powerful with the scale increasing (Manheim & Garrabrant, 2018  ###reference_b61###; Gao et al., 2022  ###reference_b35###), where the model might learn to \u201ccheat\u201d the reward system by shortcuts to obtain high scores, rather than truly learning the expected behavior. This leads the model to maximize rewards in unintended ways, significantly affecting the effectiveness and reliability of LLMs.\nTo address these issues, we propose Conditional OnLine RLHF (COOL RLHF). COOL RLHF first introduces a Conditional Reward mechanism to reconcile diverse preferences, which allows the reward model to dynamically allocate its attention to various preferences based on specific conditions, thereby optimally integrating multiple preferences. Furthermore, COOL RLHF adopts a multi-round Online RLHF strategy to enable the LLM to promptly adapt to new human feedback, mitigating the occurrence of reward hacking.\nThe training process of the Conditional Reward Model involves an extensive dataset, encompassing various fields such as dialogue, article writing, poetry, summarization, coding, mathematics, and formatted output, with up to 2.4 million binarized preference pairs. This comprehensive dataset ensures the model\u2019s broad adaptability and enhances its capability to undertake reinforcement learning in a broader and more complex array of situations. Thus, by employing the conditional system prompt method, the reward model can respond to complex human requirements, providing a more nuanced control over the reward scores during the PPO phase.\nFurthermore, to reduce the influence of the imbalance between easy and difficult samples in the dataset, we modify the original ranking loss function (Burges et al., 2005  ###reference_b18###  ###reference_b18###) inspired by Focal Loss (Lin et al., 2017  ###reference_b55###  ###reference_b55###).\nWe add a difficulty decay coefficient to the ranking loss, making the loss value larger for difficult samples and smaller for easy ones, preventing overfitting on a large number of easy samples. The focal ranking loss is formulated as\nwhere  represents the probability that  is greater than . The difficulty decay coefficient only takes effect when the model correctly predicts the preference of the training sample, i.e., , otherwise it equals to 1. The term  represents a hyper-parameter that is instrumental in modulating the difficulty decay ratio. Here we set it to 2 by default. Concurrently, to ensure the stability and consistency of the output scores from the reward model across different training, we introduce a logarithmic barrier penalty to the reward scores to confine the score distribution within a range of -5 to 5, defined as\nThis constraint is critical as it obviates the need to modify additional reward-related hyper-parameters in the PPO stage, which could potentially arise due to variations of the reward score distribution in different reward models.\nOverall, the loss function of the reward model is\nThe parameter  is a weighting coefficient that balances the contribution of  and . We set it to a default value of 0.02 based on our observation in the preliminary experimental results. These enhancements improve the robustness and consistency of the reward model, particularly in the context of datasets characterized by an imbalance between easy and difficult samples.\nIn our experiment, we align the sizes of the reward models with those of the actor models used in PPO. Following the methods described in InstructGPT(Ouyang et al., 2022  ###reference_b65###  ###reference_b65###), we initialize the reward models using the SFT model weights, modifying the output layer to a one-dimensional linear mapping layer, which was randomly initialized. Our batch construction strategy focuses on fixing the total length of preference data at 16384 tokens per batch, rather than limiting the number of preference pairs, to avoid training inefficiencies due to data padding. The maximum context length is set at 8192. A special token is appended to each sequence\u2019s end, with its output value used as the reward score. We adapt AdamW as the optimizer. The learning rate follows a cosine annealing schedule, decreasing from 1e-5 to 5e-6 and weight decay is set to 0.01. To prevent overfitting, the models are trained for one epoch.\nThe fast path in Online RLHF focuses on quickly identifying and rectifying reward hacking incidents through targeted patches to improve the reliability of the reward model. As the PPO training goes by, the LLMs are encouraged to gravitate towards high-reward regions, which usually expose more reward hacking scenarios that can be easily detected.\nAfter identifying the hacking pattern after each round of RLHF, we construct preference pairs that highlight these patterns by comparing responses generated by early and late-stage PPO models in the current round. Incorporating 20 to 100 such preference pairs into the training process is sufficient to prevent the reward model from the corresponding hacking pattern significantly. This process allows for a swift fix of the reward model to the emerging hacking behaviors, enhancing the reward model\u2019s reliability and adherence to desired outcomes.\nIn comparison to the fast path that focuses on fixing reward hacking, the slow path aims at a general improvement of the reward model\u2019s upper bound, especially the reliability and robustness of the reward model at the high-reward regions, by covering the LLMs responses from the most recent and capable models, following previous work (Bai et al., 2022  ###reference_b11###  ###reference_b11###). To achieve this, responses generated by models at various stages of training (including the SFT model, early-stage PPO model, and late-stage PPO model) are used to form pairwise comparisons. These pairs are then presented to professional human annotators to label their preferences.\nSuch a process provides a more nuanced and thorough refinement of the reward model but requires extensive human annotation time. To improve the efficiency of online RLHF, we only use the accumulated human preferences of all previous models at the launch time of our experiments (i.e., which may not include the preferences of responses produced by models at the current round due to the time cost of human labeling). By continuously updating the model based on human feedback, the Slow Path ensures that the reward model evolves in tandem with the complexities and subtleties of human preferences.\nIn our implementation of Online RLHF, we conducted three rounds of refinement. In these cycles, we gathered thousands of preference patches and online preference data in the Fast Path to update the reward model, and use all the existing human preference data of responses from previous models.\nEach round of Online RLHF provided valuable insights, allowing us to dynamically adjust and refine the reward model, thereby enhancing the overall performance and reliability of language models trained with human feedback.\n###figure_21### As is common practice, we initialize the reference model and the actor model from the SFT model weights. The critic model is initialized from the reward model (excluding the linear head) and undergoes a 50-iteration pre-training phase, during which the actor model is frozen. This phase is critical for stabilizing the value estimation in early training, thus preventing the potentially detrimental effects of unstable values. We conducted ablation studies comparing the initialization of the critic model from the reward model versus the SFT model, as show in Figure 9  ###reference_###  ###reference_###. Our results show that the critic model initialized from the reward model experiences larger losses during the first few iterations of PPO training, but after approximately 20 iterations, it consistently exhibits lower losses and leads to higher rewards for the actor model. We hypothesize that the higher loss observed during the initial stage may reveal fundamental differences between the tasks of reward modeling and critic modeling. The subsequent lower loss could be attributed to a more consistent internal understanding of the world knowledge and a better grasp of the assessment principles.\n###figure_22### As discussed earlier, our reward models are trained to adapt to various conditions. Consequently, for queries from different domains, we prepend appropriate conditional system prompts to every sampled response before calculating reward scores, as illustrated in Figure 10  ###reference_###  ###reference_###. This practice ensures that the model\u2019s responses are contextually aligned with the varied demands of different domains.\nTo mitigate the risk of catastrophic forgetting during the PPO phase, we incorporate a pre-train loss, following the InstructGPT methodology. The coefficient for the pre-train loss is set at 0.5, and the volume of the pre-train data is approximately 50% of that for PPO training. This addition helps to preserve the knowledge acquired during the initial training stages, ensuring that the model retains its foundational abilities and knowledge base while adapting to new feedback and learning through PPO.\nWe set the KL divergence coefficient to 0.01. The learning rates for the actor model and the critic model are set to 1e-6 and 5e-6, respectively. We found that a larger  value for PPO leads to higher rewards in our case, so we set it to 0.99. We adopted a slightly conservative sampling strategy, with , to strike a balance between sampling diversity and convergence speed. Unlike some conventional approaches, we do not apply value loss clipping or advantage normalization. Despite extensive RL tricks, the training remains remarkably stable, partially due to our meticulous Online RLHF efforts."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Conditional Reward Model",
            "text": "The Conditional Reward Model represents an innovative solution to the challenges inherent in the previous preference modeling of RLHF methods. Unlike conventional approaches that typically rely on multiple preference models to address preference conflicts across different domains (Figure 8  ###reference_###(a)), the Conditional Reward Model incorporates different system prompts for different kinds of preferences to effectively model a variety of preferences within a singular reward model.\n###figure_23### Specifically, as depicted in Figure 8  ###reference_###(b), the Conditional Reward Model employs different system prompts to seamlessly blend data from various fields. Since the reward model is initialized from a SFT model, which already learned to follow diverse human instructions, we also let the reward model follow different system prompts to adapt to diverse preferences of different scenarios. In the Conditional Reward Model, system prompts are not simply a component of its input; they are also a crucial tool for directing the reward score in alignment with specific preferences in varied scenarios.\nSuch an integration facilitates the management of contradictory and complex human preferences within a unified reward model without sacrificing accuracy.\nThe training process of the Conditional Reward Model involves an extensive dataset, encompassing various fields such as dialogue, article writing, poetry, summarization, coding, mathematics, and formatted output, with up to 2.4 million binarized preference pairs. This comprehensive dataset ensures the model\u2019s broad adaptability and enhances its capability to undertake reinforcement learning in a broader and more complex array of situations. Thus, by employing the conditional system prompt method, the reward model can respond to complex human requirements, providing a more nuanced control over the reward scores during the PPO phase.\nFurthermore, to reduce the influence of the imbalance between easy and difficult samples in the dataset, we modify the original ranking loss function (Burges et al., 2005  ###reference_b18###  ###reference_b18###  ###reference_b18###) inspired by Focal Loss (Lin et al., 2017  ###reference_b55###  ###reference_b55###  ###reference_b55###).\nWe add a difficulty decay coefficient to the ranking loss, making the loss value larger for difficult samples and smaller for easy ones, preventing overfitting on a large number of easy samples. The focal ranking loss is formulated as\nwhere  represents the probability that  is greater than . The difficulty decay coefficient only takes effect when the model correctly predicts the preference of the training sample, i.e., , otherwise it equals to 1. The term  represents a hyper-parameter that is instrumental in modulating the difficulty decay ratio. Here we set it to 2 by default. Concurrently, to ensure the stability and consistency of the output scores from the reward model across different training, we introduce a logarithmic barrier penalty to the reward scores to confine the score distribution within a range of -5 to 5, defined as\nThis constraint is critical as it obviates the need to modify additional reward-related hyper-parameters in the PPO stage, which could potentially arise due to variations of the reward score distribution in different reward models.\nOverall, the loss function of the reward model is\nThe parameter  is a weighting coefficient that balances the contribution of  and . We set it to a default value of 0.02 based on our observation in the preliminary experimental results. These enhancements improve the robustness and consistency of the reward model, particularly in the context of datasets characterized by an imbalance between easy and difficult samples.\nIn our experiment, we align the sizes of the reward models with those of the actor models used in PPO. Following the methods described in InstructGPT(Ouyang et al., 2022  ###reference_b65###  ###reference_b65###  ###reference_b65###), we initialize the reward models using the SFT model weights, modifying the output layer to a one-dimensional linear mapping layer, which was randomly initialized. Our batch construction strategy focuses on fixing the total length of preference data at 16384 tokens per batch, rather than limiting the number of preference pairs, to avoid training inefficiencies due to data padding. The maximum context length is set at 8192. A special token is appended to each sequence\u2019s end, with its output value used as the reward score. We adapt AdamW as the optimizer. The learning rate follows a cosine annealing schedule, decreasing from 1e-5 to 5e-6 and weight decay is set to 0.01. To prevent overfitting, the models are trained for one epoch."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Online RLHF",
            "text": "After obtaining a conditional reward model, we conduct Proximal Policy Optimization (PPO) to align the LLMs to the human preferences modeled by the reward model Ouyang et al. (2022  ###reference_b65###).\nTo address the challenge of reward hacking in the PPO stage, we introduce an Online RLHF approach, divided into two distinct pathways: a Fast Path for immediate, targeted improvements and a Slow Path for long-term, comprehensive refinement of the reward model. The Fast and Slow Paths are complementary to provide an adaptive framework for mitigating reward hacking and enhancing the performance and reliability of LLMs trained with human feedback.\nThe fast path in Online RLHF focuses on quickly identifying and rectifying reward hacking incidents through targeted patches to improve the reliability of the reward model. As the PPO training goes by, the LLMs are encouraged to gravitate towards high-reward regions, which usually expose more reward hacking scenarios that can be easily detected.\nAfter identifying the hacking pattern after each round of RLHF, we construct preference pairs that highlight these patterns by comparing responses generated by early and late-stage PPO models in the current round. Incorporating 20 to 100 such preference pairs into the training process is sufficient to prevent the reward model from the corresponding hacking pattern significantly. This process allows for a swift fix of the reward model to the emerging hacking behaviors, enhancing the reward model\u2019s reliability and adherence to desired outcomes.\nIn comparison to the fast path that focuses on fixing reward hacking, the slow path aims at a general improvement of the reward model\u2019s upper bound, especially the reliability and robustness of the reward model at the high-reward regions, by covering the LLMs responses from the most recent and capable models, following previous work (Bai et al., 2022  ###reference_b11###  ###reference_b11###  ###reference_b11###). To achieve this, responses generated by models at various stages of training (including the SFT model, early-stage PPO model, and late-stage PPO model) are used to form pairwise comparisons. These pairs are then presented to professional human annotators to label their preferences.\nSuch a process provides a more nuanced and thorough refinement of the reward model but requires extensive human annotation time. To improve the efficiency of online RLHF, we only use the accumulated human preferences of all previous models at the launch time of our experiments (i.e., which may not include the preferences of responses produced by models at the current round due to the time cost of human labeling). By continuously updating the model based on human feedback, the Slow Path ensures that the reward model evolves in tandem with the complexities and subtleties of human preferences.\nIn our implementation of Online RLHF, we conducted three rounds of refinement. In these cycles, we gathered thousands of preference patches and online preference data in the Fast Path to update the reward model, and use all the existing human preference data of responses from previous models.\nEach round of Online RLHF provided valuable insights, allowing us to dynamically adjust and refine the reward model, thereby enhancing the overall performance and reliability of language models trained with human feedback."
        },
        {
            "section_id": "4.2.3",
            "parent_section_id": "4.2",
            "section_name": "4.2.3 PPO Training Details",
            "text": "During the RL alignment phase, we adopted the standard PPO (Proximal Policy Optimization) algorithm and made several adaptions to it, ensuring a more stable training process. The framework involves four models: the actor model, critic model, reference model, and reward model. During training, the latter 2 models are frozen, and only the former 2 models are actively trained. Notably, all these models are of the same size, ensuring consistency in their capacity to process and generate data. We iterate through about 200k diverse queries in about 400 iterations and choose the best checkpoints on the validation sets for release.\n###figure_24### As is common practice, we initialize the reference model and the actor model from the SFT model weights. The critic model is initialized from the reward model (excluding the linear head) and undergoes a 50-iteration pre-training phase, during which the actor model is frozen. This phase is critical for stabilizing the value estimation in early training, thus preventing the potentially detrimental effects of unstable values. We conducted ablation studies comparing the initialization of the critic model from the reward model versus the SFT model, as show in Figure 9  ###reference_###  ###reference_###  ###reference_###. Our results show that the critic model initialized from the reward model experiences larger losses during the first few iterations of PPO training, but after approximately 20 iterations, it consistently exhibits lower losses and leads to higher rewards for the actor model. We hypothesize that the higher loss observed during the initial stage may reveal fundamental differences between the tasks of reward modeling and critic modeling. The subsequent lower loss could be attributed to a more consistent internal understanding of the world knowledge and a better grasp of the assessment principles.\n###figure_25### As discussed earlier, our reward models are trained to adapt to various conditions. Consequently, for queries from different domains, we prepend appropriate conditional system prompts to every sampled response before calculating reward scores, as illustrated in Figure 10  ###reference_###  ###reference_###  ###reference_###. This practice ensures that the model\u2019s responses are contextually aligned with the varied demands of different domains.\nTo mitigate the risk of catastrophic forgetting during the PPO phase, we incorporate a pre-train loss, following the InstructGPT methodology. The coefficient for the pre-train loss is set at 0.5, and the volume of the pre-train data is approximately 50% of that for PPO training. This addition helps to preserve the knowledge acquired during the initial training stages, ensuring that the model retains its foundational abilities and knowledge base while adapting to new feedback and learning through PPO.\nWe set the KL divergence coefficient to 0.01. The learning rates for the actor model and the critic model are set to 1e-6 and 5e-6, respectively. We found that a larger  value for PPO leads to higher rewards in our case, so we set it to 0.99. We adopted a slightly conservative sampling strategy, with , to strike a balance between sampling diversity and convergence speed. Unlike some conventional approaches, we do not apply value loss clipping or advantage normalization. Despite extensive RL tricks, the training remains remarkably stable, partially due to our meticulous Online RLHF efforts."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Long-Context Finetuning",
            "text": "To preserve the long-context capability of LLMs after fine-tuning, we keep using the long-context pre-training data in SFT and RLHF, inspired by previous work that adopts long-context pre-training corpus in SFT (Xiong et al., 2023  ###reference_b91###). Specifically, we utilize two types of data: one comprises long-context data from books, while the other is long-context data obtained from GitHub repositories and concatenated through specific paradigms, described below.\nTo enhance the data analysis capabilities of InternLM2, we choose the code repositories used in DS-1000 (Lai et al., 2023  ###reference_b51###) as the core repositories, which include Pandas, Numpy, Tensorflow, Scipy, Scikit-learn, PyTorch, and Matplotlib. We then search for repositories on GitHub with over 10,000 stars that referenced these core repositories and conducted the same filtering and data cleaning process as pre-training.\nFor each repository, we initially sort the acquired raw data using a depth-first approach, while simultaneously generating the required prompts that briefly describe the file content, as shown in Figure 11  ###reference_###. Subsequently, we concatenate the processed data in sequence until reaching a length of 32k.\nThe experimental results show that long-context code data improves not only the long-context capability of LLMs but also the code capabilities.\n###figure_26###"
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Tool-Augmented LLMs",
            "text": "We adopt a modified version of the ChatML format to enable general tool calling by introducing the \u201cenvironment\u201d role. Such a modification shares the same format in chat scenarios but provides more clear signal to the model when adopting agents. Additionally, we define two specific keywords to support the diverse purpose of AI agents, i.e., code interpreter (<|interpreter|>) and external plugins (<|plugin|>). This enables us to adopt a unified streaming format that can handle various types of plugin extensions and AI environments while being compatible with general chat. Figure 17  ###reference_###  ###reference_### shows a concrete example of streaming chat format. To fully elicit the agent ability of InternLM2, we align the agent corpus to the chat domain and disentangle it along the basic capabilities of the language model for fine-grained training, as described in Agent-FLAN (Chen et al., 2024c  ###reference_b24###  ###reference_b24###).\nWe also enhance the ability of InternLM2-Chat to solve the math problems by code interpreters, by treating the Python code interpreter as a special tool using the same schema described in Tool Learning. We adopt the reasoning interleaved with coding (RICO) strategy and construct the data in an iterative hard example mining manner, described in InternLM-Math (Ying et al., 2024  ###reference_b94###  ###reference_b94###)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Evaluation and Analysis",
            "text": "We report the result of base models in Table 5  ###reference_### and chat models in Table 6  ###reference_###.\nFor the base model, the InternLM2 series performs well among models with similar number of parameters. On 7B and 20B, InternLM2 shows a significant increase compared to InternLM2-Base, which proves that pre-training on general domain data and domain-enhanced corpus has advantages for comprehensive examination. For the AGIEval and GAOKAO tasks, which are collected from exams designed specifically for human, InternLM2 shows a greater increase compared to InternLM2-Base relative to other datasets.\nFor the chat model, the InternLM2 series also excels among models with similar number of parameters. By comparing the InternLM2-Chat-7B-SFT and InternLM2-Chat-7B models, it can be seen that COOL RLHF has little impact on comprehensive examination performance.\nWe report the result of base models in Table 7  ###reference_### and chat models in Table 8  ###reference_###. Thanks to its rigorous and high-quality training corpus, InternLM2 has demonstrated a remarkable competitive edge in tasks that involve language understanding and knowledge application. Consequently, it emerges as an excellent choice for a multitude of real-world applications where the reliance on robust language comprehension and extensive knowledge is paramount.\nReasoning ability reflects a model\u2019s capacity to understand, process, and manipulate abstract concepts, which is essential for tasks involving complex problem-solving and decision-making. We separately compare and demonstrate the performance of InternLM2 in Table 9  ###reference_### from two aspects: Base and Chat.\nBase Model:\nFor models around 7B, InternLM2-7B demonstrates superior performance on most datasets outside of BBH. Notably, its performance on WinoGrande (84.7) significantly surpasses that of Mistral-7B-v0.1 (75.3) by 9.4 points, ranking second only to its series counterpart InternLM2-20B (85.2) by less than one point. This showcases the strong commonsense reasoning capabilities of InternLM2-7B. Among all tested base models, InternLM2-20B achieves the best overall performance. Compared to InternLM2-20B-Base, the inclusion of domain-enhanced knowledge has led to noticeable improvements in commonsense reasoning, with InternLM2-20B showing an average increase of 10.4% across the tested reasoning datasets.\nChat Model:\nIn terms of reasoning within chat models, InternLM2 continues to lead, both in the 7B phase and the 1320B phase. RL models and SFT models exhibit similar effects on a significantly advanced basis of reasoning capabilities. Among these, the 7B parameter models even outperform most of the 1320B models on three test sets, such as Mixtral-8x7B-Instruct-v0.1 and Qwen-14B-Chat, and InternLM2-Chat-20B significantly outperforms GPT-3.5 across various aspects of reasoning test sets, such as in situational reasoning on HellaSwag () and challenging comprehensive reasoning on BBH (), demonstrating InternLM2\u2019s exceptional reasoning abilities in smaller-scale models.\nMathematical proficiency is an integral element of a model\u2019s cognitive and computational faculties. In Table 10  ###reference_###, we present the performance of the Base model across multiple math assessment sets of varying difficulty. At the 7B parameter scale, InternLM2-7B takes the leading positions. Specifically, in the basic arithmetic GSM8k set, InternLM2-7B (70.8) significantly outperforms other models, surpassing ChatGLM3-6B-Base (60.7) by 10.1 percentage points and nearly doubling the performance relative to InternLM2-7B-Base (36.0), demonstrating the powerful basic arithmetic capability of InternLM2-7B after incorporating domain-enhanced data. For more complex problem-solving tasks such as MATH and theorem proving in TheoremQA, InternLM2-7B, despite being slightly behind in MATH, achieves a score (10.5) that even surpassed the larger Qwen-14B-Base (10.4), indicating that InternLM2-7B excels not only in computational tasks but also in complex theorem proving. In the Bilingual math dataset MathBench, InternLM2 shows excellent performance both in English and Chinese.\nIn the 1320B parameter scale phase, InternLM2-20B outperforms all tested Base models in basic arithmetic and theorem proving, while Qwen-14B-Base demonstrates outstanding results in problem-solving tasks and both English and Chinese tests of MathBench.\nChat\nFor Chat models in Table 11  ###reference_###, InternLM2-Chat demonstrates the best performance in basic arithmetic GSM8K, complex applied mathematics MATH, and theoretical problems TheoremQA at both the 7B and 20B parameter scales. Notably, InternLM2-Chat-20B, which underwent COOL RLHF training, leads comprehensively across all metrics, outperforming the API model GPT-3.5 and the MoE model Mixtral-8x7B-Instruct-v0.1. In the bilingual tests of MathBench, InternLM2-Chat-20B also shows excellent performance.\nHumanEval\nHumanEval (Chen et al., 2021  ###reference_b19###) is a widely recognized dataset that serves as a benchmark for evaluating the performance of code generation models. It consists of 164 carefully crafted programming tasks, each composed of a Python function and an accompanying docstring to provide context and specifications. This dataset, featuring human-written code, plays a pivotal role in assessing the capabilities of Large Language Models (LLMs) when generating or completing programs.\nMBPP\nMBPP (Austin et al., 2021  ###reference_b8###) consists of 974 programming tasks that are solvable by entry-level programmers. These tasks range from simple numeric manipulations to more complex problems requiring external knowledge, such as defining specific integer sequences. We use the santinized version of MBPP, which only includes a subset of the data has been hand-verified by the authors.\nHumanEval-X HumanEval-X (Zheng et al., 2023b  ###reference_b101###) dataset is a multilingual extension of the original HumanEval benchmark, designed to evaluate the capabilities of code generation models across multiple programming languages. It consists of 164 handcrafted programming problems, each translated into five major languages: C++, Java, JavaScript, Go, and Python. This results in a total of 820 problem-solution pairs, supporting both code generation and code translation tasks. HumanEval-X allows for the assessment of a model\u2019s ability to generate functionally correct code and to translate code between different languages, using test cases to verify the correctness of the generated code. This benchmark facilitates a more comprehensive understanding of pre-trained multilingual code generation models and their potential applications in real-world programming tasks.\nTo assess the coding prowess of InternLM2, we conduct a series of experiments utilizing the widely recognized benchmarks MBPP (Austin et al., 2021  ###reference_b8###) and HumanEval (Chen et al., 2021  ###reference_b19###). Furthermore, to gauge the aptitude of code generation models for multiple programming languages, we extend our evaluation to include MBPP-CN, a Chinese adaptation of MBPP, and HumanEval-X, a multilingual expansion of the original HumanEval benchmark.\nAs depicted in Figure 13  ###reference_###, the InternLM2 model series achieve leading performance, especially on HumanEval, MBPP, and MBPP-CN, where the InternLM2-Chat-20B model surpasses the previous state-of-the-art by more than 10%, underscoring the InternLM2 series\u2019 exceptional proficiency in code generation tasks. Additionally, the InternLM2-Chat-20B model exhibits substantial improvement over the InternLM2-Chat-7B model in MBPP-CN benchmarks, yet it shows a slight decline in performance on HumanEval-X. This phenomenon might stem from the InternLM2-Chat-20B model being finely tuned for Chinese at the expense of its effectiveness in other languages.\nWe mainly evaluate the long-context modeling capability of InternLM2 on the following two benchmarks: L-Eval (An et al., 2023  ###reference_b5###) and LongBench (Bai et al., 2023b  ###reference_b12###).\nL-Eval. \nL-Eval is a long-context benchmark consisting of 18 subtasks888\nWe adopt the first version of L-Eval, corresponding to https://arxiv.org/abs/2307.11088v1  ###reference_###,\nincluding texts from various fields such as law, economy, and technology.\nL-Eval consists of 411 documents and over 2000 test cases, with an average document length of 7217 words.\nSubtasks in this dataset can be categorized into two major classes:\n5 close-ended tasks and 13 open-ended categories.\nClosed-ended tasks are evaluated using exact matching based accuracies,\nwhile open-ended tasks adopt the Rouge score as the metric.\nLongBench. \nLongBench is a long-context benchmark consisting of 21 subtasks with a total of 4750 test cases.\nIt is the first bilingual long-context benchmark, with an average English text length of 6711 words and an average Chinese text length of 13386 characters.\nThe 21 subtasks are divided into 6 types, providing a more comprehensive evaluation of the model\u2019s capabilities in various aspects.\nEvaluation Results. \nWe report the evaluation results of InternLM2 on long-context benchmarks in Table 15  ###reference_###.\nAll variants of InternLM2 have demonstrated strong long-context modeling performance across two benchmarks.\nInternLM2-Chat-20B-SFT achieves the best performance on L-Eval and outperforms its counterparts by a considerable margin.\nOn LongBench, InternLM2-Chat-7B-SFT outperforms other B Models models across 4 out of 6 subtask categories.\nIt obtains a 48.1 overall score, which is only slightly inferior to the 48.4 overall score of ChatGLM3-6B.\nMeanwhile, we noticed that for InternLM2, different parameter sizes do not lead to significant difference in long-context performance, which will be further investigated.\n###figure_27### \u201cNeedle-in-the-Haystack\u201d is a single-needle retrieval task, which is designed to test the Large Language Models\u2019 (LLMs) ability to recall a single piece of key information. This is done by inserting a crucial piece of information into a Haystack text of a target length999Text token length calculations use the GPT-4 tokenizer at various positions and then querying the model about this key information at the end of the prompt. This method precisely visualizes LLMs\u2019 recall capabilities at different positions within long texts of varying lengths. We design a Chinese Haystack following the original idea, and utilize the Skywork/ChineseDomainModelingEval dataset released by Wei et al. (2023  ###reference_b87###), ensuring diversity and quality in the sources of Chinese texts. This dataset covers a wide range of fields from finance to technology, offering high-quality, up-to-date Chinese articles and providing a stable benchmark for assessing different models\u2019 abilities to handle domain-specific long texts. For this experiment, we leverage the LMDeploy101010https://github.com/InternLM/lmdeploy  ###reference_### Contributors (2023a  ###reference_b28###) inference engine to accelerate the inference process. The results presented in Figure 13  ###reference_### effectively demonstrate InternLM2\u2019s capability for long-context modeling.\nWe utilize the external code interpreter and follow the ReAct protocol to evaluate LLM\u2019s ability to solve coding and mathematic problems. The results, as depicted in Figure 14  ###reference_###, demonstrate a substantial improvement even when using the code interpreter, particularly on the MATH dataset where the enhancement is notably significant.\nAs depicted in Figure 15  ###reference_###, regarding the recently introduced MathBench dataset, the utilization of a code interpreter results in improved performance for InternLM2\u2019s performance in most cases, and a minor decrease may be attributed to the incorrect usage of such interpreters. Moreover, notable improvements are observed within the Knowledge domain for InternLM2-20B-Chat and in the Application section for InternLM2-7B-Chat. These disparities can stem from a multitude of factors, including differences in the constitution of their respective training datasets.\nAs depicted in Figure 16  ###reference_###, the InternLM2 models consistently demonstrate superior or comparable performance compared to existing models across various benchmarks. Specifically, when comparing models of identical scale, InternLM2-Chat-7B emerges as the top performer on T-Eval and CIBench. Meanwhile, InternLM2-Chat-20B achieves competitive results on T-Eval while obtaining the highest scores on CIBench. Additionally, the InternLM2 series models achieve impressive results in Chinese, showcasing their proficiency in multiple languages.\n###figure_28### ###figure_29### AlpacaEval(Li et al., 2023b  ###reference_b53###) is a single-turn question-answer dataset with 805 questions. Its main purpose is to assess the helpfulness of model responses to humans, reflecting the alignment with human intentions through adversarial evaluation. AlpacaEval(v2) establishes a baseline model and uses GPT4-Turbo as the judge model to compare the baseline\u2019s answers with those of the model under evaluation. It selects the model that better aligns with human preferences and calculates the win rate.\nInstead of directly collecting the judge model\u2019s responses, AlpacaEval(v2) employs logit probabilities to analyze the judge model\u2019s preferences statistically. These preferences are then used as a weighting factor in the calculation of the weighted win rate.\nAs shown in Table 17  ###reference_###, InternLM2-20B achieved a win rate of 21.8, marking the highest SOTA result among the compared models and demonstrating its superior alignment performance. Moreover, the table indicates that the RLHF model outperforms the SFT model in terms of win rate, which underscores the effectiveness of the alignment strategy.\nMTBench(Zheng et al., 2023a  ###reference_b100###) is a two-round conversation dataset consisting of 80 questions that span eight dimensions: reasoning, roleplay, math, coding, writing, humanities, STEM, and information extraction. Each dimension contains 10 questions, which are subjected to two rounds of questioning. Initially, the model\u2019s ability to answer basic questions is tested; subsequently, it must follow additional, specific instructions to refine its previous response. Scores are assigned on a 1-10 scale by a judge model.\nAs we can see in Table 17  ###reference_###, InternLM2 achieves leading scores in both the 7B and 20B versions, with 7.7 and 7.9 respectively, demonstrating its reliable multi-turn conversational ability.\n###figure_30### CompassArena comprises 520 Chinese questions, encompassing knowledge, language, mathematics, reasoning, and creativity. Like AlpacaEval, it calculates the win rate of two models judged by GPT4-Turbo and employs double-blind testing by swapping the model order to mitigate position bias. As indicated in Table 17  ###reference_###, InternLM2 secures the highest win rate in the 7B version (28.7) and the 20B version (31.4).\nNote that the performance gap between InternLM2\u2019s 7B and 20B versions is relatively small. However, when compared to SFT models, InternLM2\u2019s RLHF model shows a significant improvement in performance. This suggests that the RLHF strategy substantially enhances InternLM2\u2019s alignment with human preferences, rather than just increasing the model size. Furthermore, the results broken down by category in Figure 16  ###reference_### reveal that the InternLM2 series possesses exceptionally strong Chinese creativity and language abilities, with a win rate that rivals that of GPT4-Turbo.\nAlignBench(Liu et al., 2023a  ###reference_b56###) is a Chinese subjective dataset comprising 683 question-answer pairs, categorized into eight areas, including Fundamental Language Ability, Advanced Chinese Understanding, Task-oriented Role Play, among others, covering various scenarios such as physics, history, music, and law. The dataset utilizes an in-house Judge Model called CritiqueLLM(Ke et al., 2023  ###reference_b46###) for evaluation. CritiqueLLM provides scores from 1 to 10 across various dimensions for each question and issues a final score. We present these final scores, as provided by CritiqueLLM, in Table 17  ###reference_### and detailed scores across different categories in Table 18  ###reference_###. As shown in Table 17  ###reference_###, InternLM2 achieves SOTA scores in both the 7B and 20B versions, with 6.1 and 6.8 respectively, outperforming GPT-3.5\u2019s score of 5.7.\nMoreover, both the 7B and 20B versions of InternLM2 exhibit significant performance enhancements post-RLHF compared to SFT models, underscoring the effectiveness of our RLHF strategy in improving the model\u2019s alignment with human preferences. An analysis of the detailed scores reveals areas for improvement in mathematical and reasoning abilities; however, InternLM2 excels in question-answering and role-playing tasks, offering strong subjective performance.\nIFEval(Zhou et al., 2023  ###reference_b103###) is designed to test models\u2019 instruction following ability, requiring responses to adhere to specific patterns, such as letter case restrictions and keyword limitations. IFEval encompasses 25 distinct types of instructions, constructing 541 questions that follow instructions, and employs rule-based evaluation to assess the correctness of models\u2019 responses for each question. Utilizing various statistical methods, IFEval offers four types of accuracy scores: prompt-level strict, prompt-level loose, instance-level strict, and instance-level loose. We present the average results of these four scores in Table 17  ###reference_###.\nAlthough English-based models like Llama2 and Mistral generally outperform Chinese-based models on IFEval, InternLM2 ranks second and third in the 7B phase (48.5) and the 13-20B phase (48.7), respectively. This indicates that, despite the challenges of instruction following tasks, InternLM2 maintains a leading position among models of similar size."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Overview",
            "text": "In this section, we provide a comprehensive evaluation and analysis of the language model\u2019s performance across various domains and tasks. The evaluation is structured into two main categories:\n(a) downstream tasks and (b) alignment. For each category, we further break down the evaluation into specific subtasks to provide a detailed understanding of the model\u2019s strengths and weaknesses. Lastly, we discuss the potential issue of data contamination in language models and its impact on model performance and reliability. All evaluations are performed using OpenCompass (Contributors, 2023b  ###reference_b29###) unless explicitly specified otherwise666https://github.com/open-compass/opencompass  ###reference_s###."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Performance on Downstream Tasks",
            "text": "We start by detailing evaluation protocols and performance metrics for multiple NLP tasks. We introduce datasets, explain our experimental setup, and then present results with in-depth analysis, comparing to state-of-the-art methods to showcase the effectiveness of our model. The performance assessment will be dissected through six key dimensions: (1) comprehensive examinations, (2) language and knowledge, (3) reasoning and mathematics, (4) multiple programming language\ncoding, (5) long-context modeling, (6) tool utilization.\nWe report the result of base models in Table 5  ###reference_###  ###reference_### and chat models in Table 6  ###reference_###  ###reference_###.\nFor the base model, the InternLM2 series performs well among models with similar number of parameters. On 7B and 20B, InternLM2 shows a significant increase compared to InternLM2-Base, which proves that pre-training on general domain data and domain-enhanced corpus has advantages for comprehensive examination. For the AGIEval and GAOKAO tasks, which are collected from exams designed specifically for human, InternLM2 shows a greater increase compared to InternLM2-Base relative to other datasets.\nFor the chat model, the InternLM2 series also excels among models with similar number of parameters. By comparing the InternLM2-Chat-7B-SFT and InternLM2-Chat-7B models, it can be seen that COOL RLHF has little impact on comprehensive examination performance.\nWe report the result of base models in Table 7  ###reference_###  ###reference_### and chat models in Table 8  ###reference_###  ###reference_###. Thanks to its rigorous and high-quality training corpus, InternLM2 has demonstrated a remarkable competitive edge in tasks that involve language understanding and knowledge application. Consequently, it emerges as an excellent choice for a multitude of real-world applications where the reliance on robust language comprehension and extensive knowledge is paramount.\nReasoning ability reflects a model\u2019s capacity to understand, process, and manipulate abstract concepts, which is essential for tasks involving complex problem-solving and decision-making. We separately compare and demonstrate the performance of InternLM2 in Table 9  ###reference_###  ###reference_### from two aspects: Base and Chat.\nBase Model:\nFor models around 7B, InternLM2-7B demonstrates superior performance on most datasets outside of BBH. Notably, its performance on WinoGrande (84.7) significantly surpasses that of Mistral-7B-v0.1 (75.3) by 9.4 points, ranking second only to its series counterpart InternLM2-20B (85.2) by less than one point. This showcases the strong commonsense reasoning capabilities of InternLM2-7B. Among all tested base models, InternLM2-20B achieves the best overall performance. Compared to InternLM2-20B-Base, the inclusion of domain-enhanced knowledge has led to noticeable improvements in commonsense reasoning, with InternLM2-20B showing an average increase of 10.4% across the tested reasoning datasets.\nChat Model:\nIn terms of reasoning within chat models, InternLM2 continues to lead, both in the 7B phase and the 1320B phase. RL models and SFT models exhibit similar effects on a significantly advanced basis of reasoning capabilities. Among these, the 7B parameter models even outperform most of the 1320B models on three test sets, such as Mixtral-8x7B-Instruct-v0.1 and Qwen-14B-Chat, and InternLM2-Chat-20B significantly outperforms GPT-3.5 across various aspects of reasoning test sets, such as in situational reasoning on HellaSwag () and challenging comprehensive reasoning on BBH (), demonstrating InternLM2\u2019s exceptional reasoning abilities in smaller-scale models.\nMathematical proficiency is an integral element of a model\u2019s cognitive and computational faculties. In Table 10  ###reference_###  ###reference_###, we present the performance of the Base model across multiple math assessment sets of varying difficulty. At the 7B parameter scale, InternLM2-7B takes the leading positions. Specifically, in the basic arithmetic GSM8k set, InternLM2-7B (70.8) significantly outperforms other models, surpassing ChatGLM3-6B-Base (60.7) by 10.1 percentage points and nearly doubling the performance relative to InternLM2-7B-Base (36.0), demonstrating the powerful basic arithmetic capability of InternLM2-7B after incorporating domain-enhanced data. For more complex problem-solving tasks such as MATH and theorem proving in TheoremQA, InternLM2-7B, despite being slightly behind in MATH, achieves a score (10.5) that even surpassed the larger Qwen-14B-Base (10.4), indicating that InternLM2-7B excels not only in computational tasks but also in complex theorem proving. In the Bilingual math dataset MathBench, InternLM2 shows excellent performance both in English and Chinese.\nIn the 1320B parameter scale phase, InternLM2-20B outperforms all tested Base models in basic arithmetic and theorem proving, while Qwen-14B-Base demonstrates outstanding results in problem-solving tasks and both English and Chinese tests of MathBench.\nChat\nFor Chat models in Table 11  ###reference_###  ###reference_###, InternLM2-Chat demonstrates the best performance in basic arithmetic GSM8K, complex applied mathematics MATH, and theoretical problems TheoremQA at both the 7B and 20B parameter scales. Notably, InternLM2-Chat-20B, which underwent COOL RLHF training, leads comprehensively across all metrics, outperforming the API model GPT-3.5 and the MoE model Mixtral-8x7B-Instruct-v0.1. In the bilingual tests of MathBench, InternLM2-Chat-20B also shows excellent performance.\nHumanEval\nHumanEval (Chen et al., 2021  ###reference_b19###  ###reference_b19###) is a widely recognized dataset that serves as a benchmark for evaluating the performance of code generation models. It consists of 164 carefully crafted programming tasks, each composed of a Python function and an accompanying docstring to provide context and specifications. This dataset, featuring human-written code, plays a pivotal role in assessing the capabilities of Large Language Models (LLMs) when generating or completing programs.\nMBPP\nMBPP (Austin et al., 2021  ###reference_b8###  ###reference_b8###) consists of 974 programming tasks that are solvable by entry-level programmers. These tasks range from simple numeric manipulations to more complex problems requiring external knowledge, such as defining specific integer sequences. We use the santinized version of MBPP, which only includes a subset of the data has been hand-verified by the authors.\nHumanEval-X HumanEval-X (Zheng et al., 2023b  ###reference_b101###  ###reference_b101###) dataset is a multilingual extension of the original HumanEval benchmark, designed to evaluate the capabilities of code generation models across multiple programming languages. It consists of 164 handcrafted programming problems, each translated into five major languages: C++, Java, JavaScript, Go, and Python. This results in a total of 820 problem-solution pairs, supporting both code generation and code translation tasks. HumanEval-X allows for the assessment of a model\u2019s ability to generate functionally correct code and to translate code between different languages, using test cases to verify the correctness of the generated code. This benchmark facilitates a more comprehensive understanding of pre-trained multilingual code generation models and their potential applications in real-world programming tasks.\nTo assess the coding prowess of InternLM2, we conduct a series of experiments utilizing the widely recognized benchmarks MBPP (Austin et al., 2021  ###reference_b8###  ###reference_b8###) and HumanEval (Chen et al., 2021  ###reference_b19###  ###reference_b19###). Furthermore, to gauge the aptitude of code generation models for multiple programming languages, we extend our evaluation to include MBPP-CN, a Chinese adaptation of MBPP, and HumanEval-X, a multilingual expansion of the original HumanEval benchmark.\nAs depicted in Figure 13  ###reference_###  ###reference_###, the InternLM2 model series achieve leading performance, especially on HumanEval, MBPP, and MBPP-CN, where the InternLM2-Chat-20B model surpasses the previous state-of-the-art by more than 10%, underscoring the InternLM2 series\u2019 exceptional proficiency in code generation tasks. Additionally, the InternLM2-Chat-20B model exhibits substantial improvement over the InternLM2-Chat-7B model in MBPP-CN benchmarks, yet it shows a slight decline in performance on HumanEval-X. This phenomenon might stem from the InternLM2-Chat-20B model being finely tuned for Chinese at the expense of its effectiveness in other languages.\nWe mainly evaluate the long-context modeling capability of InternLM2 on the following two benchmarks: L-Eval (An et al., 2023  ###reference_b5###  ###reference_b5###) and LongBench (Bai et al., 2023b  ###reference_b12###  ###reference_b12###).\nL-Eval. \nL-Eval is a long-context benchmark consisting of 18 subtasks888\nWe adopt the first version of L-Eval, corresponding to https://arxiv.org/abs/2307.11088v1  ###reference_###  ###reference_###,\nincluding texts from various fields such as law, economy, and technology.\nL-Eval consists of 411 documents and over 2000 test cases, with an average document length of 7217 words.\nSubtasks in this dataset can be categorized into two major classes:\n5 close-ended tasks and 13 open-ended categories.\nClosed-ended tasks are evaluated using exact matching based accuracies,\nwhile open-ended tasks adopt the Rouge score as the metric.\nLongBench. \nLongBench is a long-context benchmark consisting of 21 subtasks with a total of 4750 test cases.\nIt is the first bilingual long-context benchmark, with an average English text length of 6711 words and an average Chinese text length of 13386 characters.\nThe 21 subtasks are divided into 6 types, providing a more comprehensive evaluation of the model\u2019s capabilities in various aspects.\nEvaluation Results. \nWe report the evaluation results of InternLM2 on long-context benchmarks in Table 15  ###reference_###  ###reference_###.\nAll variants of InternLM2 have demonstrated strong long-context modeling performance across two benchmarks.\nInternLM2-Chat-20B-SFT achieves the best performance on L-Eval and outperforms its counterparts by a considerable margin.\nOn LongBench, InternLM2-Chat-7B-SFT outperforms other B Models models across 4 out of 6 subtask categories.\nIt obtains a 48.1 overall score, which is only slightly inferior to the 48.4 overall score of ChatGLM3-6B.\nMeanwhile, we noticed that for InternLM2, different parameter sizes do not lead to significant difference in long-context performance, which will be further investigated.\n###figure_31### \u201cNeedle-in-the-Haystack\u201d is a single-needle retrieval task, which is designed to test the Large Language Models\u2019 (LLMs) ability to recall a single piece of key information. This is done by inserting a crucial piece of information into a Haystack text of a target length999Text token length calculations use the GPT-4 tokenizer at various positions and then querying the model about this key information at the end of the prompt. This method precisely visualizes LLMs\u2019 recall capabilities at different positions within long texts of varying lengths. We design a Chinese Haystack following the original idea, and utilize the Skywork/ChineseDomainModelingEval dataset released by Wei et al. (2023  ###reference_b87###  ###reference_b87###), ensuring diversity and quality in the sources of Chinese texts. This dataset covers a wide range of fields from finance to technology, offering high-quality, up-to-date Chinese articles and providing a stable benchmark for assessing different models\u2019 abilities to handle domain-specific long texts. For this experiment, we leverage the LMDeploy101010https://github.com/InternLM/lmdeploy  ###reference_###  ###reference_### Contributors (2023a  ###reference_b28###  ###reference_b28###) inference engine to accelerate the inference process. The results presented in Figure 13  ###reference_###  ###reference_### effectively demonstrate InternLM2\u2019s capability for long-context modeling.\nWe utilize the external code interpreter and follow the ReAct protocol to evaluate LLM\u2019s ability to solve coding and mathematic problems. The results, as depicted in Figure 14  ###reference_###  ###reference_###, demonstrate a substantial improvement even when using the code interpreter, particularly on the MATH dataset where the enhancement is notably significant.\nAs depicted in Figure 15  ###reference_###  ###reference_###, regarding the recently introduced MathBench dataset, the utilization of a code interpreter results in improved performance for InternLM2\u2019s performance in most cases, and a minor decrease may be attributed to the incorrect usage of such interpreters. Moreover, notable improvements are observed within the Knowledge domain for InternLM2-20B-Chat and in the Application section for InternLM2-7B-Chat. These disparities can stem from a multitude of factors, including differences in the constitution of their respective training datasets.\nAs depicted in Figure 16  ###reference_###  ###reference_###, the InternLM2 models consistently demonstrate superior or comparable performance compared to existing models across various benchmarks. Specifically, when comparing models of identical scale, InternLM2-Chat-7B emerges as the top performer on T-Eval and CIBench. Meanwhile, InternLM2-Chat-20B achieves competitive results on T-Eval while obtaining the highest scores on CIBench. Additionally, the InternLM2 series models achieve impressive results in Chinese, showcasing their proficiency in multiple languages.\n###figure_32### ###figure_33###"
        },
        {
            "section_id": "5.2.1",
            "parent_section_id": "5.2",
            "section_name": "5.2.1 Comprehensive Examination",
            "text": "We conducted benchmarks on a series of exam-related datasets, which include:\nMMLU (Hendrycks et al., 2020  ###reference_b39###): A multiple-choice question dataset containing 57 subtasks, covering topics in humanities, social science, STEM, and others. We report 5-shot results.\nCMMLU (Li et al., 2023a  ###reference_b52###): A multiple-choice question dataset specific to China, containing 67 subtasks. In addition to humanities, social science, STEM, and others, it includes many Chinese-specific tasks. We report 5-shot results.\nC-Eval (Huang et al., 2023  ###reference_b43###): A multiple-choice question dataset containing 52 subtasks and 4 difficulty levels, covering topics in humanities, social science, STEM, and others. We report 5-shot results.\nAGIEval (Zhong et al., 2023  ###reference_b102###): A human-centric benchmark that includes both multiple-choice and open-ended questions. The questions are from 20 official, public, and high-standard admission and qualification exams intended for general human test-takers and report 0-shot results.\nGAOKAO-Bench (Zhang et al., 2023  ###reference_b98###): A dataset containing the Chinese college entrance examination (Gaokao) from 2010 to 2022, including both subjective and objective questions. We only evaluated the dataset of objective questions and report 0-shot results.\nWe report the result of base models in Table 5  ###reference_###  ###reference_###  ###reference_### and chat models in Table 6  ###reference_###  ###reference_###  ###reference_###.\nFor the base model, the InternLM2 series performs well among models with similar number of parameters. On 7B and 20B, InternLM2 shows a significant increase compared to InternLM2-Base, which proves that pre-training on general domain data and domain-enhanced corpus has advantages for comprehensive examination. For the AGIEval and GAOKAO tasks, which are collected from exams designed specifically for human, InternLM2 shows a greater increase compared to InternLM2-Base relative to other datasets.\nFor the chat model, the InternLM2 series also excels among models with similar number of parameters. By comparing the InternLM2-Chat-7B-SFT and InternLM2-Chat-7B models, it can be seen that COOL RLHF has little impact on comprehensive examination performance."
        },
        {
            "section_id": "5.2.2",
            "parent_section_id": "5.2",
            "section_name": "5.2.2 Language and Knowledge",
            "text": "TriviaQA (Joshi et al., 2017  ###reference_b45###): A dataset containing both reading comprehension and open-domain QA. On average, each question has 6 possible answers. We utilized the open-domain QA subset of the data and report 0-shot results.\nNaturalQuestions (Kwiatkowski et al., 2019  ###reference_b48###): A dataset of QA where the questions come from users and the answers are verified by experts. We report 0-shot results.\nC3 (Sun et al., 2020  ###reference_b81###): A free-form multiple-choice Chinese machine reading comprehension dataset. We report 0-shot results.\nRACE (Lai et al., 2017  ###reference_b50###): A reading comprehension dataset that includes English reading comprehension exam questions for Chinese middle and high school students aged 12 to 18. We use the subset for high school students and report 0-shot results.\nFLORES (Team et al., 2022  ###reference_b83###): A translation dataset extracted from Wikipedia, covering 101 languages. We evaluated the translation results from English to the other 100 languages and vice versa. For each pair of translation tasks, we selecte 100 samples and evaluate with BLEU. We report 8-shot results.\nWe report the result of base models in Table 7  ###reference_###  ###reference_###  ###reference_### and chat models in Table 8  ###reference_###  ###reference_###  ###reference_###. Thanks to its rigorous and high-quality training corpus, InternLM2 has demonstrated a remarkable competitive edge in tasks that involve language understanding and knowledge application. Consequently, it emerges as an excellent choice for a multitude of real-world applications where the reliance on robust language comprehension and extensive knowledge is paramount."
        },
        {
            "section_id": "5.2.3",
            "parent_section_id": "5.2",
            "section_name": "5.2.3 Reasoning and Mathematics",
            "text": "In this section, we will primarily validate the performance of InternLM2 in reasoning and mathematics, focusing on the following two parts of the test sets:\nReasoning Datasets:\nWinoGrande (Sakaguchi et al., 2020  ###reference_b75###): A commonsense reasoning dataset containing 44,000 multiple-choice questions with two options each. It requires the model to choose the appropriate entity word for the pronoun in the descriptive text based on the scenario.\nHellaSwag (Zellers et al., 2019  ###reference_b95###): A challenging dataset for evaluating commonsense natural language inference, consisting of 70,000 multiple-choice questions. Each question presents a scenario and four possible outcomes, asking to select the most reasonable conclusion.\nBigBench Hard (BBH) (Suzgun et al., 2023  ###reference_b82###): A test collection for large language models, BBH extracts 23 challenging tasks from BIG-Bench, where contemporary language models had not surpassed human performance at the time.\nMathematics Datasets:\nGSM8K-Test (Cobbe et al., 2021  ###reference_b27###): A dataset containing approximately 1,300 elementary-level situational math problems. The solution to these problems involves 2 to 8 steps, primarily using basic arithmetic operations (addition, subtraction, multiplication, and division) to perform a series of basic calculations to reach the final answer.\nMATH (Hendrycks et al., 2021  ###reference_b40###): A dataset of 12,500 challenging high school-level competition math problems, covering multiple areas from algebra to calculus. Each problem includes a complete step-by-step solution.\nTheoremQA (Chen et al., 2023a  ###reference_b22###): A STEM theorem-driven question and answer dataset containing 800 QA pairs, covering over 350 theorems in mathematics, EE&CS, physics, and finance. It tests the limitations of large language models in applying theorems to solve challenging university-level problems.\nMathBench (Anonymous, 2024b  ###reference_b7###): MathBench comprises 3709 questions with multiple stages of progressively increasing challenges. Each stage encompasses bilingual theoretical and application-oriented questions, with each question precisely tagged with a three-level label to indicate its fine-grained knowledge point.\nIn the evaluation of reasoning and mathematics problems, for multiple-choice questions, we predominantly use the zero-shot approach. For open-ended questions, such as those in GSM8k, MATH, and the open-ended section of MathBench, we primarily employ a few-shot methodology to enhance the model\u2019s ability to follow instructions, which facilitates the extraction of answers. To ensure consistency in evaluation results, for the base models, we utilize perplexity (PPL) evaluation as the main method for assessing multiple-choice questions.\nReasoning ability reflects a model\u2019s capacity to understand, process, and manipulate abstract concepts, which is essential for tasks involving complex problem-solving and decision-making. We separately compare and demonstrate the performance of InternLM2 in Table 9  ###reference_###  ###reference_###  ###reference_### from two aspects: Base and Chat.\nBase Model:\nFor models around 7B, InternLM2-7B demonstrates superior performance on most datasets outside of BBH. Notably, its performance on WinoGrande (84.7) significantly surpasses that of Mistral-7B-v0.1 (75.3) by 9.4 points, ranking second only to its series counterpart InternLM2-20B (85.2) by less than one point. This showcases the strong commonsense reasoning capabilities of InternLM2-7B. Among all tested base models, InternLM2-20B achieves the best overall performance. Compared to InternLM2-20B-Base, the inclusion of domain-enhanced knowledge has led to noticeable improvements in commonsense reasoning, with InternLM2-20B showing an average increase of 10.4% across the tested reasoning datasets.\nChat Model:\nIn terms of reasoning within chat models, InternLM2 continues to lead, both in the 7B phase and the 1320B phase. RL models and SFT models exhibit similar effects on a significantly advanced basis of reasoning capabilities. Among these, the 7B parameter models even outperform most of the 1320B models on three test sets, such as Mixtral-8x7B-Instruct-v0.1 and Qwen-14B-Chat, and InternLM2-Chat-20B significantly outperforms GPT-3.5 across various aspects of reasoning test sets, such as in situational reasoning on HellaSwag () and challenging comprehensive reasoning on BBH (), demonstrating InternLM2\u2019s exceptional reasoning abilities in smaller-scale models.\nMathematical proficiency is an integral element of a model\u2019s cognitive and computational faculties. In Table 10  ###reference_###  ###reference_###  ###reference_###, we present the performance of the Base model across multiple math assessment sets of varying difficulty. At the 7B parameter scale, InternLM2-7B takes the leading positions. Specifically, in the basic arithmetic GSM8k set, InternLM2-7B (70.8) significantly outperforms other models, surpassing ChatGLM3-6B-Base (60.7) by 10.1 percentage points and nearly doubling the performance relative to InternLM2-7B-Base (36.0), demonstrating the powerful basic arithmetic capability of InternLM2-7B after incorporating domain-enhanced data. For more complex problem-solving tasks such as MATH and theorem proving in TheoremQA, InternLM2-7B, despite being slightly behind in MATH, achieves a score (10.5) that even surpassed the larger Qwen-14B-Base (10.4), indicating that InternLM2-7B excels not only in computational tasks but also in complex theorem proving. In the Bilingual math dataset MathBench, InternLM2 shows excellent performance both in English and Chinese.\nIn the 1320B parameter scale phase, InternLM2-20B outperforms all tested Base models in basic arithmetic and theorem proving, while Qwen-14B-Base demonstrates outstanding results in problem-solving tasks and both English and Chinese tests of MathBench.\nChat\nFor Chat models in Table 11  ###reference_###  ###reference_###  ###reference_###, InternLM2-Chat demonstrates the best performance in basic arithmetic GSM8K, complex applied mathematics MATH, and theoretical problems TheoremQA at both the 7B and 20B parameter scales. Notably, InternLM2-Chat-20B, which underwent COOL RLHF training, leads comprehensively across all metrics, outperforming the API model GPT-3.5 and the MoE model Mixtral-8x7B-Instruct-v0.1. In the bilingual tests of MathBench, InternLM2-Chat-20B also shows excellent performance."
        },
        {
            "section_id": "5.2.4",
            "parent_section_id": "5.2",
            "section_name": "5.2.4 Coding",
            "text": "HumanEval\nHumanEval (Chen et al., 2021  ###reference_b19###  ###reference_b19###  ###reference_b19###) is a widely recognized dataset that serves as a benchmark for evaluating the performance of code generation models. It consists of 164 carefully crafted programming tasks, each composed of a Python function and an accompanying docstring to provide context and specifications. This dataset, featuring human-written code, plays a pivotal role in assessing the capabilities of Large Language Models (LLMs) when generating or completing programs.\nMBPP\nMBPP (Austin et al., 2021  ###reference_b8###  ###reference_b8###  ###reference_b8###) consists of 974 programming tasks that are solvable by entry-level programmers. These tasks range from simple numeric manipulations to more complex problems requiring external knowledge, such as defining specific integer sequences. We use the santinized version of MBPP, which only includes a subset of the data has been hand-verified by the authors.\nHumanEval-X HumanEval-X (Zheng et al., 2023b  ###reference_b101###  ###reference_b101###  ###reference_b101###) dataset is a multilingual extension of the original HumanEval benchmark, designed to evaluate the capabilities of code generation models across multiple programming languages. It consists of 164 handcrafted programming problems, each translated into five major languages: C++, Java, JavaScript, Go, and Python. This results in a total of 820 problem-solution pairs, supporting both code generation and code translation tasks. HumanEval-X allows for the assessment of a model\u2019s ability to generate functionally correct code and to translate code between different languages, using test cases to verify the correctness of the generated code. This benchmark facilitates a more comprehensive understanding of pre-trained multilingual code generation models and their potential applications in real-world programming tasks.\nTo assess the coding prowess of InternLM2, we conduct a series of experiments utilizing the widely recognized benchmarks MBPP (Austin et al., 2021  ###reference_b8###  ###reference_b8###  ###reference_b8###) and HumanEval (Chen et al., 2021  ###reference_b19###  ###reference_b19###  ###reference_b19###). Furthermore, to gauge the aptitude of code generation models for multiple programming languages, we extend our evaluation to include MBPP-CN, a Chinese adaptation of MBPP, and HumanEval-X, a multilingual expansion of the original HumanEval benchmark.\nAs depicted in Figure 13  ###reference_###  ###reference_###  ###reference_###, the InternLM2 model series achieve leading performance, especially on HumanEval, MBPP, and MBPP-CN, where the InternLM2-Chat-20B model surpasses the previous state-of-the-art by more than 10%, underscoring the InternLM2 series\u2019 exceptional proficiency in code generation tasks. Additionally, the InternLM2-Chat-20B model exhibits substantial improvement over the InternLM2-Chat-7B model in MBPP-CN benchmarks, yet it shows a slight decline in performance on HumanEval-X. This phenomenon might stem from the InternLM2-Chat-20B model being finely tuned for Chinese at the expense of its effectiveness in other languages."
        },
        {
            "section_id": "5.2.5",
            "parent_section_id": "5.2",
            "section_name": "5.2.5 Performance Before and After Enhancement Training",
            "text": "Through results in previous parts, we can clearly see that InternLM2 with capability-specific enhancement training consistently outperforms its counterpart. The overall comparison of model performance before and after this phase is presented in Figure 12  ###reference_###, where the scores for various capabilities are derived from the average of multiple evaluation sets.\nCoding: HumanEval and MBPP.\nReasoning: MATH, GSM8k, SummEdits (Laban et al., 2023  ###reference_b49###) and BBH.\nQA: HellaSwag, PIQA (Bisk et al., 2020  ###reference_b15###), WinoGrande, OpenBookQA (Mihaylov et al., 2018  ###reference_b62###), NaturalQuestions and TriviaQA.\nExamination: MMLU, AGIEval and C-Eval.\n###figure_34### It is evident that there has been a significant improvement in these capabilities. Furthermore, we also compared the performance of the base models before and after undergoing enhancement training when subjected to Supervised Fine-Tuning (SFT) in Table 14  ###reference_###. The capability dimensions are aligned with the categorization adopted in OpenCompass 777https://rank.opencompass.org.cn/leaderboard-llm-v2  ###reference_d-llm-v2###. It can be observed that SFT models trained with capability-specific enhancement achieve better performance across various capability dimensions. The SFT performance in other parts of this report is based on the base model before the enhancement training."
        },
        {
            "section_id": "5.2.6",
            "parent_section_id": "5.2",
            "section_name": "5.2.6 Long-context Modeling",
            "text": "We mainly evaluate the long-context modeling capability of InternLM2 on the following two benchmarks: L-Eval (An et al., 2023  ###reference_b5###  ###reference_b5###  ###reference_b5###) and LongBench (Bai et al., 2023b  ###reference_b12###  ###reference_b12###  ###reference_b12###).\nL-Eval. \nL-Eval is a long-context benchmark consisting of 18 subtasks888\nWe adopt the first version of L-Eval, corresponding to https://arxiv.org/abs/2307.11088v1  ###reference_###  ###reference_###  ###reference_###,\nincluding texts from various fields such as law, economy, and technology.\nL-Eval consists of 411 documents and over 2000 test cases, with an average document length of 7217 words.\nSubtasks in this dataset can be categorized into two major classes:\n5 close-ended tasks and 13 open-ended categories.\nClosed-ended tasks are evaluated using exact matching based accuracies,\nwhile open-ended tasks adopt the Rouge score as the metric.\nLongBench. \nLongBench is a long-context benchmark consisting of 21 subtasks with a total of 4750 test cases.\nIt is the first bilingual long-context benchmark, with an average English text length of 6711 words and an average Chinese text length of 13386 characters.\nThe 21 subtasks are divided into 6 types, providing a more comprehensive evaluation of the model\u2019s capabilities in various aspects.\nEvaluation Results. \nWe report the evaluation results of InternLM2 on long-context benchmarks in Table 15  ###reference_###  ###reference_###  ###reference_###.\nAll variants of InternLM2 have demonstrated strong long-context modeling performance across two benchmarks.\nInternLM2-Chat-20B-SFT achieves the best performance on L-Eval and outperforms its counterparts by a considerable margin.\nOn LongBench, InternLM2-Chat-7B-SFT outperforms other B Models models across 4 out of 6 subtask categories.\nIt obtains a 48.1 overall score, which is only slightly inferior to the 48.4 overall score of ChatGLM3-6B.\nMeanwhile, we noticed that for InternLM2, different parameter sizes do not lead to significant difference in long-context performance, which will be further investigated.\n###figure_35### \u201cNeedle-in-the-Haystack\u201d is a single-needle retrieval task, which is designed to test the Large Language Models\u2019 (LLMs) ability to recall a single piece of key information. This is done by inserting a crucial piece of information into a Haystack text of a target length999Text token length calculations use the GPT-4 tokenizer at various positions and then querying the model about this key information at the end of the prompt. This method precisely visualizes LLMs\u2019 recall capabilities at different positions within long texts of varying lengths. We design a Chinese Haystack following the original idea, and utilize the Skywork/ChineseDomainModelingEval dataset released by Wei et al. (2023  ###reference_b87###  ###reference_b87###  ###reference_b87###), ensuring diversity and quality in the sources of Chinese texts. This dataset covers a wide range of fields from finance to technology, offering high-quality, up-to-date Chinese articles and providing a stable benchmark for assessing different models\u2019 abilities to handle domain-specific long texts. For this experiment, we leverage the LMDeploy101010https://github.com/InternLM/lmdeploy  ###reference_###  ###reference_###  ###reference_### Contributors (2023a  ###reference_b28###  ###reference_b28###  ###reference_b28###) inference engine to accelerate the inference process. The results presented in Figure 13  ###reference_###  ###reference_###  ###reference_### effectively demonstrate InternLM2\u2019s capability for long-context modeling."
        },
        {
            "section_id": "5.2.7",
            "parent_section_id": "5.2",
            "section_name": "5.2.7 Tool Utilization",
            "text": "It is widely acknowledged that external tools and APIs can significantly enhance the capabilities of LLMs to tackle complex, real-world problems (Qin et al., 2023a  ###reference_b68###; b  ###reference_b69###; Schick et al., 2023  ###reference_b76###). To analyze InternLM2\u2019s proficiency in tool utilization,\nwe conduct experiments across several benchmark datasets: GSM8K (Cobbe et al., 2021  ###reference_b27###), Math (Hendrycks et al., 2021  ###reference_b40###), the recently introduced MathBench (Anonymous, 2024b  ###reference_b7###), T-Eval (Chen et al., 2023b  ###reference_b23###), and the template subset of CIBench (Anonymous, 2024a  ###reference_b6###), all employing the ReAct protocol (Yao et al., 2023  ###reference_b93###) where LLMs alternate between generating thought processes and executing actions.\nNotably, MathBench consists of 3709 questions that span mathematical concepts from primary to high school levels. This dataset enables a comprehensive evaluation of an LLM\u2019s ability to solve math problems. T-Eval (Chen et al., 2023b  ###reference_b23###) features human-verified, high-quality question instructions with corresponding step-by-step solutions. It measures an LLM\u2019s proficiency with everyday tools such as Google Search and Gaode Map across six different dimensions. CIBench, developed by our team, simulates genuine data analysis scenarios using interactive Jupyter notebooks. It encompasses multiple, consecutive tasks and covers the most commonly used Python modules in data analysis, including Pandas, Numpy, and Pytorch. This custom-made benchmark allows for a thorough assessment of an LLM\u2019s comprehensive ability in data analysis.\nWe utilize the external code interpreter and follow the ReAct protocol to evaluate LLM\u2019s ability to solve coding and mathematic problems. The results, as depicted in Figure 14  ###reference_###  ###reference_###  ###reference_###, demonstrate a substantial improvement even when using the code interpreter, particularly on the MATH dataset where the enhancement is notably significant.\nAs depicted in Figure 15  ###reference_###  ###reference_###  ###reference_###, regarding the recently introduced MathBench dataset, the utilization of a code interpreter results in improved performance for InternLM2\u2019s performance in most cases, and a minor decrease may be attributed to the incorrect usage of such interpreters. Moreover, notable improvements are observed within the Knowledge domain for InternLM2-20B-Chat and in the Application section for InternLM2-7B-Chat. These disparities can stem from a multitude of factors, including differences in the constitution of their respective training datasets.\nAs depicted in Figure 16  ###reference_###  ###reference_###  ###reference_###, the InternLM2 models consistently demonstrate superior or comparable performance compared to existing models across various benchmarks. Specifically, when comparing models of identical scale, InternLM2-Chat-7B emerges as the top performer on T-Eval and CIBench. Meanwhile, InternLM2-Chat-20B achieves competitive results on T-Eval while obtaining the highest scores on CIBench. Additionally, the InternLM2 series models achieve impressive results in Chinese, showcasing their proficiency in multiple languages.\n###figure_36### ###figure_37###"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Performance on Alignment",
            "text": "Although LLMs\u2019 objective capabilities improve through pretraining and SFT, their answer styles may not align with human preferences, necessitating further enhancement with RLHF to improve alignment. Therefore, assessing the alignment capabilities is crucial to determine whether LLMs truly meet human needs.\nIn this section, we evaluate InternLM2\u2019s performance on several popular subjective alignment datasets, comparing the performance of SFT and RLHF models. As we can see in Table 17  ###reference_###, InternLM2\u2019s overall performance in alignment tasks achieved SOTA or near-SOTA results on multiple benchmarks, demonstrating a high level of consistency between the subjective outputs of InternLM2 series models and human preferences. We detail the model performance on each dataset separately below. Additionally, the prompt templates used for subjective alignment tasks on each dataset are presented in the Appendix.\nAlpacaEval(Li et al., 2023b  ###reference_b53###  ###reference_b53###) is a single-turn question-answer dataset with 805 questions. Its main purpose is to assess the helpfulness of model responses to humans, reflecting the alignment with human intentions through adversarial evaluation. AlpacaEval(v2) establishes a baseline model and uses GPT4-Turbo as the judge model to compare the baseline\u2019s answers with those of the model under evaluation. It selects the model that better aligns with human preferences and calculates the win rate.\nInstead of directly collecting the judge model\u2019s responses, AlpacaEval(v2) employs logit probabilities to analyze the judge model\u2019s preferences statistically. These preferences are then used as a weighting factor in the calculation of the weighted win rate.\nAs shown in Table 17  ###reference_###  ###reference_###, InternLM2-20B achieved a win rate of 21.8, marking the highest SOTA result among the compared models and demonstrating its superior alignment performance. Moreover, the table indicates that the RLHF model outperforms the SFT model in terms of win rate, which underscores the effectiveness of the alignment strategy.\nMTBench(Zheng et al., 2023a  ###reference_b100###  ###reference_b100###) is a two-round conversation dataset consisting of 80 questions that span eight dimensions: reasoning, roleplay, math, coding, writing, humanities, STEM, and information extraction. Each dimension contains 10 questions, which are subjected to two rounds of questioning. Initially, the model\u2019s ability to answer basic questions is tested; subsequently, it must follow additional, specific instructions to refine its previous response. Scores are assigned on a 1-10 scale by a judge model.\nAs we can see in Table 17  ###reference_###  ###reference_###, InternLM2 achieves leading scores in both the 7B and 20B versions, with 7.7 and 7.9 respectively, demonstrating its reliable multi-turn conversational ability.\n###figure_38### CompassArena comprises 520 Chinese questions, encompassing knowledge, language, mathematics, reasoning, and creativity. Like AlpacaEval, it calculates the win rate of two models judged by GPT4-Turbo and employs double-blind testing by swapping the model order to mitigate position bias. As indicated in Table 17  ###reference_###  ###reference_###, InternLM2 secures the highest win rate in the 7B version (28.7) and the 20B version (31.4).\nNote that the performance gap between InternLM2\u2019s 7B and 20B versions is relatively small. However, when compared to SFT models, InternLM2\u2019s RLHF model shows a significant improvement in performance. This suggests that the RLHF strategy substantially enhances InternLM2\u2019s alignment with human preferences, rather than just increasing the model size. Furthermore, the results broken down by category in Figure 16  ###reference_###  ###reference_### reveal that the InternLM2 series possesses exceptionally strong Chinese creativity and language abilities, with a win rate that rivals that of GPT4-Turbo.\nAlignBench(Liu et al., 2023a  ###reference_b56###  ###reference_b56###) is a Chinese subjective dataset comprising 683 question-answer pairs, categorized into eight areas, including Fundamental Language Ability, Advanced Chinese Understanding, Task-oriented Role Play, among others, covering various scenarios such as physics, history, music, and law. The dataset utilizes an in-house Judge Model called CritiqueLLM(Ke et al., 2023  ###reference_b46###  ###reference_b46###) for evaluation. CritiqueLLM provides scores from 1 to 10 across various dimensions for each question and issues a final score. We present these final scores, as provided by CritiqueLLM, in Table 17  ###reference_###  ###reference_### and detailed scores across different categories in Table 18  ###reference_###  ###reference_###. As shown in Table 17  ###reference_###  ###reference_###, InternLM2 achieves SOTA scores in both the 7B and 20B versions, with 6.1 and 6.8 respectively, outperforming GPT-3.5\u2019s score of 5.7.\nMoreover, both the 7B and 20B versions of InternLM2 exhibit significant performance enhancements post-RLHF compared to SFT models, underscoring the effectiveness of our RLHF strategy in improving the model\u2019s alignment with human preferences. An analysis of the detailed scores reveals areas for improvement in mathematical and reasoning abilities; however, InternLM2 excels in question-answering and role-playing tasks, offering strong subjective performance.\nIFEval(Zhou et al., 2023  ###reference_b103###  ###reference_b103###) is designed to test models\u2019 instruction following ability, requiring responses to adhere to specific patterns, such as letter case restrictions and keyword limitations. IFEval encompasses 25 distinct types of instructions, constructing 541 questions that follow instructions, and employs rule-based evaluation to assess the correctness of models\u2019 responses for each question. Utilizing various statistical methods, IFEval offers four types of accuracy scores: prompt-level strict, prompt-level loose, instance-level strict, and instance-level loose. We present the average results of these four scores in Table 17  ###reference_###  ###reference_###.\nAlthough English-based models like Llama2 and Mistral generally outperform Chinese-based models on IFEval, InternLM2 ranks second and third in the 7B phase (48.5) and the 13-20B phase (48.7), respectively. This indicates that, despite the challenges of instruction following tasks, InternLM2 maintains a leading position among models of similar size."
        },
        {
            "section_id": "5.3.1",
            "parent_section_id": "5.3",
            "section_name": "5.3.1 English Subjective Evaluation",
            "text": "AlpacaEval(Li et al., 2023b  ###reference_b53###  ###reference_b53###  ###reference_b53###) is a single-turn question-answer dataset with 805 questions. Its main purpose is to assess the helpfulness of model responses to humans, reflecting the alignment with human intentions through adversarial evaluation. AlpacaEval(v2) establishes a baseline model and uses GPT4-Turbo as the judge model to compare the baseline\u2019s answers with those of the model under evaluation. It selects the model that better aligns with human preferences and calculates the win rate.\nInstead of directly collecting the judge model\u2019s responses, AlpacaEval(v2) employs logit probabilities to analyze the judge model\u2019s preferences statistically. These preferences are then used as a weighting factor in the calculation of the weighted win rate.\nAs shown in Table 17  ###reference_###  ###reference_###  ###reference_###, InternLM2-20B achieved a win rate of 21.8, marking the highest SOTA result among the compared models and demonstrating its superior alignment performance. Moreover, the table indicates that the RLHF model outperforms the SFT model in terms of win rate, which underscores the effectiveness of the alignment strategy.\nMTBench(Zheng et al., 2023a  ###reference_b100###  ###reference_b100###  ###reference_b100###) is a two-round conversation dataset consisting of 80 questions that span eight dimensions: reasoning, roleplay, math, coding, writing, humanities, STEM, and information extraction. Each dimension contains 10 questions, which are subjected to two rounds of questioning. Initially, the model\u2019s ability to answer basic questions is tested; subsequently, it must follow additional, specific instructions to refine its previous response. Scores are assigned on a 1-10 scale by a judge model.\nAs we can see in Table 17  ###reference_###  ###reference_###  ###reference_###, InternLM2 achieves leading scores in both the 7B and 20B versions, with 7.7 and 7.9 respectively, demonstrating its reliable multi-turn conversational ability.\n###figure_39###"
        },
        {
            "section_id": "5.3.2",
            "parent_section_id": "5.3",
            "section_name": "5.3.2 Chinese Subjective Evaluation",
            "text": "CompassArena comprises 520 Chinese questions, encompassing knowledge, language, mathematics, reasoning, and creativity. Like AlpacaEval, it calculates the win rate of two models judged by GPT4-Turbo and employs double-blind testing by swapping the model order to mitigate position bias. As indicated in Table 17  ###reference_###  ###reference_###  ###reference_###, InternLM2 secures the highest win rate in the 7B version (28.7) and the 20B version (31.4).\nNote that the performance gap between InternLM2\u2019s 7B and 20B versions is relatively small. However, when compared to SFT models, InternLM2\u2019s RLHF model shows a significant improvement in performance. This suggests that the RLHF strategy substantially enhances InternLM2\u2019s alignment with human preferences, rather than just increasing the model size. Furthermore, the results broken down by category in Figure 16  ###reference_###  ###reference_###  ###reference_### reveal that the InternLM2 series possesses exceptionally strong Chinese creativity and language abilities, with a win rate that rivals that of GPT4-Turbo.\nAlignBench(Liu et al., 2023a  ###reference_b56###  ###reference_b56###  ###reference_b56###) is a Chinese subjective dataset comprising 683 question-answer pairs, categorized into eight areas, including Fundamental Language Ability, Advanced Chinese Understanding, Task-oriented Role Play, among others, covering various scenarios such as physics, history, music, and law. The dataset utilizes an in-house Judge Model called CritiqueLLM(Ke et al., 2023  ###reference_b46###  ###reference_b46###  ###reference_b46###) for evaluation. CritiqueLLM provides scores from 1 to 10 across various dimensions for each question and issues a final score. We present these final scores, as provided by CritiqueLLM, in Table 17  ###reference_###  ###reference_###  ###reference_### and detailed scores across different categories in Table 18  ###reference_###  ###reference_###  ###reference_###. As shown in Table 17  ###reference_###  ###reference_###  ###reference_###, InternLM2 achieves SOTA scores in both the 7B and 20B versions, with 6.1 and 6.8 respectively, outperforming GPT-3.5\u2019s score of 5.7.\nMoreover, both the 7B and 20B versions of InternLM2 exhibit significant performance enhancements post-RLHF compared to SFT models, underscoring the effectiveness of our RLHF strategy in improving the model\u2019s alignment with human preferences. An analysis of the detailed scores reveals areas for improvement in mathematical and reasoning abilities; however, InternLM2 excels in question-answering and role-playing tasks, offering strong subjective performance."
        },
        {
            "section_id": "5.3.3",
            "parent_section_id": "5.3",
            "section_name": "5.3.3 Instruct Following Evaluation",
            "text": "IFEval(Zhou et al., 2023  ###reference_b103###  ###reference_b103###  ###reference_b103###) is designed to test models\u2019 instruction following ability, requiring responses to adhere to specific patterns, such as letter case restrictions and keyword limitations. IFEval encompasses 25 distinct types of instructions, constructing 541 questions that follow instructions, and employs rule-based evaluation to assess the correctness of models\u2019 responses for each question. Utilizing various statistical methods, IFEval offers four types of accuracy scores: prompt-level strict, prompt-level loose, instance-level strict, and instance-level loose. We present the average results of these four scores in Table 17  ###reference_###  ###reference_###  ###reference_###.\nAlthough English-based models like Llama2 and Mistral generally outperform Chinese-based models on IFEval, InternLM2 ranks second and third in the 7B phase (48.5) and the 13-20B phase (48.7), respectively. This indicates that, despite the challenges of instruction following tasks, InternLM2 maintains a leading position among models of similar size."
        },
        {
            "section_id": "5.3.4",
            "parent_section_id": "5.3",
            "section_name": "5.3.4 Ablation Study of Conditional Reward Model",
            "text": "To verify the impact of conditional system prompts, we compare the performance of the reward model trained on a heterogeneous mix of data from different domains, with and without using conditional system prompts. As illustrated in Table 19  ###reference_###, the absence of system prompts results in a significant decrease in precision across several public datasets, including scenarios such as helpful and harmless conversations (Bai et al., 2022  ###reference_b11###), content summaries (Stiennon et al., 2020  ###reference_b80###), math problems (Lightman et al., 2023  ###reference_b54###), and Reddit replies (Ethayarajh et al., 2022  ###reference_b34###). Conversely, including system prompts leads to markedly higher precision in these areas."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Discussion on Data Contamination",
            "text": "We evaluate the language modeling (LM) loss on samples (a sample is a concatenation of question and answer) from GSM8K dataset for several foundation models, results are shown in Table 20  ###reference_###. For each LLM, we compare LM loss on the training split (), the test split (), and a specially curated reference set (), generated by GPT-4, designed to mimic the GSM8K dataset. We also report two key metrics: , serving as an indicator of potential test data leakage during the training of the LLM, i.e., a lower value suggests possible leakage; and , which measures the degree of overfitting on the training split of the dataset. A higher value of  implies excessive overfitting. Outliers for both  and  are highlighted in gray."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this report, we present the InternLM2 large language model, which demonstrates exceptional performance in both subjective and objective evaluations. InternLM2 has been trained on over 2T of high-quality pre-training corpora, covering model sizes of 1.8B, 7B, and 20B, making it suitable for a variety of scenarios. To better support long contexts, InternLM2 employs GQA to reduce inference costs, and has been additionally trained on up to 32k contexts. Besides open-sourcing the model itself, we also make available the checkpoints from various phases of the training process, facilitating studies by future researches.\nIn addition to open-sourcing the model, we provide a detailed description of how we trained InternLM2, including the training framework, pre-training text data, pre-training code data, pre-training long text data, and alignment data. Furthermore, to address preference conflicts encountered during the RLHF process, we propose Conditional Online RLHF to harmonize various preferences. This information can offer insights into how to prepare pre-training data and how to train larger models more effectively."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Appendix",
            "text": ""
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T1.1.1.1.1\">Source</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.2\">Docs (M rows)</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.3\">Bytes (GB)</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.4\">Bytes-percent</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.2.1.1\">en-books</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T1.1.2.1.2\">0.50</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T1.1.2.1.3\">220.14</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T1.1.2.1.4\">1.63%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.3.2.1\">en-techlit</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.1.3.2.2\">59.27</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.1.3.2.3\">576.48</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.1.3.2.4\">4.27%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.4.3.1\">en-webpages</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.1.4.3.2\">3614.07</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.1.4.3.3\">9129.39</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.1.4.3.4\">67.51%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.5.4.1\">zh-books</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.1.5.4.2\">0.71</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.1.5.4.3\">366.82</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.1.5.4.4\">2.71%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.6.5.1\">zh-techlit</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.1.6.5.2\">89.59</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.1.6.5.3\">668.19</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.1.6.5.4\">4.94%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T1.1.7.6.1\">zh-webpages</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T1.1.7.6.2\">928.94</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T1.1.7.6.3\">2562.86</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T1.1.7.6.4\">18.95%</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Summary of the pre-train data from different sources</figcaption>\n</figure>",
            "capture": "Table 1: Summary of the pre-train data from different sources"
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S3.T2.1.1.1.1\">High</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S3.T2.1.1.1.2\">Moderate</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S3.T2.1.1.1.3\">Low</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.2.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.2.1.1\">Bytes (GB)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.1.2.1.2\">Percentage</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.2.1.3\">Bytes (GB)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.1.2.1.4\">Percentage</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.2.1.5\">Bytes (GB)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.2.1.6\">Percentage</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.3.2.1\">105.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T2.1.3.2.2\">16.8%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.3.2.3\">440.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T2.1.3.2.4\">69.9%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.3.2.5\">83.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.3.2.6\">13.3%</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Statistics of code data quality based on a learnable classifier, where high-quality data will be trained multiple times, moderate-quality data will be trained once, and low-quality will be dropped. The data of low-resource programming languages will be preserved and not taken account in the statistics.</figcaption>\n</figure>",
            "capture": "Table 2: Statistics of code data quality based on a learnable classifier, where high-quality data will be trained multiple times, moderate-quality data will be trained once, and low-quality will be dropped. The data of low-resource programming languages will be preserved and not taken account in the statistics."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T3.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T3.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.4.4.5\">Params</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.2.2.2\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.3.3.3\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.4.4.4\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.4.4.6\">Learning Rate</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.4.4.7\">Batch size</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T3.4.5.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T3.4.5.1.1\">1.8B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.4.5.1.2\">24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.4.5.1.3\">2048</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.4.5.1.4\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.4.5.1.5\">2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.4.5.1.6\">3e-4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.4.5.1.7\">4M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.4.6.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.4.6.2.1\">7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.6.2.2\">32</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.6.2.3\">4096</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.6.2.4\">8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.6.2.5\">4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.6.2.6\">3e-4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.6.2.7\">4M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.4.7.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T3.4.7.3.1\">20B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.4.7.3.2\">48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.4.7.3.3\">6144</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.4.7.3.4\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.4.7.3.5\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.4.7.3.6\">3e-4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.4.7.3.7\">5M</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Hyper-parameters for InternLM2 models.</figcaption>\n</figure>",
            "capture": "Table 3: Hyper-parameters for InternLM2 models."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T4.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T4.1.1.1.1\">Category</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T4.1.1.1.2\">Tokens(B)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T4.1.1.1.3\">Ratio(%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T4.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T4.1.2.1.1\"><span class=\"ltx_text\" id=\"S3.T4.1.2.1.1.1\">Retrieved Stem Data</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.1.2.1.2\">15.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.1.2.1.3\">65</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T4.1.3.2.1\"><span class=\"ltx_text\" id=\"S3.T4.1.3.2.1.1\">Retrieved Special Domain Data</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.1.3.2.2\">2.17</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.1.3.2.3\">8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T4.1.4.3.1\">Selected High Quality Data</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.1.4.3.2\">6.26</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.1.4.3.3\">26</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.5.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T4.1.5.4.1\">Total</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T4.1.5.4.2\">24.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T4.1.5.4.3\">100</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Data details of capability specific enhancement training</figcaption>\n</figure>",
            "capture": "Table 4: Data details of capability specific enhancement training"
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T5\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S5.T5.2\" style=\"width:433.6pt;height:329.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(15.4pt,-11.7pt) scale(1.07634961094907,1.07634961094907) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T5.2.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T5.2.2.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S5.T5.2.2.3.1.1\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.2.2.3.1.1.1\">Models</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.2.2.3.1.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.2.2.3.1.2.1\">MMLU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.2.2.3.1.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.2.2.3.1.3.1\">CMMLU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.2.2.3.1.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.2.2.3.1.4.1\">C-Eval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.2.2.3.1.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.2.2.3.1.5.1\">AGIEval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.2.2.3.1.6\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.2.2.3.1.6.1\">GAOKAO</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.2.4.2\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S5.T5.2.2.4.2.1\" style=\"padding:1pt 10.0pt;\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.4.2.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T5.2.2.4.2.2.1\">5-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.4.2.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T5.2.2.4.2.3.1\">5-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.4.2.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T5.2.2.4.2.4.1\">5-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.4.2.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T5.2.2.4.2.5.1\">0-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.4.2.6\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T5.2.2.4.2.6.1\">0-shot</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"6\" id=\"S5.T5.1.1.1.1\" style=\"background-color:#DFF5E5;padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T5.1.1.1.1.1\" style=\"background-color:#DFF5E5;\">B Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.2.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T5.2.2.5.3.1\" style=\"padding:1pt 10.0pt;\">Llama2-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.5.3.2\" style=\"padding:1pt 10.0pt;\">46.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.5.3.3\" style=\"padding:1pt 10.0pt;\">31.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.5.3.4\" style=\"padding:1pt 10.0pt;\">32.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.5.3.5\" style=\"padding:1pt 10.0pt;\">21.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.5.3.6\" style=\"padding:1pt 10.0pt;\">19.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.2.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T5.2.2.6.4.1\" style=\"padding:1pt 10.0pt;\">Mistral-7B-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.6.4.2\" style=\"padding:1pt 10.0pt;\">64.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.6.4.3\" style=\"padding:1pt 10.0pt;\">44.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.6.4.4\" style=\"padding:1pt 10.0pt;\">47.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.6.4.5\" style=\"padding:1pt 10.0pt;\">32.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.6.4.6\" style=\"padding:1pt 10.0pt;\">28.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.2.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T5.2.2.7.5.1\" style=\"padding:1pt 10.0pt;\">Baichuan2-7B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.7.5.2\" style=\"padding:1pt 10.0pt;\">54.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.7.5.3\" style=\"padding:1pt 10.0pt;\">57.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.7.5.4\" style=\"padding:1pt 10.0pt;\">56.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.7.5.5\" style=\"padding:1pt 10.0pt;\">34.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.7.5.6\" style=\"padding:1pt 10.0pt;\">34.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.2.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T5.2.2.8.6.1\" style=\"padding:1pt 10.0pt;\">ChatGLM3-6B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.8.6.2\" style=\"padding:1pt 10.0pt;\">62.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.8.6.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T5.2.2.8.6.3.1\">66.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.8.6.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T5.2.2.8.6.4.1\">67.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.8.6.5\" style=\"padding:1pt 10.0pt;\">47.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.8.6.6\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T5.2.2.8.6.6.1\">59.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.2.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T5.2.2.9.7.1\" style=\"padding:1pt 10.0pt;\">Qwen-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.9.7.2\" style=\"padding:1pt 10.0pt;\">59.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.9.7.3\" style=\"padding:1pt 10.0pt;\">62.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.9.7.4\" style=\"padding:1pt 10.0pt;\">63.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.9.7.5\" style=\"padding:1pt 10.0pt;\">45.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.9.7.6\" style=\"padding:1pt 10.0pt;\">52.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.2.10.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T5.2.2.10.8.1\" style=\"padding:1pt 10.0pt;\">InternLM2-7B-Base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.2.10.8.2\" style=\"padding:1pt 10.0pt;\">64.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.2.10.8.3\" style=\"padding:1pt 10.0pt;\">63.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.2.10.8.4\" style=\"padding:1pt 10.0pt;\">60.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.2.10.8.5\" style=\"padding:1pt 10.0pt;\">38.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.2.10.8.6\" style=\"padding:1pt 10.0pt;\">40.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.2.11.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T5.2.2.11.9.1\" style=\"padding:1pt 10.0pt;\">InternLM2-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.11.9.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T5.2.2.11.9.2.1\">65.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.11.9.3\" style=\"padding:1pt 10.0pt;\">66.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.11.9.4\" style=\"padding:1pt 10.0pt;\">65.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.11.9.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T5.2.2.11.9.5.1\">49.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.11.9.6\" style=\"padding:1pt 10.0pt;\">58.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"6\" id=\"S5.T5.2.2.2.1\" style=\"background-color:#EAE2FE;padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T5.2.2.2.1.1\" style=\"background-color:#EAE2FE;\">B Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.2.12.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T5.2.2.12.10.1\" style=\"padding:1pt 10.0pt;\">Llama2-13B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.12.10.2\" style=\"padding:1pt 10.0pt;\">55.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.12.10.3\" style=\"padding:1pt 10.0pt;\">38.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.12.10.4\" style=\"padding:1pt 10.0pt;\">40.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.12.10.5\" style=\"padding:1pt 10.0pt;\">27.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.12.10.6\" style=\"padding:1pt 10.0pt;\">18.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.2.13.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T5.2.2.13.11.1\" style=\"padding:1pt 10.0pt;\">Mixtral-8x7B-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.13.11.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T5.2.2.13.11.2.1\">71.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.13.11.3\" style=\"padding:1pt 10.0pt;\">53.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.13.11.4\" style=\"padding:1pt 10.0pt;\">55.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.13.11.5\" style=\"padding:1pt 10.0pt;\">40.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.13.11.6\" style=\"padding:1pt 10.0pt;\">32.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.2.14.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T5.2.2.14.12.1\" style=\"padding:1pt 10.0pt;\">Baichuan2-13B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.14.12.2\" style=\"padding:1pt 10.0pt;\">59.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.14.12.3\" style=\"padding:1pt 10.0pt;\">61.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.14.12.4\" style=\"padding:1pt 10.0pt;\">59.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.14.12.5\" style=\"padding:1pt 10.0pt;\">37.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.14.12.6\" style=\"padding:1pt 10.0pt;\">45.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.2.15.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T5.2.2.15.13.1\" style=\"padding:1pt 10.0pt;\">Qwen-14B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.15.13.2\" style=\"padding:1pt 10.0pt;\">67.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.15.13.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T5.2.2.15.13.3.1\">70.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.15.13.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T5.2.2.15.13.4.1\">71.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.15.13.5\" style=\"padding:1pt 10.0pt;\">52.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.15.13.6\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T5.2.2.15.13.6.1\">62.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.2.16.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T5.2.2.16.14.1\" style=\"padding:1pt 10.0pt;\">InternLM2-20B-Base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.2.16.14.2\" style=\"padding:1pt 10.0pt;\">64.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.2.16.14.3\" style=\"padding:1pt 10.0pt;\">63.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.2.16.14.4\" style=\"padding:1pt 10.0pt;\">60.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.2.16.14.5\" style=\"padding:1pt 10.0pt;\">38.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.2.16.14.6\" style=\"padding:1pt 10.0pt;\">40.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.2.17.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S5.T5.2.2.17.15.1\" style=\"padding:1pt 10.0pt;\">InternLM2-20B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T5.2.2.17.15.2\" style=\"padding:1pt 10.0pt;\">67.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T5.2.2.17.15.3\" style=\"padding:1pt 10.0pt;\">68.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T5.2.2.17.15.4\" style=\"padding:1pt 10.0pt;\">68.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T5.2.2.17.15.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T5.2.2.17.15.5.1\">53.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T5.2.2.17.15.6\" style=\"padding:1pt 10.0pt;\">57.1</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.6.1\">Comparison of Base Models on Comprehensive Examination</span>. The model name in <span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.7.2\">bold</span> indicates the top performer, while an <span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T5.8.3\">underline</span> signifies the leading model within a similar parameter size group.</figcaption>\n</figure>",
            "capture": "Table 5: Comparison of Base Models on Comprehensive Examination. The model name in bold indicates the top performer, while an underline signifies the leading model within a similar parameter size group."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T6\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S5.T6.2\" style=\"width:433.6pt;height:345.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(2.0pt,-1.6pt) scale(1.00946041894334,1.00946041894334) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T6.2.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T6.2.2.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S5.T6.2.2.3.1.1\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.2.2.3.1.1.1\">Models</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.2.2.3.1.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.2.2.3.1.2.1\">MMLU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.2.2.3.1.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.2.2.3.1.3.1\">CMMLU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.2.2.3.1.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.2.2.3.1.4.1\">C-Eval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.2.2.3.1.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.2.2.3.1.5.1\">AGIEval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.2.2.3.1.6\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.2.2.3.1.6.1\">GAOKAO</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.2.2.4.2\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S5.T6.2.2.4.2.1\" style=\"padding:1pt 10.0pt;\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.4.2.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T6.2.2.4.2.2.1\">5-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.4.2.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T6.2.2.4.2.3.1\">5-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.4.2.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T6.2.2.4.2.4.1\">5-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.4.2.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T6.2.2.4.2.5.1\">0-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.4.2.6\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T6.2.2.4.2.6.1\">0-shot</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.2.2.5.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"6\" id=\"S5.T6.2.2.5.3.1\" style=\"background-color:#A8B3E3;padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T6.2.2.5.3.1.1\" style=\"background-color:#A8B3E3;\">API Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.2.2.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T6.2.2.6.4.1\" style=\"padding:1pt 10.0pt;\">GPT-3.5</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.6.4.2\" style=\"padding:1pt 10.0pt;\">69.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.6.4.3\" style=\"padding:1pt 10.0pt;\">53.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.6.4.4\" style=\"padding:1pt 10.0pt;\">52.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.6.4.5\" style=\"padding:1pt 10.0pt;\">39.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.6.4.6\" style=\"padding:1pt 10.0pt;\">51.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" colspan=\"6\" id=\"S5.T6.1.1.1.1\" style=\"background-color:#DFF5E5;padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T6.1.1.1.1.1\" style=\"background-color:#DFF5E5;\">B Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.2.2.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T6.2.2.7.5.1\" style=\"padding:1pt 10.0pt;\">Llama2-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.7.5.2\" style=\"padding:1pt 10.0pt;\">48.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.7.5.3\" style=\"padding:1pt 10.0pt;\">30.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.7.5.4\" style=\"padding:1pt 10.0pt;\">34.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.7.5.5\" style=\"padding:1pt 10.0pt;\">28.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.7.5.6\" style=\"padding:1pt 10.0pt;\">19.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.2.2.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T6.2.2.8.6.1\" style=\"padding:1pt 10.0pt;\">Mistral-7B-Instruct-v0.2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.8.6.2\" style=\"padding:1pt 10.0pt;\">59.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.8.6.3\" style=\"padding:1pt 10.0pt;\">42.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.8.6.4\" style=\"padding:1pt 10.0pt;\">42.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.8.6.5\" style=\"padding:1pt 10.0pt;\">34.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.8.6.6\" style=\"padding:1pt 10.0pt;\">28.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.2.2.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T6.2.2.9.7.1\" style=\"padding:1pt 10.0pt;\">Baichuan2-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.9.7.2\" style=\"padding:1pt 10.0pt;\">50.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.9.7.3\" style=\"padding:1pt 10.0pt;\">53.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.9.7.4\" style=\"padding:1pt 10.0pt;\">53.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.9.7.5\" style=\"padding:1pt 10.0pt;\">35.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.9.7.6\" style=\"padding:1pt 10.0pt;\">37.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.2.2.10.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T6.2.2.10.8.1\" style=\"padding:1pt 10.0pt;\">ChatGLM3-6B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.10.8.2\" style=\"padding:1pt 10.0pt;\">58.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.10.8.3\" style=\"padding:1pt 10.0pt;\">57.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.10.8.4\" style=\"padding:1pt 10.0pt;\">59.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.10.8.5\" style=\"padding:1pt 10.0pt;\">44.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.10.8.6\" style=\"padding:1pt 10.0pt;\">56.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.2.2.11.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T6.2.2.11.9.1\" style=\"padding:1pt 10.0pt;\">Qwen-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.11.9.2\" style=\"padding:1pt 10.0pt;\">57.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.11.9.3\" style=\"padding:1pt 10.0pt;\">57.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.11.9.4\" style=\"padding:1pt 10.0pt;\">59.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.11.9.5\" style=\"padding:1pt 10.0pt;\">39.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.11.9.6\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T6.2.2.11.9.6.1\">62.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.2.2.12.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T6.2.2.12.10.1\" style=\"padding:1pt 10.0pt;\">InternLM2-Chat-7B-SFT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.2.2.12.10.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T6.2.2.12.10.2.1\">63.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.2.2.12.10.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T6.2.2.12.10.3.1\">63.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.2.2.12.10.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T6.2.2.12.10.4.1\">60.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.2.2.12.10.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T6.2.2.12.10.5.1\">49.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.2.2.12.10.6\" style=\"padding:1pt 10.0pt;\">57.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.2.2.13.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T6.2.2.13.11.1\" style=\"padding:1pt 10.0pt;\">InternLM2-Chat-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.13.11.2\" style=\"padding:1pt 10.0pt;\">63.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.13.11.3\" style=\"padding:1pt 10.0pt;\">63.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.13.11.4\" style=\"padding:1pt 10.0pt;\">60.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.13.11.5\" style=\"padding:1pt 10.0pt;\">47.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.13.11.6\" style=\"padding:1pt 10.0pt;\">58.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.2.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" colspan=\"6\" id=\"S5.T6.2.2.2.1\" style=\"background-color:#EAE2FE;padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T6.2.2.2.1.1\" style=\"background-color:#EAE2FE;\">B Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.2.2.14.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T6.2.2.14.12.1\" style=\"padding:1pt 10.0pt;\">Llama2-13B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.14.12.2\" style=\"padding:1pt 10.0pt;\">54.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.14.12.3\" style=\"padding:1pt 10.0pt;\">33.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.14.12.4\" style=\"padding:1pt 10.0pt;\">35.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.14.12.5\" style=\"padding:1pt 10.0pt;\">30.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.14.12.6\" style=\"padding:1pt 10.0pt;\">23.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.2.2.15.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T6.2.2.15.13.1\" style=\"padding:1pt 10.0pt;\">Mixtral-8x7B-Instruct-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.15.13.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T6.2.2.15.13.2.1\">70.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.15.13.3\" style=\"padding:1pt 10.0pt;\">50.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.15.13.4\" style=\"padding:1pt 10.0pt;\">54.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.15.13.5\" style=\"padding:1pt 10.0pt;\">41.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.15.13.6\" style=\"padding:1pt 10.0pt;\">42.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.2.2.16.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T6.2.2.16.14.1\" style=\"padding:1pt 10.0pt;\">Baichuan2-13B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.16.14.2\" style=\"padding:1pt 10.0pt;\">56.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.16.14.3\" style=\"padding:1pt 10.0pt;\">54.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.16.14.4\" style=\"padding:1pt 10.0pt;\">56.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.16.14.5\" style=\"padding:1pt 10.0pt;\">40.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.16.14.6\" style=\"padding:1pt 10.0pt;\">45.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.2.2.17.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T6.2.2.17.15.1\" style=\"padding:1pt 10.0pt;\">Qwen-14B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.17.15.2\" style=\"padding:1pt 10.0pt;\">66.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.17.15.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T6.2.2.17.15.3.1\">68.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.17.15.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T6.2.2.17.15.4.1\">71.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.17.15.5\" style=\"padding:1pt 10.0pt;\">46.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.2.2.17.15.6\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T6.2.2.17.15.6.1\">76.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.2.2.18.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T6.2.2.18.16.1\" style=\"padding:1pt 10.0pt;\">InternLM2-Chat-20B-SFT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.2.2.18.16.2\" style=\"padding:1pt 10.0pt;\">66.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.2.2.18.16.3\" style=\"padding:1pt 10.0pt;\">65.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.2.2.18.16.4\" style=\"padding:1pt 10.0pt;\">63.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.2.2.18.16.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T6.2.2.18.16.5.1\">51.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.2.2.18.16.6\" style=\"padding:1pt 10.0pt;\">58.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.2.2.19.17\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S5.T6.2.2.19.17.1\" style=\"padding:1pt 10.0pt;\">InternLM2-Chat-20B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.2.2.19.17.2\" style=\"padding:1pt 10.0pt;\">66.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.2.2.19.17.3\" style=\"padding:1pt 10.0pt;\">65.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.2.2.19.17.4\" style=\"padding:1pt 10.0pt;\">63.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.2.2.19.17.5\" style=\"padding:1pt 10.0pt;\">50.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.2.2.19.17.6\" style=\"padding:1pt 10.0pt;\">58.3</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.6.1\">Comparison of Chat Models on Comprehensive Examination</span>. The model name in <span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.7.2\">bold</span> indicates the top performer, while an <span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T6.8.3\">underline</span> signifies the leading model within a similar parameter size group.</figcaption>\n</figure>",
            "capture": "Table 6: Comparison of Chat Models on Comprehensive Examination. The model name in bold indicates the top performer, while an underline signifies the leading model within a similar parameter size group."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T7\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S5.T7.2\" style=\"width:433.6pt;height:291.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-11.1pt,7.4pt) scale(0.951499243828888,0.951499243828888) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T7.2.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T7.2.2.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S5.T7.2.2.3.1.1\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.2.2.3.1.1.1\">Models</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T7.2.2.3.1.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.2.2.3.1.2.1\">TriviaQA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T7.2.2.3.1.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.2.2.3.1.3.1\">NaturalQuestions</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T7.2.2.3.1.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.2.2.3.1.4.1\">C3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T7.2.2.3.1.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.2.2.3.1.5.1\">RACE-High</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T7.2.2.3.1.6\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.2.2.3.1.6.1\">FLORES</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.2.2.4.2\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S5.T7.2.2.4.2.1\" style=\"padding:1pt 10.0pt;\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.4.2.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T7.2.2.4.2.2.1\">4-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.4.2.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T7.2.2.4.2.3.1\">5-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.4.2.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T7.2.2.4.2.4.1\">5-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.4.2.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T7.2.2.4.2.5.1\">0-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.4.2.6\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T7.2.2.4.2.6.1\">0-shot</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"6\" id=\"S5.T7.1.1.1.1\" style=\"background-color:#DFF5E5;padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T7.1.1.1.1.1\" style=\"background-color:#DFF5E5;\">B Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.2.2.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T7.2.2.5.3.1\" style=\"padding:1pt 10.0pt;\">Llama2-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.5.3.2\" style=\"padding:1pt 10.0pt;\">55.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.5.3.3\" style=\"padding:1pt 10.0pt;\">28.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.5.3.4\" style=\"padding:1pt 10.0pt;\">43.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.5.3.5\" style=\"padding:1pt 10.0pt;\">37.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.5.3.6\" style=\"padding:1pt 10.0pt;\">6.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.2.2.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T7.2.2.6.4.1\" style=\"padding:1pt 10.0pt;\">Mistral-7B-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.6.4.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T7.2.2.6.4.2.1\">68.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.6.4.3\" style=\"padding:1pt 10.0pt;\">31.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.6.4.4\" style=\"padding:1pt 10.0pt;\">54.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.6.4.5\" style=\"padding:1pt 10.0pt;\">69.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.6.4.6\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T7.2.2.6.4.6.1\">6.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.2.2.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T7.2.2.7.5.1\" style=\"padding:1pt 10.0pt;\">Baichuan2-7B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.7.5.2\" style=\"padding:1pt 10.0pt;\">51.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.7.5.3\" style=\"padding:1pt 10.0pt;\">24.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.7.5.4\" style=\"padding:1pt 10.0pt;\">64.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.7.5.5\" style=\"padding:1pt 10.0pt;\">63.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.7.5.6\" style=\"padding:1pt 10.0pt;\">5.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.2.2.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T7.2.2.8.6.1\" style=\"padding:1pt 10.0pt;\">Qwen-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.8.6.2\" style=\"padding:1pt 10.0pt;\">54.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.8.6.3\" style=\"padding:1pt 10.0pt;\">22.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.8.6.4\" style=\"padding:1pt 10.0pt;\">71.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.8.6.5\" style=\"padding:1pt 10.0pt;\">83.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.8.6.6\" style=\"padding:1pt 10.0pt;\">3.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.2.2.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T7.2.2.9.7.1\" style=\"padding:1pt 10.0pt;\">ChatGLM3-6B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.9.7.2\" style=\"padding:1pt 10.0pt;\">63.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.9.7.3\" style=\"padding:1pt 10.0pt;\">38.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.9.7.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T7.2.2.9.7.4.1\">78.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.9.7.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T7.2.2.9.7.5.1\">84.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.9.7.6\" style=\"padding:1pt 10.0pt;\">0.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.2.2.10.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T7.2.2.10.8.1\" style=\"padding:1pt 10.0pt;\">InternLM2-7B-Base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.2.2.10.8.2\" style=\"padding:1pt 10.0pt;\">57.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.2.2.10.8.3\" style=\"padding:1pt 10.0pt;\">28.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.2.2.10.8.4\" style=\"padding:1pt 10.0pt;\">61.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.2.2.10.8.5\" style=\"padding:1pt 10.0pt;\">74.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.2.2.10.8.6\" style=\"padding:1pt 10.0pt;\">5.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.2.2.11.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T7.2.2.11.9.1\" style=\"padding:1pt 10.0pt;\">InternLM2-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.11.9.2\" style=\"padding:1pt 10.0pt;\">58.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.11.9.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T7.2.2.11.9.3.1\">42.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.11.9.4\" style=\"padding:1pt 10.0pt;\">74.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.11.9.5\" style=\"padding:1pt 10.0pt;\">81.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.11.9.6\" style=\"padding:1pt 10.0pt;\">5.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.2.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"6\" id=\"S5.T7.2.2.2.1\" style=\"background-color:#EAE2FE;padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T7.2.2.2.1.1\" style=\"background-color:#EAE2FE;\">B Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.2.2.12.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T7.2.2.12.10.1\" style=\"padding:1pt 10.0pt;\">Llama2-13B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.12.10.2\" style=\"padding:1pt 10.0pt;\">60.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.12.10.3\" style=\"padding:1pt 10.0pt;\">34.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.12.10.4\" style=\"padding:1pt 10.0pt;\">47.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.12.10.5\" style=\"padding:1pt 10.0pt;\">59.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.12.10.6\" style=\"padding:1pt 10.0pt;\">7.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.2.2.13.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T7.2.2.13.11.1\" style=\"padding:1pt 10.0pt;\">Mixtral-8x7B-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.13.11.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T7.2.2.13.11.2.1\">77.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.13.11.3\" style=\"padding:1pt 10.0pt;\">39.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.13.11.4\" style=\"padding:1pt 10.0pt;\">59.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.13.11.5\" style=\"padding:1pt 10.0pt;\">80.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.13.11.6\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T7.2.2.13.11.6.1\">8.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.2.2.14.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T7.2.2.14.12.1\" style=\"padding:1pt 10.0pt;\">Baichuan2-13B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.14.12.2\" style=\"padding:1pt 10.0pt;\">54.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.14.12.3\" style=\"padding:1pt 10.0pt;\">27.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.14.12.4\" style=\"padding:1pt 10.0pt;\">65.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.14.12.5\" style=\"padding:1pt 10.0pt;\">75.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.14.12.6\" style=\"padding:1pt 10.0pt;\">6.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.2.2.15.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T7.2.2.15.13.1\" style=\"padding:1pt 10.0pt;\">Qwen-14B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.15.13.2\" style=\"padding:1pt 10.0pt;\">62.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.15.13.3\" style=\"padding:1pt 10.0pt;\">37.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.15.13.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T7.2.2.15.13.4.1\">80.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.15.13.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T7.2.2.15.13.5.1\">90.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.2.2.15.13.6\" style=\"padding:1pt 10.0pt;\">5.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.2.2.16.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T7.2.2.16.14.1\" style=\"padding:1pt 10.0pt;\">InternLM2-20B-Base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.2.2.16.14.2\" style=\"padding:1pt 10.0pt;\">63.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.2.2.16.14.3\" style=\"padding:1pt 10.0pt;\">30.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.2.2.16.14.4\" style=\"padding:1pt 10.0pt;\">67.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.2.2.16.14.5\" style=\"padding:1pt 10.0pt;\">75.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.2.2.16.14.6\" style=\"padding:1pt 10.0pt;\">6.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.2.2.17.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S5.T7.2.2.17.15.1\" style=\"padding:1pt 10.0pt;\">InternLM2-20B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T7.2.2.17.15.2\" style=\"padding:1pt 10.0pt;\">60.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T7.2.2.17.15.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T7.2.2.17.15.3.1\">44.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T7.2.2.17.15.4\" style=\"padding:1pt 10.0pt;\">79.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T7.2.2.17.15.5\" style=\"padding:1pt 10.0pt;\">72.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T7.2.2.17.15.6\" style=\"padding:1pt 10.0pt;\">6.5</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.6.1\">Comparison of Base Models on Language &amp; Knowledge</span>. The model name in <span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.7.2\">bold</span> indicates the top performer, while an <span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T7.8.3\">underline</span> signifies the leading model within a similar parameter size group.</figcaption>\n</figure>",
            "capture": "Table 7: Comparison of Base Models on Language & Knowledge. The model name in bold indicates the top performer, while an underline signifies the leading model within a similar parameter size group."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T8\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S5.T8.2\" style=\"width:433.6pt;height:307.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-24.4pt,17.3pt) scale(0.898848072682853,0.898848072682853) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T8.2.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T8.2.2.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S5.T8.2.2.3.1.1\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.2.2.3.1.1.1\">Models</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.2.2.3.1.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.2.2.3.1.2.1\">TriviaQA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.2.2.3.1.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.2.2.3.1.3.1\">NaturalQuestions</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.2.2.3.1.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.2.2.3.1.4.1\">C3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.2.2.3.1.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.2.2.3.1.5.1\">RACE-High</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.2.2.3.1.6\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.2.2.3.1.6.1\">FLORES</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.2.2.4.2\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S5.T8.2.2.4.2.1\" style=\"padding:1pt 10.0pt;\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.4.2.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T8.2.2.4.2.2.1\">4-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.4.2.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T8.2.2.4.2.3.1\">5-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.4.2.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T8.2.2.4.2.4.1\">5-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.4.2.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T8.2.2.4.2.5.1\">0-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.4.2.6\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T8.2.2.4.2.6.1\">0-shot</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.2.2.5.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"6\" id=\"S5.T8.2.2.5.3.1\" style=\"background-color:#A8B3E3;padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T8.2.2.5.3.1.1\" style=\"background-color:#A8B3E3;\"> API Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.2.2.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T8.2.2.6.4.1\" style=\"padding:1pt 10.0pt;\">GPT-3.5</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.6.4.2\" style=\"padding:1pt 10.0pt;\">69.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.6.4.3\" style=\"padding:1pt 10.0pt;\">53.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.6.4.4\" style=\"padding:1pt 10.0pt;\">52.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.6.4.5\" style=\"padding:1pt 10.0pt;\">39.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.6.4.6\" style=\"padding:1pt 10.0pt;\">51.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" colspan=\"6\" id=\"S5.T8.1.1.1.1\" style=\"background-color:#DFF5E5;padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T8.1.1.1.1.1\" style=\"background-color:#DFF5E5;\">B Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.2.2.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T8.2.2.7.5.1\" style=\"padding:1pt 10.0pt;\">Llama2-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.7.5.2\" style=\"padding:1pt 10.0pt;\">46.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.7.5.3\" style=\"padding:1pt 10.0pt;\">19.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.7.5.4\" style=\"padding:1pt 10.0pt;\">51.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.7.5.5\" style=\"padding:1pt 10.0pt;\">3.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.7.5.6\" style=\"padding:1pt 10.0pt;\">5.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.2.2.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T8.2.2.8.6.1\" style=\"padding:1pt 10.0pt;\">Mistral-7B-Instruct-v0.2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.8.6.2\" style=\"padding:1pt 10.0pt;\">35.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.8.6.3\" style=\"padding:1pt 10.0pt;\">8.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.8.6.4\" style=\"padding:1pt 10.0pt;\">66.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.8.6.5\" style=\"padding:1pt 10.0pt;\">71.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.8.6.6\" style=\"padding:1pt 10.0pt;\">12.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.2.2.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T8.2.2.9.7.1\" style=\"padding:1pt 10.0pt;\">Baichuan2-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.9.7.2\" style=\"padding:1pt 10.0pt;\">37.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.9.7.3\" style=\"padding:1pt 10.0pt;\">12.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.9.7.4\" style=\"padding:1pt 10.0pt;\">78.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.9.7.5\" style=\"padding:1pt 10.0pt;\">67.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.9.7.6\" style=\"padding:1pt 10.0pt;\">11.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.2.2.10.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T8.2.2.10.8.1\" style=\"padding:1pt 10.0pt;\">Qwen-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.10.8.2\" style=\"padding:1pt 10.0pt;\">46.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.10.8.3\" style=\"padding:1pt 10.0pt;\">18.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.10.8.4\" style=\"padding:1pt 10.0pt;\">84.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.10.8.5\" style=\"padding:1pt 10.0pt;\">74.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.10.8.6\" style=\"padding:1pt 10.0pt;\">12.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.2.2.11.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T8.2.2.11.9.1\" style=\"padding:1pt 10.0pt;\">ChatGLM3-6B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.11.9.2\" style=\"padding:1pt 10.0pt;\">38.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.11.9.3\" style=\"padding:1pt 10.0pt;\">14.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.11.9.4\" style=\"padding:1pt 10.0pt;\">79.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.11.9.5\" style=\"padding:1pt 10.0pt;\">79.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.11.9.6\" style=\"padding:1pt 10.0pt;\">9.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.2.2.12.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T8.2.2.12.10.1\" style=\"padding:1pt 10.0pt;\">InternLM2-Chat-7B-SFT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.2.2.12.10.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T8.2.2.12.10.2.1\">51.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.2.2.12.10.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T8.2.2.12.10.3.1\">24.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.2.2.12.10.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T8.2.2.12.10.4.1\">91.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.2.2.12.10.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T8.2.2.12.10.5.1\">85.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.2.2.12.10.6\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T8.2.2.12.10.6.1\">15.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.2.2.13.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T8.2.2.13.11.1\" style=\"padding:1pt 10.0pt;\">InternLM2-Chat-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.13.11.2\" style=\"padding:1pt 10.0pt;\">50.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.13.11.3\" style=\"padding:1pt 10.0pt;\">24.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.13.11.4\" style=\"padding:1pt 10.0pt;\">91.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.13.11.5\" style=\"padding:1pt 10.0pt;\">85.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.13.11.6\" style=\"padding:1pt 10.0pt;\">15.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.2.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" colspan=\"6\" id=\"S5.T8.2.2.2.1\" style=\"background-color:#EAE2FE;padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T8.2.2.2.1.1\" style=\"background-color:#EAE2FE;\">B Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.2.2.14.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T8.2.2.14.12.1\" style=\"padding:1pt 10.0pt;\">Llama2-13B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.14.12.2\" style=\"padding:1pt 10.0pt;\">54.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.14.12.3\" style=\"padding:1pt 10.0pt;\">25.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.14.12.4\" style=\"padding:1pt 10.0pt;\">56.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.14.12.5\" style=\"padding:1pt 10.0pt;\">19.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.14.12.6\" style=\"padding:1pt 10.0pt;\">6.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.2.2.15.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T8.2.2.15.13.1\" style=\"padding:1pt 10.0pt;\">Mixtral-8x7B-Instruct-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.15.13.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T8.2.2.15.13.2.1\">57.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.15.13.3\" style=\"padding:1pt 10.0pt;\">22.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.15.13.4\" style=\"padding:1pt 10.0pt;\">82.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.15.13.5\" style=\"padding:1pt 10.0pt;\">81.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.15.13.6\" style=\"padding:1pt 10.0pt;\">17.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.2.2.16.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T8.2.2.16.14.1\" style=\"padding:1pt 10.0pt;\">Baichuan2-13B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.16.14.2\" style=\"padding:1pt 10.0pt;\">40.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.16.14.3\" style=\"padding:1pt 10.0pt;\">12.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.16.14.4\" style=\"padding:1pt 10.0pt;\">84.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.16.14.5\" style=\"padding:1pt 10.0pt;\">73.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.16.14.6\" style=\"padding:1pt 10.0pt;\">12.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.2.2.17.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T8.2.2.17.15.1\" style=\"padding:1pt 10.0pt;\">Qwen-14B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.17.15.2\" style=\"padding:1pt 10.0pt;\">54.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.17.15.3\" style=\"padding:1pt 10.0pt;\">22.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.17.15.4\" style=\"padding:1pt 10.0pt;\">91.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.17.15.5\" style=\"padding:1pt 10.0pt;\">84.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.2.2.17.15.6\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T8.2.2.17.15.6.1\">17.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.2.2.18.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T8.2.2.18.16.1\" style=\"padding:1pt 10.0pt;\">InternLM2-Chat-20B-SFT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.2.2.18.16.2\" style=\"padding:1pt 10.0pt;\">55.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.2.2.18.16.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T8.2.2.18.16.3.1\">27.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.2.2.18.16.4\" style=\"padding:1pt 10.0pt;\">93.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.2.2.18.16.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T8.2.2.18.16.5.1\">87.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.2.2.18.16.6\" style=\"padding:1pt 10.0pt;\">17.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.2.2.19.17\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S5.T8.2.2.19.17.1\" style=\"padding:1pt 10.0pt;\">InternLM2-Chat-20B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.2.2.19.17.2\" style=\"padding:1pt 10.0pt;\">53.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.2.2.19.17.3\" style=\"padding:1pt 10.0pt;\">25.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.2.2.19.17.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T8.2.2.19.17.4.1\">93.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.2.2.19.17.5\" style=\"padding:1pt 10.0pt;\">87.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.2.2.19.17.6\" style=\"padding:1pt 10.0pt;\">16.9</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.6.1\">Comparison of Chat Models on Language &amp; Knowledge</span>. The model name in <span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.7.2\">bold</span> indicates the top performer, while an <span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T8.8.3\">underline</span> signifies the leading model within a similar parameter size group.</figcaption>\n</figure>",
            "capture": "Table 8: Comparison of Chat Models on Language & Knowledge. The model name in bold indicates the top performer, while an underline signifies the leading model within a similar parameter size group."
        },
        "9": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T9\">\n<div class=\"ltx_flex_figure ltx_flex_table\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle\" id=\"S5.T9.4\" style=\"width:216.8pt;\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T9.4.4\" style=\"width:433.6pt;height:484.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(80.0pt,-89.4pt) scale(1.584560216084,1.584560216084) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T9.4.4.4\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T9.4.4.4.5.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T9.4.4.4.5.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T9.4.4.4.5.1.1.1\">Models</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T9.4.4.4.5.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T9.4.4.4.5.1.2.1\">WinoGrande</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T9.4.4.4.5.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T9.4.4.4.5.1.3.1\">HellaSwag</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T9.4.4.4.5.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T9.4.4.4.5.1.4.1\">BBH</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.4.4.4.6.2\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S5.T9.4.4.4.6.2.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.6.2.2\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T9.4.4.4.6.2.2.1\">0-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.6.2.3\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T9.4.4.4.6.2.3.1\">0-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.6.2.4\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T9.4.4.4.6.2.4.1\">0-shot</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.2.2.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"4\" id=\"S5.T9.2.2.2.2.2\" style=\"background-color:#DFF5E5;\">\n<span class=\"ltx_text ltx_font_italic\" id=\"S5.T9.2.2.2.2.2.1\" style=\"background-color:#DFF5E5;\"> </span>\n</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.4.4.4.7.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.4.4.4.7.3.1\">ChatGLM3-6B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.7.3.2\">51.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.7.3.3\">76.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.7.3.4\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T9.4.4.4.7.3.4.1\">66.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.4.4.4.8.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.4.4.4.8.4.1\">Baichuan2-7B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.8.4.2\">68.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.8.4.3\">70.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.8.4.4\">41.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.4.4.4.9.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.4.4.4.9.5.1\">Llama2-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.9.5.2\">69.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.9.5.3\">74.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.9.5.4\">39.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.4.4.4.10.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.4.4.4.10.6.1\">Mistral-7B-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.10.6.2\">75.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.10.6.3\">78.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.10.6.4\">56.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.4.4.4.11.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.4.4.4.11.7.1\">Qwen-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.11.7.2\">68.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.11.7.3\">75.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.11.7.4\">45.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.4.4.4.12.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T9.4.4.4.12.8.1\">InternLM2-7B-Base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T9.4.4.4.12.8.2\">74.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T9.4.4.4.12.8.3\">76.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T9.4.4.4.12.8.4\">55.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.4.4.4.13.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.4.4.4.13.9.1\">InternLM2-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.13.9.2\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T9.4.4.4.13.9.2.1\">84.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.13.9.3\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T9.4.4.4.13.9.3.1\">79.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.13.9.4\">65.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.4.4.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" colspan=\"4\" id=\"S5.T9.4.4.4.4.2\" style=\"background-color:#EAE2FE;\">\n<span class=\"ltx_text ltx_font_italic\" id=\"S5.T9.4.4.4.4.2.1\" style=\"background-color:#EAE2FE;\"> </span>\n</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.4.4.4.14.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.4.4.4.14.10.1\">Llama2-13B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.14.10.2\">72.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.14.10.3\">77.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.14.10.4\">47.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.4.4.4.15.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.4.4.4.15.11.1\">Baichuan2-13B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.15.11.2\">70.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.15.11.3\">73.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.15.11.4\">48.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.4.4.4.16.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.4.4.4.16.12.1\">Qwen-14B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.16.12.2\">67.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.16.12.3\">80.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.16.12.4\">53.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.4.4.4.17.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.4.4.4.17.13.1\">Mixtral-8x7B-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.17.13.2\">77.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.17.13.3\">81.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.4.4.4.17.13.4\">68.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.4.4.4.18.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T9.4.4.4.18.14.1\">InternLM2-20B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T9.4.4.4.18.14.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T9.4.4.4.18.14.2.1\">85.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T9.4.4.4.18.14.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T9.4.4.4.18.14.3.1\">81.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T9.4.4.4.18.14.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T9.4.4.4.18.14.4.1\">72.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.4.4.4.19.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S5.T9.4.4.4.19.15.1\">InternLM2-20B-Base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T9.4.4.4.19.15.2\">76.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T9.4.4.4.19.15.3\">78.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T9.4.4.4.19.15.4\">62.0</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\">Base Model Results.</figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle\" id=\"S5.T9.8\" style=\"width:216.8pt;\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S5.T9.8.4\" style=\"width:433.6pt;height:493.8pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(66.6pt,-75.9pt) scale(1.44372640134618,1.44372640134618) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T9.8.4.4\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T9.8.4.4.5.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T9.8.4.4.5.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T9.8.4.4.5.1.1.1\">Models</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T9.8.4.4.5.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T9.8.4.4.5.1.2.1\">WinoGrande</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T9.8.4.4.5.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T9.8.4.4.5.1.3.1\">HellaSwag</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T9.8.4.4.5.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T9.8.4.4.5.1.4.1\">BBH</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.8.4.4.6.2\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S5.T9.8.4.4.6.2.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.6.2.2\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T9.8.4.4.6.2.2.1\">0-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.6.2.3\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T9.8.4.4.6.2.3.1\">0-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.6.2.4\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T9.8.4.4.6.2.4.1\">0-shot</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.8.4.4.7.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"4\" id=\"S5.T9.8.4.4.7.3.1\" style=\"background-color:#A8B3E3;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T9.8.4.4.7.3.1.1\" style=\"background-color:#A8B3E3;\">\u2605API Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.8.4.4.8.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.8.4.4.8.4.1\">GPT-3.5</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.8.4.2\">68.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.8.4.3\">70.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.8.4.4\">41.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.6.2.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" colspan=\"4\" id=\"S5.T9.6.2.2.2.2\" style=\"background-color:#DFF5E5;\">\n<span class=\"ltx_text ltx_font_italic\" id=\"S5.T9.6.2.2.2.2.1\" style=\"background-color:#DFF5E5;\"> </span>\n</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.8.4.4.9.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.8.4.4.9.5.1\">ChatGLM3-6B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.9.5.2\">61.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.9.5.3\">73.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.9.5.4\">56.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.8.4.4.10.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.8.4.4.10.6.1\">Llama2-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.10.6.2\">51.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.10.6.3\">49.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.10.6.4\">41.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.8.4.4.11.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.8.4.4.11.7.1\">Baichuan2-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.11.7.2\">49.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.11.7.3\">36.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.11.7.4\">35.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.8.4.4.12.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.8.4.4.12.8.1\">Mistral-7B-Instruct-v0.2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.12.8.2\">50.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.12.8.3\">64.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.12.8.4\">46.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.8.4.4.13.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.8.4.4.13.9.1\">Qwen-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.13.9.2\">54.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.13.9.3\">61.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.13.9.4\">45.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.8.4.4.14.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T9.8.4.4.14.10.1\">InternLM2-Chat-7B-SFT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T9.8.4.4.14.10.2\">65.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T9.8.4.4.14.10.3\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T9.8.4.4.14.10.3.1\">83.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T9.8.4.4.14.10.4\">60.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.8.4.4.15.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.8.4.4.15.11.1\">InternLM2-Chat-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.15.11.2\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T9.8.4.4.15.11.2.1\">65.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.15.11.3\">83.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.15.11.4\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T9.8.4.4.15.11.4.1\">61.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.8.4.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" colspan=\"4\" id=\"S5.T9.8.4.4.4.2\" style=\"background-color:#EAE2FE;\">\n<span class=\"ltx_text ltx_font_italic\" id=\"S5.T9.8.4.4.4.2.1\" style=\"background-color:#EAE2FE;\"> </span>\n</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.8.4.4.16.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.8.4.4.16.12.1\">Llama2-13B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.16.12.2\">50.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.16.12.3\">57.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.16.12.4\">49.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.8.4.4.17.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.8.4.4.17.13.1\">Baichuan2-13B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.17.13.2\">50.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.17.13.3\">34.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.17.13.4\">42.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.8.4.4.18.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.8.4.4.18.14.1\">Mixtral-8x7B-Instruct-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.18.14.2\">60.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.18.14.3\">80.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.18.14.4\">57.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.8.4.4.19.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T9.8.4.4.19.15.1\">Qwen-14B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.19.15.2\">55.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.19.15.3\">79.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T9.8.4.4.19.15.4\">55.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.8.4.4.20.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T9.8.4.4.20.16.1\">InternLM2-Chat-20B-SFT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T9.8.4.4.20.16.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T9.8.4.4.20.16.2.1\">75.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T9.8.4.4.20.16.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T9.8.4.4.20.16.3.1\">85.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T9.8.4.4.20.16.4\">66.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.8.4.4.21.17\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S5.T9.8.4.4.21.17.1\">InternLM2-Chat-20B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T9.8.4.4.21.17.2\">74.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T9.8.4.4.21.17.3\">85.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T9.8.4.4.21.17.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T9.8.4.4.21.17.4.1\">68.3</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\">Chat Model Results.</figcaption>\n</figure>\n</div>\n</div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T9.10.1\">Comparison of Reasoning Tasks</span></figcaption>\n</figure>",
            "capture": "Base Model Results."
        },
        "10": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T10\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T10.4\" style=\"width:433.6pt;height:313.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(5.2pt,-3.7pt) scale(1.02439839072197,1.02439839072197) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T10.4.4\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T10.4.4.5.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T10.4.4.5.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T10.4.4.5.1.1.1\">Models</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T10.4.4.5.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T10.4.4.5.1.2.1\">GSM8K</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T10.4.4.5.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T10.4.4.5.1.3.1\">MATH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T10.4.4.5.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T10.4.4.5.1.4.1\">TheoremQA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T10.4.4.5.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T10.4.4.5.1.5.1\">MathBench-CN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T10.4.4.5.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T10.4.4.5.1.6.1\">MathBench-EN</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.4.4.6.2\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S5.T10.4.4.6.2.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.6.2.2\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T10.4.4.6.2.2.1\">4-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.6.2.3\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T10.4.4.6.2.3.1\">4-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.6.2.4\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T10.4.4.6.2.4.1\">0-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.6.2.5\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T10.4.4.6.2.5.1\">0-shot&amp;4-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.6.2.6\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T10.4.4.6.2.6.1\">0-shot&amp;4-shot</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.2.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"6\" id=\"S5.T10.2.2.2.2\" style=\"background-color:#DFF5E5;\">\n<span class=\"ltx_text ltx_font_italic\" id=\"S5.T10.2.2.2.2.1\" style=\"background-color:#DFF5E5;\"> </span>\n</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.4.4.7.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T10.4.4.7.3.1\">ChatGLM3-6B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.7.3.2\">60.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.7.3.3\">19.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.7.3.4\">6.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.7.3.5\">32.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.7.3.6\">29.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.4.4.8.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T10.4.4.8.4.1\">Baichuan2-7B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.8.4.2\">24.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.8.4.3\">5.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.8.4.4\">1.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.8.4.5\">21.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.8.4.6\">15.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.4.4.9.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T10.4.4.9.5.1\">Llama2-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.9.5.2\">16.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.9.5.3\">3.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.9.5.4\">1.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.9.5.5\">4.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.9.5.6\">7.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.4.4.10.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T10.4.4.10.6.1\">Mistral-7B-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.10.6.2\">47.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.10.6.3\">11.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.10.6.4\">1.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.10.6.5\">19.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.10.6.6\">27.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.4.4.11.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T10.4.4.11.7.1\">Qwen-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.11.7.2\">54.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.11.7.3\">13.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.11.7.4\">5.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.11.7.5\">26.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.11.7.6\">21.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.4.4.12.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T10.4.4.12.8.1\">InternLM2-7B-Base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T10.4.4.12.8.2\">36.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T10.4.4.12.8.3\">8.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T10.4.4.12.8.4\">2.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T10.4.4.12.8.5\">14.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T10.4.4.12.8.6\">18.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.4.4.13.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T10.4.4.13.9.1\">InternLM2-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.13.9.2\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T10.4.4.13.9.2.1\">70.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.13.9.3\">20.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.13.9.4\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T10.4.4.13.9.4.1\">10.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.13.9.5\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T10.4.4.13.9.5.1\">37.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.13.9.6\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T10.4.4.13.9.6.1\">33.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.4.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" colspan=\"6\" id=\"S5.T10.4.4.4.2\" style=\"background-color:#EAE2FE;\">\n<span class=\"ltx_text ltx_font_italic\" id=\"S5.T10.4.4.4.2.1\" style=\"background-color:#EAE2FE;\"> </span>\n</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.4.4.14.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T10.4.4.14.10.1\">Llama2-13B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.14.10.2\">28.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.14.10.3\">4.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.14.10.4\">2.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.14.10.5\">10.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.14.10.6\">18.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.4.4.15.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T10.4.4.15.11.1\">Baichuan2-13B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.15.11.2\">52.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.15.11.3\">10.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.15.11.4\">4.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.15.11.5\">28.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.15.11.6\">24.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.4.4.16.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T10.4.4.16.12.1\">Qwen-14B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.16.12.2\">60.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.16.12.3\">25.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.16.12.4\">10.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.16.12.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T10.4.4.16.12.5.1\">46.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.16.12.6\">39.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.4.4.17.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T10.4.4.17.13.1\">Mixtral-8x7B-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.17.13.2\">66.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.17.13.3\">22.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.17.13.4\">2.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.17.13.5\">31.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T10.4.4.17.13.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T10.4.4.17.13.6.1\">40.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.4.4.18.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T10.4.4.18.14.1\">InternLM2-20B-Base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T10.4.4.18.14.2\">54.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T10.4.4.18.14.3\">13.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T10.4.4.18.14.4\">3.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T10.4.4.18.14.5\">27.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T10.4.4.18.14.6\">26.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.4.4.19.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S5.T10.4.4.19.15.1\">InternLM2-20B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T10.4.4.19.15.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T10.4.4.19.15.2.1\">76.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T10.4.4.19.15.3\">25.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T10.4.4.19.15.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T10.4.4.19.15.4.1\">13.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T10.4.4.19.15.5\">38.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T10.4.4.19.15.6\">36.9</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 10: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T10.8.1\">Comparison of Base Models on Math Tasks.</span> The model name in <span class=\"ltx_text ltx_font_bold\" id=\"S5.T10.9.2\">bold</span> indicates the top performer among Open-Source or API models, while an <span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T10.10.3\">underline</span> signifies the leading model within a similar parameter size group.</figcaption>\n</figure>",
            "capture": "Table 10: Comparison of Base Models on Math Tasks. The model name in bold indicates the top performer among Open-Source or API models, while an underline signifies the leading model within a similar parameter size group."
        },
        "11": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T11\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T11.4\" style=\"width:433.6pt;height:329.6pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-8.2pt,6.2pt) scale(0.963628057064515,0.963628057064515) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T11.4.4\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T11.4.4.5.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T11.4.4.5.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T11.4.4.5.1.1.1\">Models/Datasets</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T11.4.4.5.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T11.4.4.5.1.2.1\">GSM8K</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T11.4.4.5.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T11.4.4.5.1.3.1\">MATH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T11.4.4.5.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T11.4.4.5.1.4.1\">TheoremQA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T11.4.4.5.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T11.4.4.5.1.5.1\">MathBench-CN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T11.4.4.5.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T11.4.4.5.1.6.1\">MathBench-EN</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.4.4.6.2\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S5.T11.4.4.6.2.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.6.2.2\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T11.4.4.6.2.2.1\">4-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.6.2.3\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T11.4.4.6.2.3.1\">4-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.6.2.4\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T11.4.4.6.2.4.1\">0-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.6.2.5\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T11.4.4.6.2.5.1\">0-shot&amp;8-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.6.2.6\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T11.4.4.6.2.6.1\">0-shot&amp;8-shot</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.4.4.7.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"6\" id=\"S5.T11.4.4.7.3.1\" style=\"background-color:#A8B3E3;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T11.4.4.7.3.1.1\" style=\"background-color:#A8B3E3;\">\u2605API Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.4.4.8.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T11.4.4.8.4.1\">GPT-3.5</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.8.4.2\">78.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.8.4.3\">28.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.8.4.4\">9.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.8.4.5\">26.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.8.4.6\">42.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.2.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" colspan=\"6\" id=\"S5.T11.2.2.2.2\" style=\"background-color:#DFF5E5;\">\n<span class=\"ltx_text ltx_font_italic\" id=\"S5.T11.2.2.2.2.1\" style=\"background-color:#DFF5E5;\"> </span>\n</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.4.4.9.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T11.4.4.9.5.1\">ChatGLM3-6B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.9.5.2\">53.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.9.5.3\">20.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.9.5.4\">9.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.9.5.5\">18.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.9.5.6\">21.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.4.4.10.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T11.4.4.10.6.1\">Llama2-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.10.6.2\">28.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.10.6.3\">4.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.10.6.4\">0.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.10.6.5\">3.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.10.6.6\">7.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.4.4.11.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T11.4.4.11.7.1\">Baichuan2-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.11.7.2\">32.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.11.7.3\">5.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.11.7.4\">2.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.11.7.5\">16.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.11.7.6\">17.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.4.4.12.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T11.4.4.12.8.1\">Mistral-7B-Instruct-v0.2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.12.8.2\">48.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.12.8.3\">8.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.12.8.4\">2.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.12.8.5\">14.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.12.8.6\">28.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.4.4.13.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T11.4.4.13.9.1\">Qwen-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.13.9.2\">44.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.13.9.3\">12.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.13.9.4\">5.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.13.9.5\">31.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.13.9.6\">29.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.4.4.14.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T11.4.4.14.10.1\">InternLM2-Chat-7B-SFT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T11.4.4.14.10.2\">68.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T11.4.4.14.10.3\">23.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T11.4.4.14.10.4\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T11.4.4.14.10.4.1\">11.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T11.4.4.14.10.5\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T11.4.4.14.10.5.1\">43.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T11.4.4.14.10.6\">40.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.4.4.15.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T11.4.4.15.11.1\">InternLM2-Chat-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.15.11.2\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T11.4.4.15.11.2.1\">70.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.15.11.3\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T11.4.4.15.11.3.1\">23.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.15.11.4\">9.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.15.11.5\">38.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.15.11.6\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T11.4.4.15.11.6.1\">42.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.4.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" colspan=\"6\" id=\"S5.T11.4.4.4.2\" style=\"background-color:#EAE2FE;\">\n<span class=\"ltx_text ltx_font_italic\" id=\"S5.T11.4.4.4.2.1\" style=\"background-color:#EAE2FE;\"> </span>\n</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.4.4.16.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T11.4.4.16.12.1\">Llama2-13B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.16.12.2\">43.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.16.12.3\">5.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.16.12.4\">1.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.16.12.5\">6.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.16.12.6\">15.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.4.4.17.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T11.4.4.17.13.1\">Baichuan2-13B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.17.13.2\">56.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.17.13.3\">4.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.17.13.4\">3.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.17.13.5\">28.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.17.13.6\">29.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.4.4.18.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T11.4.4.18.14.1\">Mixtral-8x7B-Instruct-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.18.14.2\">71.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.18.14.3\">22.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.18.14.4\">9.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.18.14.5\">27.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.18.14.6\">44.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.4.4.19.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T11.4.4.19.15.1\">Qwen-14B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.19.15.2\">57.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.19.15.3\">27.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.19.15.4\">8.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.19.15.5\">47.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T11.4.4.19.15.6\">40.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.4.4.20.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T11.4.4.20.16.1\">InternLM2-Chat-20B-SFT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T11.4.4.20.16.2\">77.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T11.4.4.20.16.3\">31.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T11.4.4.20.16.4\">13.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T11.4.4.20.16.5\">47.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T11.4.4.20.16.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T11.4.4.20.16.6.1\">50.6</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.4.4.21.17\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S5.T11.4.4.21.17.1\">InternLM2-Chat-20B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T11.4.4.21.17.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T11.4.4.21.17.2.1\">79.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T11.4.4.21.17.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T11.4.4.21.17.3.1\">32.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T11.4.4.21.17.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T11.4.4.21.17.4.1\">14.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T11.4.4.21.17.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T11.4.4.21.17.5.1\">48.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T11.4.4.21.17.6\">48.5</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 11: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T11.8.1\">Comparison of Chat Models on Math Tasks</span>. Models are categorized by their parameter size and type, highlighting top performers in each category with <span class=\"ltx_text ltx_font_bold\" id=\"S5.T11.9.2\">bold</span> for overall leaders and <span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T11.10.3\">underline</span> for leaders within their parameter group.</figcaption>\n</figure>",
            "capture": "Table 11: Comparison of Chat Models on Math Tasks. Models are categorized by their parameter size and type, highlighting top performers in each category with bold for overall leaders and underline for leaders within their parameter group."
        },
        "12": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T12\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S5.T12.2\" style=\"width:390.3pt;height:306.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.6pt,-0.4pt) scale(1.002938924064,1.002938924064) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T12.2.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T12.2.2.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S5.T12.2.2.3.1.1\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T12.2.2.3.1.1.1\">Models</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T12.2.2.3.1.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T12.2.2.3.1.2.1\">HumanEval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T12.2.2.3.1.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T12.2.2.3.1.3.1\">HumanEval-X</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T12.2.2.3.1.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T12.2.2.3.1.4.1\">MBPP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T12.2.2.3.1.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T12.2.2.3.1.5.1\">MBPP-CN</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.2.2.4.2\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.2.2.4.2.1\" style=\"padding:1pt 10.0pt;\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.4.2.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T12.2.2.4.2.2.1\">4-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.4.2.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T12.2.2.4.2.3.1\">5-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.4.2.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T12.2.2.4.2.4.1\">5-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.4.2.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T12.2.2.4.2.5.1\">0-shot</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" colspan=\"5\" id=\"S5.T12.1.1.1.1\" style=\"background-color:#DFF5E5;padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T12.1.1.1.1.1\" style=\"background-color:#DFF5E5;\">B Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.2.2.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.2.2.5.3.1\" style=\"padding:1pt 10.0pt;\">Llama2-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.5.3.2\" style=\"padding:1pt 10.0pt;\">14.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.5.3.3\" style=\"padding:1pt 10.0pt;\">11.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.5.3.4\" style=\"padding:1pt 10.0pt;\">28.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.5.3.5\" style=\"padding:1pt 10.0pt;\">16.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.2.2.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.2.2.6.4.1\" style=\"padding:1pt 10.0pt;\">Mistral-7B-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.6.4.2\" style=\"padding:1pt 10.0pt;\">27.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.6.4.3\" style=\"padding:1pt 10.0pt;\">28.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.6.4.4\" style=\"padding:1pt 10.0pt;\">47.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.6.4.5\" style=\"padding:1pt 10.0pt;\">35.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.2.2.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.2.2.7.5.1\" style=\"padding:1pt 10.0pt;\">Baichuan2-7B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.7.5.2\" style=\"padding:1pt 10.0pt;\">22.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.7.5.3\" style=\"padding:1pt 10.0pt;\">16.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.7.5.4\" style=\"padding:1pt 10.0pt;\">35.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.7.5.5\" style=\"padding:1pt 10.0pt;\">23.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.2.2.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.2.2.8.6.1\" style=\"padding:1pt 10.0pt;\">Qwen-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.8.6.2\" style=\"padding:1pt 10.0pt;\">36.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.8.6.3\" style=\"padding:1pt 10.0pt;\">24.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.8.6.4\" style=\"padding:1pt 10.0pt;\">33.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.8.6.5\" style=\"padding:1pt 10.0pt;\">27.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.2.2.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.2.2.9.7.1\" style=\"padding:1pt 10.0pt;\">ChatGLM3-6B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.9.7.2\" style=\"padding:1pt 10.0pt;\">45.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.9.7.3\" style=\"padding:1pt 10.0pt;\">38.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.9.7.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T12.2.2.9.7.4.1\">68.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.9.7.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T12.2.2.9.7.5.1\">50.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.2.2.10.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T12.2.2.10.8.1\" style=\"padding:1pt 10.0pt;\">InternLM2-7B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T12.2.2.10.8.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T12.2.2.10.8.2.1\">56.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T12.2.2.10.8.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T12.2.2.10.8.3.1\">39.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T12.2.2.10.8.4\" style=\"padding:1pt 10.0pt;\">51.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T12.2.2.10.8.5\" style=\"padding:1pt 10.0pt;\">45.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.2.2.11.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.2.2.11.9.1\" style=\"padding:1pt 10.0pt;\">InternLM2-7B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.11.9.2\" style=\"padding:1pt 10.0pt;\">31.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.11.9.3\" style=\"padding:1pt 10.0pt;\">28.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.11.9.4\" style=\"padding:1pt 10.0pt;\">54.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.11.9.5\" style=\"padding:1pt 10.0pt;\">40.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.2.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" colspan=\"5\" id=\"S5.T12.2.2.2.1\" style=\"background-color:#EAE2FE;padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T12.2.2.2.1.1\" style=\"background-color:#EAE2FE;\">B Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.2.2.12.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.2.2.12.10.1\" style=\"padding:1pt 10.0pt;\">Llama2-13B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.12.10.2\" style=\"padding:1pt 10.0pt;\">18.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.12.10.3\" style=\"padding:1pt 10.0pt;\">17.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.12.10.4\" style=\"padding:1pt 10.0pt;\">38.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.12.10.5\" style=\"padding:1pt 10.0pt;\">25.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.2.2.13.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.2.2.13.11.1\" style=\"padding:1pt 10.0pt;\">Mixtral-8x7B-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.13.11.2\" style=\"padding:1pt 10.0pt;\">32.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.13.11.3\" style=\"padding:1pt 10.0pt;\">38.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.13.11.4\" style=\"padding:1pt 10.0pt;\">59.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.13.11.5\" style=\"padding:1pt 10.0pt;\">43.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.2.2.14.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.2.2.14.12.1\" style=\"padding:1pt 10.0pt;\">Baichuan2-13B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.14.12.2\" style=\"padding:1pt 10.0pt;\">23.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.14.12.3\" style=\"padding:1pt 10.0pt;\">19.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.14.12.4\" style=\"padding:1pt 10.0pt;\">44.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.14.12.5\" style=\"padding:1pt 10.0pt;\">30.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.2.2.15.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.2.2.15.13.1\" style=\"padding:1pt 10.0pt;\">Qwen-14B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.15.13.2\" style=\"padding:1pt 10.0pt;\">30.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.15.13.3\" style=\"padding:1pt 10.0pt;\">31.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.15.13.4\" style=\"padding:1pt 10.0pt;\">52.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T12.2.2.15.13.5\" style=\"padding:1pt 10.0pt;\">41.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.2.2.16.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T12.2.2.16.14.1\" style=\"padding:1pt 10.0pt;\">InternLM2-20B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T12.2.2.16.14.2\" style=\"padding:1pt 10.0pt;\">48.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T12.2.2.16.14.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T12.2.2.16.14.3.1\">48.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T12.2.2.16.14.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T12.2.2.16.14.4.1\">63.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T12.2.2.16.14.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T12.2.2.16.14.5.1\">53.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.2.2.17.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S5.T12.2.2.17.15.1\" style=\"padding:1pt 10.0pt;\">InternLM2-20B-Base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T12.2.2.17.15.2\" style=\"padding:1pt 10.0pt;\">32.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T12.2.2.17.15.3\" style=\"padding:1pt 10.0pt;\">31.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T12.2.2.17.15.4\" style=\"padding:1pt 10.0pt;\">59.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T12.2.2.17.15.5\" style=\"padding:1pt 10.0pt;\">44.4</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 12: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T12.6.1\">Comparison of Base Models on Python Coding</span>. The model name in <span class=\"ltx_text ltx_font_bold\" id=\"S5.T12.7.2\">bold</span> indicates the top performer, while an <span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T12.8.3\">underline</span> signifies the leading model within a similar parameter size group.</figcaption>\n</figure>",
            "capture": "Table 12: Comparison of Base Models on Python Coding. The model name in bold indicates the top performer, while an underline signifies the leading model within a similar parameter size group."
        },
        "13": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T13\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S5.T13.2\" style=\"width:390.3pt;height:287.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-12.8pt,9.4pt) scale(0.938550766610387,0.938550766610387) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T13.2.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T13.2.2.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S5.T13.2.2.3.1.1\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T13.2.2.3.1.1.1\">Models</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T13.2.2.3.1.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T13.2.2.3.1.2.1\">HumanEval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T13.2.2.3.1.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T13.2.2.3.1.3.1\">HumanEval-X</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T13.2.2.3.1.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T13.2.2.3.1.4.1\">MBPP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T13.2.2.3.1.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T13.2.2.3.1.5.1\">MBPP-CN</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.2.2.4.2\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S5.T13.2.2.4.2.1\" style=\"padding:1pt 10.0pt;\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.4.2.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T13.2.2.4.2.2.1\">4-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.4.2.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T13.2.2.4.2.3.1\">5-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.4.2.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T13.2.2.4.2.4.1\">5-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.4.2.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T13.2.2.4.2.5.1\">0-shot</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" colspan=\"5\" id=\"S5.T13.1.1.1.1\" style=\"background-color:#DFF5E5;padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T13.1.1.1.1.1\" style=\"background-color:#DFF5E5;\">B Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.2.2.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T13.2.2.5.3.1\" style=\"padding:1pt 10.0pt;\">Llama2-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.5.3.2\" style=\"padding:1pt 10.0pt;\">15.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.5.3.3\" style=\"padding:1pt 10.0pt;\">10.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.5.3.4\" style=\"padding:1pt 10.0pt;\">30.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.5.3.5\" style=\"padding:1pt 10.0pt;\">17.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.2.2.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T13.2.2.6.4.1\" style=\"padding:1pt 10.0pt;\">Mistral-7B-Instruct-v0.2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.6.4.2\" style=\"padding:1pt 10.0pt;\">35.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.6.4.3\" style=\"padding:1pt 10.0pt;\">27.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.6.4.4\" style=\"padding:1pt 10.0pt;\">23.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.6.4.5\" style=\"padding:1pt 10.0pt;\">17.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.2.2.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T13.2.2.7.5.1\" style=\"padding:1pt 10.0pt;\">Baichuan2-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.7.5.2\" style=\"padding:1pt 10.0pt;\">17.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.7.5.3\" style=\"padding:1pt 10.0pt;\">15.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.7.5.4\" style=\"padding:1pt 10.0pt;\">37.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.7.5.5\" style=\"padding:1pt 10.0pt;\">20.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.2.2.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T13.2.2.8.6.1\" style=\"padding:1pt 10.0pt;\">Qwen-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.8.6.2\" style=\"padding:1pt 10.0pt;\">36.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.8.6.3\" style=\"padding:1pt 10.0pt;\">24.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.8.6.4\" style=\"padding:1pt 10.0pt;\">33.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.8.6.5\" style=\"padding:1pt 10.0pt;\">27.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.2.2.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T13.2.2.9.7.1\" style=\"padding:1pt 10.0pt;\">ChatGLM3-6B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.9.7.2\" style=\"padding:1pt 10.0pt;\">53.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.9.7.3\" style=\"padding:1pt 10.0pt;\">17.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.9.7.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T13.2.2.9.7.4.1\">54.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.9.7.5\" style=\"padding:1pt 10.0pt;\">38.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.2.2.10.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T13.2.2.10.8.1\" style=\"padding:1pt 10.0pt;\">InternLM2-Chat-7B-SFT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T13.2.2.10.8.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T13.2.2.10.8.2.1\">61.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T13.2.2.10.8.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T13.2.2.10.8.3.1\">43.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T13.2.2.10.8.4\" style=\"padding:1pt 10.0pt;\">47.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T13.2.2.10.8.5\" style=\"padding:1pt 10.0pt;\">44.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.2.2.11.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T13.2.2.11.9.1\" style=\"padding:1pt 10.0pt;\">InternLM2-Chat-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.11.9.2\" style=\"padding:1pt 10.0pt;\">59.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.11.9.3\" style=\"padding:1pt 10.0pt;\">41.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.11.9.4\" style=\"padding:1pt 10.0pt;\">52.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.11.9.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T13.2.2.11.9.5.1\">46.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.2.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" colspan=\"5\" id=\"S5.T13.2.2.2.1\" style=\"background-color:#EAE2FE;padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T13.2.2.2.1.1\" style=\"background-color:#EAE2FE;\">B Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.2.2.12.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T13.2.2.12.10.1\" style=\"padding:1pt 10.0pt;\">Llama2-13B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.12.10.2\" style=\"padding:1pt 10.0pt;\">8.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.12.10.3\" style=\"padding:1pt 10.0pt;\">12.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.12.10.4\" style=\"padding:1pt 10.0pt;\">19.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.12.10.5\" style=\"padding:1pt 10.0pt;\">22.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.2.2.13.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T13.2.2.13.11.1\" style=\"padding:1pt 10.0pt;\">Mixtral-8x7B-Instruct-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.13.11.2\" style=\"padding:1pt 10.0pt;\">32.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.13.11.3\" style=\"padding:1pt 10.0pt;\">38.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.13.11.4\" style=\"padding:1pt 10.0pt;\">59.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.13.11.5\" style=\"padding:1pt 10.0pt;\">43.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.2.2.14.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T13.2.2.14.12.1\" style=\"padding:1pt 10.0pt;\">Baichuan2-13B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.14.12.2\" style=\"padding:1pt 10.0pt;\">19.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.14.12.3\" style=\"padding:1pt 10.0pt;\">18.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.14.12.4\" style=\"padding:1pt 10.0pt;\">40.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.14.12.5\" style=\"padding:1pt 10.0pt;\">27.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.2.2.15.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T13.2.2.15.13.1\" style=\"padding:1pt 10.0pt;\">Qwen-14B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.15.13.2\" style=\"padding:1pt 10.0pt;\">41.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.15.13.3\" style=\"padding:1pt 10.0pt;\">29.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.15.13.4\" style=\"padding:1pt 10.0pt;\">29.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.2.2.15.13.5\" style=\"padding:1pt 10.0pt;\">27.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.2.2.16.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T13.2.2.16.14.1\" style=\"padding:1pt 10.0pt;\">InternLM2-Chat-20B-SFT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T13.2.2.16.14.2\" style=\"padding:1pt 10.0pt;\">67.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T13.2.2.16.14.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T13.2.2.16.14.3.1\">42.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T13.2.2.16.14.4\" style=\"padding:1pt 10.0pt;\">63.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T13.2.2.16.14.5\" style=\"padding:1pt 10.0pt;\">52.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.2.2.17.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S5.T13.2.2.17.15.1\" style=\"padding:1pt 10.0pt;\">InternLM2-Chat-20B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T13.2.2.17.15.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T13.2.2.17.15.2.1\">67.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T13.2.2.17.15.3\" style=\"padding:1pt 10.0pt;\">39.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T13.2.2.17.15.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T13.2.2.17.15.4.1\">65.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T13.2.2.17.15.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T13.2.2.17.15.5.1\">54.0</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 13: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T13.6.1\">Comparison of Chat Models on Python Coding</span>. The model name in <span class=\"ltx_text ltx_font_bold\" id=\"S5.T13.7.2\">bold</span> indicates the top performer, while an <span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T13.8.3\">underline</span> signifies the leading model within a similar parameter size group.</figcaption>\n</figure>",
            "capture": "Table 13: Comparison of Chat Models on Python Coding. The model name in bold indicates the top performer, while an underline signifies the leading model within a similar parameter size group."
        },
        "14": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T14\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T14.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T14.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T14.1.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T14.1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S5.T14.1.1.1.1.1.1\">Model</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T14.1.1.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T14.1.1.1.2.1\">\n<span class=\"ltx_p\" id=\"S5.T14.1.1.1.2.1.1\">Language</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T14.1.1.1.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T14.1.1.1.3.1\">\n<span class=\"ltx_p\" id=\"S5.T14.1.1.1.3.1.1\">Knowledge</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T14.1.1.1.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T14.1.1.1.4.1\">\n<span class=\"ltx_p\" id=\"S5.T14.1.1.1.4.1.1\">Reason</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T14.1.1.1.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T14.1.1.1.5.1\">\n<span class=\"ltx_p\" id=\"S5.T14.1.1.1.5.1.1\">Code</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T14.1.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T14.1.2.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T14.1.2.1.1.1\">\n<span class=\"ltx_p\" id=\"S5.T14.1.2.1.1.1.1\">SFT from InternLM2-7B-Base</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T14.1.2.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T14.1.2.1.2.1\">\n<span class=\"ltx_p\" id=\"S5.T14.1.2.1.2.1.1\">66.64</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T14.1.2.1.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T14.1.2.1.3.1\">\n<span class=\"ltx_p\" id=\"S5.T14.1.2.1.3.1.1\">58.35</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T14.1.2.1.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T14.1.2.1.4.1\">\n<span class=\"ltx_p\" id=\"S5.T14.1.2.1.4.1.1\">69.30</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T14.1.2.1.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T14.1.2.1.5.1\">\n<span class=\"ltx_p\" id=\"S5.T14.1.2.1.5.1.1\">38.79</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.3.2\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T14.1.3.2.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T14.1.3.2.1.1\">\n<span class=\"ltx_p\" id=\"S5.T14.1.3.2.1.1.1\">SFT from InternLM2-7B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T14.1.3.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T14.1.3.2.2.1\">\n<span class=\"ltx_p\" id=\"S5.T14.1.3.2.2.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T14.1.3.2.2.1.1.1\">69.28</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T14.1.3.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T14.1.3.2.3.1\">\n<span class=\"ltx_p\" id=\"S5.T14.1.3.2.3.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T14.1.3.2.3.1.1.1\">62.11</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T14.1.3.2.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T14.1.3.2.4.1\">\n<span class=\"ltx_p\" id=\"S5.T14.1.3.2.4.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T14.1.3.2.4.1.1.1\">75.18</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T14.1.3.2.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T14.1.3.2.5.1\">\n<span class=\"ltx_p\" id=\"S5.T14.1.3.2.5.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T14.1.3.2.5.1.1.1\">41.40</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 14: </span>Comparative performance of a 7B SFT model trained from InternLM2-7B-Base and from InternLM2-7B. </figcaption>\n</figure>",
            "capture": "Table 14: Comparative performance of a 7B SFT model trained from InternLM2-7B-Base and from InternLM2-7B. "
        },
        "15": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T15\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S5.T15.2\" style=\"width:433.6pt;height:233pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-67.9pt,36.5pt) scale(0.761440720701178,0.761440720701178) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T15.2.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T15.2.2.3.1\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T15.2.2.3.1.1\" style=\"padding:1pt 10.0pt;\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\" id=\"S5.T15.2.2.3.1.2\" style=\"padding:1pt 10.0pt;\">L-Eval</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"7\" id=\"S5.T15.2.2.3.1.3\" style=\"padding:1pt 10.0pt;\">LongBench</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.2.2.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T15.2.2.4.2.1\" style=\"padding:1pt 10.0pt;\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T15.2.2.4.2.2\" style=\"padding:1pt 10.0pt;\">Close</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T15.2.2.4.2.3\" style=\"padding:1pt 10.0pt;\">Open</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T15.2.2.4.2.4\" style=\"padding:1pt 10.0pt;\">Avg.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T15.2.2.4.2.5\" style=\"padding:1pt 10.0pt;\">Single</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T15.2.2.4.2.6\" style=\"padding:1pt 10.0pt;\">Multi</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T15.2.2.4.2.7\" style=\"padding:1pt 10.0pt;\">Summ</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T15.2.2.4.2.8\" style=\"padding:1pt 10.0pt;\">FSL</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T15.2.2.4.2.9\" style=\"padding:1pt 10.0pt;\">Syn</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T15.2.2.4.2.10\" style=\"padding:1pt 10.0pt;\">Code</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T15.2.2.4.2.11\" style=\"padding:1pt 10.0pt;\">Avg.</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T15.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"11\" id=\"S5.T15.1.1.1.1\" style=\"background-color:#DFF5E5;padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T15.1.1.1.1.1\" style=\"background-color:#DFF5E5;\">B Models</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.2.2.5.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T15.2.2.5.1.1\" style=\"padding:1pt 10.0pt;\">Llama2-7B-Chat</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.5.1.2\" style=\"padding:1pt 10.0pt;\">20.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.5.1.3\" style=\"padding:1pt 10.0pt;\">33.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.2.2.5.1.4\" style=\"padding:1pt 10.0pt;\">29.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.5.1.5\" style=\"padding:1pt 10.0pt;\">23.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.5.1.6\" style=\"padding:1pt 10.0pt;\">17.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.5.1.7\" style=\"padding:1pt 10.0pt;\">18.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.5.1.8\" style=\"padding:1pt 10.0pt;\">7.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.5.1.9\" style=\"padding:1pt 10.0pt;\">5.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.5.1.10\" style=\"padding:1pt 10.0pt;\">15.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.5.1.11\" style=\"padding:1pt 10.0pt;\">14.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.2.2.6.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T15.2.2.6.2.1\" style=\"padding:1pt 10.0pt;\">Mistral-7B-Instruct-v0.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.6.2.2\" style=\"padding:1pt 10.0pt;\">54.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.6.2.3\" style=\"padding:1pt 10.0pt;\">34.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.2.2.6.2.4\" style=\"padding:1pt 10.0pt;\">39.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.6.2.5\" style=\"padding:1pt 10.0pt;\">31.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.6.2.6\" style=\"padding:1pt 10.0pt;\">26.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.6.2.7\" style=\"padding:1pt 10.0pt;\">21.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.6.2.8\" style=\"padding:1pt 10.0pt;\">46.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.6.2.9\" style=\"padding:1pt 10.0pt;\">59.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.6.2.10\" style=\"padding:1pt 10.0pt;\">44.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.6.2.11\" style=\"padding:1pt 10.0pt;\">38.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.2.2.7.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T15.2.2.7.3.1\" style=\"padding:1pt 10.0pt;\">Baichuan2-7B-Chat</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.7.3.2\" style=\"padding:1pt 10.0pt;\">36.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.7.3.3\" style=\"padding:1pt 10.0pt;\">32.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.2.2.7.3.4\" style=\"padding:1pt 10.0pt;\">33.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.7.3.5\" style=\"padding:1pt 10.0pt;\">30.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.7.3.6\" style=\"padding:1pt 10.0pt;\">18.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.7.3.7\" style=\"padding:1pt 10.0pt;\">21.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.7.3.8\" style=\"padding:1pt 10.0pt;\">42.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.7.3.9\" style=\"padding:1pt 10.0pt;\">6.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.7.3.10\" style=\"padding:1pt 10.0pt;\">36.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.7.3.11\" style=\"padding:1pt 10.0pt;\">25.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.2.2.8.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T15.2.2.8.4.1\" style=\"padding:1pt 10.0pt;\">Qwen-7B-Chat</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.8.4.2\" style=\"padding:1pt 10.0pt;\">13.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.8.4.3\" style=\"padding:1pt 10.0pt;\">32.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.2.2.8.4.4\" style=\"padding:1pt 10.0pt;\">27.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.8.4.5\" style=\"padding:1pt 10.0pt;\">27.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.8.4.6\" style=\"padding:1pt 10.0pt;\">14.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.8.4.7\" style=\"padding:1pt 10.0pt;\">21.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.8.4.8\" style=\"padding:1pt 10.0pt;\">21.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.8.4.9\" style=\"padding:1pt 10.0pt;\">8.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.8.4.10\" style=\"padding:1pt 10.0pt;\">28.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.8.4.11\" style=\"padding:1pt 10.0pt;\">20.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.2.2.9.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T15.2.2.9.5.1\" style=\"padding:1pt 10.0pt;\">ChatGLM3-6B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.9.5.2\" style=\"padding:1pt 10.0pt;\">59.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.9.5.3\" style=\"padding:1pt 10.0pt;\">35.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.2.2.9.5.4\" style=\"padding:1pt 10.0pt;\">41.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.9.5.5\" style=\"padding:1pt 10.0pt;\">40.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.9.5.6\" style=\"padding:1pt 10.0pt;\">45.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.9.5.7\" style=\"padding:1pt 10.0pt;\">26.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.9.5.8\" style=\"padding:1pt 10.0pt;\">56.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.9.5.9\" style=\"padding:1pt 10.0pt;\">65.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.9.5.10\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T15.2.2.9.5.10.1\">57.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.9.5.11\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.2.2.9.5.11.1\">48.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.2.2.10.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T15.2.2.10.6.1\" style=\"padding:1pt 10.0pt;\">InternLM2-Chat-7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.10.6.2\" style=\"padding:1pt 10.0pt;\">68.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.10.6.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T15.2.2.10.6.3.1\">40.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.2.2.10.6.4\" style=\"padding:1pt 10.0pt;\">48.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.10.6.5\" style=\"padding:1pt 10.0pt;\">45.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.10.6.6\" style=\"padding:1pt 10.0pt;\">43.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.10.6.7\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.2.2.10.6.7.1\">26.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.10.6.8\" style=\"padding:1pt 10.0pt;\">58.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.10.6.9\" style=\"padding:1pt 10.0pt;\">66.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.10.6.10\" style=\"padding:1pt 10.0pt;\">36.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.10.6.11\" style=\"padding:1pt 10.0pt;\">46.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.2.2.11.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T15.2.2.11.7.1\" style=\"padding:1pt 10.0pt;\">InternLM2-Chat-7B-SFT</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.11.7.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T15.2.2.11.7.2.1\">68.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.11.7.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T15.2.2.11.7.3.1\">40.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.2.2.11.7.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T15.2.2.11.7.4.1\">48.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.11.7.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.2.2.11.7.5.1\">47.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.11.7.6\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T15.2.2.11.7.6.1\">45.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.11.7.7\" style=\"padding:1pt 10.0pt;\">25.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.11.7.8\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.2.2.11.7.8.1\">59.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.11.7.9\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T15.2.2.11.7.9.1\">67.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.11.7.10\" style=\"padding:1pt 10.0pt;\">43.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.11.7.11\" style=\"padding:1pt 10.0pt;\">48.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.2.2.2\">\n<td class=\"ltx_td ltx_align_center\" colspan=\"11\" id=\"S5.T15.2.2.2.1\" style=\"background-color:#EAE2FE;padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T15.2.2.2.1.1\" style=\"background-color:#EAE2FE;\">B Models</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.2.2.12.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T15.2.2.12.8.1\" style=\"padding:1pt 10.0pt;\">Llama2-13B-Chat</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.12.8.2\" style=\"padding:1pt 10.0pt;\">27.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.12.8.3\" style=\"padding:1pt 10.0pt;\">34.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.2.2.12.8.4\" style=\"padding:1pt 10.0pt;\">32.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.12.8.5\" style=\"padding:1pt 10.0pt;\">18.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.12.8.6\" style=\"padding:1pt 10.0pt;\">9.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.12.8.7\" style=\"padding:1pt 10.0pt;\">18.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.12.8.8\" style=\"padding:1pt 10.0pt;\">6.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.12.8.9\" style=\"padding:1pt 10.0pt;\">10.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.12.8.10\" style=\"padding:1pt 10.0pt;\">18.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.12.8.11\" style=\"padding:1pt 10.0pt;\">13.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.2.2.13.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T15.2.2.13.9.1\" style=\"padding:1pt 10.0pt;\">Mixtral-8x7B-Instruct-v0.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.13.9.2\" style=\"padding:1pt 10.0pt;\">65.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.13.9.3\" style=\"padding:1pt 10.0pt;\">35.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.2.2.13.9.4\" style=\"padding:1pt 10.0pt;\">43.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.13.9.5\" style=\"padding:1pt 10.0pt;\">35.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.13.9.6\" style=\"padding:1pt 10.0pt;\">25.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.13.9.7\" style=\"padding:1pt 10.0pt;\">17.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.13.9.8\" style=\"padding:1pt 10.0pt;\">35.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.13.9.9\" style=\"padding:1pt 10.0pt;\">68.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.13.9.10\" style=\"padding:1pt 10.0pt;\">40.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.13.9.11\" style=\"padding:1pt 10.0pt;\">37.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.2.2.14.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T15.2.2.14.10.1\" style=\"padding:1pt 10.0pt;\">Baichuan2-13B-Chat</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.14.10.2\" style=\"padding:1pt 10.0pt;\">40.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.14.10.3\" style=\"padding:1pt 10.0pt;\">32.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.2.2.14.10.4\" style=\"padding:1pt 10.0pt;\">34.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.14.10.5\" style=\"padding:1pt 10.0pt;\">34.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.14.10.6\" style=\"padding:1pt 10.0pt;\">29.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.14.10.7\" style=\"padding:1pt 10.0pt;\">21.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.14.10.8\" style=\"padding:1pt 10.0pt;\">45.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.14.10.9\" style=\"padding:1pt 10.0pt;\">4.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.14.10.10\" style=\"padding:1pt 10.0pt;\">51.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.14.10.11\" style=\"padding:1pt 10.0pt;\">30.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.2.2.15.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T15.2.2.15.11.1\" style=\"padding:1pt 10.0pt;\">Qwen-14B-Chat</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.15.11.2\" style=\"padding:1pt 10.0pt;\">35.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.15.11.3\" style=\"padding:1pt 10.0pt;\">33.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.2.2.15.11.4\" style=\"padding:1pt 10.0pt;\">34.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.15.11.5\" style=\"padding:1pt 10.0pt;\">32.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.15.11.6\" style=\"padding:1pt 10.0pt;\">19.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.15.11.7\" style=\"padding:1pt 10.0pt;\">22.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.15.11.8\" style=\"padding:1pt 10.0pt;\">36.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.15.11.9\" style=\"padding:1pt 10.0pt;\">23.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.15.11.10\" style=\"padding:1pt 10.0pt;\">42.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.15.11.11\" style=\"padding:1pt 10.0pt;\">29.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.2.2.16.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T15.2.2.16.12.1\" style=\"padding:1pt 10.0pt;\">InternLM2-Chat-20B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.16.12.2\" style=\"padding:1pt 10.0pt;\">68.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.16.12.3\" style=\"padding:1pt 10.0pt;\">40.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.2.2.16.12.4\" style=\"padding:1pt 10.0pt;\">48.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.16.12.5\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T15.2.2.16.12.5.1\">46.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.16.12.6\" style=\"padding:1pt 10.0pt;\">46.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.16.12.7\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T15.2.2.16.12.7.1\">26.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.16.12.8\" style=\"padding:1pt 10.0pt;\">49.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.16.12.9\" style=\"padding:1pt 10.0pt;\">67.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.16.12.10\" style=\"padding:1pt 10.0pt;\">32.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.16.12.11\" style=\"padding:1pt 10.0pt;\">44.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.2.2.17.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S5.T15.2.2.17.13.1\" style=\"padding:1pt 10.0pt;\">InternLM2-Chat-20B-SFT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T15.2.2.17.13.2\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.2.2.17.13.2.1\">68.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T15.2.2.17.13.3\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.2.2.17.13.3.1\">42.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T15.2.2.17.13.4\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.2.2.17.13.4.1\">49.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T15.2.2.17.13.5\" style=\"padding:1pt 10.0pt;\">46.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T15.2.2.17.13.6\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.2.2.17.13.6.1\">48.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T15.2.2.17.13.7\" style=\"padding:1pt 10.0pt;\">25.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T15.2.2.17.13.8\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T15.2.2.17.13.8.1\">51.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T15.2.2.17.13.9\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.2.2.17.13.9.1\">67.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T15.2.2.17.13.10\" style=\"padding:1pt 10.0pt;\">40.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T15.2.2.17.13.11\" style=\"padding:1pt 10.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T15.2.2.17.13.11.1\">46.8</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 15: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.22.1\">Comparison of Chat Models on Long-Context Benchmarks</span>. <span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.23.2\">Bold</span> indicates the top performer;\nan <span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T15.24.3\">underline</span> signifies the leading model within the same parameter size group.\nFor each benchmark and task group, we report the average accuracies of intra-group subtasks.\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.25.4\">L-Eval abbrs</span>: Close  close-ended; Open  open-ended.\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.26.5\">LongBench abbrs</span>: Single  single-document; Multi  multi-document; Summ  summarization; FSL  few shot learning; Syn  synthetic.\nAll results are obtained with the OpenCompass\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Contributors, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.17297v1#bib.bib29\" title=\"\">2023b</a>)</cite> evaluation toolkit. </figcaption>\n</figure>",
            "capture": "Table 15: Comparison of Chat Models on Long-Context Benchmarks. Bold indicates the top performer;\nan underline signifies the leading model within the same parameter size group.\nFor each benchmark and task group, we report the average accuracies of intra-group subtasks.\nL-Eval abbrs: Close  close-ended; Open  open-ended.\nLongBench abbrs: Single  single-document; Multi  multi-document; Summ  summarization; FSL  few shot learning; Syn  synthetic.\nAll results are obtained with the OpenCompass\u00a0(Contributors, 2023b) evaluation toolkit. "
        },
        "16": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T16\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T16.2\" style=\"width:318.0pt;height:261.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-27.2pt,22.3pt) scale(0.854106712292819,0.854106712292819) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T16.2.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T16.2.2.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S5.T16.2.2.3.1.1\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T16.2.2.3.1.1.1\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" id=\"S5.T16.2.2.3.1.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T16.2.2.3.1.2.1\">T-Eval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S5.T16.2.2.3.1.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T16.2.2.3.1.3.1\">CIBench</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T16.2.2.4.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.4.2.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">English</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.4.2.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Chinese</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T16.2.2.4.2.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Avg</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.4.2.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">English</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.4.2.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Chinese</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.4.2.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Avg</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T16.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"7\" id=\"S5.T16.1.1.1.1\" style=\"background-color:#DFF5E5;padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T16.1.1.1.1.1\" style=\"background-color:#DFF5E5;\">B Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T16.2.2.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T16.2.2.5.3.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Llama2-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.5.3.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">37.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.5.3.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">37.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T16.2.2.5.3.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">37.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.5.3.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">12.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.5.3.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">10.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.5.3.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">11.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T16.2.2.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T16.2.2.6.4.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">ChatGLM3-6B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.6.4.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">62.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.6.4.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">63.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T16.2.2.6.4.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">63.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.6.4.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">29.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.6.4.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">17.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.6.4.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">23.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T16.2.2.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T16.2.2.7.5.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Qwen-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.7.5.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">57.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.7.5.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">61.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T16.2.2.7.5.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">59.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.7.5.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">47.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.7.5.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">29.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.7.5.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">38.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T16.2.2.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T16.2.2.8.6.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Qwen1.5-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.8.6.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">54.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.8.6.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">50.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T16.2.2.8.6.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">52.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.8.6.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">44.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.8.6.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">26.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.8.6.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">35.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T16.2.2.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T16.2.2.9.7.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Mistral-7B-Instruct-v0.2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.9.7.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">39.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.9.7.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">38.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T16.2.2.9.7.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">39.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.9.7.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">47.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.9.7.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">38.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.9.7.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">42.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T16.2.2.10.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T16.2.2.10.8.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">InternLM2-Chat-7B-SFT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T16.2.2.10.8.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">64.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T16.2.2.10.8.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T16.2.2.10.8.3.1\">66.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T16.2.2.10.8.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">65.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T16.2.2.10.8.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">52.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T16.2.2.10.8.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">27.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T16.2.2.10.8.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">39.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T16.2.2.11.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T16.2.2.11.9.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">InternLM2-Chat-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.11.9.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T16.2.2.11.9.2.1\">66.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.11.9.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">65.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T16.2.2.11.9.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T16.2.2.11.9.4.1\">65.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.11.9.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T16.2.2.11.9.5.1\">57.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.11.9.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T16.2.2.11.9.6.1\">40.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.11.9.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T16.2.2.11.9.7.1\">48.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T16.2.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"7\" id=\"S5.T16.2.2.2.1\" style=\"background-color:#EAE2FE;padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T16.2.2.2.1.1\" style=\"background-color:#EAE2FE;\">B Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T16.2.2.12.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T16.2.2.12.10.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Llama2-13B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.12.10.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">48.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.12.10.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">44.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T16.2.2.12.10.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">46.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.12.10.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">20.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.12.10.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">12.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.12.10.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">16.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T16.2.2.13.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T16.2.2.13.11.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Qwen-14B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.13.11.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">63.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.13.11.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T16.2.2.13.11.3.1\">68.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T16.2.2.13.11.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T16.2.2.13.11.4.1\">66.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.13.11.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">54.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.13.11.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">40.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.13.11.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">47.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T16.2.2.14.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T16.2.2.14.12.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Qwen1.5-14B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.14.12.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">67.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.14.12.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">63.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T16.2.2.14.12.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">65.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.14.12.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">57.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.14.12.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">49.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.14.12.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">53.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T16.2.2.15.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T16.2.2.15.13.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Mistral-8x7B-Instruct-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.15.13.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">58.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.15.13.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">58.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T16.2.2.15.13.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">58.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.15.13.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">42.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.15.13.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">40.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.2.2.15.13.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">41.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T16.2.2.16.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T16.2.2.16.14.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">InternLM2-Chat-20B-SFT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T16.2.2.16.14.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">64.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T16.2.2.16.14.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">64.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T16.2.2.16.14.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">64.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T16.2.2.16.14.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">49.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T16.2.2.16.14.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">43.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T16.2.2.16.14.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">46.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T16.2.2.17.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S5.T16.2.2.17.15.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">InternLM2-Chat-20B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T16.2.2.17.15.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T16.2.2.17.15.2.1\">65.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T16.2.2.17.15.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">62.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T16.2.2.17.15.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">64.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T16.2.2.17.15.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T16.2.2.17.15.5.1\">56.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T16.2.2.17.15.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T16.2.2.17.15.6.1\">54.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T16.2.2.17.15.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T16.2.2.17.15.7.1\">55.4</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 16: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T16.6.1\">Results on T-Eval and CIBench.</span> <span class=\"ltx_text ltx_font_bold\" id=\"S5.T16.7.2\">Bold</span> indicates the top performer;\nan <span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T16.8.3\">underline</span> signifies the leading model within the same parameter size group. </figcaption>\n</figure>",
            "capture": "Table 16: Results on T-Eval and CIBench. Bold indicates the top performer;\nan underline signifies the leading model within the same parameter size group. "
        },
        "17": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T17\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T17.5\" style=\"width:433.6pt;height:333pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-6.6pt,5.1pt) scale(0.970366912463731,0.970366912463731) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T17.5.5\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T17.1.1.1\">\n<th class=\"ltx_td ltx_nopad ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T17.1.1.1.1\"><svg height=\"19.07\" overflow=\"visible\" version=\"1.1\" width=\"113.7\"><g transform=\"translate(0,19.07) scale(1,-1)\"><path d=\"M 0,19.07 113.7,0\" stroke=\"#000000\" stroke-width=\"0.4\"></path><g class=\"ltx_svg_fog\" transform=\"translate(0,0)\"><g transform=\"translate(0,9.61) scale(1, -1)\"><foreignobject height=\"9.61\" overflow=\"visible\" width=\"43.13\">\n<span class=\"ltx_inline-block\" id=\"S5.T17.1.1.1.1.pic1.1.1\">\n<span class=\"ltx_inline-block ltx_align_left\" id=\"S5.T17.1.1.1.1.pic1.1.1.1\">\n<span class=\"ltx_p\" id=\"S5.T17.1.1.1.1.pic1.1.1.1.1\">Models</span>\n</span>\n</span></foreignobject></g></g><g class=\"ltx_svg_fog\" transform=\"translate(56.85,9.61)\"><g transform=\"translate(0,9.46) scale(1, -1)\"><foreignobject height=\"9.46\" overflow=\"visible\" width=\"56.85\">\n<span class=\"ltx_inline-block\" id=\"S5.T17.1.1.1.1.pic1.2.1\">\n<span class=\"ltx_inline-block ltx_align_right\" id=\"S5.T17.1.1.1.1.pic1.2.1.1\">\n<span class=\"ltx_p\" id=\"S5.T17.1.1.1.1.pic1.2.1.1.1\">Datasets</span>\n</span>\n</span></foreignobject></g></g></g></svg></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T17.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T17.1.1.1.2.1\">AlpacaEval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T17.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T17.1.1.1.3.1\">MTBench</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T17.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T17.1.1.1.4.1\">CompassArena</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T17.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T17.1.1.1.5.1\">AlignBench</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T17.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T17.1.1.1.6.1\">IFEval</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.5.5.6.1\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S5.T17.5.5.6.1.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.6.1.2\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T17.5.5.6.1.2.1\">WinRate</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.6.1.3\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T17.5.5.6.1.3.1\">0-10 Score</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.6.1.4\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T17.5.5.6.1.4.1\">WinRate</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.6.1.5\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T17.5.5.6.1.5.1\">0-10 Score</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.6.1.6\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T17.5.5.6.1.6.1\">0-100 Acc</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.5.5.7.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"6\" id=\"S5.T17.5.5.7.2.1\" style=\"background-color:#A8B3E3;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T17.5.5.7.2.1.1\" style=\"background-color:#A8B3E3;\">\u2605API Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.5.5.8.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T17.5.5.8.3.1\">GPT-3.5</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.8.3.2\">9.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.8.3.3\">8.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.8.3.4\">24.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.8.3.5\">5.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.8.3.6\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.3.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" colspan=\"6\" id=\"S5.T17.3.3.3.2\" style=\"background-color:#DFF5E5;\">\n<span class=\"ltx_text ltx_font_italic\" id=\"S5.T17.3.3.3.2.1\" style=\"background-color:#DFF5E5;\"> </span>\n</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.5.5.9.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T17.5.5.9.4.1\">ChatGLM3-6B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.9.4.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.9.4.3\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.9.4.4\">17.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.9.4.5\">5.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.9.4.6\">33.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.5.5.10.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T17.5.5.10.5.1\">Llama2-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.10.5.2\">5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.10.5.3\">6.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.10.5.4\">6.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.10.5.5\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.10.5.6\">44.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.5.5.11.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T17.5.5.11.6.1\">Baichuan2-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.11.6.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.11.6.3\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.11.6.4\">16.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.11.6.5\">5.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.11.6.6\">42.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.5.5.12.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T17.5.5.12.7.1\">Mistral-7B-Instruct-v0.2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.12.7.2\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T17.5.5.12.7.2.1\">14.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.12.7.3\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.12.7.4\">14.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.12.7.5\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.12.7.6\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T17.5.5.12.7.6.1\">57.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.5.5.13.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T17.5.5.13.8.1\">Qwen-7B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.13.8.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.13.8.3\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.13.8.4\">15.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.13.8.5\">4.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.13.8.6\">37.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.5.5.14.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T17.5.5.14.9.1\">InternLM2-Chat-7B-SFT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T17.5.5.14.9.2\">6.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T17.5.5.14.9.3\">7.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T17.5.5.14.9.4\">14.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T17.5.5.14.9.5\">4.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T17.5.5.14.9.6\">48.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.5.5.15.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T17.5.5.15.10.1\">InternLM2-Chat-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.15.10.2\">11.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.15.10.3\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T17.5.5.15.10.3.1\">7.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.15.10.4\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T17.5.5.15.10.4.1\">28.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.15.10.5\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T17.5.5.15.10.5.1\">6.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.15.10.6\">45.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.5.5.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" colspan=\"6\" id=\"S5.T17.5.5.5.2\" style=\"background-color:#EAE2FE;\">\n<span class=\"ltx_text ltx_font_italic\" id=\"S5.T17.5.5.5.2.1\" style=\"background-color:#EAE2FE;\"> </span>\n</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.5.5.16.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T17.5.5.16.11.1\">Llama2-13B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.16.11.2\">7.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.16.11.3\">6.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.16.11.4\">9.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.16.11.5\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.16.11.6\">46.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.5.5.17.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T17.5.5.17.12.1\">Baichuan2-13B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.17.12.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.17.12.3\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.17.12.4\">20.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.17.12.5\">5.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.17.12.6\">42.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.5.5.18.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T17.5.5.18.13.1\">Mixtral-8x7B-Instruct-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.18.13.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.18.13.3\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T17.5.5.18.13.3.1\">8.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.18.13.4\">18.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.18.13.5\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.18.13.6\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T17.5.5.18.13.6.1\">56.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.5.5.19.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T17.5.5.19.14.1\">Qwen-14B-Chat</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.19.14.2\">7.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.19.14.3\">7.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.19.14.4\">24.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.19.14.5\">5.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.5.5.19.14.6\">43.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.5.5.20.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T17.5.5.20.15.1\">InternLM2-Chat-20B-SFT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T17.5.5.20.15.2\">8.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T17.5.5.20.15.3\">7.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T17.5.5.20.15.4\">15.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T17.5.5.20.15.5\">5.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T17.5.5.20.15.6\">48.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.5.5.21.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S5.T17.5.5.21.16.1\">InternLM2-Chat-20B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T17.5.5.21.16.2\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T17.5.5.21.16.2.1\">21.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T17.5.5.21.16.3\">7.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T17.5.5.21.16.4\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T17.5.5.21.16.4.1\">31.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T17.5.5.21.16.5\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S5.T17.5.5.21.16.5.1\">6.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T17.5.5.21.16.6\">42.2</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 17: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T17.9.1\">Comparison of Models on Alignment Benchmarks</span>. Models are categorized by their parameter size and type, highlighting top performers in each category with <span class=\"ltx_text ltx_font_bold\" id=\"S5.T17.10.2\">bold</span> for overall leaders and <span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T17.11.3\">underline</span> for leaders within their parameter group.</figcaption>\n</figure>",
            "capture": "Table 17: Comparison of Models on Alignment Benchmarks. Models are categorized by their parameter size and type, highlighting top performers in each category with bold for overall leaders and underline for leaders within their parameter group."
        },
        "18": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T18\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T18.1\" style=\"width:346.9pt;height:84.7pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-10.9pt,2.7pt) scale(0.940807642643545,0.940807642643545) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T18.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T18.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S5.T18.1.1.1.1.1\">Models</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T18.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T18.1.1.1.1.2.1\">K</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T18.1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T18.1.1.1.1.3.1\">U</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T18.1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T18.1.1.1.1.4.1\">L</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T18.1.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T18.1.1.1.1.5.1\">M</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T18.1.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T18.1.1.1.1.6.1\">W</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T18.1.1.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T18.1.1.1.1.7.1\">QA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T18.1.1.1.1.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T18.1.1.1.1.8.1\">RP</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T18.1.1.1.1.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T18.1.1.1.1.9.1\">RE</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T18.1.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T18.1.1.2.1.1\">InternLM2-Chat-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T18.1.1.2.1.2\">7.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T18.1.1.2.1.3\">7.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T18.1.1.2.1.4\">6.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T18.1.1.2.1.5\">4.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T18.1.1.2.1.6\">7.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T18.1.1.2.1.7\">8.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T18.1.1.2.1.8\">7.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T18.1.1.2.1.9\">5.22</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T18.1.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T18.1.1.3.2.1\">InternLM2-Chat-7B-SFT</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T18.1.1.3.2.2\">5.83</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T18.1.1.3.2.3\">5.81</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T18.1.1.3.2.4\">5.63</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T18.1.1.3.2.5\">3.82</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T18.1.1.3.2.6\">6.25</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T18.1.1.3.2.7\">6.89</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T18.1.1.3.2.8\">6.46</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T18.1.1.3.2.9\">3.51</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T18.1.1.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T18.1.1.4.3.1\">InternLM2-Chat-20B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T18.1.1.4.3.2\">8.02</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T18.1.1.4.3.3\">7.88</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T18.1.1.4.3.4\">7.65</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T18.1.1.4.3.5\">5.38</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T18.1.1.4.3.6\">7.93</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T18.1.1.4.3.7\">8.26</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T18.1.1.4.3.8\">8.09</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T18.1.1.4.3.9\">5.76</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T18.1.1.5.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S5.T18.1.1.5.4.1\">InternLM2-Chat-20B-SFT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T18.1.1.5.4.2\">6.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T18.1.1.5.4.3\">6.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T18.1.1.5.4.4\">5.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T18.1.1.5.4.5\">4.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T18.1.1.5.4.6\">6.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T18.1.1.5.4.7\">6.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T18.1.1.5.4.8\">6.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T18.1.1.5.4.9\">4.35</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 18: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T18.3.1\">Detail Scores of InternLM2 Series on AlignBench.</span> K: Knowledge, U: Understadning, L: Language, M: Mathematics, W: Writing, RP: Role Play, RE: Reasoning.</figcaption>\n</figure>",
            "capture": "Table 18: Detail Scores of InternLM2 Series on AlignBench. K: Knowledge, U: Understadning, L: Language, M: Mathematics, W: Writing, RP: Role Play, RE: Reasoning."
        },
        "19": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T19\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T19.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T19.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S5.T19.1.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T19.1.1.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.1.1.2.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.1.1.2.1.1\">Anthropic Helpful-base</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T19.1.1.1.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.1.1.3.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.1.1.3.1.1\">Anthropic Helpful-online</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T19.1.1.1.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.1.1.4.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.1.1.4.1.1\">Anthropic Harmless-base</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T19.1.1.1.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.1.1.5.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.1.1.5.1.1\">OpenAI Summ.</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T19.1.1.1.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.1.1.6.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.1.1.6.1.1\">Stanford SHP</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T19.1.1.1.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.1.1.7.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.1.1.7.1.1\">PRM 800k</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T19.1.2.1\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T19.1.2.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.2.1.1.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.2.1.1.1.1\">UltraRM</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T19.1.2.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.2.1.2.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.2.1.2.1.1\">75.02</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T19.1.2.1.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.2.1.3.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.2.1.3.1.1\">65.34</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T19.1.2.1.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.2.1.4.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.2.1.4.1.1\">37.02</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T19.1.2.1.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.2.1.5.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.2.1.5.1.1\">74.48</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T19.1.2.1.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.2.1.6.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.2.1.6.1.1\">74.36</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T19.1.2.1.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.2.1.7.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.2.1.7.1.1\">48.77</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T19.1.3.2\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r\" id=\"S5.T19.1.3.2.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.3.2.1.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.3.2.1.1.1\">QwenRM</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T19.1.3.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.3.2.2.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.3.2.2.1.1\">73.98</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T19.1.3.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.3.2.3.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.3.2.3.1.1\">64.57</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T19.1.3.2.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.3.2.4.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.3.2.4.1.1\">-</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T19.1.3.2.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.3.2.5.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.3.2.5.1.1\">69.99</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T19.1.3.2.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.3.2.6.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.3.2.6.1.1\">60.10</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T19.1.3.2.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.3.2.7.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.3.2.7.1.1\">70.52</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T19.1.4.3\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r\" id=\"S5.T19.1.4.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.4.3.1.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.4.3.1.1.1\">Ours w/o cond.</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T19.1.4.3.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.4.3.2.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.4.3.2.1.1\">73.26</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T19.1.4.3.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.4.3.3.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.4.3.3.1.1\">64.45</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T19.1.4.3.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.4.3.4.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.4.3.4.1.1\">72.48</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T19.1.4.3.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.4.3.5.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.4.3.5.1.1\">68.25</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T19.1.4.3.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.4.3.6.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.4.3.6.1.1\">69.24</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T19.1.4.3.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.4.3.7.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.4.3.7.1.1\">72.11</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T19.1.5.4\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S5.T19.1.5.4.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.5.4.1.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.5.4.1.1.1\">Ours w/ cond.</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T19.1.5.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.5.4.2.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.5.4.2.1.1\">75.10</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T19.1.5.4.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.5.4.3.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.5.4.3.1.1\">66.98</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T19.1.5.4.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.5.4.4.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.5.4.4.1.1\">73.19</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T19.1.5.4.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.5.4.5.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.5.4.5.1.1\">75.37</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T19.1.5.4.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.5.4.6.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.5.4.6.1.1\">75.76</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T19.1.5.4.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.1.5.4.7.1\">\n<span class=\"ltx_p\" id=\"S5.T19.1.5.4.7.1.1\">77.18</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 19: </span>Comparative performance of UltraRM-13B<cite class=\"ltx_cite ltx_citemacro_citep\">(Cui et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.17297v1#bib.bib30\" title=\"\">2023</a>)</cite>, QwenRM<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.17297v1#bib.bib10\" title=\"\">2023a</a>)</cite>, and ours 7B reward model trained with and without conditional system prompts. The table demonstrates a marked improvement in precision when conditional prompts are used.</figcaption>\n</figure>",
            "capture": "Table 19: Comparative performance of UltraRM-13B(Cui et\u00a0al., 2023), QwenRM(Bai et\u00a0al., 2023a), and ours 7B reward model trained with and without conditional system prompts. The table demonstrates a marked improvement in precision when conditional prompts are used."
        },
        "20": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T20\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T20.7\" style=\"width:281.9pt;height:307.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(8.8pt,-9.6pt) scale(1.0667851306864,1.0667851306864) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T20.7.7\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T20.5.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S5.T20.5.5.5.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.5.5.5.6.1\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T20.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T20.2.2.2.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T20.3.3.3.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T20.4.4.4.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T20.5.5.5.5\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.6.6.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"6\" id=\"S5.T20.6.6.6.1\" style=\"background-color:#DFF5E5;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T20.6.6.6.1.1\" style=\"background-color:#DFF5E5;\">B Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.7.7.8.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T20.7.7.8.1.1\">Llama2-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.8.1.2\">1.49</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.8.1.3\">1.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.8.1.4\">1.49</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.8.1.5\">0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.8.1.6\">-0.01</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.7.7.9.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T20.7.7.9.2.1\">Mistral-7B-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.9.2.2\">1.43</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.9.2.3\">1.44</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.9.2.4\">1.41</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.9.2.5\">0.02</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.9.2.6\">-0.01</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.7.7.10.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T20.7.7.10.3.1\">Baichuan2-7B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.10.3.2\">1.47</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.10.3.3\">1.48</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.10.3.4\">1.45</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.10.3.5\">0.02</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.10.3.6\">-0.01</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.7.7.11.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T20.7.7.11.4.1\">Qwen-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.11.4.2\">1.33</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.11.4.3\">0.78</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.11.4.4\">1.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.11.4.5\">0.13</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.11.4.6\" style=\"background-color:#DFDFDF;\"><span class=\"ltx_text\" id=\"S5.T20.7.7.11.4.6.1\" style=\"background-color:#DFDFDF;\">0.55</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.7.7.12.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T20.7.7.12.5.1\">ChatGLM3-6B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.12.5.2\">1.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.12.5.3\">1.16</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.12.5.4\">1.35</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.12.5.5\">-0.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.12.5.6\">0.14</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.7.7.13.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T20.7.7.13.6.1\">InternLM2-7B-Base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T20.7.7.13.6.2\">1.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T20.7.7.13.6.3\">1.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T20.7.7.13.6.4\">1.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T20.7.7.13.6.5\">0.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T20.7.7.13.6.6\">-0.02</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.7.7.14.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T20.7.7.14.7.1\">InternLM2-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.14.7.2\">1.48</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.14.7.3\">1.14</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.14.7.4\">1.46</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.14.7.5\">0.02</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.14.7.6\" style=\"background-color:#DFDFDF;\"><span class=\"ltx_text\" id=\"S5.T20.7.7.14.7.6.1\" style=\"background-color:#DFDFDF;\">0.34</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.7.7.7\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"6\" id=\"S5.T20.7.7.7.1\" style=\"background-color:#EAE2FE;\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T20.7.7.7.1.1\" style=\"background-color:#EAE2FE;\"> B Models</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.7.7.15.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T20.7.7.15.8.1\">Llama2-13B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.15.8.2\">1.42</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.15.8.3\">1.42</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.15.8.4\">1.45</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.15.8.5\">-0.03</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.15.8.6\">0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.7.7.16.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T20.7.7.16.9.1\">Mixtral-8x7B-v0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.16.9.2\">1.34</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.16.9.3\">1.35</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.16.9.4\">1.35</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.16.9.5\">-0.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.16.9.6\">-0.01</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.7.7.17.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T20.7.7.17.10.1\">Baichuan2-13B-Base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.17.10.2\">1.13</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.17.10.3\">0.76</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.17.10.4\">1.19</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.17.10.5\">-0.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.17.10.6\" style=\"background-color:#DFDFDF;\"><span class=\"ltx_text\" id=\"S5.T20.7.7.17.10.6.1\" style=\"background-color:#DFDFDF;\">0.37</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.7.7.18.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T20.7.7.18.11.1\">Qwen-14B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.18.11.2\">1.15</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.18.11.3\">0.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.18.11.4\">1.27</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.18.11.5\">-0.12</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T20.7.7.18.11.6\" style=\"background-color:#DFDFDF;\"><span class=\"ltx_text\" id=\"S5.T20.7.7.18.11.6.1\" style=\"background-color:#DFDFDF;\">0.65</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.7.7.19.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T20.7.7.19.12.1\">InternLM2-20B-Base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T20.7.7.19.12.2\">1.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T20.7.7.19.12.3\">1.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T20.7.7.19.12.4\">1.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T20.7.7.19.12.5\">0.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T20.7.7.19.12.6\">-0.01</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.7.7.20.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S5.T20.7.7.20.13.1\">InternLM2-20B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T20.7.7.20.13.2\">1.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T20.7.7.20.13.3\">1.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T20.7.7.20.13.4\">1.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T20.7.7.20.13.5\">0.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T20.7.7.20.13.6\" style=\"background-color:#DFDFDF;\"><span class=\"ltx_text\" id=\"S5.T20.7.7.20.13.6.1\" style=\"background-color:#DFDFDF;\">0.35</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 20: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.9.1\">Evaluation of Base Models on GSM8K Contamination</span>.</figcaption>\n</figure>",
            "capture": "Table 20: Evaluation of Base Models on GSM8K Contamination."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.17297v1_figure_1.png",
            "caption": "Figure 1: Model FLOPs Utilization (MFU) of training InternLM-7B with InternEvo. We benchmark training performance using a sequence length of 4096 tokens with varying GPU numbers, and benchmark training performance on 128 GPUs with varying sequence lengths."
        },
        "2": {
            "figure_path": "2403.17297v1_figure_2.png",
            "caption": "Figure 2: Different weight matrix layouts cause different complexity when changing the tensor parallelism (TP) size."
        },
        "3": {
            "figure_path": "2403.17297v1_figure_3.png",
            "caption": "Figure 3: Data Process Pipeline"
        },
        "4": {
            "figure_path": "2403.17297v1_figure_4.png",
            "caption": "Figure 4: Statistics of code data in our pre-training corpus."
        },
        "5": {
            "figure_path": "2403.17297v1_figure_5.png",
            "caption": "Figure 5: The pipeline of iterative refinement annotation process for a code quality classifier."
        },
        "6": {
            "figure_path": "2403.17297v1_figure_6.png",
            "caption": "(a)"
        },
        "7": {
            "figure_path": "2403.17297v1_figure_7.png",
            "caption": "(b)"
        },
        "8": {
            "figure_path": "2403.17297v1_figure_8.png",
            "caption": "Figure 7: The distribution of SFT data instances."
        },
        "9": {
            "figure_path": "2403.17297v1_figure_9.png",
            "caption": "Figure 8: Architecture of the Conditional Reward Model. (a) LLaMA2 employs distinct reward models to address the issue of preference conflicts. (b) The proposed conditional reward model (ours) utilizes conditional system prompts to reconcile preference data across various domains, enabling the modeling of multiple preferences using a single reward model."
        },
        "10": {
            "figure_path": "2403.17297v1_figure_10.png",
            "caption": "Figure 9: Ablation study on critic model initialization. Note that we used logarithmic coordinates in critic loss due to its large dynamic range."
        },
        "11": {
            "figure_path": "2403.17297v1_figure_11.png",
            "caption": "Figure 10: Illustration of conditional PPO training. An appropriate conditional system prompt is added to query & response before reward model assesment. Note that this system prompt is agnostic to the other three models."
        },
        "12": {
            "figure_path": "2403.17297v1_figure_12.png",
            "caption": "Figure 11: Illustration of the process to obtain long-context code data."
        },
        "13": {
            "figure_path": "2403.17297v1_figure_13.png",
            "caption": "Figure 12: The performance before and after the capability specific enhancement training phase, the darker areas mean the performance before the training."
        },
        "14": {
            "figure_path": "2403.17297v1_figure_14.png",
            "caption": "Figure 13: Results on Needle in the Haystack(Chinese)."
        },
        "15": {
            "figure_path": "2403.17297v1_figure_15.png",
            "caption": "Figure 14: Results on GSM8K (4-shot) and MATH (4-shot) with and w/o Code Interpreter."
        },
        "16": {
            "figure_path": "2403.17297v1_figure_16.png",
            "caption": "Figure 15: Results on MathBench with and without ReAct."
        },
        "17": {
            "figure_path": "2403.17297v1_figure_17.png",
            "caption": "Figure 16: Detail Results on CompassArena of InternLM2 Series."
        },
        "18": {
            "figure_path": "2403.17297v1_figure_18.png",
            "caption": "Figure 17: Illustraction of streaming format adopted by InternLM2-Chat to conduct function calls, especially JSON format to ease downstream applications."
        }
    },
    "references": [
        {
            "1": {
                "title": "https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/ai-services/openai/includes/chat-markup-language.md.",
                "author": "chat markup language.",
                "venue": "Accessed: 2024-02-06.",
                "url": null
            }
        },
        {
            "2": {
                "title": "https://github.com/ggerganov/llama.cpp, 2023.",
                "author": "llama.cpp: Port of facebook\u2019s llama model in c/c++.",
                "venue": null,
                "url": null
            }
        },
        {
            "3": {
                "title": "GQA: training generalized multi-query transformer models from\nmulti-head checkpoints.",
                "author": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico\nLebr\u00f3n, and Sumit Sanghai.",
                "venue": "In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),\nProceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 4895\u20134901. Association for Computational Linguistics, 2023.",
                "url": null
            }
        },
        {
            "4": {
                "title": "The falcon series of open language models.",
                "author": "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,\nRuxandra Cojocaru, M\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel\nHesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune,\nBaptiste Pannier, and Guilherme Penedo.",
                "venue": "CoRR, abs/2311.16867, 2023.",
                "url": null
            }
        },
        {
            "5": {
                "title": "L-eval: Instituting standardized evaluation for long context language\nmodels.",
                "author": "Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and\nXipeng Qiu.",
                "venue": "CoRR, abs/2307.11088, 2023.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Cibench: Evaluating your llms with a code interpreter plugin.",
                "author": "Anonymous.",
                "venue": "In Openreview, 2024a.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Mathbench: Evaluating the theory and application proficiency of llms\nwith a hierarchical mathematics benchmark.",
                "author": "Anonymous.",
                "venue": "In Openreview, 2024b.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Program synthesis with large language models.",
                "author": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski,\nDavid Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al.",
                "venue": "arXiv preprint arXiv:2108.07732, 2021.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Layer normalization.",
                "author": "Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.",
                "venue": "CoRR, abs/1607.06450, 2016.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Qwen technical report.",
                "author": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,\nWenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji\nLin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,\nXingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,\nShijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,\nJian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan,\nJianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou,\nJingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.",
                "venue": "CoRR, abs/2309.16609, 2023a.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Training a helpful and harmless assistant with reinforcement learning\nfrom human feedback, 2022.",
                "author": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,\nDawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph,\nSaurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage,\nZac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna\nKravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown,\nJack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.",
                "venue": null,
                "url": null
            }
        },
        {
            "12": {
                "title": "Longbench: A bilingual, multitask benchmark for long context\nunderstanding.",
                "author": "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang,\nZhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and\nJuanzi Li.",
                "venue": "CoRR, abs/2308.14508, 2023b.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Trafilatura: A web scraping library and command-line tool for text\ndiscovery and extraction.",
                "author": "Adrien Barbaresi.",
                "venue": "In Heng Ji, Jong C. Park, and Rui Xia (eds.), Proceedings of\nthe Joint Conference of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on\nNatural Language Processing, ACL 2021 - System Demonstrations, Online,\nAugust 1-6, 2021, pp.  122\u2013131. Association for Computational Linguistics,\n2021.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Deepseek LLM: scaling open-source language models with longtermism.",
                "author": "Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng,\nHonghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao,\nRuiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying\nHe, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K.\nLi, Wenfeng Liang, Fangyun Lin, Alex X. Liu, Bo Liu, Wen Liu, Xiaodong Liu,\nXin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao\nNie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren,\nChong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang\nSun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui\nWang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang\nXiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping\nYu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan\nZhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao,\nShangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou.",
                "venue": "CoRR, abs/2401.02954, 2024.",
                "url": null
            }
        },
        {
            "15": {
                "title": "PIQA: reasoning about physical commonsense in natural language.",
                "author": "Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi.",
                "venue": "In The Thirty-Fourth AAAI Conference on Artificial\nIntelligence, AAAI 2020, The Thirty-Second Innovative Applications of\nArtificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium\non Educational Advances in Artificial Intelligence, EAAI 2020, New York,\nNY, USA, February 7-12, 2020, pp.  7432\u20137439. AAAI Press, 2020.",
                "url": null
            }
        },
        {
            "16": {
                "title": "On the resemblance and containment of documents.",
                "author": "Andrei Z. Broder.",
                "venue": "In Bruno Carpentieri, Alfredo De Santis, Ugo Vaccaro, and James A.\nStorer (eds.), Compression and Complexity of SEQUENCES 1997,\nPositano, Amalfitan Coast, Salerno, Italy, June 11-13, 1997, Proceedings,\npp.  21\u201329. IEEE, 1997.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Language models are few-shot learners.",
                "author": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei.",
                "venue": "In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell,\nMaria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural\nInformation Processing Systems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Learning to rank using gradient descent.",
                "author": "Christopher J. C. Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds,\nNicole Hamilton, and Gregory N. Hullender.",
                "venue": "In Luc De Raedt and Stefan Wrobel (eds.), Machine Learning,\nProceedings of the Twenty-Second International Conference (ICML 2005),\nBonn, Germany, August 7-11, 2005, volume 119 of ACM International\nConference Proceeding Series, pp.  89\u201396. ACM, 2005.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Evaluating large language models trained on code.",
                "author": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira\nPinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al.",
                "venue": "arXiv preprint arXiv:2107.03374, 2021.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Internevo: Efficient long-sequence large language model training via\nhybrid parallelism and redundant sharding.",
                "author": "Qiaoling Chen, Diandian Gu, Guoteng Wang, Xun Chen, YingTong Xiong, Ting Huang,\nQinghao Hu, Xin Jin, Yonggang Wen, Tianwei Zhang, et al.",
                "venue": "arXiv preprint arXiv:2401.09149, 2024a.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Amsp: Reducing communication overhead of zero for efficient llm\ntraining, 2024b.",
                "author": "Qiaoling Chen, Qinghao Hu, Guoteng Wang, Yingtong Xiong, Ting Huang, Xun Chen,\nYang Gao, Hang Yan, Yonggang Wen, Tianwei Zhang, and Peng Sun.",
                "venue": null,
                "url": null
            }
        },
        {
            "22": {
                "title": "Theoremqa: A theorem-driven question answering dataset.",
                "author": "Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi\nWang, and Tony Xia.",
                "venue": "In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),\nProceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 7889\u20137901. Association for Computational Linguistics, 2023a.",
                "url": null
            }
        },
        {
            "23": {
                "title": "T-eval: Evaluating the tool utilization capability step by step.",
                "author": "Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu, Miao Zheng,\nJingming Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, et al.",
                "venue": "arXiv preprint arXiv:2312.14033, 2023b.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Agent-flan: Designing data and methods of effective agent tuning for\nlarge language models.",
                "author": "Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Dahua Lin, Kai Chen, and\nFeng Zhao.",
                "venue": "work in progress, 2024c.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Palm: Scaling language modeling with pathways.",
                "author": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,\nAbhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,\nEmily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm\nLevskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David\nLuan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David\nDohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai,\nThanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica\nMoreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi\nWang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,\nKathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah\nFiedel.",
                "venue": "J. Mach. Learn. Res., 24:240:1\u2013240:113, 2023.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Deep reinforcement learning from human preferences.",
                "author": "Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and\nDario Amodei.",
                "venue": "In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach,\nRob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances\nin Neural Information Processing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9, 2017, Long Beach, CA,\nUSA, pp.  4299\u20134307, 2017.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Training verifiers to solve math word problems.",
                "author": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz\nKaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,\nChristopher Hesse, and John Schulman.",
                "venue": "CoRR, abs/2110.14168, 2021.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Lmdeploy: A toolkit for compressing, deploying, and serving llm.",
                "author": "LMDeploy Contributors.",
                "venue": "https://github.com/InternLM/lmdeploy, 2023a.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Opencompass: A universal evaluation platform for foundation models.",
                "author": "OpenCompass Contributors.",
                "venue": "https://github.com/open-compass/opencompass,\n2023b.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Ultrafeedback: Boosting language models with high-quality feedback,\n2023.",
                "author": "Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie,\nZhiyuan Liu, and Maosong Sun.",
                "venue": null,
                "url": null
            }
        },
        {
            "31": {
                "title": "Safe rlhf: Safe reinforcement learning from human feedback, 2023.",
                "author": "Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou\nWang, and Yaodong Yang.",
                "venue": null,
                "url": null
            }
        },
        {
            "32": {
                "title": "Flashattention-2: Faster attention with better parallelism and work\npartitioning.",
                "author": "Tri Dao.",
                "venue": "CoRR, abs/2307.08691, 2023.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Documenting large webtext corpora: A case study on the colossal\nclean crawled corpus.",
                "author": "Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco, Dirk\nGroeneveld, Margaret Mitchell, and Matt Gardner.",
                "venue": "In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and\nScott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event\n/ Punta Cana, Dominican Republic, 7-11 November, 2021, pp.  1286\u20131305.\nAssociation for Computational Linguistics, 2021.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Understanding dataset difficulty with -usable\ninformation.",
                "author": "Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta.",
                "venue": "In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari,\nGang Niu, and Sivan Sabato (eds.), Proceedings of the 39th\nInternational Conference on Machine Learning, volume 162 of\nProceedings of Machine Learning Research, pp.  5988\u20136008. PMLR,\n17\u201323 Jul 2022.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Scaling laws for reward model overoptimization.",
                "author": "Leo Gao, John Schulman, and Jacob Hilton.",
                "venue": "Oct 2022.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Retrieval-augmented generation for large language models: A survey.",
                "author": "Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai,\nJiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang.",
                "venue": "CoRR, abs/2312.10997, 2023.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Olmo: Accelerating the science of language models.",
                "author": "Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind\nTafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane\nArora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan,\nJennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William\nMerrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam,\nMatthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk,\nSaurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell\nWortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer,\nJesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi.",
                "venue": "CoRR, abs/2402.00838, 2024.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Deepseek-coder: When the large language model meets programming - the\nrise of code intelligence.",
                "author": "Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting\nChen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang.",
                "venue": "CoRR, abs/2401.14196, 2024.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Measuring massive multitask language understanding.",
                "author": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn\nSong, and Jacob Steinhardt.",
                "venue": "arXiv preprint arXiv:2009.03300, 2020.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Measuring mathematical problem solving with the MATH dataset.",
                "author": "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric\nTang, Dawn Song, and Jacob Steinhardt.",
                "venue": "In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings\nof the Neural Information Processing Systems Track on Datasets and Benchmarks\n1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Characterization of large language model development in the\ndatacenter.",
                "author": "Qinghao Hu, Zhisheng Ye, Zerui Wang, Guoteng Wang, Meng Zhang, Qiaoling Chen,\nPeng Sun, Dahua Lin, Xiaolin Wang, Yingwei Luo, et al.",
                "venue": "In USENIX Symposium on Networked Systems Design and\nImplementation (NSDI\u201924), 2024.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Gpipe: Efficient training of giant neural networks using pipeline\nparallelism.",
                "author": "Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen,\nHyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al.",
                "venue": "Advances in neural information processing systems, 32, 2019.",
                "url": null
            }
        },
        {
            "43": {
                "title": "C-eval: A multi-level multi-discipline chinese evaluation suite for\nfoundation models.",
                "author": "Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su,\nJunteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and\nJunxian He.",
                "venue": "arXiv preprint arXiv:2305.08322, 2023.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Mistral 7b.",
                "author": "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\nDevendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne\nLachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang,\nTimoth\u00e9e Lacroix, and William El Sayed.",
                "venue": "CoRR, abs/2310.06825, 2023.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Triviaqa: A large scale distantly supervised challenge dataset for\nreading comprehension.",
                "author": "Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.",
                "venue": "arXiv preprint arXiv:1705.03551, 2017.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Critiquellm: Scaling llm-as-critic for effective and explainable\nevaluation of large language model generation.",
                "author": "Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan\nWang, Aohan Zeng, Yuxiao Dong, Hongning Wang, et al.",
                "venue": "arXiv preprint arXiv:2311.18702, 2023.",
                "url": null
            }
        },
        {
            "47": {
                "title": "Reducing activation recomputation in large transformer models.",
                "author": "Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael\nAndersch, Mohammad Shoeybi, and Bryan Catanzaro.",
                "venue": "Proceedings of Machine Learning and Systems, 5, 2023.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Natural questions: a benchmark for question answering research.",
                "author": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey,\nJacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang,\nAndrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.",
                "venue": "Transactions of the Association of Computational Linguistics,\n2019.",
                "url": null
            }
        },
        {
            "49": {
                "title": "Summedits: Measuring LLM ability at factual reasoning through the\nlens of summarization.",
                "author": "Philippe Laban, Wojciech Kryscinski, Divyansh Agarwal, Alexander R. Fabbri,\nCaiming Xiong, Shafiq Joty, and Chien-Sheng Wu.",
                "venue": "In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),\nProceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 9662\u20139676. Association for Computational Linguistics, 2023.",
                "url": null
            }
        },
        {
            "50": {
                "title": "RACE: Large-scale ReAding comprehension dataset from\nexaminations.",
                "author": "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy.",
                "venue": "In Proceedings of the 2017 Conference on Empirical Methods in\nNatural Language Processing, pp.  785\u2013794, Copenhagen, Denmark, September\n2017. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "51": {
                "title": "Ds-1000: A natural and reliable benchmark for data science code\ngeneration.",
                "author": "Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke\nZettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu.",
                "venue": "In International Conference on Machine Learning, pp. 18319\u201318345. PMLR, 2023.",
                "url": null
            }
        },
        {
            "52": {
                "title": "Cmmlu: Measuring massive multitask language understanding in chinese,\n2023a.",
                "author": "Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan\nDuan, and Timothy Baldwin.",
                "venue": null,
                "url": null
            }
        },
        {
            "53": {
                "title": "Alpacaeval: An automatic evaluator of instruction-following models.",
                "author": "Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos\nGuestrin, Percy Liang, and Tatsunori B. Hashimoto.",
                "venue": "https://github.com/tatsu-lab/alpaca_eval, 2023b.",
                "url": null
            }
        },
        {
            "54": {
                "title": "Let\u2019s verify step by step.",
                "author": "Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy\nLee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.",
                "venue": "arXiv preprint arXiv:2305.20050, 2023.",
                "url": null
            }
        },
        {
            "55": {
                "title": "Focal loss for dense object detection.",
                "author": "Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr\nDoll\u00e1r.",
                "venue": "In IEEE International Conference on Computer Vision, ICCV\n2017, Venice, Italy, October 22-29, 2017, pp.  2999\u20133007. IEEE Computer\nSociety, 2017.",
                "url": null
            }
        },
        {
            "56": {
                "title": "Alignbench: Benchmarking chinese alignment of large language models.",
                "author": "Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale\nCheng, Pei Ke, Yifan Xu, Weng Lam Tam, et al.",
                "venue": "arXiv preprint arXiv:2311.18743, 2023a.",
                "url": null
            }
        },
        {
            "57": {
                "title": "Scaling laws of rope-based extrapolation.",
                "author": "Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, and Dahua Lin.",
                "venue": "CoRR, abs/2310.05209, 2023b.",
                "url": null
            }
        },
        {
            "58": {
                "title": "Dynamically scaled rope further increases performance of long context\nllama with zero fine-tuning, July 2023.",
                "author": "LocalLLaMA.",
                "venue": "URL\nhttps://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/.",
                "url": null
            }
        },
        {
            "59": {
                "title": "Decoupled weight decay regularization.",
                "author": "Ilya Loshchilov and Frank Hutter.",
                "venue": "In 7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.",
                "url": null
            }
        },
        {
            "60": {
                "title": "Longwanjuan: Towards systematic measurement for long text quality,\n2024.",
                "author": "Kai Lv, Xiaoran Liu, Qipeng Guo, Hang Yan, Conghui He, Xipeng Qiu, and Dahua\nLin.",
                "venue": null,
                "url": null
            }
        },
        {
            "61": {
                "title": "Categorizing variants of goodhart\u2019s law.",
                "author": "David Manheim and Scott Garrabrant.",
                "venue": "arXiv: Artificial Intelligence,arXiv: Artificial Intelligence,\nMar 2018.",
                "url": null
            }
        },
        {
            "62": {
                "title": "Can a suit of armor conduct electricity? A new dataset for open\nbook question answering.",
                "author": "Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.",
                "venue": "In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii\n(eds.), Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, Brussels, Belgium, October 31 - November 4,\n2018, pp.  2381\u20132391. Association for Computational Linguistics, 2018.",
                "url": null
            }
        },
        {
            "63": {
                "title": "Mixed precision training.",
                "author": "Sharan Narang, Gregory Diamos, Erich Elsen, Paulius Micikevicius, Jonah Alben,\nDavid Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh\nVenkatesh, et al.",
                "venue": "In Int. Conf. on Learning Representation, 2017.",
                "url": null
            }
        },
        {
            "64": {
                "title": "GPT-4 technical report.",
                "author": "OpenAI.",
                "venue": "CoRR, abs/2303.08774, 2023.",
                "url": null
            }
        },
        {
            "65": {
                "title": "Training language models to follow instructions with human feedback.",
                "author": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.",
                "venue": "In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho,\nand A. Oh (eds.), Advances in Neural Information Processing Systems 35:\nAnnual Conference on Neural Information Processing Systems 2022, NeurIPS\n2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.",
                "url": null
            }
        },
        {
            "66": {
                "title": "Nemotron-4 15b technical report.",
                "author": "Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep\nSubramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush\nDattagupta, et al.",
                "venue": "arXiv preprint arXiv:2402.16819, 2024.",
                "url": null
            }
        },
        {
            "67": {
                "title": "The refinedweb dataset for falcon LLM: outperforming curated\ncorpora with web data, and web data only.",
                "author": "Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,\nAlessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay.",
                "venue": "CoRR, abs/2306.01116, 2023.",
                "url": null
            }
        },
        {
            "68": {
                "title": "Tool learning with foundation models.",
                "author": "Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni\nZeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong\nWang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai\nXu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu,\nZhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi\nYan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng\nJi, Zhiyuan Liu, and Maosong Sun.",
                "venue": "CoRR, abs/2304.08354, 2023a.",
                "url": null
            }
        },
        {
            "69": {
                "title": "Toolllm: Facilitating large language models to master 16000+\nreal-world apis.",
                "author": "Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin,\nXin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie\nZhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun.",
                "venue": "CoRR, abs/2307.16789, 2023b.",
                "url": null
            }
        },
        {
            "70": {
                "title": "Scaling language models: Methods, analysis & insights from\ntraining gopher.",
                "author": "Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,\nH. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,\nEliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,\nGeorge van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang,\nAmelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan\nUesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,\nErich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme\nSutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens,\nXiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,\nDomenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,\nMaria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas\nPajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien\nde Masson d\u2019Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor\nBabuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James\nBradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger, Iason\nGabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris\nDyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis\nHassabis, Koray Kavukcuoglu, and Geoffrey Irving.",
                "venue": "CoRR, abs/2112.11446, 2021.",
                "url": null
            }
        },
        {
            "71": {
                "title": "Zero: Memory optimizations toward training trillion parameter models.",
                "author": "Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.",
                "venue": "In SC20: International Conference for High Performance\nComputing, Networking, Storage and Analysis, pp.  1\u201316. IEEE, 2020.",
                "url": null
            }
        },
        {
            "72": {
                "title": "Deepspeed: System optimizations enable training deep learning models\nwith over 100 billion parameters.",
                "author": "Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.",
                "venue": "In Proceedings of the 26th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining, pp.  3505\u20133506, 2020.",
                "url": null
            }
        },
        {
            "73": {
                "title": "Code llama: Open foundation models for code.",
                "author": "Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat,\nXiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my\nRapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt,\nCristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre\nD\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas\nUsunier, Thomas Scialom, and Gabriel Synnaeve.",
                "venue": "CoRR, abs/2308.12950, 2023.",
                "url": null
            }
        },
        {
            "74": {
                "title": "How to train data-efficient llms.",
                "author": "Noveen Sachdeva, Benjamin Coleman, Wang-Cheng Kang, Jianmo Ni, Lichan Hong,\nEd H. Chi, James Caverlee, Julian J. McAuley, and Derek Zhiyuan Cheng.",
                "venue": "CoRR, abs/2402.09668, 2024.",
                "url": null
            }
        },
        {
            "75": {
                "title": "Winogrande: An adversarial winograd schema challenge at scale.",
                "author": "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.",
                "venue": "In The Thirty-Fourth AAAI Conference on Artificial\nIntelligence, AAAI 2020, The Thirty-Second Innovative Applications of\nArtificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium\non Educational Advances in Artificial Intelligence, EAAI 2020, New York,\nNY, USA, February 7-12, 2020, pp.  8732\u20138740. AAAI Press, 2020.",
                "url": null
            }
        },
        {
            "76": {
                "title": "Toolformer: Language models can teach themselves to use tools.",
                "author": "Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria\nLomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.",
                "venue": "CoRR, abs/2302.04761, 2023.",
                "url": null
            }
        },
        {
            "77": {
                "title": "Proximal policy optimization algorithms.",
                "author": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.",
                "venue": "CoRR, abs/1707.06347, 2017.",
                "url": null
            }
        },
        {
            "78": {
                "title": "GLU variants improve transformer.",
                "author": "Noam Shazeer.",
                "venue": "CoRR, abs/2002.05202, 2020.",
                "url": null
            }
        },
        {
            "79": {
                "title": "Megatron-lm: Training multi-billion parameter language models using\nmodel parallelism.",
                "author": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,\nand Bryan Catanzaro.",
                "venue": "arXiv preprint arXiv:1909.08053, 2019.",
                "url": null
            }
        },
        {
            "80": {
                "title": "Learning to summarize from human feedback.",
                "author": "Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea\nVoss, Alec Radford, Dario Amodei, and Paul Christiano.",
                "venue": "In NeurIPS, 2020.",
                "url": null
            }
        },
        {
            "81": {
                "title": "Investigating prior knowledge for challenging chinese machine reading\ncomprehension.",
                "author": "Kai Sun, Dian Yu, Dong Yu, and Claire Cardie.",
                "venue": "Transactions of the Association for Computational Linguistics,\n2020.",
                "url": null
            }
        },
        {
            "82": {
                "title": "Challenging big-bench tasks and whether chain-of-thought can solve\nthem.",
                "author": "Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann,\nYi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny\nZhou, and Jason Wei.",
                "venue": "In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.),\nFindings of the Association for Computational Linguistics: ACL 2023,\nToronto, Canada, July 9-14, 2023, pp.  13003\u201313051. Association for\nComputational Linguistics, 2023.",
                "url": null
            }
        },
        {
            "83": {
                "title": "No language left behind: Scaling human-centered machine translation.",
                "author": "NLLB Team, Marta R. Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad,\nKenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht,\nJean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi\nAkula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John\nHoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit,\nChau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov,\nAngela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm\u00e1n, Philipp Koehn,\nAlexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and\nJeff Wang.",
                "venue": "2022.",
                "url": null
            }
        },
        {
            "84": {
                "title": "Llama: Open and efficient foundation language models.",
                "author": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric\nHambro, Faisal Azhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard Grave,\nand Guillaume Lample.",
                "venue": "CoRR, abs/2302.13971, 2023a.",
                "url": null
            }
        },
        {
            "85": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine\nBabaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,\nDan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem\nCucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar\nHosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier\nMartinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\nSilva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\nRoss Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan\nZarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,\nAur\u00e9lien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.",
                "venue": "CoRR, abs/2307.09288, 2023b.",
                "url": null
            }
        },
        {
            "86": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.",
                "venue": "In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach,\nRob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances\nin Neural Information Processing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9, 2017, Long Beach, CA,\nUSA, pp.  5998\u20136008, 2017.",
                "url": null
            }
        },
        {
            "87": {
                "title": "Skywork: A more open bilingual foundation model, 2023.",
                "author": "Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye\nLi, Cheng Cheng, Weiwei L\u00fc, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie\nWu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei\nLin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi\nPeng, Xiaojuan Liang, Shuicheng Yan, Han Fang, and Yahui Zhou.",
                "venue": null,
                "url": null
            }
        },
        {
            "88": {
                "title": "Qurating: Selecting high-quality data for training language models.",
                "author": "Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen.",
                "venue": "CoRR, abs/2402.09739, 2024.",
                "url": null
            }
        },
        {
            "89": {
                "title": "Fine-grained human feedback gives better rewards for language model\ntraining.",
                "author": "Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj\nAmmanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi.",
                "venue": "arXiv preprint arXiv:2306.01693, 2023.",
                "url": null
            }
        },
        {
            "90": {
                "title": "The rise and potential of large language model based agents: A\nsurvey.",
                "author": "Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming\nZhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang,\nLimao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang\nLiu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang,\nWenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui.",
                "venue": "CoRR, abs/2309.07864, 2023.",
                "url": null
            }
        },
        {
            "91": {
                "title": "Effective long-context scaling of foundation models.",
                "author": "Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui\nHou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz,\nMadian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela\nFan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma.",
                "venue": "CoRR, abs/2309.16039, 2023.",
                "url": null
            }
        },
        {
            "92": {
                "title": "Baichuan 2: Open large-scale language models.",
                "author": "Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu\nLv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu,\nGuangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang,\nHui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song,\nLifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan\nNie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng,\nWeipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu,\nXuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao,\nYupeng Zhang, Zenan Zhou, and Zhiying Wu.",
                "venue": "CoRR, abs/2309.10305, 2023.",
                "url": null
            }
        },
        {
            "93": {
                "title": "React: Synergizing reasoning and acting in language models.",
                "author": "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R.\nNarasimhan, and Yuan Cao.",
                "venue": "In The Eleventh International Conference on Learning\nRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net,\n2023.",
                "url": null
            }
        },
        {
            "94": {
                "title": "Internlm-math: Open math large language models toward verifiable\nreasoning, 2024.",
                "author": "Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei,\nYichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu,\nShuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang\nYan, Xipeng Qiu, Jiayu Wang, Kai Chen, and Dahua Lin.",
                "venue": null,
                "url": null
            }
        },
        {
            "95": {
                "title": "Hellaswag: Can a machine really finish your sentence?",
                "author": "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.",
                "venue": "In Anna Korhonen, David R. Traum, and Llu\u00eds M\u00e0rquez\n(eds.), Proceedings of the 57th Conference of the Association for\nComputational Linguistics, ACL 2019, Florence, Italy, July 28- August 2,\n2019, Volume 1: Long Papers, pp.  4791\u20134800. Association for Computational\nLinguistics, 2019.",
                "url": null
            }
        },
        {
            "96": {
                "title": "GLM-130B: an open bilingual pre-trained model.",
                "author": "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi\nYang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue,\nJidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie\nTang.",
                "venue": "In The Eleventh International Conference on Learning\nRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net,\n2023.",
                "url": null
            }
        },
        {
            "97": {
                "title": "Root mean square layer normalization.",
                "author": "Biao Zhang and Rico Sennrich.",
                "venue": "In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence\nd\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances\nin Neural Information Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, pp.  12360\u201312371, 2019.",
                "url": null
            }
        },
        {
            "98": {
                "title": "Evaluating the performance of large language models on gaokao\nbenchmark.",
                "author": "Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu.",
                "venue": "2023.",
                "url": null
            }
        },
        {
            "99": {
                "title": "Mics: Near-linear scaling for training gigantic model on public.",
                "author": "Zhen Zhang, Shuai Zheng, Yida Wang, Justin Chiu, George Karypis, Trishul\nChilimbi, Mu Li, and Xin Jin.",
                "venue": "Proceedings of the VLDB Endowment, 16(1):37\u201350, 2022.",
                "url": null
            }
        },
        {
            "100": {
                "title": "Judging llm-as-a-judge with mt-bench and chatbot arena,\n2023a.",
                "author": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E.\nGonzalez, and Ion Stoica.",
                "venue": null,
                "url": null
            }
        },
        {
            "101": {
                "title": "Codegeex: A pre-trained model for code generation with multilingual\nevaluations on humaneval-x.",
                "author": "Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang,\nLei Shen, Andi Wang, Yang Li, et al.",
                "venue": "arXiv preprint arXiv:2303.17568, 2023b.",
                "url": null
            }
        },
        {
            "102": {
                "title": "Agieval: A human-centric benchmark for evaluating foundation models,\n2023.",
                "author": "Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin\nSaied, Weizhu Chen, and Nan Duan.",
                "venue": null,
                "url": null
            }
        },
        {
            "103": {
                "title": "Instruction-following evaluation for large language models.",
                "author": "Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu,\nYi Luan, Denny Zhou, and Le Hou.",
                "venue": "arXiv preprint arXiv:2311.07911, 2023.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.17297v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "1",
            "3",
            "4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5.2",
            "5.2.1",
            "5.2.3",
            "5.2.4",
            "5.2.5",
            "5.2.6",
            "5.2.7"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.3.3",
            "4.2.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.17297v1",
        "paper_title": "InternLM2 Technical Report",
        "research_background": "### Motivation\nThe primary motivation for this paper arises from the significant advancements in Large Language Models (LLMs) following the introduction of popular models such as ChatGPT and GPT-4. These models have demonstrated substantial empathy and problem-solving capabilities, prompting discussions about the approaching era of Artificial General Intelligence (AGI). Despite this excitement, replicating the capabilities of these proprietary models remains challenging. The open-source community has been actively engaging to close the gap between proprietary LLMs and their open-source counterparts. This paper introduces InternLM2, an open-source LLM aimed at outperforming existing models and advancing the capabilities of open-source LLMs.\n\n### Research Problem\nThe research problem addressed in this paper is threefold:\n1. **Bridging the Capability Gap**: Developing an open-source LLM, InternLM2, that can rival or surpass existing proprietary and notable open-source models in terms of performance.\n2. **Efficient Long-Context Handling**: Effectively extending the context length of LLMs to support applications requiring long-context capabilities.\n3. **Optimizing Training Techniques**: Enhancing the model's alignment with human instructions and values through innovative supervised fine-tuning and reinforcement learning methodologies, specifically addressing challenges in long-context processing and alignment with human feedback.\n\n### Relevant Prior Work\nThe paper builds on substantial prior work in the field of LLMs, particularly focusing on the following contributions:\n1. **Popular LLMs**: The paper references influential models such as ChatGPT and GPT-4 by OpenAI (2023), which have set new standards in LLM capabilities.\n2. **Open-Source LLMs**:\n    - **LLaMA** by Touvron et al. (2023a, b): Made significant advances in the open-source LLM domain.\n    - **Qwen** by Bai et al. (2023a): Another notable open-source LLM making considerable strides.\n    - **Mistral** by Jiang et al. (2023): Contributed to the development of effective LLMs.\n    - **Deepseek** by Bi et al. (2024): Added valuable knowledge to the community.\n3. **Training Techniques**: Discusses the major phases of LLM development, including pre-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF).\n    - **RLHF** detailed by Ouyang et al. (2022): Highlights the importance of aligning LLMs with human values and preferences.\n4. **Context Length in LLMs**:\n    - **Retrieval-Augmented Generation (RAG)** (Gao et al., 2023): Emphasized the importance of long-context handling.\n    - **Agents** (Xi et al., 2023): Underlined the necessity of extending context length for various applications.\n\nThis paper also introduces certain methodologies and architectural innovations such as Group Query Attention (GQA) and Conditional Online RLHF (COOL RLHF) to improve memory efficiency and reconcile diverse preferences during RLHF, respectively. These innovations are built upon the foundational work of earlier models and training techniques.",
        "methodology": "The methodology section of the InternLM2 Technical Report illustrates a comprehensive and innovative approach to developing their Large Language Model (LLM), InternLM2. The model aims to bridge the performance gap between open-source LLMs and proprietary versions like OpenAI's ChatGPT and GPT-4. Below, I summarize the key components and innovations of the proposed method or model as detailed in the report:\n\n### Development Phases\nInternLM2\u2019s development involves several key phases:\n1. **Pre-training**\n   - Utilizing a vast corpus of natural text, code, and long-context data consisting of trillions of tokens to build a strong foundational knowledge base.\n   - Emphasis on data preparation, which is crucial for quality outcomes, covering extensive details on processing text, code, and long-context data.\n   \n2. **Supervised Fine-Tuning (SFT)**\n   - Enhances the model\u2019s ability to adhere to human instructions by fine-tuning it on curated datasets.\n   \n3. **Reinforcement Learning from Human Feedback (RLHF)**\n   - Further aligns the model with human values using feedback mechanisms.\n\n### Long-Context Processing\nInternLM2 focuses on improving long-context capabilities essential for applications like Retrieval-Augmented Generation (RAG) and AI agents:\n- **Group Query Attention (GQA)**\n  - This mechanism reduces memory usage during the inference of long sequences.\n- **Incremental Context Training**\n  - Initial pre-training on 4k context texts, switching to 32k context texts for in-depth training.\n  - Using positional encoding extrapolation techniques to reach effective performance on tests involving up to 200k contexts.\n\n### Conditional Online RLHF (COOL RLHF)\nAn innovation in RLHF to fine-tune InternLM2 under conditional contexts:\n- **Conditional Reward Model**\n  - Harmonizes varied, potentially conflicting preferences.\n- **Proximal Policy Optimization (PPO)**\n  - Applied iteratively to mitigate reward hacking across phases.\n\n### Models and Contributions\n1. **Open-Sourced Models**\n   - InternLM2 models released in various sizes (1.8B, 7B, 20B parameters) to the community for both subjective and objective evaluation.\n   - Versions available from different stages such as pre-RLHF (InternLM2-Chat-{size}-SFT) and post-RLHF (InternLM2-Chat-{size}).\n\n2. **200k Context Window Efficiency**\n   - Demonstrates superior long-context performance by almost flawlessly identifying key elements in the \"Needle-in-a-Haystack\" task.\n\n3. **Data Preparation Guidance**\n   - Detailed instructions on preparing pre-training, domain-specific enhancement, SFT, and RLHF data to aid the community in LLM training.\n\n4. **Innovative RLHF Techniques**\n   - Implementing COOL RLHF to enhance dialogue performance, with in-depth analyses comparing subjective and objective results of RLHF.\n\nIn summary, the methodology details a systematic and innovative approach to developing InternLM2, from extensive pre-training and fine-tuning to advanced RLHF techniques. These innovations are designed to improve both the model's ability to handle extended contexts and align it more closely with human expectations, significantly contributing to the open-source AI community.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Experiment Setup:**\n1. **Datasets:**\n   - **Instruction Data:** Used during Supervised Fine-Tuning (SFT) to train the models on following diverse human instructions.\n   - **Extensive Dataset for Conditional Reward Model Training:** Covers dialogue, article writing, poetry, summarization, coding, mathematics, and formatted output. This dataset includes up to 2.4 million binarized preference pairs.\n   - **Pre-training Data for Long-context Capability:** Utilized during SFT and RLHF to maintain long-context capabilities of LLMs.\n\n2. **Baseline Models:**\n   - **Supervised Fine-Tuning (SFT) Models:** These models are fine-tuned to follow human instructions using high-quality instruction data.\n   - **InstructGPT (Ouyang et al., 2022):** Methods referenced for initializing the models and defining aspects of PPO training.\n\n3. **Evaluation Metrics:**\n   - **Reward Scores During PPO:** Evaluated by a novel conditional reward model to ensure alignment with multiple human preferences like accuracy, helpfulness, and harmlessness.\n   - **KL Divergence Coefficient:** To prevent catastrophic forgetting during PPO.\n   - **Pre-train Loss Coefficient:** To ensure retention of knowledge acquired during initial training stages.\n   - **Higher Rewards Consistency and Stability:** By focusing on improvements in the reward model\u2019s reliability and consistency.\n\n**Main Experimental Results:**\n1. **Alignment Process:**\n   - **SFT and RLHF Improved Capability:** The methods leveraged for fine-tuning include adding conditional system prompts and using long-context pre-training data, which were successful in maintaining the model's foundational abilities while adapting to new feedback.\n   - **Conditional Reward Model:** Successfully incorporated a difficulty decay coefficient and a logarithmic barrier penalty to enhance robustness, particularly in datasets imbalanced between easy and difficult samples. The focal ranking loss and new reward constraints improved the model's reliability.\n\n2. **Online RLHF:**\n   - **Three Rounds of Online RLHF:** Engaged in a three-round online RLHF to identify and rectify reward hacking incidents and improve the model's capabilities. Fast-path and slow-path methodologies were combined for swift and comprehensive refinement.\n   - **Consistency in Rewards:** The critic model initialization from the reward model led to a more stable value estimation in early training, resulting in higher overall rewards for the actor model during PPO training.\n\n3. **Enhancements:**\n   - **Tool Utilization Capability:** The introduction of roles like \"environment\" and specific keywords allowed for a unified streaming format compatible with general chat and various plugin extensions.\n   - **Code Interpretation for Mathematics:** Leveraging a special tool mode for a Python code interpreter enhanced the model's ability to solve math problems effectively.\n\n**Overall Performance and Stability:**\n- The careful combination of various methodologies and targeted refinements has not only ensured enhanced performance across multiple domains but also maintained training stability. The systematic approach to fine-tuning, reward modeling, and leveraging feedback led to higher rewards and better alignment with human expectations."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To enhance the reasoning, mathematical problem-solving, and knowledge memorizing capabilities of the InternLM2 model by incorporating high-quality capability-related data during pre-training.",
            "experiment_process": "An enriched dataset consisting of high-quality retrieved data and various types of open-source data from the Hugging Face platform was collected, totaling 24 billion tokens. Specific steps included filtering out test set-related data and running a contamination test. The model was trained with a smaller learning rate and batch size to better fit the data. The performance of the InternLM2 model was evaluated pre- and post-enhancement training, and checkpoints were named InternLM2-{size}-Base and InternLM2-{size}, respectively.",
            "result_discussion": "InternLM2 exhibited substantial performance improvements in coding, reasoning, question answering, and examinations after the enhancement training phase. This suggests that the targeted dataset and tailored training approach effectively enhanced the model's key capabilities.",
            "ablation_id": "2403.17297v1.No1"
        },
        {
            "research_objective": "To investigate the initialization strategy for the critic model in PPO training by comparing initialization from the reward model versus the SFT model and optimize the PPO training process for improved stability and performance.",
            "experiment_process": "The RL alignment phase used the PPO algorithm adapted for stability. Four models were involved: actor, critic, reference, and reward, where only the actor and critic models were actively trained. The critic model was pre-trained for 50 iterations with the actor model frozen. An ablation study compared the initialization of the critic model from the reward model vs. the SFT model, tracking losses and rewards during PPO training. Additional strategies included conditioning responses with system prompts, incorporating pre-train loss to offset catastrophic forgetting, and setting specific learning rates and coefficients for training stability.",
            "result_discussion": "Results indicated that initializing the critic model from the reward model led to higher initial losses but subsequently resulted in lower losses and higher rewards for the actor model after around 20 iterations. This suggests that while the initial higher loss might reflect fundamental differences between reward and critic modeling tasks, the subsequent lower loss points to a more robust value estimation and model performance. The study indicates that the reward model initialization yields better long-term stability and performance for PPO training.",
            "ablation_id": "2403.17297v1.No2"
        }
    ]
}