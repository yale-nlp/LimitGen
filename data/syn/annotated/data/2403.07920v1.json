{
    "title": "ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training",
    "abstract": "We propose ProtLLM, a versatile cross-modal large language model (LLM) for both protein-centric and protein-language tasks. ProtLLM features a unique dynamic protein mounting mechanism, enabling it to handle complex inputs where the natural language text is interspersed with an arbitrary number of proteins. Besides, we propose the protein-as-word language modeling approach to train ProtLLM. By developing a specialized protein vocabulary, we equip the model with the capability to predict not just natural language but also proteins from a vast pool of candidates. Additionally, we construct a large-scale interleaved protein-text dataset, named InterPT, for pre-training. This dataset comprehensively encompasses both (1) structured data sources like protein annotations and (2) unstructured data sources like biological research papers, thereby endowing ProtLLM with crucial knowledge for understanding proteins. We evaluate ProtLLM on classic supervised protein-centric tasks and explore its novel protein-language applications. Experimental results demonstrate that ProtLLM not only achieves superior performance against protein-specialized baselines on protein-centric tasks but also induces zero-shot and in-context learning capabilities on protein-language tasks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### Understanding proteins is essential for unraveling the mysteries of life and enabling artificial intelligence systems to advance bioscience research Wang et al. (2023a  ###reference_b42###). Thanks to the development of deep learning techniques, neural network models encompass extensive protein-centric applications, such as protein-folding prediction Jumper et al. (2021  ###reference_b18###), protein-protein interaction analysis Li et al. (2018  ###reference_b19###); Su et al. (2023  ###reference_b39###), function prediction Zhang et al. (2023a  ###reference_b52###), etc.\nProtein representation learning methods typically employ large-scale pre-training, which learns unsupervised protein representations on massive protein sequences with masked language modeling Rives et al. (2021  ###reference_b38###), or autoregressive language modeling Elnaggar et al. (2020  ###reference_b8###).\nIn addition to protein-centric tasks, recent studies have attempted to extend protein models to protein-language scenarios.\nProtST Xu et al. (2023b  ###reference_b47###) integrates textual information into the protein encoder through multimodal pre-training on protein-text pairs, achieving zero-shot text-to-protein retrieval.\nFang et al. (2023  ###reference_b10###) introduces an instruction dataset tailored for the biomolecular domain and investigates how fine-tuned LLM performs on protein-domain instruction-following tasks, such as function description generation.\nDespite the success of protein representation methods on specific tasks, developing a model that excels in both protein-centric and protein-language tasks is still under-explored, facing three main challenges. Firstly, architectures are designed for particular downstream tasks, making it difficult to accommodate a wide range of tasks simultaneously. Secondly, current methods primarily derive cross-modal supervision from explicitly annotated protein-text pairs, which is not scalable to large-scale pre-training. Lastly, supporting a variable number of proteins in the input sequence introduces computational uncertainty in each training step, leading to inefficiencies during pre-training.\nIn this work, we propose ProtLLM, which is a versatile LLM for both protein-centric and protein-language tasks. Instead of designing for specific tasks, ProtLLM supports complex interleaved protein-text inputs and outputs, which enables our model to simultaneously handle diverse downstream tasks without re-designing task-specific architecture (see Figure 1  ###reference_### for illustrations). Specifically, our dynamic protein mounting mechanism enables the model to seamlessly process text interspersed with an arbitrary number of proteins. Besides, we propose protein-as-word language modeling to ensure interleaved protein-text outputs. By building a protein vocabulary, ProtLLM is trained to autoregressively predict words and proteins from their respective vocabularies.\nAdditionally, we present a large-scale interleaved protein-text dataset, named InterPT, for ProtLLM pre-training. InterPT is constructed from diverse data sources, consisting of both structured data such as paired protein annotation data, and unstructured data from biological research papers, which encourages ProtLLM to harness crucial knowledge from the scientific articles.\nWe conduct extensive experiments on a wide range of downstream tasks, ranging from classic supervised protein-centric tasks to novel protein-language applications. Experimental results demonstrate that ProtLLM outperforms specialized baselines on protein-centric tasks. ProtLLM also unlocks the in-context learning capability for protein-protein interaction prediction, and achieves zero-shot text-guided functional protein retrieval.\nOur contributions are as follows:\nWe propose ProtLLM, a versatile cross-modal LLM for both protein-centric and protein-language tasks. ProtLLM could process complex interleaved protein-text inputs and outputs, thereby supporting diverse tasks.\nWe introduce a large-scale pre-training dataset, InterPT, interleaving proteins and text from both structured data sources and unstructured multi-protein scientific articles.\nWe show that ProtLLM achieves superior results on protein-centric tasks against protein-specialized baselines, and induces zero-shot and in-context learning capabilities."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Large Language Models",
            "text": "The evolution of LLMs has been a cornerstone in the field of natural language processing , showcasing extraordinary capabilities across a broad spectrum of tasks Devlin et al. (2018  ###reference_b7###); Raffel et al. (2020  ###reference_b37###); Brown et al. (2020  ###reference_b2###); OpenAI (2023  ###reference_b33###); Touvron et al. (2023  ###reference_b40###); Longpre et al. (2023  ###reference_b26###); Chowdhery et al. (2022  ###reference_b4###). These models, once thought to be limited to text-based tasks, have now crossed boundaries into areas traditionally dominated by human expertise, including mathematical problem-solving Wei et al. (2022  ###reference_b44###); Imani et al. (2023  ###reference_b16###), drug discovery Liang et al. (2023  ###reference_b20###); Liu et al. (2023b  ###reference_b23###), and complex decision making Yu et al. (2023  ###reference_b49###); Ma et al. (2023  ###reference_b27###). Recent explorations further extend LLMs\u2019 expertise into the multimodal domain where they demonstrate significant promise in processing and generating content from diverse modalities Huang et al. (2023  ###reference_b15###); Zhu et al. (2023  ###reference_b55###); Liu et al. (2023a  ###reference_b22###); Zhao et al. (2023  ###reference_b54###); Wu et al. (2023  ###reference_b45###). Most of these works focus on aligning pre-trained encoders from various modalities with LLMs through instruction tuning, thus equipping LLMs to interpret multimodal inputs. In the realm of scientific research, specialized molecular LLMs have been devised for tasks like molecular property prediction Liu et al. (2023c  ###reference_b24###), captioning Fang et al. (2023  ###reference_b10###), and retrieval Liu et al. (2023d  ###reference_b25###). Despite these advances, the progress in protein understanding with LLMs lags, hindered by the scarcity of comprehensive datasets for alignment and the absence of efficient architectures to model protein-language sequences."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Protein Representation Learning",
            "text": "Current mainstream methods for protein understanding tasks have focused on protein representation learning. Protein language models (PLMs) Elnaggar et al. (2020  ###reference_b8###); Rives et al. (2021  ###reference_b38###); Meier et al. (2021  ###reference_b30###); Lin et al. (2022  ###reference_b21###) have marked significant progress in the area by training the protein sequence encoders on massive protein sequence data.\nProtein structure encoding methods aim to learn coarse-grained amino-acid-level representations Gligorijevi\u0107 et al. (2021  ###reference_b11###); Fan et al. (2022  ###reference_b9###); Zhang et al. (2023a  ###reference_b52###); Xu et al. (2023a  ###reference_b46###) or fine-grained atom-level representations Hermosilla et al. (2021  ###reference_b13###); Jing et al. (2021  ###reference_b17###); Zhang et al. (2023b  ###reference_b53###).\nDespite the success in protein modeling, protein-related text data are left unexplored, which contains valuable supervision signals crucial for understanding proteins.\nTo enhance protein understanding with text supervision, OntoProtein Zhang et al. (2022a  ###reference_b50###) leverages knowledge graphs, utilizing gene ontology annotations to implicitly enrich protein representation with textual information. ProtST Xu et al. (2023b  ###reference_b47###) integrates textual information into the protein encoder through multimodal pre-training on protein-text pairs, achieving zero-shot text-to-protein retrieval.\nMol-Instruction Fang et al. (2023  ###reference_b10###) introduces a comprehensive instruction dataset specialized for biomolecules and further fine-tunes LLMs on this dataset. Similarly, InstructProtein Wang et al. (2023b  ###reference_b43###) improves the quality of instruction datasets by sampling protein-text pairs from a structured knowledge graph. This line of work focuses on aligning protein with human language using LLMs. However, a limitation of these approaches lies in their direct incorporation of protein sequences into LLMs as text, leading to suboptimal protein modeling due to the LLMs not being pre-trained on extensive protein sequence datasets. In contrast, ProtLLM provides a versatile framework that excels in both classic protein-centric tasks and novel protein-text applications.\n###figure_2###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "In this section, we elaborate on our proposed method, ProtLLM, which is illustrated in Figure 2  ###reference_###. Initially, we detail the model architecture in Section 3.1  ###reference_.SSS0.Px1###. Subsequently, the pre-training strategy is explained, introducing the concept of protein-as-word modeling, as outlined in Section 3.2  ###reference_###. We then present the uniquely constructed interleaved protein-text dataset, InterPT, in Section 3.3  ###reference_###. Lastly, we explore the application of ProtLLM on a variety of tasks in Section 3.4  ###reference_###.\nProtLLM consists of an LLM for natural language modeling, a protein encoder, and cross-modal connectors that connect the protein encoder and the LLM.\nWe use LLaMA-7b Touvron et al. (2023  ###reference_b40###) as the backbone of ProtLLM, which is an autoregressive Transformer language model pre-trained on large-scale natural language data. To make ProtLLM understand protein sequences (i.e., sequences of amino acid tokens, which are the primary structure of proteins), we employ ProtST Xu et al. (2023b  ###reference_b47###) as the protein encoder. ProtST follows the backbone architecture of ESM-2 Lin et al. (2022  ###reference_b21###) and introduces an additional two-layer MLP projection head. Pre-trained on large-scale protein-text pairs with contrastive learning, ProtST learns protein representations that are well-aligned with text. Besides, we introduce cross-modal connectors that connect the LLM with the protein encoder, thereby enabling ProtLLM to accept multimodal inputs. Specifically, ProtLLM has two cross-modal connector layers, which are placed at the input layer and the output layer of the LLM, respectively. The input-layer connector is a trainable projection matrix and transforms the output vectors from the protein representation space to the LLM representation space. Similarly, the output-layer connector transforms the LLM output vectors back to the protein representation space. Significantly, the output-layer connector also serves as a prediction head, allowing our model to perform protein retrieval and multi-choice protein answering tasks without requiring the LLM to generate complicated protein names.\nProtLLM considers not only structured protein-text paired data but also free-form interleaved protein-text sequences. Although the widely used encoder-decoder architecture can handle paired data, it encounters difficulties when dealing with interleaved protein-text inputs with multiple proteins. Therefore, we propose dynamic protein mounting, which allows ProtLLM to accept an arbitrary number of proteins as either input. Specifically, given an input sequence interleaved with proteins and text,\n...[text] [protein] [text] [protein] [text]...\nwe do not directly feed the protein sequence to the LLM, but replace sequences with mount points.\n...[text] <PROT> [mount] </PROT> [text] ...\nAt each mount point, we mount the protein encoder to the LLM with the cross-modal connector. Additionally, these mount points are delineated by protein tags, signaling to the LLM that it is receiving protein vector inputs at these positions, rather than text data.\nWe introduce the protein-as-word language modeling training objective, which unifies protein prediction and word prediction as an autoregressive language modeling task. Consider an input sequence interleaved with  tokens , where the -th token  represents either a natural language token or a protein. The protein-as-word language modeling object is to maximize the likelihood:\nwhere  is a categorical probability distribution over a natural language vocabulary when predicting natural words, or a protein vocabulary when predicting proteins. The probability is computed by\nwhere  is the last-layer LLM hidden states of ;  is the word embedding of the word  from the natural language vocabulary ;  stands for the output connector matrix, and  is the protein embeddings of the protein  from the protein vocabulary .\nTo construct the protein vocabulary, we collect all protein sequences in the training data. We then filter out proteins present in the downstream test sets to prevent data leakage. Finally, we compile a vocabulary consisting of the  proteins.\nAlthough our dynamic protein mounting design introduces flexibility for the input format, it also introduces computational uncertainty into the pre-training process, i.e., the computational cost of each step can vary significantly with the number of input proteins. Consequently, the throughput is limited by the worst case, leading to markedly reduced training efficiency. To accelerate the pre-training, we build a protein cache where we store all the pre-computed protein vectors encoded by the protein encoder. With the protein cache, we eliminate the heavy computational cost of the protein encoder, thereby accelerating the pre-training procedure with stable throughput. Besides, we utilize LoRA Hu et al. (2022  ###reference_b14###) for efficient training.\nMulti-protein scientific articles describe complex relationships among different proteins found in biological research, where each sample could contain multiple proteins. Unlike data presented in structured formats such as pairs or knowledge graphs, these articles offer detailed insights in unstructured natural language.\nGuided by the recording in the STRING database Mering et al. (2003  ###reference_b31###) of multi-protein interactions and the scientific articles supporting them, we retrieve all involved articles from the PubMed database Canese and Weis (2013  ###reference_b3###), specifically selecting instances where multiple proteins co-occur within the same paragraph.\nAll proteins in these paragraphs are linked to the UniProt database Consortium (2015  ###reference_b5###) for their amino acid sequences. Finally, we collect K interleaved protein-text sequences from PubMed articles.\nThis data maps individual proteins to their textual annotations such as function descriptions. We integrate two data sources, i.e., the UniProt database Consortium (2015  ###reference_b5###) and the STRING database Mering et al. (2003  ###reference_b31###), adding up to K protein-annotation pairs.\nGiven such a pair, we utilize it for two tasks, i.e., protein-to-text prediction and text-to-protein prediction, with the probability of  and , respectively. Besides, during pre-training, we interleave the data into longer sequences by concatenating multiple pairs into a single sequence, which has two advantages: (1) this operation can bridge the data length gap across different data sources and reduce the number of padding tokens, leading to higher training efficiency; (2) training multiple pairs in a single sequence encourages the model to obtain in-context learning capabilities Gu et al. (2023  ###reference_b12###).\nThis data is in the instruction-following style Ouyang et al. (2022  ###reference_b34###), typically requiring the model to generate open-ended text given a protein and an instruction Fang et al. (2023  ###reference_b10###). We select the data items of proteins from the Mol-Instructions dataset Fang et al. (2023  ###reference_b10###) and include them into InterPT. Similar to the processing of protein-annotation pairs, we also concatenate multiple instruction-following data into a single pre-training example, so as to improve training efficiency and acquire in-context learning capabilities.\nThe best practice for adapting ProtLLM to downstream tasks is supervised fine-tuning when training data are available. Since ProtLLM supports flexible input and output formats, we can simply transform the downstream task data into an interleaved format and directly perform protein-as-word language modeling for supervised fine-tuning. The input and output prompt format for each downstream task can be found in the Appendix A  ###reference_###. During fine-tuning, we also apply the LoRA adapter to the LLM for efficient fine-tuning while preventing the model from overfitting several proteins in the training set.\nIn-context learning is a promising capability of LLM, which can adapt the LLM to specific tasks with a few examples without training the model. ProtLLM can achieve in-context learning by pretending a few demonstration examples. To the best of our knowledge, ProtLLM is the first protein-language LLM that is capable of in-context learning.\nModel\nPre-training\nEC\nGO-BP\nGO-MF\nGO-CC\nPPI\n\nProtein\nText\nAUPR\n\nAUPR\n\nAUPR\n\nAUPR\n\nACC\n\n\n\nDeepFRI\n\u2713\n\u2717\n0.546\n0.631\n0.282\n0.399\n0.462\n0.465\n0.363\n0.460\n-\n\nGearNet\n\u2713\n\u2717\n0.892\n0.874\n0.292\n0.490\n0.596\n0.650\n0.226\n0.486\n73.86\n\nProtBert\n\u2713\n\u2717\n0.859\n0.838\n0.188\n0.279\n0.464\n0.456\n0.234\n0.408\n77.32\n\nESM-1b\n\u2713\n\u2717\n0.884\n0.869\n0.332\n0.452\n0.630\n0.659\n0.324\n0.477\n82.22\n\nESM-2\n\u2713\n\u2717\n0.888\n0.874\n0.340\n0.472\n0.643\n0.662\n0.350\n0.472\n86.90\n\nOntoProtein\n\u2713\n\u2713\n0.854\n0.841\n0.284\n0.436\n0.603\n0.631\n0.300\n0.441\n70.42\n\nProtST\n\u2713\n\u2713\n0.898\n0.878\n0.342\n0.482\n0.647\n0.668\n0.364\n0.487\n88.19\n\nProtLLM\n\u2713\n\u2713\n0.874\n0.860\n0.349\n0.503\n0.652\n0.668\n0.469\n0.596\n89.87\nFor another interesting application, ProtLLM can be programmed to execute protein retrieval with customized requirements by following instructions. In Section 4.3  ###reference_###, we show that ProtLLM can well retrieve functional proteins based only on function descriptions, and it can be further improved by prepending a one-shot demonstration."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "ProtLLM Framework",
            "text": "ProtLLM consists of an LLM for natural language modeling, a protein encoder, and cross-modal connectors that connect the protein encoder and the LLM.\nWe use LLaMA-7b Touvron et al. (2023  ###reference_b40###  ###reference_b40###) as the backbone of ProtLLM, which is an autoregressive Transformer language model pre-trained on large-scale natural language data. To make ProtLLM understand protein sequences (i.e., sequences of amino acid tokens, which are the primary structure of proteins), we employ ProtST Xu et al. (2023b  ###reference_b47###  ###reference_b47###) as the protein encoder. ProtST follows the backbone architecture of ESM-2 Lin et al. (2022  ###reference_b21###  ###reference_b21###) and introduces an additional two-layer MLP projection head. Pre-trained on large-scale protein-text pairs with contrastive learning, ProtST learns protein representations that are well-aligned with text. Besides, we introduce cross-modal connectors that connect the LLM with the protein encoder, thereby enabling ProtLLM to accept multimodal inputs. Specifically, ProtLLM has two cross-modal connector layers, which are placed at the input layer and the output layer of the LLM, respectively. The input-layer connector is a trainable projection matrix and transforms the output vectors from the protein representation space to the LLM representation space. Similarly, the output-layer connector transforms the LLM output vectors back to the protein representation space. Significantly, the output-layer connector also serves as a prediction head, allowing our model to perform protein retrieval and multi-choice protein answering tasks without requiring the LLM to generate complicated protein names.\nProtLLM considers not only structured protein-text paired data but also free-form interleaved protein-text sequences. Although the widely used encoder-decoder architecture can handle paired data, it encounters difficulties when dealing with interleaved protein-text inputs with multiple proteins. Therefore, we propose dynamic protein mounting, which allows ProtLLM to accept an arbitrary number of proteins as either input. Specifically, given an input sequence interleaved with proteins and text,\n...[text] [protein] [text] [protein] [text]...\nwe do not directly feed the protein sequence to the LLM, but replace sequences with mount points.\n...[text] <PROT> [mount] </PROT> [text] ...\nAt each mount point, we mount the protein encoder to the LLM with the cross-modal connector. Additionally, these mount points are delineated by protein tags, signaling to the LLM that it is receiving protein vector inputs at these positions, rather than text data."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "ProtLLM Pre-Training",
            "text": "We introduce the protein-as-word language modeling training objective, which unifies protein prediction and word prediction as an autoregressive language modeling task. Consider an input sequence interleaved with  tokens , where the -th token  represents either a natural language token or a protein. The protein-as-word language modeling object is to maximize the likelihood:\nwhere  is a categorical probability distribution over a natural language vocabulary when predicting natural words, or a protein vocabulary when predicting proteins. The probability is computed by\nwhere  is the last-layer LLM hidden states of ;  is the word embedding of the word  from the natural language vocabulary ;  stands for the output connector matrix, and  is the protein embeddings of the protein  from the protein vocabulary .\nTo construct the protein vocabulary, we collect all protein sequences in the training data. We then filter out proteins present in the downstream test sets to prevent data leakage. Finally, we compile a vocabulary consisting of the  proteins.\nAlthough our dynamic protein mounting design introduces flexibility for the input format, it also introduces computational uncertainty into the pre-training process, i.e., the computational cost of each step can vary significantly with the number of input proteins. Consequently, the throughput is limited by the worst case, leading to markedly reduced training efficiency. To accelerate the pre-training, we build a protein cache where we store all the pre-computed protein vectors encoded by the protein encoder. With the protein cache, we eliminate the heavy computational cost of the protein encoder, thereby accelerating the pre-training procedure with stable throughput. Besides, we utilize LoRA Hu et al. (2022  ###reference_b14###  ###reference_b14###) for efficient training."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "InterPT: Interleaving Protein-Text Data",
            "text": "We propose a large-scale interleaved protein-text multimodal dataset, named InterPT, to pre-train ProtLLM with comprehensive protein-related knowledge. This dataset encompasses three types of data sources, i.e., multi-protein scientific articles, protein-annotation pairs, and protein instruction-following data. The statistics of each component are listed in Table 1  ###reference_###.\n\nMulti-protein scientific articles describe complex relationships among different proteins found in biological research, where each sample could contain multiple proteins. Unlike data presented in structured formats such as pairs or knowledge graphs, these articles offer detailed insights in unstructured natural language. Guided by the recording in the STRING database Mering et al. (2003  ###reference_b31###  ###reference_b31###) of multi-protein interactions and the scientific articles supporting them, we retrieve all involved articles from the PubMed database Canese and Weis (2013  ###reference_b3###  ###reference_b3###), specifically selecting instances where multiple proteins co-occur within the same paragraph. All proteins in these paragraphs are linked to the UniProt database Consortium (2015  ###reference_b5###  ###reference_b5###) for their amino acid sequences. Finally, we collect K interleaved protein-text sequences from PubMed articles. This data maps individual proteins to their textual annotations such as function descriptions. We integrate two data sources, i.e., the UniProt database Consortium (2015  ###reference_b5###  ###reference_b5###) and the STRING database Mering et al. (2003  ###reference_b31###  ###reference_b31###), adding up to K protein-annotation pairs.\n\nThis data is in the instruction-following style Ouyang et al. (2022  ###reference_b34###  ###reference_b34###), typically requiring the model to generate open-ended text given a protein and an instruction Fang et al. (2023  ###reference_b10###  ###reference_b10###). We select the data items of proteins from the Mol-Instructions dataset Fang et al. (2023  ###reference_b10###  ###reference_b10###) and include them into InterPT."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Applying ProtLLM to Diverse Tasks",
            "text": "The best practice for adapting ProtLLM to downstream tasks is supervised fine-tuning when training data are available. Since ProtLLM supports flexible input and output formats, we can simply transform the downstream task data into an interleaved format and directly perform protein-as-word language modeling for supervised fine-tuning. The input and output prompt format for each downstream task can be found in the Appendix A  ###reference_###  ###reference_###. During fine-tuning, we also apply the LoRA adapter to the LLM for efficient fine-tuning while preventing the model from overfitting several proteins in the training set.\nIn-context learning is a promising capability of LLM, which can adapt the LLM to specific tasks with a few examples without training the model. ProtLLM can achieve in-context learning by pretending a few demonstration examples. To the best of our knowledge, ProtLLM is the first protein-language LLM that is capable of in-context learning.\nModel\nPre-training\nEC\nGO-BP\nGO-MF\nGO-CC\nPPI\n\nProtein\nText\nAUPR\n\nAUPR\n\nAUPR\n\nAUPR\n\nACC\n\n\n\nDeepFRI\n\u2713\n\u2717\n0.546\n0.631\n0.282\n0.399\n0.462\n0.465\n0.363\n0.460\n-\n\nGearNet\n\u2713\n\u2717\n0.892\n0.874\n0.292\n0.490\n0.596\n0.650\n0.226\n0.486\n73.86\n\nProtBert\n\u2713\n\u2717\n0.859\n0.838\n0.188\n0.279\n0.464\n0.456\n0.234\n0.408\n77.32\n\nESM-1b\n\u2713\n\u2717\n0.884\n0.869\n0.332\n0.452\n0.630\n0.659\n0.324\n0.477\n82.22\n\nESM-2\n\u2713\n\u2717\n0.888\n0.874\n0.340\n0.472\n0.643\n0.662\n0.350\n0.472\n86.90\n\nOntoProtein\n\u2713\n\u2713\n0.854\n0.841\n0.284\n0.436\n0.603\n0.631\n0.300\n0.441\n70.42\n\nProtST\n\u2713\n\u2713\n0.898\n0.878\n0.342\n0.482\n0.647\n0.668\n0.364\n0.487\n88.19\n\nProtLLM\n\u2713\n\u2713\n0.874\n0.860\n0.349\n0.503\n0.652\n0.668\n0.469\n0.596\n89.87\nFor another interesting application, ProtLLM can be programmed to execute protein retrieval with customized requirements by following instructions. In Section 4.3  ###reference_###  ###reference_###, we show that ProtLLM can well retrieve functional proteins based only on function descriptions, and it can be further improved by prepending a one-shot demonstration."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We evaluate ProtLLM on three types of downstream tasks: (1) protein-centric tasks, which include supervised fine-tuning on conventional benchmarks for protein understanding; (2) protein-text in-context learning, where we show the unique ability of ProtLLM by in-context learning on protein-protein interaction prediction; (3) text-guided functional protein retrieval, where we conduct a real-world enzyme mining task as a proof-of-concept study to validate the retrieval capability of ProtLLM. We present detailed hyperparameters, and prompt templates for pre-training and fine-tuning in Appendix A  ###reference_###.\nFollowing the settings in PEER benchmark Xu et al. (2022  ###reference_b48###), we adopt three standard tasks in protein understanding to validate our method. Enzyme Commission (EC) number prediction Gligorijevi\u0107 et al. (2021  ###reference_b11###) aims to predict all possible EC numbers of a protein simultaneously, reflecting the chemical reactions it catalyzes. Gene Ontology (GO) term prediction Gligorijevi\u0107 et al. (2021  ###reference_b11###) extends as a multi-label classification task, seeking to predict whether a protein belongs to specific GO terms. The GO benchmark is categorized into three branches, namely biological process (BP), molecular function (MF), and cellular component (CC). Protein-Protein Interaction (PPI) prediction aims to determine whether two given proteins interact or not. We adopt the human PPI dataset Pan et al. (2010  ###reference_b35###) for experiments.\nTo evaluate performances on multi-label classification tasks including EC and GO prediction, we report pair-centric area under precision-recall curve (AUPR) values and , a widely used metric in the CAFA challenges Radivojac et al. (2013  ###reference_b36###). PPI prediction results are evaluated by mean accuracy. These metrics require the soft probability of each target label. To achieve this, we initially extract the probabilities of \u201cYes\u201d for the positive label and \u201cNo\u201d for the negative label, respectively. Then, these probabilities are normalized via the softmax function to get the final predicted probabilities.\nWe compare  ProtLLM with seven existing protein representation learning methods. As shown in Table 2  ###reference_###, these methods can be categorized into two distinct categories: protein-only approaches and protein-text learning approaches. The former encompasses sequence-based models including ProtBert Elnaggar et al. (2020  ###reference_b8###), ESM-1b Rives et al. (2021  ###reference_b38###), and ESM-2 Lin et al. (2022  ###reference_b21###), which are pre-trained using extensive collections of protein sequences, alongside structure-based models, such as DeepFRI Gligorijevi\u0107 et al. (2021  ###reference_b11###), and GearNet Zhang et al. (2022b  ###reference_b51###). The latter, protein-text learning approaches, includes OntoProtein Zhang et al. (2022a  ###reference_b50###), and ProtST Xu et al. (2023b  ###reference_b47###).\nNote that Mol-Instructions Fang et al. (2023  ###reference_b10###) and InstructProtein Wang et al. (2023b  ###reference_b43###) also belong to the protein-text learning approaches. However, their methods directly take protein sequences as human language and tokenize the data using byte-pair encoding. Contrasting with the protein-as-word strategy in ProtLLM, this exponentially increases the context length, rendering the evaluation on tasks with extensive label sets, like EC and GO prediction, or those requiring multiple protein inputs, such as PPI prediction, impractical for their approaches.\nThe results are shown in Table 2  ###reference_###. ProtLLM consistently shows competitive or even superior performance compared to both protein-only and protein-text approaches across all benchmarks, indicating the effectiveness of our proposed framework on conventional close-ended protein understanding tasks. Remarkably, ProtLLM obtain   and  AUPR on GO-CC, which outperforms ProtST by a large margin. As depicted in Section 3.1  ###reference_###, ProtLLM directly uses pre-trained ProtST as the protein encoder, with the key difference lying in our LLM decoder and pre-training stage for alignment. By comparing ProtLLM with ProtST, the overall improvements strongly highlight the potential benefits of incorporating richer protein-text information and scaling the size of the language model.\nMoreover, ProtLLM also outperforms two structure-based models on GO and PPI prediction despite we only leverage sequence information during training. Protein structures encode rich information and have direct relations to their functions. This opens up a promising direction to further incorporate protein structure into our framework, which we leave for future work.\n###figure_3### We directly evaluate the pre-trained ProtLLM model on the human PPI task without updating any parameters. For the -shot in-context learning, we randomly sample  examples from the validation set as the demonstrations and prepend them to each test sequence. Both the demonstration example and test example are prompted with the same template. For example, a one-shot prompted input is as follows:\nDo <PROT> [mount] </PROT> and <PROT> [mount] </PROT> interact with each other? Yes\\n Do <PROT> [mount] </PROT> and <PROT> [mount] </PROT> interact with each other?\nThe protein sequences of demonstration and test examples are first encoded by the protein encoder and then fed to the language model at each mount point. The final answer is predicted by selecting the verbalizer, i.e., \u201cYes\u201d or \u201cNo\u201d, with the higher probability. Besides, to understand how multi-protein pre-training data from scientific articles improves ProtLLM, we also evaluate a variant of our model by removing the multi-protein scientific articles from the pre-training corpora.\n###figure_4### Figure 3  ###reference_### presents the in-context learning performance on human PPI with varying numbers of demonstration examples. Our model consistently achieves higher PPI accuracy with an increasing number of demonstration examples, demonstrating its effective in-context learning capability for protein-centric tasks.\nIn comparison, the model performs drastically worse upon removing the multi-protein scientific articles, and fails to learn in context with the , , and  demonstrations.\nWe believe that the in-context learning capability of our model could empower biologists to apply it to specialized tasks that lack annotated data, using minimal examples. Our experiments on enzyme mining illustrate a tangible application of in-context learning, as detailed in Section 4.3  ###reference_###.\nThis experiment aims to study the capability of ProtLLM to retrieve functional proteins based on text prompts and demonstrations. For this purpose, we apply ProtLLM to enzyme mining, which is a critical stage in enzyme and metabolic engineering pipelines. In this experiment, we evaluate our model on mining carboxylate reductases that transform various ketoacids into their corresponding aldehydes. Four ketoacid reactants, i.e., 2-ketoisovaleric acid (IsoC5), pyruvic acid (C3), 2-ketovaleric acid (C5), and 2-ketooctanoic acid (C8), studied in Mak et al. (2015  ###reference_b28###) are employed for evaluation.\nUsing a reported enzyme for IsoC5, ketoisovalerate decarboxylase (KIVD) De La Plaza et al. (2004  ###reference_b6###), as the query, we first search for a pool of enzyme candidates by BLASTp McGinnis and Madden (2004  ###reference_b29###), where the pools with the size of 500 and 1000 are respectively tested.\nWe then leverage ProtLLM to retrieve active enzymes from the pool for each reactant in two modes. In the zero-shot retrieval setting, given the prompt:\nIdentify the enzymes: {Reactant}  Isobutanal. <PROT>\ndescribing the reaction from reactant (IsoC5, C3, C5 or C8) to product, ProtLLM generates a protein embedding at the token <PROT>. Then, we encode all the candidate enzymes as embeddings with the protein encoder. Finally, we utilize this embedding to rank all enzyme candidates by comparing embedding similarity. For in-context learning, we further add a one-shot demonstration of carboxylate reductase before the prompt above to facilitate enzyme mining. The demonstration is:\nIndentify the enzymes: Oxidizes aldehydes to the corresponding carboxylic acids with a preference for aromatic aldehydes. <PROT> [mount] </PROT>\nwhere the \u201c[mount]\u201d token is represented by the protein embedding of PaoC Neumann et al. (2009  ###reference_b32###), a typical carboxylate reductase.\nIn Table 3  ###reference_###, we report the recall of active enzymes found in Mak et al. (2015  ###reference_b28###) at top 10, 20, and 50 ranked candidates. It is observed that in-context learning outperforms zero-shot retrieval on 18 out of 24 metrics, which verifies that ProtLLM can learn from a few demonstrations and improve its enzyme mining performance based on such knowledge. To study the top-ranked enzymes by ProtLLM more in-depth, we employ AutoDock Vina Trott and Olson (2010  ###reference_b41###) to further screen the top-20 enzymes found by in-context learning and pick the one with the lowest Vina energy for visualization. As shown in Figure 4  ###reference_###, the lead enzymes selected in this way are all with good properties, possessing high enzyme activity (i.e., high  and  values measured by Mak et al. (2015  ###reference_b28###)) and low binding energy measured by AutoDock Vina. These results altogether prove the effectiveness of ProtLLM on enzyme mining."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Protein-Centric Tasks",
            "text": "Following the settings in PEER benchmark Xu et al. (2022  ###reference_b48###  ###reference_b48###), we adopt three standard tasks in protein understanding to validate our method. Enzyme Commission (EC) number prediction Gligorijevi\u0107 et al. (2021  ###reference_b11###  ###reference_b11###) aims to predict all possible EC numbers of a protein simultaneously, reflecting the chemical reactions it catalyzes. Gene Ontology (GO) term prediction Gligorijevi\u0107 et al. (2021  ###reference_b11###  ###reference_b11###) extends as a multi-label classification task, seeking to predict whether a protein belongs to specific GO terms. The GO benchmark is categorized into three branches, namely biological process (BP), molecular function (MF), and cellular component (CC). Protein-Protein Interaction (PPI) prediction aims to determine whether two given proteins interact or not. We adopt the human PPI dataset Pan et al. (2010  ###reference_b35###  ###reference_b35###) for experiments.\nTo evaluate performances on multi-label classification tasks including EC and GO prediction, we report pair-centric area under precision-recall curve (AUPR) values and , a widely used metric in the CAFA challenges Radivojac et al. (2013  ###reference_b36###  ###reference_b36###). PPI prediction results are evaluated by mean accuracy. These metrics require the soft probability of each target label. To achieve this, we initially extract the probabilities of \u201cYes\u201d for the positive label and \u201cNo\u201d for the negative label, respectively. Then, these probabilities are normalized via the softmax function to get the final predicted probabilities.\nWe compare  ProtLLM with seven existing protein representation learning methods. As shown in Table 2  ###reference_###  ###reference_###, these methods can be categorized into two distinct categories: protein-only approaches and protein-text learning approaches. The former encompasses sequence-based models including ProtBert Elnaggar et al. (2020  ###reference_b8###  ###reference_b8###), ESM-1b Rives et al. (2021  ###reference_b38###  ###reference_b38###), and ESM-2 Lin et al. (2022  ###reference_b21###  ###reference_b21###), which are pre-trained using extensive collections of protein sequences, alongside structure-based models, such as DeepFRI Gligorijevi\u0107 et al. (2021  ###reference_b11###  ###reference_b11###), and GearNet Zhang et al. (2022b  ###reference_b51###  ###reference_b51###). The latter, protein-text learning approaches, includes OntoProtein Zhang et al. (2022a  ###reference_b50###  ###reference_b50###), and ProtST Xu et al. (2023b  ###reference_b47###  ###reference_b47###).\nNote that Mol-Instructions Fang et al. (2023  ###reference_b10###  ###reference_b10###) and InstructProtein Wang et al. (2023b  ###reference_b43###  ###reference_b43###) also belong to the protein-text learning approaches. However, their methods directly take protein sequences as human language and tokenize the data using byte-pair encoding. Contrasting with the protein-as-word strategy in ProtLLM, this exponentially increases the context length, rendering the evaluation on tasks with extensive label sets, like EC and GO prediction, or those requiring multiple protein inputs, such as PPI prediction, impractical for their approaches.\nThe results are shown in Table 2  ###reference_###  ###reference_###. ProtLLM consistently shows competitive or even superior performance compared to both protein-only and protein-text approaches across all benchmarks, indicating the effectiveness of our proposed framework on conventional close-ended protein understanding tasks. Remarkably, ProtLLM obtain   and  AUPR on GO-CC, which outperforms ProtST by a large margin. As depicted in Section 3.1  ###reference_###  ###reference_###, ProtLLM directly uses pre-trained ProtST as the protein encoder, with the key difference lying in our LLM decoder and pre-training stage for alignment. By comparing ProtLLM with ProtST, the overall improvements strongly highlight the potential benefits of incorporating richer protein-text information and scaling the size of the language model.\nMoreover, ProtLLM also outperforms two structure-based models on GO and PPI prediction despite we only leverage sequence information during training. Protein structures encode rich information and have direct relations to their functions. This opens up a promising direction to further incorporate protein structure into our framework, which we leave for future work.\n###figure_5###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Unlocking In-Context Learning",
            "text": "In-context learning is the capability that rapidly adapts the model to specific tasks using only a few annotated demonstration examples, which is originally found in autoregressive language models Brown et al. (2020  ###reference_b2###) and then is extended to visual language models Alayrac et al. (2022  ###reference_b1###). In this section, we investigate whether ProtLLM can achieve in-context learning on the human protein-protein interaction (PPI) prediction task.\nWe directly evaluate the pre-trained ProtLLM model on the human PPI task without updating any parameters. For the -shot in-context learning, we randomly sample  examples from the validation set as the demonstrations and prepend them to each test sequence. Both the demonstration example and test example are prompted with the same template. For example, a one-shot prompted input is as follows:\nDo <PROT> [mount] </PROT> and <PROT> [mount] </PROT> interact with each other? Yes\\n Do <PROT> [mount] </PROT> and <PROT> [mount] </PROT> interact with each other?\nThe protein sequences of demonstration and test examples are first encoded by the protein encoder and then fed to the language model at each mount point. The final answer is predicted by selecting the verbalizer, i.e., \u201cYes\u201d or \u201cNo\u201d, with the higher probability. Besides, to understand how multi-protein pre-training data from scientific articles improves ProtLLM, we also evaluate a variant of our model by removing the multi-protein scientific articles from the pre-training corpora.\n###figure_6### Figure 3  ###reference_###  ###reference_### presents the in-context learning performance on human PPI with varying numbers of demonstration examples. Our model consistently achieves higher PPI accuracy with an increasing number of demonstration examples, demonstrating its effective in-context learning capability for protein-centric tasks.\nIn comparison, the model performs drastically worse upon removing the multi-protein scientific articles, and fails to learn in context with the , , and  demonstrations.\nWe believe that the in-context learning capability of our model could empower biologists to apply it to specialized tasks that lack annotated data, using minimal examples. Our experiments on enzyme mining illustrate a tangible application of in-context learning, as detailed in Section 4.3  ###reference_###  ###reference_###."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Text-Guided Functional Protein Retrieval",
            "text": "Reactant\nMethod\nPool Size: 500\nPool Size: 1000\n\nTop-10\nTop-20\nTop-50\nTop-10\nTop-20\nTop-50\n\n\n\nIsoC5\nZero-shot\n0.40\n0.40\n0.40\n0.33\n0.33\n0.33\n\nIn-context\n0.60\n0.80\n0.80\n0.50\n0.67\n0.67\n\nC3\nZero-shot\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\nIn-context\n1.0\n1.0\n1.0\n0.67\n0.67\n0.67\n\nC5\nZero-shot\n0.40\n0.40\n0.40\n0.25\n0.25\n0.25\n\nIn-context\n0.60\n0.80\n0.80\n0.38\n0.50\n0.50\n\nC8\nZero-shot\n0.33\n0.33\n0.50\n0.22\n0.22\n0.33\n\nIn-context\n0.83\n0.83\n0.83\n0.56\n0.56\n0.56\nThis experiment aims to study the capability of ProtLLM to retrieve functional proteins based on text prompts and demonstrations. For this purpose, we apply ProtLLM to enzyme mining, which is a critical stage in enzyme and metabolic engineering pipelines. In this experiment, we evaluate our model on mining carboxylate reductases that transform various ketoacids into their corresponding aldehydes. Four ketoacid reactants, i.e., 2-ketoisovaleric acid (IsoC5), pyruvic acid (C3), 2-ketovaleric acid (C5), and 2-ketooctanoic acid (C8), studied in Mak et al. (2015  ###reference_b28###  ###reference_b28###) are employed for evaluation.\nUsing a reported enzyme for IsoC5, ketoisovalerate decarboxylase (KIVD) De La Plaza et al. (2004  ###reference_b6###  ###reference_b6###), as the query, we first search for a pool of enzyme candidates by BLASTp McGinnis and Madden (2004  ###reference_b29###  ###reference_b29###), where the pools with the size of 500 and 1000 are respectively tested.\nWe then leverage ProtLLM to retrieve active enzymes from the pool for each reactant in two modes. In the zero-shot retrieval setting, given the prompt:\nIdentify the enzymes: {Reactant}  Isobutanal. <PROT>\ndescribing the reaction from reactant (IsoC5, C3, C5 or C8) to product, ProtLLM generates a protein embedding at the token <PROT>. Then, we encode all the candidate enzymes as embeddings with the protein encoder. Finally, we utilize this embedding to rank all enzyme candidates by comparing embedding similarity. For in-context learning, we further add a one-shot demonstration of carboxylate reductase before the prompt above to facilitate enzyme mining. The demonstration is:\nIndentify the enzymes: Oxidizes aldehydes to the corresponding carboxylic acids with a preference for aromatic aldehydes. <PROT> [mount] </PROT>\nwhere the \u201c[mount]\u201d token is represented by the protein embedding of PaoC Neumann et al. (2009  ###reference_b32###  ###reference_b32###), a typical carboxylate reductase.\nIn Table 3  ###reference_###  ###reference_###, we report the recall of active enzymes found in Mak et al. (2015  ###reference_b28###  ###reference_b28###) at top 10, 20, and 50 ranked candidates. It is observed that in-context learning outperforms zero-shot retrieval on 18 out of 24 metrics, which verifies that ProtLLM can learn from a few demonstrations and improve its enzyme mining performance based on such knowledge. To study the top-ranked enzymes by ProtLLM more in-depth, we employ AutoDock Vina Trott and Olson (2010  ###reference_b41###  ###reference_b41###) to further screen the top-20 enzymes found by in-context learning and pick the one with the lowest Vina energy for visualization. As shown in Figure 4  ###reference_###  ###reference_###, the lead enzymes selected in this way are all with good properties, possessing high enzyme activity (i.e., high  and  values measured by Mak et al. (2015  ###reference_b28###  ###reference_b28###)) and low binding energy measured by AutoDock Vina. These results altogether prove the effectiveness of ProtLLM on enzyme mining."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we present ProtLLM, a versatile LLM designed to tackle both protein-centric and protein-language tasks. Through dynamic\nprotein mounting and protein-as-word modeling, ProtLLM adeptly handles complex interleaved protein-text data, seamlessly unifying a wide array of protein tasks via a natural language interface. Besides, we construct a large-scale protein-language pre-training dataset, called InterPT, which encourages the model to learn from diverse data sources ranging from structured paired data to unstructured multi-protein scientific articles. Extensive experiments demonstrate that ProtLLM not only achieves competitive performance against specialized baselines across standard protein-centric benchmarks but also paves the way for exploring novel protein-language applications."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Experimental Details",
            "text": ""
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S3.T1.4\" style=\"width:190.9pt;height:74.7pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-19.5pt,7.6pt) scale(0.83,0.83) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.4.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.4.4.5.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.4.4.5.1.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.4.4.5.1.1.1\">Data Source</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.4.4.5.1.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.4.4.5.1.2.1\">Data Type</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.4.4.5.1.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.4.4.5.1.3.1\">Size</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.1.1.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">PubMed</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.1.1.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Multi-protein articles</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T1.1.1.1.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.2.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.2.2.2.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">UniProt</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.2.2.2.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Annotations</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.2.2.2.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.3.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.3.3.3.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">STRING</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.3.3.3.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Annotations</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.3.3.3.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.4.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T1.4.4.4.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Mol-Instructions</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T1.4.4.4.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Instruction-following data</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T1.4.4.4.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Category and statistics of <span class=\"ltx_text\" id=\"S3.T1.6.1\">InterPT</span> components.</figcaption>\n</figure>",
            "capture": "Table 1: Category and statistics of InterPT components."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<p class=\"ltx_p ltx_align_center\" id=\"S3.T2.4\"><span class=\"ltx_text\" id=\"S3.T2.4.4\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S3.T2.4.4.4\" style=\"width:339.5pt;height:180pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\" id=\"S3.T2.4.4.4.4\"><span class=\"ltx_text\" id=\"S3.T2.4.4.4.4.4\" style=\"color:#000000;\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.4.4.4.4.4.4\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\" id=\"S3.T2.4.4.4.4.4.4.5.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2\" id=\"S3.T2.4.4.4.4.4.4.5.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.4.4.4.5.1.1.1\">Model</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2\" id=\"S3.T2.4.4.4.4.4.4.5.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.4.4.4.5.1.2.1\">Pre-training</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2\" id=\"S3.T2.4.4.4.4.4.4.5.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.4.4.4.5.1.3.1\">EC</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2\" id=\"S3.T2.4.4.4.4.4.4.5.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.4.4.4.5.1.4.1\">GO-BP</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2\" id=\"S3.T2.4.4.4.4.4.4.5.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.4.4.4.5.1.5.1\">GO-MF</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2\" id=\"S3.T2.4.4.4.4.4.4.5.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.4.4.4.5.1.6.1\">GO-CC</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.4.4.4.4.4.4.5.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.4.4.4.5.1.7.1\">PPI</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T2.4.4.4.4.4.4.4\">\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.4.5\">Protein</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.4.6\">Text</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.4.7\">AUPR</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S3.T2.1.1.1.1.1.1.1.1\"></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.4.8\">AUPR</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S3.T2.2.2.2.2.2.2.2.2\"></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.4.9\">AUPR</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S3.T2.3.3.3.3.3.3.3.3\"></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.4.10\">AUPR</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.4.4\"></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.4.11\">ACC</span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S3.T2.4.4.4.4.4.4.6.1\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.6.1.1\">DeepFRI</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.6.1.2\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.6.1.3\">\u2717</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.6.1.4\">0.546</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.6.1.5\">0.631</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.6.1.6\">0.282</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.6.1.7\">0.399</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.6.1.8\">0.462</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.6.1.9\">0.465</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.6.1.10\">0.363</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.6.1.11\">0.460</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.4.4.4.4.4.6.1.12\">-</span></span>\n<span class=\"ltx_tr\" id=\"S3.T2.4.4.4.4.4.4.7.2\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.7.2.1\">GearNet</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.7.2.2\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.7.2.3\">\u2717</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.7.2.4\">0.892</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.7.2.5\">0.874</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.7.2.6\">0.292</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.7.2.7\">0.490</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.7.2.8\">0.596</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.7.2.9\">0.650</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.7.2.10\">0.226</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.7.2.11\">0.486</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.7.2.12\">73.86</span></span>\n<span class=\"ltx_tr\" id=\"S3.T2.4.4.4.4.4.4.8.3\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.8.3.1\">ProtBert</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.8.3.2\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.8.3.3\">\u2717</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.8.3.4\">0.859</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.8.3.5\">0.838</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.8.3.6\">0.188</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.8.3.7\">0.279</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.8.3.8\">0.464</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.8.3.9\">0.456</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.8.3.10\">0.234</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.8.3.11\">0.408</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.8.3.12\">77.32</span></span>\n<span class=\"ltx_tr\" id=\"S3.T2.4.4.4.4.4.4.9.4\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.9.4.1\">ESM-1b</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.9.4.2\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.9.4.3\">\u2717</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.9.4.4\">0.884</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.9.4.5\">0.869</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.9.4.6\">0.332</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.9.4.7\">0.452</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.9.4.8\">0.630</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.9.4.9\">0.659</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.9.4.10\">0.324</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.9.4.11\">0.477</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.9.4.12\">82.22</span></span>\n<span class=\"ltx_tr\" id=\"S3.T2.4.4.4.4.4.4.10.5\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.10.5.1\">ESM-2</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.10.5.2\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.10.5.3\">\u2717</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.10.5.4\">0.888</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.10.5.5\">0.874</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.10.5.6\">0.340</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.10.5.7\">0.472</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.10.5.8\">0.643</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.10.5.9\">0.662</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.10.5.10\">0.350</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.10.5.11\">0.472</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.10.5.12\">86.90</span></span>\n<span class=\"ltx_tr\" id=\"S3.T2.4.4.4.4.4.4.11.6\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.11.6.1\">OntoProtein</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.11.6.2\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.11.6.3\">\u2713</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.11.6.4\">0.854</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.11.6.5\">0.841</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.11.6.6\">0.284</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.11.6.7\">0.436</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.11.6.8\">0.603</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.11.6.9\">0.631</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.11.6.10\">0.300</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.11.6.11\">0.441</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.11.6.12\">70.42</span></span>\n<span class=\"ltx_tr\" id=\"S3.T2.4.4.4.4.4.4.12.7\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.12.7.1\">ProtST</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.12.7.2\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.12.7.3\">\u2713</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.12.7.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.4.4.4.12.7.4.1\">0.898</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.12.7.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.4.4.4.12.7.5.1\">0.878</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.12.7.6\">0.342</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.12.7.7\">0.482</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.12.7.8\">0.647</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.12.7.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.4.4.4.12.7.9.1\">0.668</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.12.7.10\">0.364</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.12.7.11\">0.487</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4.4.4.4.12.7.12\">88.19</span></span>\n<span class=\"ltx_tr\" id=\"S3.T2.4.4.4.4.4.4.13.8\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.13.8.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T2.4.4.4.4.4.4.13.8.1.1\">ProtLLM</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.4.4.4.4.4.4.13.8.2\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.13.8.3\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.4.4.4.4.4.4.13.8.4\">0.874</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.13.8.5\">0.860</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.4.4.4.4.4.4.13.8.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.4.4.4.13.8.6.1\">0.349</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.13.8.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.4.4.4.13.8.7.1\">0.503</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.4.4.4.4.4.4.13.8.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.4.4.4.13.8.8.1\">0.652</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.13.8.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.4.4.4.13.8.9.1\">0.668</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.4.4.4.4.4.4.13.8.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.4.4.4.13.8.10.1\">0.469</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T2.4.4.4.4.4.4.13.8.11\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.4.4.4.13.8.11.1\">0.596</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.4.4.4.4.4.4.13.8.12\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.4.4.4.13.8.12.1\">89.87</span></span></span>\n</span>\n</span></span></span>\n</span></span></span></p>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Comparative benchmark results on protein-centric tasks. We use AUPR and  on EC and GO prediction and accuracy (%) on PPI prediction. Bold figures denote the best performance. \u2018-\u2019 indicates not applicable.</figcaption>\n</figure>",
            "capture": "Table 2: Comparative benchmark results on protein-centric tasks. We use AUPR and  on EC and GO prediction and accuracy (%) on PPI prediction. Bold figures denote the best performance. \u2018-\u2019 indicates not applicable."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<p class=\"ltx_p ltx_align_center\" id=\"S4.T3.1\"><span class=\"ltx_text\" id=\"S4.T3.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T3.1.1.1\" style=\"width:269.0pt;height:180pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.1.1\"><span class=\"ltx_text\" id=\"S4.T3.1.1.1.1.1\" style=\"color:#000000;\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.1.1.1.1.1.1\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.1.1.1.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2\" id=\"S4.T3.1.1.1.1.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.1.1.1.1\">Reactant</span></span>\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2\" id=\"S4.T3.1.1.1.1.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.1.1.2.1\">Method</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_3\" id=\"S4.T3.1.1.1.1.1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.1.1.3.1\">Pool Size: 500</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3\" id=\"S4.T3.1.1.1.1.1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.1.1.4.1\">Pool Size: 1000</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.1.1.2.2\">\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.2.2.1\">Top-10</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.2.2.2\">Top-20</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.2.2.3\">Top-50</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.2.2.4\">Top-10</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.2.2.5\">Top-20</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.2.2.6\">Top-50</span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.1.1.3.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2\" id=\"S4.T3.1.1.1.1.1.1.3.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.3.1.1.1\">IsoC5</span></span>\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.3.1.2\">Zero-shot</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.3.1.3\">0.40</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.3.1.4\">0.40</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.3.1.5\">0.40</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.3.1.6\">0.33</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.3.1.7\">0.33</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.3.1.8\">0.33</span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.1.1.4.2\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.4.2.1\">In-context</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.1.1.4.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.4.2.2.1\">0.60</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.1.1.4.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.4.2.3.1\">0.80</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.4.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.4.2.4.1\">0.80</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.1.1.4.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.4.2.5.1\">0.50</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.1.1.4.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.4.2.6.1\">0.67</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.1.1.4.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.4.2.7.1\">0.67</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.1.1.5.3\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2\" id=\"S4.T3.1.1.1.1.1.1.5.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.5.3.1.1\">C3</span></span>\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.5.3.2\">Zero-shot</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.5.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.5.3.3.1\">1.0</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.5.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.5.3.4.1\">1.0</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.5.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.5.3.5.1\">1.0</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.5.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.5.3.6.1\">1.0</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.5.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.5.3.7.1\">1.0</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.5.3.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.5.3.8.1\">1.0</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.1.1.6.4\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.6.4.1\">In-context</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.1.1.6.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.6.4.2.1\">1.0</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.1.1.6.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.6.4.3.1\">1.0</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.6.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.6.4.4.1\">1.0</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.1.1.6.4.5\">0.67</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.1.1.6.4.6\">0.67</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.1.1.6.4.7\">0.67</span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.1.1.7.5\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2\" id=\"S4.T3.1.1.1.1.1.1.7.5.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.7.5.1.1\">C5</span></span>\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.7.5.2\">Zero-shot</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.7.5.3\">0.40</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.7.5.4\">0.40</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.7.5.5\">0.40</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.7.5.6\">0.25</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.7.5.7\">0.25</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.7.5.8\">0.25</span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.1.1.8.6\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.8.6.1\">In-context</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.1.1.8.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.8.6.2.1\">0.60</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.1.1.8.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.8.6.3.1\">0.80</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.8.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.8.6.4.1\">0.80</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.1.1.8.6.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.8.6.5.1\">0.38</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.1.1.8.6.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.8.6.6.1\">0.50</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.1.1.8.6.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.8.6.7.1\">0.50</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.1.1.9.7\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2\" id=\"S4.T3.1.1.1.1.1.1.9.7.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.9.7.1.1\">C8</span></span>\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.9.7.2\">Zero-shot</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.9.7.3\">0.33</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.9.7.4\">0.33</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.9.7.5\">0.50</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.9.7.6\">0.22</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.9.7.7\">0.22</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.9.7.8\">0.33</span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.1.1.10.8\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.10.8.1\">In-context</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.1.1.1.1.1.10.8.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.10.8.2.1\">0.83</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.1.1.1.1.1.10.8.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.10.8.3.1\">0.83</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.10.8.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.10.8.4.1\">0.83</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.1.1.1.1.1.10.8.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.10.8.5.1\">0.56</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.1.1.1.1.1.10.8.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.10.8.6.1\">0.56</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.1.1.1.1.1.10.8.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1.10.8.7.1\">0.56</span></span></span>\n</span>\n</span></span></span>\n</span></span></span></p>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Performance comparisons between zero-shot retrieval and in-context learning on enzyme mining. <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.1\">Top-10, 20 and 50 Recall</span> are reported.</figcaption>\n</figure>",
            "capture": "Table 3: Performance comparisons between zero-shot retrieval and in-context learning on enzyme mining. Top-10, 20 and 50 Recall are reported."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T4\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A1.T4.16\" style=\"width:176.3pt;height:256.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-4.6pt,6.8pt) scale(0.95,0.95) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A1.T4.16.16\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T4.16.16.17.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"A1.T4.16.16.17.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.16.16.17.1.1.1\">Hyperparameter</span></th>\n<td class=\"ltx_td ltx_border_tt\" id=\"A1.T4.16.16.17.1.2\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A1.T4.1.1.1.2\">Batch size</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T4.1.1.1.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.2.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T4.2.2.2.2\">Sequence length</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T4.2.2.2.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.3.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T4.3.3.3.2\">Training steps</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T4.3.3.3.1\">\nK</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.16.16.18.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T4.16.16.18.2.1\">Optimizer</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T4.16.16.18.2.2\">AdamW</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.6.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T4.4.4.4.1\">Adam \n</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T4.6.6.6.3\">(, )</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.8.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T4.7.7.7.1\">Adam \n</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T4.8.8.8.2\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.9.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T4.9.9.9.2\">Learning rate</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T4.9.9.9.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.16.16.19.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T4.16.16.19.3.1\">Learning rate schedule</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T4.16.16.19.3.2\">Cosine decay</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.10.10.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T4.10.10.10.2\">Warmup ratio</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T4.10.10.10.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.11.11.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T4.11.11.11.2\">Weight decay</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T4.11.11.11.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.13.13.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T4.12.12.12.1\">LoRA \n</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T4.13.13.13.2\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.15.15.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T4.14.14.14.1\">LoRA \n</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T4.15.15.15.2\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.16.16.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T4.16.16.16.2\">LoRA dropout</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T4.16.16.16.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.16.16.20.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A1.T4.16.16.20.4.1\">LoRA modules</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A1.T4.16.16.20.4.2\">All linear modules</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>\nPre-training hyperparameters of <span class=\"ltx_text ltx_font_smallcaps\" id=\"A1.T4.18.1\">ProtLLM</span>.\n</figcaption>\n</figure>",
            "capture": "Table 4: \nPre-training hyperparameters of ProtLLM.\n"
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T5\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A1.T5.42\" style=\"width:374.2pt;height:222.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-9.8pt,5.9pt) scale(0.95,0.95) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A1.T5.42.42\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A1.T5.42.42.43.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"A1.T5.42.42.43.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.42.42.43.1.1.1\">Hyperparameter</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T5.42.42.43.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.42.42.43.1.2.1\">EC</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T5.42.42.43.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.42.42.43.1.3.1\">GO-BP</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T5.42.42.43.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.42.42.43.1.4.1\">GO-MF</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T5.42.42.43.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.42.42.43.1.5.1\">GO-CC</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T5.42.42.43.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.42.42.43.1.6.1\">PPI</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T5.5.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A1.T5.5.5.5.6\">Batch size</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.2.2\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.3.3.3.3\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.4.4.4.4\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.5.5.5.5\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.10.10.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T5.10.10.10.6\">Training steps</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.6.6.6.1\">\nK</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.7.7.7.2\">\nK</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.8.8.8.3\">\nK</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.9.9.9.4\">\nK</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.10.10.10.5\">\nK</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.42.42.44.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T5.42.42.44.1.1\">Optimizer</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.42.42.44.1.2\">AdamW</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.42.42.44.1.3\">AdamW</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.42.42.44.1.4\">AdamW</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.42.42.44.1.5\">AdamW</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.42.42.44.1.6\">AdamW</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.15.15.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T5.15.15.15.6\">Learning rate</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.11.11.11.1\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.12.12.12.2\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.13.13.13.3\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.14.14.14.4\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.15.15.15.5\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.42.42.45.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T5.42.42.45.2.1\">Learning rate schedule</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.42.42.45.2.2\">Cosine decay</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.42.42.45.2.3\">Cosine decay</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.42.42.45.2.4\">Cosine decay</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.42.42.45.2.5\">Cosine decay</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.42.42.45.2.6\">Cosine decay</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.20.20.20\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T5.20.20.20.6\">Warmup ratio</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.16.16.16.1\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.17.17.17.2\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.18.18.18.3\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.19.19.19.4\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.20.20.20.5\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.25.25.25\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T5.25.25.25.6\">Weight decay</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.21.21.21.1\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.22.22.22.2\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.23.23.23.3\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.24.24.24.4\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.25.25.25.5\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.31.31.31\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T5.26.26.26.1\">LoRA \n</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.27.27.27.2\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.28.28.28.3\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.29.29.29.4\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.30.30.30.5\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.31.31.31.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.37.37.37\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T5.32.32.32.1\">LoRA \n</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.33.33.33.2\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.34.34.34.3\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.35.35.35.4\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.36.36.36.5\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.37.37.37.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.42.42.42\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T5.42.42.42.6\">LoRA dropout</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.38.38.38.1\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.39.39.39.2\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.40.40.40.3\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.41.41.41.4\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.42.42.42.5\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.42.42.46.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T5.42.42.46.3.1\">LoRA modules</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.42.42.46.3.2\">All linear</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.42.42.46.3.3\">All linear</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.42.42.46.3.4\">All linear</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.42.42.46.3.5\">All linear</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T5.42.42.46.3.6\">All linear</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.42.42.47.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A1.T5.42.42.47.4.1\">Update protein encoder</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A1.T5.42.42.47.4.2\">Yes</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A1.T5.42.42.47.4.3\">Yes</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A1.T5.42.42.47.4.4\">Yes</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A1.T5.42.42.47.4.5\">Yes</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A1.T5.42.42.47.4.6\">No</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>\nFine-tuning hyperparameters of <span class=\"ltx_text ltx_font_smallcaps\" id=\"A1.T5.44.1\">ProtLLM</span> on various downstream tasks.\n</figcaption>\n</figure>",
            "capture": "Table 5: \nFine-tuning hyperparameters of ProtLLM on various downstream tasks.\n"
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T6\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A1.T6.2\" style=\"width:394.9pt;height:58.4pt;vertical-align:-0.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-49.4pt,7.2pt) scale(0.8,0.8) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A1.T6.2.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A1.T6.2.2.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T6.2.2.3.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.2.2.3.1.1.1\">Task</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T6.2.2.3.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.2.2.3.1.2.1\">Prompt template</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T6.2.2.3.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.2.2.3.1.3.1\">Verbalizer</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T6.2.2.4.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T6.2.2.4.1.1\">GO</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T6.2.2.4.1.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T6.2.2.4.1.2.1\">&lt;PROT&gt; [mount] &lt;/PROT&gt; Does the protein belong to [name], which is [description]?</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T6.2.2.4.1.3\"><span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T6.2.2.4.1.3.1\">Yes/No</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.2.2.5.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T6.2.2.5.2.1\">EC</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T6.2.2.5.2.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T6.2.2.5.2.2.1\">&lt;PROT&gt; [mount] &lt;/PROT&gt; Does the protein catalyze [name], which is [description]?</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T6.2.2.5.2.3\"><span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T6.2.2.5.2.3.1\">Yes/No</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.2.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T6.2.2.2.3\">PPI</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T6.2.2.2.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T6.2.2.2.2.2\">Do &lt;PROT&gt; [mount] &lt;/PROT&gt; and &lt;PROT&gt; [mount] &lt;/PROT&gt; interact with each other?</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T6.2.2.2.4\"><span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T6.2.2.2.4.1\">Yes/No</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>\nPrompt templates for each task.\n</figcaption>\n</figure>",
            "capture": "Table 6: \nPrompt templates for each task.\n"
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.07920v1_figure_1.png",
            "caption": "Figure 1: Unlike existing protein representation models that focus on protein-text pairs or protein-only data, ProtLLM can handle complex inputs with multiple proteins interleaved with text, thereby learning crucial knowledge from scientific papers and supporting diverse downstream tasks."
        },
        "2": {
            "figure_path": "2403.07920v1_figure_2.png",
            "caption": "Figure 2: An overview of ProtLLM. The architecture of ProtLLM consists of an autoregressive transformer, a protein encoder, and cross-modal connectors. With dynamic protein mounting, ProtLLM adeptly handles free-form interleaved protein-text sequences with an arbitrary number of proteins in the input. ProtLLM is pre-trained with protein-as-word language modeling that unifies word and protein prediction by constructing a protein vocabulary."
        },
        "3": {
            "figure_path": "2403.07920v1_figure_3.png",
            "caption": "Figure 3: In-context learning results on human PPI."
        },
        "4": {
            "figure_path": "2403.07920v1_figure_4.png",
            "caption": "Figure 4: Top-1 enzyme mining results based on ProtLLM retrieval and AutoDock Vina post-screening. Kc\u2062a\u2062t/KMsubscript\ud835\udc3e\ud835\udc50\ud835\udc4e\ud835\udc61subscript\ud835\udc3e\ud835\udc40K_{cat}/K_{M}italic_K start_POSTSUBSCRIPT italic_c italic_a italic_t end_POSTSUBSCRIPT / italic_K start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT and Kc\u2062a\u2062tsubscript\ud835\udc3e\ud835\udc50\ud835\udc4e\ud835\udc61K_{cat}italic_K start_POSTSUBSCRIPT italic_c italic_a italic_t end_POSTSUBSCRIPT measure enzyme activity (higher the better). Vina energy measures binding affinity (lower the better)."
        }
    },
    "references": [
        {
            "1": {
                "title": "Flamingo: a visual language model for few-shot learning.",
                "author": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022.",
                "venue": "Advances in Neural Information Processing Systems, 35:23716\u201323736.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Language models are few-shot learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.",
                "venue": "Advances in Neural Information Processing Systems, 33:1877\u20131901.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Pubmed: the bibliographic database.",
                "author": "Kathi Canese and Sarah Weis. 2013.",
                "venue": "The NCBI handbook, 2(1).",
                "url": null
            }
        },
        {
            "4": {
                "title": "PaLM: Scaling language modeling with pathways.",
                "author": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.",
                "venue": "arXiv preprint arXiv:2204.02311.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Uniprot: a hub for protein information.",
                "author": "UniProt Consortium. 2015.",
                "venue": "Nucleic acids research, 43(D1):D204\u2013D212.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Biochemical and molecular characterization of -ketoisovalerate decarboxylase, an enzyme involved in the formation of aldehydes from amino acids by lactococcus lactis.",
                "author": "Marta De La Plaza, Pilar Fern\u00e1ndez de Palencia, Carmen Pel\u00e1ez, and Teresa Requena. 2004.",
                "venue": "FEMS microbiology letters, 238(2):367\u2013374.",
                "url": null
            }
        },
        {
            "7": {
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.",
                "venue": "arXiv preprint arXiv:1810.04805.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Prottrans: towards cracking the language of life\u2019s code through self-supervised deep learning and high performance computing.",
                "author": "Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. 2020.",
                "venue": "arXiv preprint arXiv:2007.06225.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Continuous-discrete convolution for geometry-sequence modeling in proteins.",
                "author": "Hehe Fan, Zhangyang Wang, Yi Yang, and Mohan Kankanhalli. 2022.",
                "venue": "In The Eleventh International Conference on Learning Representations.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models.",
                "author": "Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and Huajun Chen. 2023.",
                "venue": "arXiv preprint arXiv:2306.08018.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Structure-based protein function prediction using graph convolutional networks.",
                "author": "Vladimir Gligorijevi\u0107, P Douglas Renfrew, Tomasz Kosciolek, Julia Koehler Leman, Daniel Berenberg, Tommi Vatanen, Chris Chandler, Bryn C Taylor, Ian M Fisk, Hera Vlamakis, et al. 2021.",
                "venue": "Nature communications, 12(1):3168.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Pre-training to learn in context.",
                "author": "Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4849\u20134870, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-long.267"
            }
        },
        {
            "13": {
                "title": "Intrinsic-extrinsic convolution and pooling for learning on 3d protein structures.",
                "author": "Pedro Hermosilla, Marco Sch\u00e4fer, Mat\u011bj Lang, Gloria Fackelmann, Pere Pau V\u00e1zquez, Barbora Kozl\u00edkov\u00e1, Michael Krone, Tobias Ritschel, and Timo Ropinski. 2021.",
                "venue": "International Conference on Learning Representations.",
                "url": null
            }
        },
        {
            "14": {
                "title": "LoRA: Low-rank adaptation of large language models.",
                "author": "Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022.",
                "venue": "In International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=nZeVKeeFYf9"
            }
        },
        {
            "15": {
                "title": "Language is not all you need: Aligning perception with language models.",
                "author": "Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. 2023.",
                "venue": "arXiv preprint arXiv:2302.14045.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Mathprompter: Mathematical reasoning using large language models.",
                "author": "Shima Imani, Liang Du, and Harsh Shrivastava. 2023.",
                "venue": "arXiv preprint arXiv:2303.05398.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Learning from protein structure with geometric vector perceptrons.",
                "author": "Bowen Jing, Stephan Eismann, Pratham N. Soni, and Ron O. Dror. 2021.",
                "venue": "In International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=1YLJDvSx6J4"
            }
        },
        {
            "18": {
                "title": "Highly accurate protein structure prediction with alphafold.",
                "author": "John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al. 2021.",
                "venue": "Nature, 596(7873):583\u2013589.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Deep neural network based predictions of protein interactions using primary sequences.",
                "author": "Hang Li, Xiu-Jun Gong, Hua Yu, and Chang Zhou. 2018.",
                "venue": "Molecules, 23(8):1923.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Drugchat: towards enabling chatgpt-like capabilities on drug molecule graphs.",
                "author": "Youwei Liang, Ruiyi Zhang, Li Zhang, and Pengtao Xie. 2023.",
                "venue": "arXiv preprint arXiv:2309.03907.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Language models of protein sequences at the scale of evolution enable accurate structure prediction.",
                "author": "Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, et al. 2022.",
                "venue": "bioRxiv.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Visual instruction tuning.",
                "author": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023a.",
                "venue": "arXiv preprint arXiv:2304.08485.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Chatgpt-powered conversational drug editing using retrieval and domain feedback.",
                "author": "Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, and Chaowei Xiao. 2023b.",
                "venue": "arXiv preprint arXiv:2305.18090.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Molxpt: Wrapping molecules with text for generative pre-training.",
                "author": "Zequn Liu, Wei Zhang, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Ming Zhang, and Tie-Yan Liu. 2023c.",
                "venue": "arXiv preprint arXiv:2305.10688.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter.",
                "author": "Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. 2023d.",
                "venue": "arXiv preprint arXiv:2310.12798.",
                "url": null
            }
        },
        {
            "26": {
                "title": "The flan collection: Designing data and methods for effective instruction tuning.",
                "author": "Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023.",
                "venue": "arXiv preprint arXiv:2301.13688.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Eureka: Human-level reward design via coding large language models.",
                "author": "Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023.",
                "venue": "arXiv preprint arXiv: Arxiv-2310.12931.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Integrative genomic mining for enzyme function to enable engineering of a non-natural biosynthetic pathway.",
                "author": "Wai Shun Mak, Stephen Tran, Ryan Marcheschi, Steve Bertolani, James Thompson, David Baker, James C Liao, and Justin B Siegel. 2015.",
                "venue": "Nature communications, 6(1):10005.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Blast: at the core of a powerful and diverse set of sequence analysis tools.",
                "author": "Scott McGinnis and Thomas L Madden. 2004.",
                "venue": "Nucleic acids research, 32(suppl_2):W20\u2013W25.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Language models enable zero-shot prediction of the effects of mutations on protein function.",
                "author": "Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. 2021.",
                "venue": "bioRxiv.",
                "url": null
            }
        },
        {
            "31": {
                "title": "String: a database of predicted functional associations between proteins.",
                "author": "Christian von Mering, Martijn Huynen, Daniel Jaeggi, Steffen Schmidt, Peer Bork, and Berend Snel. 2003.",
                "venue": "Nucleic acids research, 31(1):258\u2013261.",
                "url": null
            }
        },
        {
            "32": {
                "title": "A periplasmic aldehyde oxidoreductase represents the first molybdopterin cytosine dinucleotide cofactor containing molybdo-flavoenzyme from escherichia coli.",
                "author": "Meina Neumann, Gerd Mittelst\u00e4dt, Chantal Iobbi-Nivol, Miguel Saggu, Friedhelm Lendzian, Peter Hildebrandt, and Silke Leimk\u00fchler. 2009.",
                "venue": "The FEBS journal, 276(10):2762\u20132774.",
                "url": null
            }
        },
        {
            "33": {
                "title": "GPT-4 technical report.",
                "author": "OpenAI. 2023.",
                "venue": "arXiv preprint arXiv:2303.08774.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Training language models to follow instructions with human feedback.",
                "author": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.",
                "venue": "In Advances in Neural Information Processing Systems, volume 35, pages 27730\u201327744. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf"
            }
        },
        {
            "35": {
                "title": "Large-scale prediction of human protein- protein interactions from amino acid sequence based on latent topic features.",
                "author": "Xiao-Yong Pan, Ya-Nan Zhang, and Hong-Bin Shen. 2010.",
                "venue": "Journal of proteome research, 9(10):4992\u20135001.",
                "url": null
            }
        },
        {
            "36": {
                "title": "A large-scale evaluation of computational protein function prediction.",
                "author": "Predrag Radivojac, Wyatt T Clark, Tal Ronnen Oron, Alexandra M Schnoes, Tobias Wittkop, Artem Sokolov, Kiley Graim, Christopher Funk, Karin Verspoor, Asa Ben-Hur, et al. 2013.",
                "venue": "Nature methods, 10(3):221\u2013227.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer.",
                "author": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.",
                "venue": "JMLR.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences.",
                "author": "Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. 2021.",
                "venue": "Proceedings of the National Academy of Sciences, 118(15).",
                "url": null
            }
        },
        {
            "39": {
                "title": "Saprot: Protein language modeling with structure-aware vocabulary.",
                "author": "Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, and Fajie Yuan. 2023.",
                "venue": "bioRxiv, pages 2023\u201310.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Llama: Open and efficient foundation language models.",
                "author": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023.",
                "venue": "arXiv preprint arXiv:2302.13971.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading.",
                "author": "Oleg Trott and Arthur J Olson. 2010.",
                "venue": "Journal of computational chemistry, 31(2):455\u2013461.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Scientific discovery in the age of artificial intelligence.",
                "author": "Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. 2023a.",
                "venue": "Nature, 620(7972):47\u201360.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Instructprotein: Aligning human and protein language via knowledge instruction.",
                "author": "Zeyuan Wang, Qiang Zhang, Keyan Ding, Ming Qin, Xiang Zhuang, Xiaotong Li, and Huajun Chen. 2023b.",
                "venue": "arXiv preprint arXiv:2310.03269.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Chain-of-thought prompting elicits reasoning in large language models.",
                "author": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022.",
                "venue": "NeurIPS.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Next-gpt: Any-to-any multimodal llm.",
                "author": "Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2023.",
                "venue": "arXiv preprint arXiv:2309.05519.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Eurnet: Efficient multi-range relational modeling of protein structure.",
                "author": "Minghao Xu, Yuanfan Guo, Yi Xu, Jian Tang, Xinlei Chen, and Yuandong Tian. 2023a.",
                "venue": "In ICLR 2023 - Machine Learning for Drug Discovery workshop.",
                "url": "https://openreview.net/forum?id=7fJC3tA1Ny"
            }
        },
        {
            "47": {
                "title": "Protst: Multi-modality learning of protein sequences and biomedical texts.",
                "author": "Minghao Xu, Xinyu Yuan, Santiago Miret, and Jian Tang. 2023b.",
                "venue": null,
                "url": null
            }
        },
        {
            "48": {
                "title": "Peer: a comprehensive and multi-task benchmark for protein sequence understanding.",
                "author": "Minghao Xu, Zuobai Zhang, Jiarui Lu, Zhaocheng Zhu, Yangtian Zhang, Ma Chang, Runcheng Liu, and Jian Tang. 2022.",
                "venue": "Advances in Neural Information Processing Systems, 35:35156\u201335173.",
                "url": null
            }
        },
        {
            "49": {
                "title": "Language to rewards for robotic skill synthesis.",
                "author": "Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, et al. 2023.",
                "venue": "arXiv preprint arXiv:2306.08647.",
                "url": null
            }
        },
        {
            "50": {
                "title": "Ontoprotein: Protein pretraining with gene ontology embedding.",
                "author": "Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Haosen Hong, Shumin Deng, Qiang Zhang, Jiazhang Lian, and Huajun Chen. 2022a.",
                "venue": "In International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=yfe1VMYAXa4"
            }
        },
        {
            "51": {
                "title": "Protein representation learning by geometric structure pretraining.",
                "author": "Zuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan, Aurelie Lozano, Payel Das, and Jian Tang. 2022b.",
                "venue": "arXiv preprint arXiv:2203.06125.",
                "url": null
            }
        },
        {
            "52": {
                "title": "Protein representation learning by geometric structure pretraining.",
                "author": "Zuobai Zhang, Minghao Xu, Arian Rokkum Jamasb, Vijil Chenthamarakshan, Aurelie Lozano, Payel Das, and Jian Tang. 2023a.",
                "venue": "In The Eleventh International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=to3qCB3tOh9"
            }
        },
        {
            "53": {
                "title": "Pre-training protein encoder via siamese sequence-structure diffusion trajectory prediction.",
                "author": "Zuobai Zhang, Minghao Xu, Aurelie Lozano, Vijil Chenthamarakshan, Payel Das, and Jian Tang. 2023b.",
                "venue": "In Annual Conference on Neural Information Processing Systems.",
                "url": null
            }
        },
        {
            "54": {
                "title": "Graphtext: Graph reasoning in text space.",
                "author": "Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, and Jian Tang. 2023.",
                "venue": "arXiv preprint arXiv:2310.01089.",
                "url": null
            }
        },
        {
            "55": {
                "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models.",
                "author": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.",
                "venue": "arXiv preprint arXiv:2304.10592.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.07920v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.07920v1",
        "paper_title": "ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training",
        "research_background": "## Motivation\nThe motivation for the paper stems from the essential need to understand proteins to advance bioscience research using artificial intelligence systems. The rapid development of deep learning techniques has led to numerous protein-centric applications such as protein-folding prediction, protein-protein interaction analysis, and function prediction. Despite these advancements, existing protein representation learning methods predominantly focus on specific tasks and architectures, thereby limiting their versatility across diverse protein and protein-language tasks.\n\n## Research Problem\nThe primary research problem addressed in the paper is the development of a model that excels in both protein-centric and protein-language tasks. This problem is challenging due to three main issues:\n1. Task-specific model architectures make it difficult to accommodate a wide range of tasks simultaneously.\n2. Current methods rely on explicitly annotated protein-text pairs for cross-modal supervision, which is not scalable for large-scale pre-training.\n3. Supporting a variable number of proteins in the input sequence introduces computational inefficiencies during pre-training.\n\n## Relevant Prior Work\n1. **Protein Representation Learning**:\n   - Large-scale pre-training methods for protein representation learning typically use masked language modeling or autoregressive language modeling to learn unsupervised representations from massive protein sequences (e.g., Rives et al., 2021; Elnaggar et al., 2020).\n\n2. **Protein-Language Scenarios**:\n   - ProtST (Xu et al., 2023b) integrates textual information into the protein encoder via multimodal pre-training on protein-text pairs, enabling zero-shot text-to-protein retrieval.\n   - Fang et al. (2023) introduced an instruction dataset specific to the biomolecular domain and evaluated how fine-tuned large language models (LLMs) perform on protein-domain instruction-following tasks such as function description generation.\n\n## Contributions\n- **ProtLLM**: The paper proposes ProtLLM, a versatile large language model (LLM) capable of handling both protein-centric and protein-language tasks by supporting interleaved protein-text inputs and outputs. \n- **Dynamic Protein Mounting Mechanism**: ProtLLM employs a dynamic protein mounting mechanism to process text interspersed with an arbitrary number of proteins.\n- **Protein-as-Word Language Modeling**: By building a protein vocabulary, ProtLLM is trained to autoregressively predict both words and protein sequences.\n- **InterPT Dataset**: The authors introduce a large-scale pre-training dataset, InterPT, constructed from a combination of structured protein annotation data and unstructured biological research papers to enrich ProtLLM with essential knowledge.\n- **Experimental Validation**: Extensive experiments demonstrate ProtLLM's superior performance on classic supervised protein-centric tasks and its capabilities in novel protein-language applications, including zero-shot text-guided functional protein retrieval and in-context learning for protein-protein interaction prediction.\n\nThese contributions collectively aim to overcome the challenges in creating a versatile model that excels across diverse protein-related tasks without necessitating task-specific architectural designs.",
        "methodology": "**Methodology:**\nIn this section, we elaborate on our proposed method, ProtLLM, which offers a sophisticated architecture, an innovative pre-training strategy, and a cleverly constructed dataset, aiming to enhance the model's performance on various tasks related to protein and natural language interactions.\n\n1. **Model Architecture (Section 3.1 ###reference_.SSS0.Px1###):**\nProtLLM comprises three main components:\n- **LLM for Natural Language Modeling:** We use LLaMA-7b (Touvron et al., 2023 ###reference_b40###) as the backbone, an autoregressive Transformer model pre-trained on large-scale natural language data.\n- **Protein Encoder:** ProtST (Xu et al., 2023b ###reference_b47###) is employed, which follows the backbone architecture of ESM-2 (Lin et al., 2022 ###reference_b21###) and includes a two-layer MLP projection head. ProtST is pre-trained on protein-text pairs using contrastive learning to align protein representations with text.\n- **Cross-Modal Connectors:** These connectors bridge the LLM and protein encoder, allowing ProtLLM to handle multimodal inputs. Two layers, situated at the LLM's input and output layers, transform vectors between the protein and LLM representation spaces, facilitating seamless interactions.\n\n2. **Pre-Training Strategy: Protein-as-Word Modeling (Section 3.2 ###reference_###):**\nProtLLM introduces dynamic protein mounting and a unique training objective:\n- **Dynamic Protein Mounting:** Allows ProtLLM to handle sequences interleaved with multiple proteins. Protein sequences are replaced by mount points in the input sequence, with the protein encoder mounted at these points.\n- **Protein-as-Word Language Modeling:** Unifies protein and word prediction tasks into an autoregressive language modeling framework. Tokens in the input sequence can be either natural language or protein tokens. The goal is to maximize the likelihood of predicting these tokens accurately.\n\nTo facilitate training, a protein cache is used, storing pre-computed protein vectors to speed up the pre-training process. Additionally, LoRA (Hu et al., 2022 ###reference_b14###) is utilized for efficient training.\n\n3. **Interleaved Protein-Text Dataset (InterPT) (Section 3.3 ###reference_###):**\nInterPT is constructed specifically for ProtLLM by integrating data from the STRING (Mering et al., 2003 ###reference_b31###) and UniProt (Consortium, 2015 ###reference_b5###) databases. The dataset includes multi-protein scientific articles retrieved from PubMed (Canese and Weis, 2013 ###reference_b3###), annotated with protein-text pairs. By concatenating multiple pairs into single sequences, the dataset is optimized to enhance training efficiency and encourage in-context learning abilities.\n\n4. **Application of ProtLLM (Section 3.4 ###reference_###):**\nFor downstream tasks, ProtLLM can be fine-tuned using supervised learning, transforming data into an interleaved format for protein-as-word language modeling. Moreover, ProtLLM can also perform in-context learning, showcasing its ability to handle specific tasks with few examples without further training.\n\nOverall, ProtLLM's innovative structure, pre-training strategy, and uniquely compiled dataset aim to push the boundaries of protein and natural language interactions, demonstrating superior performance across various applications as detailed in subsequent sections.",
        "main_experiment_and_results": "### Main Experiment Setup and Results for ProtLLM Evaluation\n\n#### Experiment Setup\n\nWe evaluated ProtLLM on three downstream tasks:\n\n1. **Protein-centric tasks**: Supervised fine-tuning on standard benchmarks for protein understanding.\n2. **Protein-text in-context learning**: In-context learning on protein-protein interaction (PPI) prediction.\n3. **Text-guided functional protein retrieval**: A real-world enzyme mining task to validate retrieval capabilities.\n\nFor protein understanding, we utilized three key tasks as defined by the PEER benchmark:\n- **Enzyme Commission (EC) number prediction**: Predicts all possible EC numbers of a protein as a multi-label classification task.\n- **Gene Ontology (GO) term prediction**: Also a multi-label classification task, categorized into biological process (BP), molecular function (MF), and cellular component (CC).\n- **Protein-Protein Interaction (PPI) prediction**: Determines whether two proteins interact, using the human PPI dataset.\n\n#### Datasets and Baselines\n\nWe compared ProtLLM with seven protein representation learning methods categorized into:\n- **Protein-only approaches**: Including sequence-based models like ProtBert, ESM-1b, and ESM-2, and structure-based models like DeepFRI and GearNet.\n- **Protein-text learning approaches**: Including OntoProtein and ProtST.\n\n#### Evaluation Metrics\n\nFor multi-label classification (EC and GO prediction), we used pair-centric area under the precision-recall curve (AUPR) values. For PPI prediction, we reported mean accuracy. Predicted probabilities were normalized using the softmax function.\n\n#### Experimental Results\n\nProtLLM demonstrated competitive or superior performance across benchmarks compared to both protein-only and protein-text approaches, indicating the effectiveness of the proposed framework for conventional protein understanding tasks. Notably, ProtLLM achieved high AUPR values on GO-CC, surpassing ProtST substantially.\n\nFurthermore, ProtLLM outperformed structure-based models on GO and PPI predictions, despite only utilizing sequence information during training, suggesting future potential in incorporating protein structure data.\n\nFor the PPI prediction task using in-context learning, the evaluation showed ProtLLM's effectiveness, consistently achieving higher accuracy with an increasing number of demonstrations. In enzyme mining, ProtLLM demonstrated strong capability in retrieving functional proteins based on text prompts, outperforming zero-shot retrieval in most metrics and selecting high-activity enzymes with low binding energy for further screening.\n\nOverall, these results underscore ProtLLM's proficiency in protein understanding and retrieval tasks, positioning it as a powerful tool in protein research and highlighting promising directions for further enhancement by integrating structural data."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the effectiveness of ProtLLM on conventional close-ended protein understanding tasks and compare it against other protein representation learning methods.",
            "experiment_process": "The experiments were conducted on three standard protein understanding tasks: Enzyme Commission (EC) number prediction, Gene Ontology (GO) term prediction, and Protein-Protein Interaction (PPI) prediction. The human PPI dataset and benchmarks from literature were used for the evaluations. Performance metrics included pair-centric area under the precision-recall curve (AUPR) and mean accuracy, with soft probability predictions normalized via the softmax function. ProtLLM's performance was compared against seven existing protein representation learning methods, categorized into protein-only and protein-text learning approaches.",
            "result_discussion": "ProtLLM demonstrated competitive or superior performance compared to other methods across all benchmarks, notably outperforming models like ProtST in tasks involving GO predictions. The results highlighted the potential benefits of incorporating richer protein-text information and scaling the size of the language model. Additionally, ProtLLM outperformed structure-based models in GO and PPI prediction tasks, despite only utilizing sequence information during training.",
            "ablation_id": "2403.07920v1.No1"
        },
        {
            "research_objective": "To investigate ProtLLM's capability to achieve in-context learning on the human protein-protein interaction (PPI) prediction task.",
            "experiment_process": "Pre-trained ProtLLM was directly evaluated on the PPI task without parameter updates. Different numbers of demonstration examples were sampled from the validation set and prepended to each test sequence. Demonstration and test examples were encoded by the protein encoder and fed to the language model. The model predicted responses by selecting the higher probability between 'Yes' or 'No'. A variant without multi-protein scientific articles in the pre-training corpus was also evaluated to understand its impact.",
            "result_discussion": "ProtLLM showed effective in-context learning capability, achieving higher PPI accuracy with more demonstration examples. Removing multi-protein scientific articles from the pre-training data significantly worsened performance. These results suggest that ProtLLM can adapt to specialized tasks with minimal examples, empowering biologists for tasks lacking annotated data.",
            "ablation_id": "2403.07920v1.No2"
        },
        {
            "research_objective": "To study ProtLLM's capability to retrieve functional proteins based on text prompts and demonstrations, specifically for enzyme mining in metabolic engineering.",
            "experiment_process": "ProtLLM was used for enzyme mining to transform ketoacids into aldehydes. Pools of enzyme candidates were created using BLASTp, with pool sizes of 500 and 1000. ProtLLM retrieved active enzymes from the pools in two modes: zero-shot retrieval and in-context learning. In zero-shot, embedding similarity was used to rank enzyme candidates. In-context learning added a one-shot demonstration before the prompt. The recall of active enzymes in top-10, top-20, and top-50 ranked candidates was reported.",
            "result_discussion": "In-context learning outperformed zero-shot retrieval in most metrics, demonstrating that ProtLLM can improve enzyme mining performance with a few demonstrations. Further screening of top-20 enzymes confirmed that selected enzymes had high activity and low binding energy, validating ProtLLM's effectiveness in enzyme mining.",
            "ablation_id": "2403.07920v1.No3"
        }
    ]
}