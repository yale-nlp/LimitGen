{
    "title": "MAGPIE: Multi-Task Analysis of Media-Bias Generalization with Pre-Trained Identification of Expressions",
    "abstract": "Media bias detection poses a complex, multifaceted problem traditionally tackled using single-task models and small in-domain datasets, consequently lacking generalizability.\nTo address this, we introduce MAGPIE, the first large-scale multi-task pre-training approach explicitly tailored for media bias detection.\nTo enable pre-training at scale, we present Large Bias Mixture (LBM), a compilation of 59 bias-related tasks.\nMAGPIE outperforms previous approaches in media bias detection on the Bias Annotation By Experts (BABE) dataset, with a relative improvement of 3.3% F1-score.\nMAGPIE also performs better than previous models on 5 out of 8 tasks in the Media Bias Identification Benchmark (MBIB).\nUsing a RoBERTa encoder, MAGPIE needs only 15% of finetuning steps compared to single-task approaches.\nOur evaluation shows, for instance, that tasks like sentiment and emotionality boost all learning, all tasks enhance fake news detection, and scaling tasks leads to the best results.\nMAGPIE confirms that MTL is a promising approach for addressing media bias detection, enhancing the accuracy and efficiency of existing models. Furthermore, LBM is the first available resource collection focused on media bias MTL.\n\n\n\nKeywords:\u2009Media bias, Multi-task learning, Text classification",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Media bias is the skewed portrayal of information favoring certain group interests Recasens et al. (2013  ###reference_b76###), which manifests in multiple facets, including political, gender, racial, and linguistic biases. Such subtypes of bias, which can intersect and coexist in complex combinations, make the classification of media bias a challenging task Raza et al. (2022  ###reference_b74###). Existing research on media bias detection primarily involves training classifiers on small in-domain datasets Krieger et al. (2022  ###reference_b46###), which exhibit limited generalizability across diverse domains Wessel et al. (2023  ###reference_b104###).\nThis paper builds upon the work of Wessel et al. (2023  ###reference_b104###), emphasizing that the multifaceted nature of media bias detection requires a shift from isolated approaches to multi-task methodologies, considering a broad spectrum of bias types and datasets. The recent advancements in Multi-Task Learning (MTL) Aribandi et al. (2021a  ###reference_b4###); Chen et al. (2021  ###reference_b15###); Kirstein et al. (2022  ###reference_b44###) open up promising opportunities to overcome these challenges by enabling knowledge transfer across domains and tasks. Despite the potential, a comprehensive MTL approach for media bias detection is yet to be realized. The only other media bias MTL method Spinde et al. (2022  ###reference_b90###) underperforms due to its narrow task focus and does not surpass baseline outcomes.\n###figure_1### In this study, we make five main contributions:\nWe present MAGPIE (Multi-Task Media-Bias Analysis Generalization for Pre-Trained Identification of Expressions) the first large-scale multi-task pre-training approach for media bias detection.\nBy including diverse biases like persuasive and subjective, a classifier trained on top of MAGPIE correctly classifies sentences that state-of-the-art single-task models misidentify (we show an example in Figure 1  ###reference_###).\nWe introduce LBM (Large Bias Mixture), a pre-training composition of 59 bias-related tasks encompassing wide range of biases such as linguistic bias, gender bias and group bias.\nWe provide an analysis of a task selection and demonstrate the effectiveness of scaling the number of tasks.\nWe demonstrate that MAGPIE outperforms the previous state-of-the-art model by 3.3% on the Media Bias Annotation by Experts (BABE) dataset Spinde et al. (2021c  ###reference_b91###) and achieves state-of-the-art results on the Media Bias Identification Benchmark (MBIB) collection Wessel et al. (2023  ###reference_b104###).\nWe make all resources, including datasets, training framework, documentation, and models, publicly available on GitHub.\ngithub.com/magpie-multi-task  ###reference_-multi-task###\nThese contributions highlight the potential of MTL in improving media bias detection.\nOur findings show, e.g., that tasks like sentiment and emotionality enhance overall learning, all tasks boost fake news detection, and scaling tasks leads to optimal results.\nAnother key insight of our research is the value of MTL in contexts where the primary dataset is small111For example, the Media Bias Annotation by Experts (BABE) dataset Spinde et al. (2021c  ###reference_b91###)..\nBy learning generalized bias knowledge from a range of tasks, we can improve the accuracy and efficiency of existing models, even in the face of limited data.\nOverall, our research offers a novel multi-task learning approach with a variety of media bias tasks, contributing to the understanding of media bias detection."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Media Bias",
            "text": "Media bias is a complex issue Lee et al. (2021a  ###reference_b49###); Recasens et al. (2013  ###reference_b76###); Raza et al. (2022  ###reference_b74###) composed of varying definitions of bias subtypes such as linguistic bias, context bias, or group bias Wessel et al. (2023  ###reference_b104###).\nIn their literature review, Spinde et al. (2023  ###reference_b87###) provide an extensive overview of research on media bias and related subtypes of bias.\nMedia bias detection approaches have evolved from hand-crafted features Recasens et al. (2013  ###reference_b76###); Hube and Fetahu (2018  ###reference_b40###); Spinde et al. (2021d  ###reference_b92###) to neural models Spinde et al. (2022  ###reference_b90###); Chen et al. (2021  ###reference_b15###); Spinde et al. (2021c  ###reference_b91###); Huguet Cabot et al. (2021  ###reference_b42###); Sinha and Dasgupta (2021  ###reference_b83###); Raza et al. (2022  ###reference_b74###).\nHowever, existing models, so far, focus only on single tasks and saturate quickly on smaller datasets Wessel et al. (2023  ###reference_b104###).\nAs most neural approaches require large quantities of data, those relying on single and small datasets cannot provide a realistic scenario for their solutions (e.g., Fan et al. (2019  ###reference_b28###)).\nWe will first provide an overview of existing datasets and then show how to exploit their diversity within the media bias domain.\nMedia bias tasks and datasets mainly cover individual, self-contained tasks such as binary classifications Recasens et al. (2013  ###reference_b76###); Spinde et al. (2021b  ###reference_b89###), which, so far, are not explored in relation to each other Spinde et al. (2023  ###reference_b87###).\nWessel et al. (2023  ###reference_b104###) systematically form the media bias detection benchmark MBIB by reviewing over 100 media bias datasets and consolidating 22 of them into eight distinct tasks like linguistic, racial, and political bias.\nTheir study highlights that methods only focused on one of these tasks exhibit limitations in their detection capabilities.\nMAGPIE encompasses all the tasks identified in the MBIB but also significantly expands its scope by incorporating an additional 51 media bias-related tasks due to a variety of limitations in MBIB (see Section 3.1  ###reference_###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Multi-Task Learning",
            "text": "MTL shows significant improvements in various NLP tasks, including sentiment analysis He et al. (2019  ###reference_b36###), text summarization Kirstein et al. (2022  ###reference_b44###), and natural language understanding Raffel et al. (2020  ###reference_b73###).\nIn MTL, a model leverages knowledge gained from one task to improve the performance of others.\nAribandi et al. (2021b  ###reference_b5###) demonstrate that increasing the number of tasks generally leads to improved performance for downstream NLP applications. Aghajanyan et al. (2021  ###reference_b1###) show that pre-finetuning, a large-scale multi-task learning phase, consistently improves the performance and efficiency of pre-trained models across diverse tasks, with results improving linearly with the number of tasks beyond a certain threshold.\nAs described above, media bias can be seen as a composite problem composed of various interrelated bias types Wessel et al. (2023  ###reference_b104###). In the realm of Natural Language Understanding (NLU), MTL has proven to be highly effective when incorporating related tasks Aribandi et al. (2021b  ###reference_b5###). For instance, benchmarks such as GLUE and SuperGLUE successfully decompose the NLU problem into a suite of proxy tasks, including paraphrase detection and semantic evaluation, thereby substantially improving performance across a range of NLU tasks Wang et al. (2018  ###reference_b99###, 2019  ###reference_b98###). Motivated by this success in NLU, we propose to jointly learn from different bias types within the media bias domain. With this approach, we aim to treat media bias not as a singular entity but as many interconnected issues.\nThe selection of tasks is pivotal to the efficacy of MTL.\nThere have been several attempts to automate task selection, including learning the data selection with Bayesian optimization Bingel and S\u00f8gaard (2017  ###reference_b10###) or estimating task relations Ruder and Plank (2017  ###reference_b77###).\nThe most model-agnostic approach is GradTS Ma et al. (2021  ###reference_b56###), which is highly scalable due to low resource requirements, and is therefore implemented within MAGPIE.\nGradTS accumulates gradients of attention heads and selects tasks based on their correlation with the primary task\u2019s attention.\nThe selected tasks are trained jointly and share representations across tasks."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Methodolodgy",
            "text": "We implement MAGPIE using pre-finetuning as introduced in Aghajanyan et al. (2021  ###reference_b1###) (See also Section 2  ###reference_###).\nAs such, MAGPIE is an encoder-only MTL transformer model pre-finetuned on 59 media bias-related auxiliary tasks provided by Large Bias Mixture (LBM), an extensive task collection of bias-related datasets222LBM is also published within this paper, see Section 3.1  ###reference_###..\nWe incorporate a novel approach of a Head-Specific Early Stopping and Resurrection to effectively handle tasks of varying sizes (Section 3.3.1  ###reference_.SSS1###).\n###figure_2### As outlined in Figure 2  ###reference_###, our first step involves constructing the LBM. Following this, we define the model and multi-task learning (MTL) framework employed to train MAGPIE, which includes optimization strategies, task sampling, and auxiliary task selection. Lastly, we evaluate MAGPIE on two primary resources: the Media Bias Annotation by Experts (BABE) dataset333BABE provides high-quality labels that capture a broad range of linguistic biases, thus allowing us to evaluate our model\u2019s generalizability within a single dataset context. Spinde et al. (2021c  ###reference_b91###), and the Media Bias Identification Benchmark (MBIB) collection444MBIB provides a variety of bias tasks for testing and is currently the most extensive media bias benchmark available. Wessel et al. (2023  ###reference_b104###)."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   The LBM (Large Bias Mixture) task collection",
            "text": "Currently, MBIB is the only collection of media bias tasks. However, it does not include tasks that constitute a form of media bias555As per the definition of directly and indirectly related media bias tasks given by Spinde et al. (2023  ###reference_b87###). more indirectly, such as sentiment analysis or emotion classification. MAGPIE aims to integrate tasks both directly linked to media bias and those peripherally related, such as sentiment analysis, to provide broader coverage of linguistic features in the media bias context. Therefore, we introduce Large Bias Mixture (LBM), a more general collection of relevant media bias and media bias-related tasks, more suitable for our MTL approach. We show our task selection process in Figure 3  ###reference_###. First, we manually assess a list of 115 media bias-related datasets categorized into task families composed by Wessel et al. (2023  ###reference_b104###). A task family is a conceptual grouping of tasks that share similar objectives, such as those related to gender bias, encompassing pronoun coreference resolution, gender classification, and sexism detection. We use this notion of task families to analyze general knowledge transfer between media bias tasks, such as proposed by Aribandi et al. (2021b  ###reference_b5###) for general language tasks. We filter the collection of the 115 datasets based on the following criteria666We base these on the criteria set by Wessel et al. (2023  ###reference_b104###). However, we acknowledge that determining the dataset quality remains a manual and subjective choice.: Accessibility: Datasets have to be publicly accessible. Text granularity: We only use datasets labeled on a sentence level (not on, for instance, article or word level) Quality of annotations: We exclude datasets with little to no documentation or unreliable annotation sources. Duplicates: We filter out datasets that contain full or partial duplicates of each other Of the 115 datasets collected, we discard 11 datasets that are not publicly available. We discard 52 with article-level annotations and 5 with annotations on other levels777One dataset has annotations on only words, two on users, and three on outlets.. We remove 5 datasets due to low quality or unreliable annotations and discard 4 duplicates. Applying these criteria leaves 38 datasets. Including 8 handpicked datasets not originally listed gives us 46 datasets. These are categorized into task families ensuring no overlap and more than two datasets per family. Finally, ten datasets with multi-level annotations, e.g., token and sentence level, are split into tasks, yielding final number of 59 tasks. The final Large Bias Mixture (LBM) includes 59 tasks, categorized into 9 distinct task families, encompassing 1,210,084 labeled sentences. We make the LBM publicly accessible, to facilitate research in media bias detection and other computational-social-science tasks. References and short descriptions of all datasets and corresponding tasks can be found in Table 4  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   The Base Model",
            "text": "In terms of the base language model for our procedure, we adopt a pre-trained RoBERTa Liu et al. (2019  ###reference_b54###) encoder due to its proven state-of-the-art performances across various media bias applications Spinde et al. (2021c  ###reference_b91###); Krieger et al. (2022  ###reference_b46###)."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   The MTL framework",
            "text": "Pre-finetuning.\nTo effectively harness the generalization abilities of MTL for media bias detection, we adopt a pre-finetuning procedure Aghajanyan et al. (2021  ###reference_b1###).\nPre-finetuning is a complementary approach to pre-training, where a model, already pre-trained on a typically unsupervised task, is subjected to another intermediate step of pre-training.\nWhile incorporating MTL directly into the pre-training stage has demonstrated performance gains Aribandi et al. (2021b  ###reference_b5###), we opt for pre-finetuning as it offers significantly reduced computational demands while still capitalizing on the benefits of MTL Aghajanyan et al. (2021  ###reference_b1###).\nSharing representations.\nWe use hard parameter sharing to share the underlying encoder among all tasks while using separate task-specific heads.\nFor each task, we attach dense layers, or \"heads\", to the shared encoder.\nThese heads are optimized individually per task while the shared encoder learns general bias representations.\nHowever, multi-task optimization presents challenges due to differing gradient directions and magnitudes Yu et al. (2020  ###reference_b108###).\nFor instance, two tasks, A and B, may have opposing gradients with the same magnitude, nullifying their sum.\nOn the other hand, if Task A\u2019s gradient greatly surpasses that of Task B, gradient A becomes dominant.\nWe counter the gradient misalignment by using a variation of the PCGrad de-confliction algorithm and loss scaling Yu et al. (2020  ###reference_b108###).\nConflicting gradients and loss scaling.\nIn multi-task training involving  tasks, encoder parameters receive  potentially conflicting gradients. Efficient handling of this conflict, such as PCGrad Yu et al. (2020  ###reference_b108###), requires storing a set of gradients for each task involved in the update, leading to infeasible memory requirements among our 59 LBM tasks.\nTherefore, we propose a variation of PCGrad we call PCGrad-online which preserves the fundamental idea of the original algorithm but is more memory efficient, requiring only one set of gradients instead of  sets per update.\nAdopting Muppet\u2019s method, we solve the issue of varying task gradient magnitudes by re-scaling the task loss with the inverse log size of its output space, ensuring balanced gradients and preventing task dominance in training steps Aghajanyan et al. (2021  ###reference_b1###)."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1.   Data sampling and early stopping",
            "text": "To prevent large tasks from dominating the optimization, we ensure uniform data distribution by sampling one fixed-size sub-batch from each task per training step, a regular approach in MTL Aribandi et al. (2021b  ###reference_b5###); Spinde et al. (2022  ###reference_b90###).\nWe also employ early stopping as a regularization for each task individually to prevent over-fitting of tasks that converge faster.\nHowever, these methods often fall short when confronted with tasks of varied complexity and differing convergence speeds, which both is the case for tasks in LBM.\nWhen task A stops early while task B takes longer to converge, the latent representation of the shared encoder shifts toward task B.\nWe aim to mitigate this issue by employing a training strategy that tackles the latent representation shift using two complementary approaches:\nHead-Specific-Early-Stopping (HSES)\nResurrection\nHSES. When the task stops, we stop updating its specific head parameters while still backpropagating its language model gradients. This method stems from the observation that not all tasks benefit from the shared layers\u2019 continuous learning, especially after they have reached an optimal state.\nResurrection. When the task stops, we allow it to resume the training after its validation loss starts to increase again.\nThis enables the task to adapt its head parameters to the latest latent representation.\nHSES maintains quality of faster-converging tasks, while Resurrection allows further adaptation when needed. Their combination aims for balanced, adaptive learning for tasks with varied complexities and convergence speeds. We perform a preliminary evaluation of the effectiveness of this method in the in Section 4.4  ###reference_###. However, we stress the need for further extensive analysis in Limitations  ###reference_###."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2.   Auxiliary task selection",
            "text": "Multi-Task Learning often involves selecting well-used datasets, leading to potential selection biases.\nFurthermore, manually handpicking datasets becomes challenging due to varying and potentially ambiguous bias annotation schemes.\nTo automate the process of task selection, we utilize the GradTS algorithm Ma et al. (2021  ###reference_b56###).\nWe choose GradTS due to its demonstrated efficiency and its simplicity of implementation, which enhances its usability.\nGradient-based Task selection.\nIn line with GradTS, we construct auxiliary tasks as follows. We individually train all tasks, accumulate absolute gradients, then extract and layer-wise normalize these in the attention heads, forming a 12x12 importance matrix888RoBERTa has 12 attention heads on each of the 12 layers. for each task. Tasks are sorted by correlation between each task\u2019s matrix and BABE task\u2019s matrix. We pre-finetune  models on the first  tasks from the sorted list, where  varies from 1 to  and  is the size of LBM. The BABE task is then evaluated on these pre-finetuned models, with the optimal  determined by evaluation loss.\nFor further details on the GradTS algorithm, please see Ma et al. (2021  ###reference_b56###)."
        },
        {
            "section_id": "3.3.3",
            "parent_section_id": "3.3",
            "section_name": "3.3.3.   Hyperparameters and training stability",
            "text": "Finetuning transformer models on small datasets often leads to inconsistent outcomes, such as overfitting, training instabilities, or significant performance variabilities Dodge et al. (2020  ###reference_b26###).\nTo ensure reproducibility and consistency, we use a fixed random seed for each run and perform individual task hyperparameter grid searches. To negate the influence of random weight initializations, we evaluate and compare models using 30 random seeds, reporting average performance.\nWe use  as a random seed for all experiments. For the final evaluation, where we employ 30 random seeds, we use values from .\nFor optimizing the model, we use a per-task batch size\nof 32, an AdamW optimizer, and a polynomial learning scheduler.\nWe run all experiments on 1 NVIDIA TESLA A100 GPU with 40 GB of memory."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Empirical Results",
            "text": "In this section, we present the results of our MTL approach.\nFirst, we report the set of auxiliary tasks selected by GradTS for pre-finetuning.\nNext, we assess how the model pre-finetuned on the GradTS set performs during subsequent finetuning on the BABE task, compared to a random choice of tasks and a full set of LBM tasks.\nWe also compare the MTL approach to a single-task baseline and multiple MTL baselines and evaluate the performance of our best model, MAGPIE, on the MBIB benchmark.\nThen, we analyze the LBM taxonomy through a study on knowledge transfer between families.\nLastly, we evaluate the effects of the proposed methods, HSES and Resurrection, through a preliminary study."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Auxiliary tasks selection",
            "text": "We select suitable auxiliary tasks by calculating the correlation between attention-head importance matrices.\nWe use Kendall\u2019s correlation coefficient, as suggested by Puth et al. (2015  ###reference_b72###).\nWe find a local minimum for the BABE evaluation loss when pre-finetuning on the first  most correlated tasks.\nThe final set of the ten most correlated tasks referred to as gradts set, is displayed in Table 1  ###reference_###.\nThe tasks in the gradts set demonstrate a strong semantic connection to media bias, encompassing areas such as lexical bias, rumor detection, and fake news detection."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Evaluation",
            "text": "First, we finetune a model pre-finetuned on three different multi-task sets on the BABE task and compare it against the single-task RoBERTa baseline.\nThe multi-task sets are the following:\nMTL:Random - Random Subset of 10 tasks\nMTL:GradTS - Subset of 10 tasks selected by GradTS algorithm\nMTL:All - Set of all tasks\nWe also evaluate the model pre-finetuned on the set of all tasks on MBIB.\nWe follow the guidelines set by Wessel et al. (2023  ###reference_b104###) for the evaluation.\nGiven that MAGPIE\u2019s pre-training data includes portions of the MBIB data, we ensure that the test set for each task in MBIB is not exposed to the model during its training or validation phases.\n###figure_4### Multi-Task Learning Performance.\nSection 4.2  ###reference_### summarizes our performance results on the BABE dataset.\nWe observe that all of our MTL pre-finetuning schemas lead to performance improvements.\nIn particular, pre-finetuning on all tasks from LBM yields a SOTA performance on the BABE dataset, achieving an 84.1% F1 score and a relative improvement of 3.3% compared to the previous baseline by Spinde et al. (2021c  ###reference_b91###).\nWhile both MTL baselines - Muppet Aghajanyan et al. (2021  ###reference_b1###) and UnifiedM2 Lee et al. (2021b  ###reference_b50###) outperform single-task baseline, they underperform all of our MTL models.\nTask scaling.\nGradTS task selection outperforms random tasks on average performance, yet our experiment suggests task number scaling is more crucial. This is consistent with Muppet and ExT5 results Aghajanyan et al. (2021  ###reference_b1###); Aribandi et al. (2021b  ###reference_b5###), indicating MTL can compensate for scarce high-quality media bias datasets through general bias representation from other tasks. It also supports Kirstein et al. (2022  ###reference_b44###)\u2019s finding that sufficient related tasks can substitute the original task.\nOn MBIB, our performance surpasses that of the five models evaluated by Wessel et al. (2023  ###reference_b104###) in 5 out of 8 tasks.\nAdditionally, it improves upon the current baseline with an average F1-Score of 0.7692 across all tasks. However, this improvement is only marginal.\nMore importantly, MAGPIE outperforms the single-task RoBERTa baseline on MBIB, showing that an MTL approach is beneficial in media bias detection.\n###figure_5### Step efficiency.\nIn addition to the performance improvements achieved, we also assess our model training efficiency.\nIn Figure 5  ###reference_###, we show the F1 score on the development set for the BABE task, averaged over all 30 runs.\nOur findings show that Multi-Task Learning only requires 1\u03035% of the training steps used in single-task finetuning on BABE.\nThis result demonstrates the high training-step efficiency of MAGPIE in media bias classification, making MTL implementations in the media bias domain more viable in the future."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   LBM taxonomy analysis",
            "text": "In Section 3.1  ###reference_###, following Aribandi et al. (2021b  ###reference_b5###), we introduce data task families. Aribandi et al. (2021b  ###reference_b5###) uses task families for selection and knowledge transfer.\nTo assess task families\u2019 significance in LBM taxonomy, we train each pair of families together, investigating knowledge transfer.\nTo account for potential negative transfer within families, we first calculate the average transfer within each family and use it as a baseline for measuring transfer between families.\nWe train tasks from the same family together and report the average change in task performance, as depicted in Figure 6  ###reference_###.\nNegative knowledge transfer is prevalent across most of our task families.\nHowever, we observe two exceptions: the hate-speech and stance detection families, where multi-task training leads to an average improvement in performance.\n###figure_6### Next, we measure knowledge transfer between families by training each pair together. We report the average impact of each family on others and the average benefit each family gets through training with others, summarized in Table 3  ###reference_###.\nOur results show that, on average, the Emotionality and Sentiment analysis families provide positive transfer learning to other families.\nConversely, we observe that the Fake News family benefits from knowledge transfer from every other family, with an average improvement of 1.7%.\nOn the other hand, the Emotionality family is significantly impaired by negative transfer from other task families.\n###figure_7### The full table of transfers can be found in Figure 7  ###reference_###.\nConsidering that only two families show positive transfer learning, with marginal effects of 0.11% and 0.34%, we conclude that the task families used in the construction of LBM are generally unsuitable for effectively utilizing knowledge transfer. We discuss this again in Section 5  ###reference_###."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "4.4.   Resurrection and HSES evaluation",
            "text": "To evaluate the Resurrection and HSES methods in combination with other training strategies, we run a grid search on the following training strategies: HSES, Resurrection, Loss Scaling and Gradient Aggregation.\nWe calculate the average evaluation loss for both Resurrection and HSES methods across 20 tasks randomly selected from the LBM collection.\nThe boxplot in Figure 8  ###reference_### shows that both methods reduce the loss by 5% and decrease the variance across different training setups by 85%.\nHowever, we hypothesize that a random constellation of tasks999Particularly variance in task sizes and quality. can have a non-trivial effect on the evaluation of our technique; thus, we opt for robust examination of the methods in future work.\n###figure_8### ###figure_9###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Conclusion",
            "text": "This paper contributes to media bias detection by the development of MAGPIE, our large-scale multi-task learning (MTL) approach that enhances traditional models with a multi-task structure.\nAdditionally, we present Large Bias Mixture (LBM), a compilation of 59 bias-related tasks. This broad collection serves as a resource for the pre-training of MAGPIE. To the best of our knowledge, it is the first available MTL resource tailored specifically towards media bias.\nOur study investigates the dynamics of transfer learning across tasks and task families within media bias. Despite the occurrence of negative transfer among several tasks, scaling the pre-training setup to all collected tasks in Multi-Task Learning results in a 3.3% improvement over the previous state-of-the-art, making it the biggest advancement in neural media bias classification so far.\nFurthermore, we report that finetuning MAGPIE on the BABE dataset only requires 15% of steps compared to RoBERTa single-task approaches.\nThese findings underscore the effectiveness and potency of Multi-Task Learning in highly specific classification domains such as media bias.\nWhile results suggest benefits in scaling tasks, we see more promise in novel tasks rooted in media bias, suggesting deeper exploration over simply expanding the task spectrum.\nUnderstanding families and tasks in datasets necessitates systematic analysis of label definitions, rater agreement, and inter-relatedness of dataset creation strategies.\nAs media bias is emerging globally, incorporating multilingual models is a natural extension."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Bibliographical References",
            "text": ""
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T1.1.1.2\">Task Type</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.1\">\n (correlation)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.2.1.1\">Persuasive techniques</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.2.1.2\">0.73</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.3.2.1\">Lexical/Informational bias</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.3.2.2\">0.72</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.4.3.1\">Rumour detection</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.4.3.2\">0.69</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.5.4.1\">Sentiment analysis</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.5.4.2\">0.68</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.6.5.1\">Global warming stance detection</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.6.5.2\">0.68</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.7.6.1\">Subjective bias</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.7.6.2\">0.67</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.8.7.1\">Subjective bias</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.8.7.2\">0.67</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.9.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.9.8.1\">Veracity classification</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.9.8.2\">0.64</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.10.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.10.9.1\">Gender bias</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.10.9.2\">0.64</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.11.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T1.1.11.10.1\">Fake news detection</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.11.10.2\">0.63</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Attention importance correlation wrt media bias task.</figcaption>\n</figure>",
            "capture": "Table 1: Attention importance correlation wrt media bias task."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table ltx_align_center\" id=\"S4.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.1\">Model</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.2.1\">F1</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.3.1\">Acc</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.4.1\">loss</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt\" id=\"S4.T2.1.2.2.1\">Baseline (RoBERTa base)</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T2.1.2.2.2\">80.83 (\u00b10.69)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T2.1.2.2.3\">81.19 (\u00b10.69)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T2.1.2.2.4\">43.6 (\u00b13.54)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S4.T2.1.3.3.1\">DA-RoBERTa</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T2.1.3.3.2\">77.83 (\u00b11.4)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T2.1.3.3.3\">78.56 (\u00b11.3)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T2.1.3.3.4\">47.84 (\u00b12.97)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S4.T2.1.4.4.1\">MUPPET</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T2.1.4.4.2\">80.56 (\u00b11.3)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T2.1.4.4.3\">81.18 (\u00b11.16)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T2.1.4.4.4\">44.19 (\u00b14.65)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S4.T2.1.5.5.1\">UnifiedM2</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T2.1.5.5.2\">81.91 (\u00b10.91)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T2.1.5.5.3\">82.41 (\u00b10.88)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T2.1.5.5.4\">44.86 (\u00b13.99)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S4.T2.1.6.6.1\">MTL:Random</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T2.1.6.6.2\">81.88 (\u00b11.02)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T2.1.6.6.3\">82.28 (\u00b10.97)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T2.1.6.6.4\">40.35 (\u00b11.73)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S4.T2.1.7.7.1\">MTL:GradTS</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T2.1.7.7.2\">82.32 (\u00b10.79)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T2.1.7.7.3\">82.64 (\u00b10.8)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T2.1.7.7.4\">40.96 (\u00b12.36)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\" id=\"S4.T2.1.8.8.1\">MTL:All</th>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S4.T2.1.8.8.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.8.8.2.1\">84.1</span> (\u00b11.33)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S4.T2.1.8.8.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.8.8.3.1\">84.44</span> (\u00b11.25)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S4.T2.1.8.8.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.8.8.4.1\">39.46</span> (\u00b12.41)</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Performance of two MTL baseline models (Muppet, UnifiedM2) two single-task baslines (RoBERTa and DA-RoBERTa) and our three MTL models, on fine-tuning on BABE dataset and evaluating on the held-out test set. The results are averaged over 30 random seeds.</figcaption>\n</figure>",
            "capture": "Table 2: Performance of two MTL baseline models (Muppet, UnifiedM2) two single-task baslines (RoBERTa and DA-RoBERTa) and our three MTL models, on fine-tuning on BABE dataset and evaluating on the held-out test set. The results are averaged over 30 random seeds."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1\">Task Family</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.2\">Transfer <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.2.1\">from</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.3\">Transfer <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.3.1\">to</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" id=\"S4.T3.1.2.1.1\">media bias</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T3.1.2.1.2\" style=\"background-color:#ECBDBD;\"><span class=\"ltx_text\" id=\"S4.T3.1.2.1.2.1\" style=\"background-color:#ECBDBD;\">-2.07%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T3.1.2.1.3\">-0.94%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S4.T3.1.3.2.1\">subjectivity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.3.2.2\">-1.26%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.3.2.3\">0.89%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S4.T3.1.4.3.1\">hate speech</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.4.3.2\">-0.87%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.4.3.3\">0.17%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S4.T3.1.5.4.1\">gender bias</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.5.4.2\">-1.01%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.5.4.3\">-1.07%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S4.T3.1.6.5.1\">sentiment analysis</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.6.5.2\" style=\"background-color:#B7EBC6;\"><span class=\"ltx_text\" id=\"S4.T3.1.6.5.2.1\" style=\"background-color:#B7EBC6;\">0.11%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.6.5.3\">0.72%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S4.T3.1.7.6.1\">fake news</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.7.6.2\">-0.13%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.7.6.3\" style=\"background-color:#B7EBC6;\"><span class=\"ltx_text\" id=\"S4.T3.1.7.6.3.1\" style=\"background-color:#B7EBC6;\">1.79%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S4.T3.1.8.7.1\">group bias</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.8.7.2\">-1.04%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.8.7.3\">0.09%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.9.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S4.T3.1.9.8.1\">emotionality</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.9.8.2\" style=\"background-color:#B7EBC6;\"><span class=\"ltx_text\" id=\"S4.T3.1.9.8.2.1\" style=\"background-color:#B7EBC6;\">0.34%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.9.8.3\" style=\"background-color:#ECBDBD;\"><span class=\"ltx_text\" id=\"S4.T3.1.9.8.3.1\" style=\"background-color:#ECBDBD;\">-6.56%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.10.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r\" id=\"S4.T3.1.10.9.1\">stance detection</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T3.1.10.9.2\">-0.79%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T3.1.10.9.3\">-1.83%</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Evaluation of averaged transfer between task families.</figcaption>\n</figure>",
            "capture": "Table 3: Evaluation of averaged transfer between task families."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"A0.T4\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A0.T4.1\" style=\"width:358.9pt;height:1006.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<p class=\"ltx_p\" id=\"A0.T4.1.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1\" style=\"font-size:50%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A0.T4.1.1.1.1\">\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.2.1\">\n<span class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.2.1.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.2.1.1.1\" style=\"color:#000000;\">Task Family</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.2.1.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.2.1.2.1\" style=\"color:#000000;\">Dataset</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.2.1.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.2.1.3.1\" style=\"color:#000000;\"># sentences</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.2.1.4\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.2.1.4.1\" style=\"color:#000000;\">Task</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.3.2\">\n<span class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_4\" id=\"A0.T4.1.1.1.1.3.2.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.3.2.1.1\" style=\"color:#000000;\">Subjective bias</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"A0.T4.1.1.1.1.3.2.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.3.2.2.1\" style=\"color:#000000;\">SUBJ </span><cite class=\"ltx_cite ltx_citemacro_cite\">Pang and Lee <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.3.2.2.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib67\" title=\"\">2004</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.3.2.2.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"A0.T4.1.1.1.1.3.2.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.3.2.3.1\" style=\"color:#000000;\">10.000</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"A0.T4.1.1.1.1.3.2.4\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.3.2.4.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.4.3\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.4.3.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.4.3.1.1\" style=\"color:#000000;\">Wiki Neutrality Corpus </span><cite class=\"ltx_cite ltx_citemacro_cite\">Pryzant et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.4.3.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib70\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.4.3.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.4.3.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.4.3.2.1\" style=\"color:#000000;\">52.036</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.4.3.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.4.3.3.1\" style=\"color:#000000;\">Token-level Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.5.4\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.5.4.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.5.4.1.1\" style=\"color:#000000;\">NewsWCL50 </span><cite class=\"ltx_cite ltx_citemacro_cite\">Hamborg et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.5.4.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib35\" title=\"\">2019</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.5.4.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.5.4.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.5.4.2.1\" style=\"color:#000000;\">731</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.5.4.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.5.4.3.1\" style=\"color:#000000;\">Regression</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.6.5\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.6.5.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.6.5.1.1\" style=\"color:#000000;\">CW_HARD </span><cite class=\"ltx_cite ltx_citemacro_cite\">Hube and Fetahu <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.6.5.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib41\" title=\"\">2019</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.6.5.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.6.5.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.6.5.2.1\" style=\"color:#000000;\">6.843</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.6.5.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.6.5.3.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.7.6\">\n<span class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_7\" id=\"A0.T4.1.1.1.1.7.6.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.7.6.1.1\" style=\"color:#000000;\">News bias</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.7.6.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.7.6.2.1\" style=\"color:#000000;\">MultiDimNews </span><cite class=\"ltx_cite ltx_citemacro_cite\">F\u00e4rber et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.7.6.2.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib31\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.7.6.2.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.7.6.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.7.6.3.1\" style=\"color:#000000;\">2.015</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.7.6.4\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.7.6.4.1\" style=\"color:#000000;\">Multi-Label Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.8.7\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2\" id=\"A0.T4.1.1.1.1.8.7.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.8.7.1.1\" style=\"color:#000000;\">BASIL <cite class=\"ltx_cite ltx_citemacro_cite\">Fan et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib28\" title=\"\">2019</a>)</cite></span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2\" id=\"A0.T4.1.1.1.1.8.7.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.8.7.2.1\" style=\"color:#000000;\">7.987</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.8.7.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.8.7.3.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.9.8\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.9.8.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.9.8.1.1\" style=\"color:#000000;\">Token-Level Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.10.9\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.10.9.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.10.9.1.1\" style=\"color:#000000;\">Starbucks </span><cite class=\"ltx_cite ltx_citemacro_cite\">Lim et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.10.9.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib51\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.10.9.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.10.9.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.10.9.2.1\" style=\"color:#000000;\">866</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.10.9.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.10.9.3.1\" style=\"color:#000000;\">Regression</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.11.10\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.11.10.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.11.10.1.1\" style=\"color:#000000;\">SemEval2023Task3 </span><cite class=\"ltx_cite ltx_citemacro_cite\">Piskorski et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.11.10.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib68\" title=\"\">2023</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.11.10.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.11.10.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.11.10.2.1\" style=\"color:#000000;\">5.219</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.11.10.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.11.10.3.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.12.11\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2\" id=\"A0.T4.1.1.1.1.12.11.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.12.11.1.1\" style=\"color:#000000;\">BABE <cite class=\"ltx_cite ltx_citemacro_cite\">Spinde et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib91\" title=\"\">2021c</a>)</cite></span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2\" id=\"A0.T4.1.1.1.1.12.11.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.12.11.2.1\" style=\"color:#000000;\">3.672</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.12.11.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.12.11.3.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.13.12\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.13.12.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.13.12.1.1\" style=\"color:#000000;\">Token-Level Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.14.13\">\n<span class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_12\" id=\"A0.T4.1.1.1.1.14.13.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.14.13.1.1\" style=\"color:#000000;\">Hate speech</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.14.13.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.14.13.2.1\" style=\"color:#000000;\">OffensiveLanguage </span><cite class=\"ltx_cite ltx_citemacro_cite\">Davidson et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.14.13.2.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib21\" title=\"\">2017</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.14.13.2.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.14.13.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.14.13.3.1\" style=\"color:#000000;\">23.198</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.14.13.4\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.14.13.4.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.15.14\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.15.14.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.15.14.1.1\" style=\"color:#000000;\">OnlineHarassmentDataset </span><cite class=\"ltx_cite ltx_citemacro_cite\">Golbeck et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.15.14.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib33\" title=\"\">2017</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.15.14.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.15.14.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.15.14.2.1\" style=\"color:#000000;\">19.613</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.15.14.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.15.14.3.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.16.15\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.16.15.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.16.15.1.1\" style=\"color:#000000;\">WikiDetoxToxicity </span><cite class=\"ltx_cite ltx_citemacro_cite\">Wulczyn et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.16.15.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib106\" title=\"\">2017a</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.16.15.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.16.15.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.16.15.2.1\" style=\"color:#000000;\">138.827</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.16.15.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.16.15.3.1\" style=\"color:#000000;\">Regression</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.17.16\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2\" id=\"A0.T4.1.1.1.1.17.16.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.17.16.1.1\" style=\"color:#000000;\">WikiDetoxAggression <cite class=\"ltx_cite ltx_citemacro_cite\">Wulczyn et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib106\" title=\"\">2017a</a>)</cite></span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2\" id=\"A0.T4.1.1.1.1.17.16.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.17.16.2.1\" style=\"color:#000000;\">101.159</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.17.16.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.17.16.3.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.18.17\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.18.17.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.18.17.1.1\" style=\"color:#000000;\">Regression</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.19.18\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.19.18.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.19.18.1.1\" style=\"color:#000000;\">Jigsaw </span><cite class=\"ltx_cite ltx_citemacro_cite\">AI <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.19.18.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib2\" title=\"\">2019</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.19.18.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.19.18.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.19.18.2.1\" style=\"color:#000000;\">101.060</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.19.18.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.19.18.3.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.20.19\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.20.19.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.20.19.1.1\" style=\"color:#000000;\">MeTooMA </span><cite class=\"ltx_cite ltx_citemacro_cite\">Gautam et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.20.19.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib32\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.20.19.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.20.19.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.20.19.2.1\" style=\"color:#000000;\">7.388</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.20.19.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.20.19.3.1\" style=\"color:#000000;\">Multi-Label Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.21.20\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.21.20.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.21.20.1.1\" style=\"color:#000000;\">WikiMadlibs </span><cite class=\"ltx_cite ltx_citemacro_cite\">Dixon et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.21.20.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib25\" title=\"\">2018</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.21.20.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.21.20.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.21.20.2.1\" style=\"color:#000000;\">74.972</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.21.20.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.21.20.3.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.22.21\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3\" id=\"A0.T4.1.1.1.1.22.21.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.22.21.1.1\" style=\"color:#000000;\">HateXplain <cite class=\"ltx_cite ltx_citemacro_cite\">Mathew et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib59\" title=\"\">2021</a>)</cite></span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3\" id=\"A0.T4.1.1.1.1.22.21.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.22.21.2.1\" style=\"color:#000000;\">18.962</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.22.21.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.22.21.3.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.23.22\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.23.22.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.23.22.1.1\" style=\"color:#000000;\">Multi-Label Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.24.23\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.24.23.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.24.23.1.1\" style=\"color:#000000;\">Token-Level Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.25.24\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.25.24.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.25.24.1.1\" style=\"color:#000000;\">HateSpeechTwitter </span><cite class=\"ltx_cite ltx_citemacro_cite\">Founta et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.25.24.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib30\" title=\"\">2018</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.25.24.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.25.24.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.25.24.2.1\" style=\"color:#000000;\">48.572</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.25.24.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.25.24.3.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.26.25\">\n<span class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_6\" id=\"A0.T4.1.1.1.1.26.25.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.26.25.1.1\" style=\"color:#000000;\">Gender bias</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.26.25.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.26.25.2.1\" style=\"color:#000000;\">GAP </span><cite class=\"ltx_cite ltx_citemacro_cite\">Webster et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.26.25.2.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib102\" title=\"\">2018</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.26.25.2.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.26.25.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.26.25.3.1\" style=\"color:#000000;\">4.373</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.26.25.4\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.26.25.4.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.27.26\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.27.26.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.27.26.1.1\" style=\"color:#000000;\">RtGender </span><cite class=\"ltx_cite ltx_citemacro_cite\">Voigt et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.27.26.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib96\" title=\"\">2018a</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.27.26.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.27.26.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.27.26.2.1\" style=\"color:#000000;\">21.690</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.27.26.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.27.26.3.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.28.27\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.28.27.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.28.27.1.1\" style=\"color:#000000;\">MDGender </span><cite class=\"ltx_cite ltx_citemacro_cite\">Dinan et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.28.27.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib22\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.28.27.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.28.27.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.28.27.2.1\" style=\"color:#000000;\">2.332</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.28.27.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.28.27.3.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.29.28\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.29.28.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.29.28.1.1\" style=\"color:#000000;\">TRAC2 </span><cite class=\"ltx_cite ltx_citemacro_cite\">Safi\u00a0Samghabadi et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.29.28.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib78\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.29.28.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.29.28.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.29.28.2.1\" style=\"color:#000000;\">3.983</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.29.28.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.29.28.3.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.30.29\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.30.29.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.30.29.1.1\" style=\"color:#000000;\">Funpedia </span><cite class=\"ltx_cite ltx_citemacro_cite\">Miller et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.30.29.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib60\" title=\"\">2017</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.30.29.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.30.29.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.30.29.2.1\" style=\"color:#000000;\">11.256</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.30.29.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.30.29.3.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.31.30\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.31.30.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.31.30.1.1\" style=\"color:#000000;\">WizardsOfWikipedia </span><cite class=\"ltx_cite ltx_citemacro_cite\">Dinan et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.31.30.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib24\" title=\"\">2019</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.31.30.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.31.30.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.31.30.2.1\" style=\"color:#000000;\">29.777</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.31.30.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.31.30.3.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.32.31\">\n<span class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_5\" id=\"A0.T4.1.1.1.1.32.31.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.32.31.1.1\" style=\"color:#000000;\">Sentiment analysis</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.32.31.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.32.31.2.1\" style=\"color:#000000;\">SST2 </span><cite class=\"ltx_cite ltx_citemacro_cite\">Socher et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.32.31.2.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib85\" title=\"\">2013a</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.32.31.2.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.32.31.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.32.31.3.1\" style=\"color:#000000;\">9.436</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.32.31.4\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.32.31.4.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.33.32\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.33.32.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.33.32.1.1\" style=\"color:#000000;\">IMDB </span><cite class=\"ltx_cite ltx_citemacro_cite\">Maas et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.33.32.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib57\" title=\"\">2011a</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.33.32.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.33.32.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.33.32.2.1\" style=\"color:#000000;\">13.139</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.33.32.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.33.32.3.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.34.33\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.34.33.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.34.33.1.1\" style=\"color:#000000;\">MPQA </span><cite class=\"ltx_cite ltx_citemacro_cite\">Wilson <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.34.33.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib105\" title=\"\">2008</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.34.33.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.34.33.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.34.33.2.1\" style=\"color:#000000;\">3.508</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.34.33.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.34.33.3.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.35.34\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.35.34.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.35.34.1.1\" style=\"color:#000000;\">SemEval2014 </span><cite class=\"ltx_cite ltx_citemacro_cite\">Pontiki et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.35.34.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib69\" title=\"\">2014</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.35.34.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.35.34.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.35.34.2.1\" style=\"color:#000000;\">5.794</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.35.34.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.35.34.3.1\" style=\"color:#000000;\">Token-Level Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.36.35\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.36.35.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.36.35.1.1\" style=\"color:#000000;\">AmazonReviews </span><cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.36.35.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib110\" title=\"\">2015b</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.36.35.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.36.35.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.36.35.2.1\" style=\"color:#000000;\">167.396</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.36.35.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.36.35.3.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.37.36\">\n<span class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4\" id=\"A0.T4.1.1.1.1.37.36.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.37.36.1.1\" style=\"color:#000000;\">Fake news</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.37.36.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.37.36.2.1\" style=\"color:#000000;\">LIAR </span><cite class=\"ltx_cite ltx_citemacro_cite\">Wang <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.37.36.2.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib100\" title=\"\">2017a</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.37.36.2.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.37.36.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.37.36.3.1\" style=\"color:#000000;\">12.742</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.37.36.4\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.37.36.4.1\" style=\"color:#000000;\">Regression</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.38.37\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.38.37.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.38.37.1.1\" style=\"color:#000000;\">FakeNewsNet </span><cite class=\"ltx_cite ltx_citemacro_cite\">Shu et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.38.37.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib80\" title=\"\">2020a</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.38.37.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.38.37.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.38.37.2.1\" style=\"color:#000000;\">21.299</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.38.37.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.38.37.3.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.39.38\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2\" id=\"A0.T4.1.1.1.1.39.38.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.39.38.1.1\" style=\"color:#000000;\">PHEME <cite class=\"ltx_cite ltx_citemacro_cite\">Kochkina et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib45\" title=\"\">2018</a>)</cite></span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2\" id=\"A0.T4.1.1.1.1.39.38.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.39.38.2.1\" style=\"color:#000000;\">5.022</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.39.38.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.39.38.3.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.40.39\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.40.39.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.40.39.1.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.41.40\">\n<span class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_5\" id=\"A0.T4.1.1.1.1.41.40.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.41.40.1.1\" style=\"color:#000000;\">Emotionality</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2\" id=\"A0.T4.1.1.1.1.41.40.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.41.40.2.1\" style=\"color:#000000;\">GoodNewsEveryone <cite class=\"ltx_cite ltx_citemacro_cite\">Bostan et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib11\" title=\"\">2020</a>)</cite></span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2\" id=\"A0.T4.1.1.1.1.41.40.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.41.40.3.1\" style=\"color:#000000;\">4.428</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.41.40.4\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.41.40.4.1\" style=\"color:#000000;\">Token-Level Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.42.41\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.42.41.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.42.41.1.1\" style=\"color:#000000;\">Token-Level Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.43.42\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.43.42.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.43.42.1.1\" style=\"color:#000000;\">BU-NEMO </span><cite class=\"ltx_cite ltx_citemacro_cite\">Reardon et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.43.42.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib75\" title=\"\">2022</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.43.42.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.43.42.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.43.42.2.1\" style=\"color:#000000;\">12.576</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.43.42.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.43.42.3.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.44.43\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.44.43.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.44.43.1.1\" style=\"color:#000000;\">EmotionTweets </span><cite class=\"ltx_cite ltx_citemacro_cite\">Krommyda et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.44.43.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib48\" title=\"\">2021b</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.44.43.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.44.43.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.44.43.2.1\" style=\"color:#000000;\">195.744</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.44.43.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.44.43.3.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.45.44\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.45.44.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.45.44.1.1\" style=\"color:#000000;\">DebateEffects </span><cite class=\"ltx_cite ltx_citemacro_cite\">Sridhar and Getoor <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.45.44.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib93\" title=\"\">2019</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.45.44.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.45.44.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.45.44.2.1\" style=\"color:#000000;\">6.941</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.45.44.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.45.44.3.1\" style=\"color:#000000;\">Regression</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.46.45\">\n<span class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_10\" id=\"A0.T4.1.1.1.1.46.45.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.46.45.1.1\" style=\"color:#000000;\">Group bias</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3\" id=\"A0.T4.1.1.1.1.46.45.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.46.45.2.1\" style=\"color:#000000;\">CrowSPairs <cite class=\"ltx_cite ltx_citemacro_cite\">Nangia et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib66\" title=\"\">2020</a>)</cite></span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3\" id=\"A0.T4.1.1.1.1.46.45.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.46.45.3.1\" style=\"color:#000000;\">3.009</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.46.45.4\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.46.45.4.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.47.46\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.47.46.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.47.46.1.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.48.47\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.48.47.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.48.47.1.1\" style=\"color:#000000;\">Token-Level Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.49.48\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2\" id=\"A0.T4.1.1.1.1.49.48.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.49.48.1.1\" style=\"color:#000000;\">StereoSet <cite class=\"ltx_cite ltx_citemacro_cite\">Nadeem et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib65\" title=\"\">2021b</a>)</cite></span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2\" id=\"A0.T4.1.1.1.1.49.48.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.49.48.2.1\" style=\"color:#000000;\">4.170</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.49.48.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.49.48.3.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.50.49\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.50.49.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.50.49.1.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.51.50\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2\" id=\"A0.T4.1.1.1.1.51.50.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.51.50.1.1\" style=\"color:#000000;\">StereotypeDataset <cite class=\"ltx_cite ltx_citemacro_cite\">Pujari et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib71\" title=\"\">2022</a>)</cite></span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2\" id=\"A0.T4.1.1.1.1.51.50.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.51.50.2.1\" style=\"color:#000000;\">2.208</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.51.50.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.51.50.3.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.52.51\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.52.51.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.52.51.1.1\" style=\"color:#000000;\">Multi-Label Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.53.52\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3\" id=\"A0.T4.1.1.1.1.53.52.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.53.52.1.1\" style=\"color:#000000;\">RedditBias <cite class=\"ltx_cite ltx_citemacro_cite\">Barikeri et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib8\" title=\"\">2021b</a>)</cite></span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3\" id=\"A0.T4.1.1.1.1.53.52.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.53.52.2.1\" style=\"color:#000000;\">10.395</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.53.52.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.53.52.3.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.54.53\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.54.53.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.54.53.1.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.55.54\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.55.54.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.55.54.1.1\" style=\"color:#000000;\">Token-Level Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.56.55\">\n<span class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_5\" id=\"A0.T4.1.1.1.1.56.55.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.56.55.1.1\" style=\"color:#000000;\">Stance detection</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.56.55.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.56.55.2.1\" style=\"color:#000000;\">SemEval2023Task4 </span><cite class=\"ltx_cite ltx_citemacro_cite\">Mirzakhmedova et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.56.55.2.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib61\" title=\"\">2023</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.56.55.2.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.56.55.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.56.55.3.1\" style=\"color:#000000;\">5.219</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.56.55.4\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.56.55.4.1\" style=\"color:#000000;\">Binary Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.57.56\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.57.56.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.57.56.1.1\" style=\"color:#000000;\">VaccineLies </span><cite class=\"ltx_cite ltx_citemacro_cite\">Weinzierl and Harabagiu <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.57.56.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib103\" title=\"\">2022</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.57.56.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.57.56.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.57.56.2.1\" style=\"color:#000000;\">4.497</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.57.56.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.57.56.3.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.58.57\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.58.57.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.58.57.1.1\" style=\"color:#000000;\">SemEval2016Task6 </span><cite class=\"ltx_cite ltx_citemacro_cite\">Mohammad et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.58.57.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib62\" title=\"\">2016</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.58.57.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.58.57.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.58.57.2.1\" style=\"color:#000000;\">4.849</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.58.57.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.58.57.3.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.59.58\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.59.58.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.59.58.1.1\" style=\"color:#000000;\">WTWT </span><cite class=\"ltx_cite ltx_citemacro_cite\">Conforti et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.59.58.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib18\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.59.58.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.59.58.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.59.58.2.1\" style=\"color:#000000;\">24.681</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.59.58.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.59.58.3.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.60.59\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.60.59.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.60.59.1.1\" style=\"color:#000000;\">MultiTargetStance </span><cite class=\"ltx_cite ltx_citemacro_cite\">Sobhani et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.60.59.1.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib84\" title=\"\">2017</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.60.59.1.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.60.59.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.60.59.2.1\" style=\"color:#000000;\">4.430</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.60.59.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.60.59.3.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.61.60\">\n<span class=\"ltx_td ltx_border_l ltx_border_r\" id=\"A0.T4.1.1.1.1.61.60.1\"></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.61.60.2\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.61.60.2.1\" style=\"color:#000000;\">GWSD </span><cite class=\"ltx_cite ltx_citemacro_cite\">Luo et\u00a0al. <span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.61.60.2.2.1.1.1\" style=\"color:#000000;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07910v2#bib.bib55\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.61.60.2.3.2.2.1\" style=\"color:#000000;\">)</span></cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.61.60.3\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.61.60.3.1\" style=\"color:#000000;\">2.010</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A0.T4.1.1.1.1.61.60.4\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.61.60.4.1\" style=\"color:#000000;\">Multi-Class Classification</span></span></span>\n<span class=\"ltx_tr\" id=\"A0.T4.1.1.1.1.1\">\n<span class=\"ltx_td ltx_border_b ltx_border_l ltx_border_r ltx_border_tt\" id=\"A0.T4.1.1.1.1.1.2\"></span>\n<span class=\"ltx_td ltx_border_b ltx_border_r ltx_border_tt\" id=\"A0.T4.1.1.1.1.1.3\"></span>\n<span class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_tt\" id=\"A0.T4.1.1.1.1.1.1\"><span class=\"ltx_text\" id=\"A0.T4.1.1.1.1.1.1.1\" style=\"color:#000000;\"> 1.210.084</span></span>\n<span class=\"ltx_td ltx_border_b ltx_border_r ltx_border_tt\" id=\"A0.T4.1.1.1.1.1.4\"></span></span>\n</span>\n</span><span class=\"ltx_text\" id=\"A0.T4.1.1.1.2\" style=\"color:#000000;\"></span></span></p>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:50%;\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>References and description to all 59 Tasks (46 datasets) in LBM collection.</figcaption>\n</figure>",
            "capture": "Table 4: References and description to all 59 Tasks (46 datasets) in LBM collection."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.07910v2_figure_1.png",
            "caption": "Figure 1: \nMAGPIE has a pre-trained representation of multiple biases (persuasive, subjective, etc.).\nThis enables it to outperform models based on single-task learning (STL) paradigms."
        },
        "2": {
            "figure_path": "2403.07910v2_figure_2.png",
            "caption": "Figure 2: The process of training and evaluating MAGPIE. The purple steps describe the construction and usage of LBM, the yellow the model training, and the green the model evaluation."
        },
        "3": {
            "figure_path": "2403.07910v2_figure_3.png",
            "caption": "Figure 3: The workflow of collecting datasets from the initial list to the final LBM collection."
        },
        "4": {
            "figure_path": "2403.07910v2_figure_4.png",
            "caption": "Figure 4: Final F1 score on a test set averaged over 30 random seeds. All three MTL approaches outperform baseline STL finetuning. Pre-finetuning on all LBM tasks results in significantly improved performance."
        },
        "5": {
            "figure_path": "2403.07910v2_figure_5.png",
            "caption": "Figure 5: Evaluation F1 score during the final finetuning where MTL: All shows superior performance in training-step efficiency. The values are averaged over 30 random seeds. The bands mark the lowest and highest values."
        },
        "6": {
            "figure_path": "2403.07910v2_figure_6.png",
            "caption": "Figure 6: Average performance change per task family. Stance detection and hate speech are the only families, on average, benefitting from Multi-Task Learning."
        },
        "7": {
            "figure_path": "2403.07910v2_figure_7.png",
            "caption": "Figure 7: Average performance change when training tasks together. The change is measured with respect to transfer within each respective family (see results in section Figure 6.). The values in the horizontal axis represent the received transfer by the family on the y-axis. E.g., when training Emotionality and Subjective bias together, Emotionality gets worse by 11% whereas Subjectivity improves by 1.5%."
        },
        "8": {
            "figure_path": "2403.07910v2_figure_8.png",
            "caption": "(a) Resurrection"
        },
        "9": {
            "figure_path": "2403.07910v2_figure_9.png",
            "caption": "(b) HSES"
        }
    },
    "references": [
        {
            "1": {
                "title": "Muppet: Massive multi-task representations with pre-finetuning.",
                "author": "Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. 2021.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5799\u20135811, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.emnlp-main.468"
            }
        },
        {
            "2": {
                "title": "Jigsaw unintended bias in toxicity classification.",
                "author": "Jigsaw/Conversation AI. 2019.",
                "venue": null,
                "url": "https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/data"
            }
        },
        {
            "3": {
                "title": "Stance detection on social media: State of the art and trends.",
                "author": "Abeer ALDayel and Walid Magdy. 2021.",
                "venue": "Information Processing & Management, 58(4):102597.",
                "url": "https://doi.org/10.1016/j.ipm.2021.102597"
            }
        },
        {
            "4": {
                "title": "How reliable are model diagnostics?",
                "author": "Vamsi Aribandi, Yi Tay, and Donald Metzler. 2021a.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1778\u20131785, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.findings-acl.155"
            }
        },
        {
            "5": {
                "title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning.",
                "author": "Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Prakash Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler. 2021b.",
                "venue": "CoRR, abs/2111.10952.",
                "url": "https://arxiv.org/abs/2111.10952"
            }
        },
        {
            "6": {
                "title": "Gendered Ambiguous Pronouns Shared Task: Boosting Model Confidence by Evidence Pooling.",
                "author": "Sandeep Attree. 2019.",
                "venue": null,
                "url": "https://doi.org/10.48550/ARXIV.1906.00839"
            }
        },
        {
            "7": {
                "title": "RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models.",
                "author": "Soumya Barikeri, Anne Lauscher, Ivan Vuli\u0107, and Goran Glava\u0161. 2021a.",
                "venue": null,
                "url": "https://doi.org/10.48550/ARXIV.2106.03521"
            }
        },
        {
            "8": {
                "title": "RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models.",
                "author": "Soumya Barikeri, Anne Lauscher, Ivan Vuli\u0107, and Goran Glava\u0161. 2021b.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1941\u20131955, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.acl-long.151"
            }
        },
        {
            "9": {
                "title": "Testing and Comparing Computational Approaches for Identifying the Language of Framing in Political News.",
                "author": "Eric Baumer, Elisha Elovic, Ying Qin, Francesca Polletta, and Geri Gay. 2015.",
                "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1472\u20131482, Denver, Colorado. Association for Computational Linguistics.",
                "url": "https://doi.org/10.3115/v1/N15-1171"
            }
        },
        {
            "10": {
                "title": "Identifying beneficial task relations for multi-task learning in deep neural networks.",
                "author": "Joachim Bingel and Anders S\u00f8gaard. 2017.",
                "venue": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 164\u2013169, Valencia, Spain. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/E17-2026"
            }
        },
        {
            "11": {
                "title": "GoodNewsEveryone: A Corpus of News Headlines Annotated with Emotions, Semantic Roles, and Reader Perception.",
                "author": "Laura Ana Maria Bostan, Evgeny Kim, and Roman Klinger. 2020.",
                "venue": "In Proceedings of the 12th Language Resources and Evaluation Conference, pages 1554\u20131566, Marseille, France. European Language Resources Association.",
                "url": "https://aclanthology.org/2020.lrec-1.194"
            }
        },
        {
            "12": {
                "title": "Natural Fibre Twines, 3rd edition.",
                "author": "BSI. 1973a.",
                "venue": "British Standards Institution, London.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Natural fibre twines.",
                "author": "BSI. 1973b.",
                "venue": "BS 2570, British Standards Institution, London.",
                "url": null
            }
        },
        {
            "14": {
                "title": "The use of user modelling to guide inference and learning.",
                "author": "A. Castor and L. E. Pollux. 1992.",
                "venue": "Applied Intelligence, 2(1):37\u201353.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Multi-Task Learning in Natural Language Processing: An Overview.",
                "author": "Shijie Chen, Yu Zhang, and Qiang Yang. 2021.",
                "venue": "arXiv:2109.09138 [cs].",
                "url": "http://arxiv.org/abs/2109.09138"
            }
        },
        {
            "16": {
                "title": "Case-Based Reasoning, 2nd edition.",
                "author": "J.L. Chercheur. 1994.",
                "venue": "Morgan Kaufman Publishers, San Mateo, CA.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Conditions on transformations.",
                "author": "N. Chomsky. 1973.",
                "venue": "In A festschrift for Morris Halle, New York. Holt, Rinehart & Winston.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Will-they-won\u2019t-they: A very large dataset for stance detection on Twitter.",
                "author": "Costanza Conforti, Jakob Berndt, Mohammad Taher Pilehvar, Chryssi Giannitsarou, Flavio Toxvaerd, and Nigel Collier. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1715\u20131724, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.157"
            }
        },
        {
            "19": {
                "title": "Does Gender Matter in the News? Detecting and Examining Gender Bias in News Articles.",
                "author": "Jamell Dacon and Haochen Liu. 2021.",
                "venue": "In Companion Proceedings of the Web Conference 2021, WWW \u201921, pages 385\u2013392, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/3442442.3452325"
            }
        },
        {
            "20": {
                "title": "Trump vs. Hillary: What Went Viral During the 2016 US Presidential Election.",
                "author": "Kareem Darwish, Walid Magdy, and Tahar Zanouda. 2017.",
                "venue": "In Social Informatics, Lecture Notes in Computer Science, pages 143\u2013161, Cham. Springer International Publishing.",
                "url": "https://doi.org/10.1007/978-3-319-67217-5_10"
            }
        },
        {
            "21": {
                "title": "Automated hate speech detection and the problem of offensive language.",
                "author": "Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017.",
                "venue": "In Proceedings of the International AAAI Conference on Web and Social Media, volume 11.",
                "url": "https://doi.org/https://doi.org/10.1609/icwsm.v11i1.14955"
            }
        },
        {
            "22": {
                "title": "Multi-dimensional gender bias classification.",
                "author": "Emily Dinan, Angela Fan, Ledell Wu, Jason Weston, Douwe Kiela, and Adina Williams. 2020.",
                "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 314\u2013331, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.emnlp-main.23"
            }
        },
        {
            "23": {
                "title": "Wizard of Wikipedia: Knowledge-Powered Conversational agents.",
                "author": "Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2018.",
                "venue": null,
                "url": "https://doi.org/10.48550/ARXIV.1811.01241"
            }
        },
        {
            "24": {
                "title": "Wizard of Wikipedia: Knowledge-powered Conversational Agents.",
                "author": "Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2019.",
                "venue": "In Proceedings of the International Conference on Learning Representations (ICLR).",
                "url": null
            }
        },
        {
            "25": {
                "title": "Measuring and Mitigating Unintended Bias in Text Classification.",
                "author": "Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. 2018.",
                "venue": "In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, AIES \u201918, pages 67\u201373, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/3278721.3278729"
            }
        },
        {
            "26": {
                "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping.",
                "author": "Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. 2020.",
                "venue": null,
                "url": "https://doi.org/10.48550/ARXIV.2002.06305"
            }
        },
        {
            "27": {
                "title": "The Limits of Interpretation.",
                "author": "Umberto Eco. 1990.",
                "venue": "Indian University Press.",
                "url": null
            }
        },
        {
            "28": {
                "title": "In plain sight: Media bias through the lens of factual reporting.",
                "author": "Lisa Fan, Marshall White, Eva Sharma, Ruisi Su, Prafulla Kumar Choubey, Ruihong Huang, and Lu Wang. 2019.",
                "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6343\u20136349, Hong Kong, China. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D19-1664"
            }
        },
        {
            "29": {
                "title": "Emergent: a novel data-set for stance classification.",
                "author": "W. Ferreira and A. Vlachos. 2016.",
                "venue": null,
                "url": "http://aclweb.org/anthology/N/N16/N16-1138.pdf"
            }
        },
        {
            "30": {
                "title": "Large Scale Crowdsourcing and Characterization of Twitter Abusive Behavior.",
                "author": "Antigoni Founta, Constantinos Djouvas, Despoina Chatzakou, Ilias Leontiadis, Jeremy Blackburn, Gianluca Stringhini, Athena Vakali, Michael Sirivianos, and Nicolas Kourtellis. 2018.",
                "venue": "Proceedings of the International AAAI Conference on Web and Social Media, 12(1).",
                "url": "https://doi.org/10.1609/icwsm.v12i1.14991"
            }
        },
        {
            "31": {
                "title": "A Multidimensional Dataset Based on Crowdsourcing for Analyzing and Detecting News Bias.",
                "author": "Michael F\u00e4rber, Victoria Burkard, Adam Jatowt, and Sora Lim. 2020.",
                "venue": "In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, CIKM \u201920, pages 3007\u20133014, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/3340531.3412876"
            }
        },
        {
            "32": {
                "title": "#MeTooMA: Multi-Aspect Annotations of Tweets Related to the MeToo Movement.",
                "author": "Akash Gautam, Puneet Mathur, Rakesh Gosangi, Debanjan Mahata, Ramit Sawhney, and Rajiv Ratn Shah. 2020.",
                "venue": "Proceedings of the International AAAI Conference on Web and Social Media, 14:209\u2013216.",
                "url": "https://ojs.aaai.org/index.php/ICWSM/article/view/7292"
            }
        },
        {
            "33": {
                "title": "A Large Labeled Corpus for Online Harassment Research.",
                "author": "Jennifer Golbeck, Zahra Ashktorab, Rashad O. Banjo, Alexandra Berlinger, Siddharth Bhagwan, Cody Buntain, Paul Cheakalos, Alicia A. Geller, Quint Gergory, Rajesh Kumar Gnanasekaran, Raja Rajan Gunasekaran, Kelly M. Hoffman, Jenny Hottle, Vichita Jienjitlert, Shivika Khare, Ryan Lau, Marianna J. Martindale, Shalmali Naik, Heather L. Nixon, Piyush Ramachandran, Kristine M. Rogers, Lisa Rogers, Meghna Sardana Sarin, Gaurav Shahane, Jayanee Thanki, Priyanka Vengataraman, Zijian Wan, and Derek Michael Wu. 2017.",
                "venue": "In Proceedings of the 2017 ACM on Web Science Conference, pages 229\u2013233, Troy New York USA. ACM.",
                "url": "https://doi.org/10.1145/3091478.3091509"
            }
        },
        {
            "34": {
                "title": "Automatic Detection of Sexist Statements Commonly Used at the Workplace.",
                "author": "Dylan Grosz and Patricia Conde-Cespedes. 2020.",
                "venue": "In Wei Lu and Kenny Q. Zhu, editors, Trends and Applications in Knowledge Discovery and Data Mining, volume 12237, pages 104\u2013115. Springer International Publishing, Cham.",
                "url": "https://doi.org/10.1007/978-3-030-60470-7_11"
            }
        },
        {
            "35": {
                "title": "Automated identification of media bias in news articles: an interdisciplinary literature review.",
                "author": "Felix Hamborg, Karsten Donnay, and Bela Gipp. 2019.",
                "venue": "International Journal on Digital Libraries, 20(4):391\u2013415.",
                "url": "https://doi.org/10.1007/s00799-018-0261-y"
            }
        },
        {
            "36": {
                "title": "An interactive multi-task learning network for end-to-end aspect-based sentiment analysis.",
                "author": "Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel Dahlmeier. 2019.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 504\u2013515, Florence, Italy. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P19-1048"
            }
        },
        {
            "37": {
                "title": "Infusing Knowledge from Wikipedia to Enhance Stance Detection.",
                "author": "Zihao He, Negar Mokhberian, and Kristina Lerman. 2022.",
                "venue": null,
                "url": "https://doi.org/10.48550/ARXIV.2204.03839"
            }
        },
        {
            "38": {
                "title": "Elementary Statistics, 3rd edition.",
                "author": "Paul Gerhard Hoel. 1971a.",
                "venue": "Wiley series in probability and mathematical statistics. Wiley, New York, Chichester.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Elementary Statistics, 3rd edition, Wiley series in probability and mathematical statistics, pages 19\u201333. Wiley, New York, Chichester.",
                "author": "Paul Gerhard Hoel. 1971b.",
                "venue": "ISBN 0 471 40300.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Detecting Biased Statements in Wikipedia.",
                "author": "Christoph Hube and Besnik Fetahu. 2018.",
                "venue": "In Companion of the The Web Conference 2018 on The Web Conference 2018 - WWW \u201918, pages 1779\u20131786, Lyon, France. ACM Press.",
                "url": "https://doi.org/10.1145/3184558.3191640"
            }
        },
        {
            "41": {
                "title": "Neural Based Statement Classification for Biased Language.",
                "author": "Christoph Hube and Besnik Fetahu. 2019.",
                "venue": "In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM \u201919, pages 195\u2013203, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/3289600.3291018"
            }
        },
        {
            "42": {
                "title": "Us vs. Them: A Dataset of Populist Attitudes, News Bias and Emotions.",
                "author": "Pere-Llu\u00eds Huguet Cabot, David Abadi, Agneta Fischer, and Ekaterina Shutova. 2021.",
                "venue": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1921\u20131945. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.eacl-main.165"
            }
        },
        {
            "43": {
                "title": "Language: Its Nature, Development, and Origin.",
                "author": "Otto Jespersen. 1922.",
                "venue": "Allen and Unwin.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Analyzing multi-task learning for abstractive text summarization.",
                "author": "Frederic Thomas Kirstein, Jan Philip Wahle, Terry Ruas, and Bela Gipp. 2022.",
                "venue": "In Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM), pages 54\u201377, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2022.gem-1.5"
            }
        },
        {
            "45": {
                "title": "All-in-one: Multi-task learning for rumour verification.",
                "author": "Elena Kochkina, Maria Liakata, and Arkaitz Zubiaga. 2018.",
                "venue": "In Proceedings of the 27th International Conference on Computational Linguistics, pages 3402\u20133413, Santa Fe, New Mexico, USA. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/C18-1288"
            }
        },
        {
            "46": {
                "title": "A Domain-adaptive Pre-training Approach for Language Bias Detection in News.",
                "author": "David Krieger, Timo Spinde, Terry Ruas, Juhi Kulshrestha, and Bela Gipp. 2022.",
                "venue": "In 2022 ACM/IEEE Joint Conference on Digital Libraries (JCDL), Cologne, Germany.",
                "url": "https://doi.org/10.1145/3529372.3530932"
            }
        },
        {
            "47": {
                "title": "An Experimental Analysis of Data Annotation Methodologies for Emotion Detection in Short Text Posted on Social Media.",
                "author": "Maria Krommyda, Anastasios Rigos, Kostas Bouklas, and Angelos Amditis. 2021a.",
                "venue": "Informatics, 8(1):19.",
                "url": "https://doi.org/10.3390/informatics8010019"
            }
        },
        {
            "48": {
                "title": "An Experimental Analysis of Data Annotation Methodologies for Emotion Detection in Short Text Posted on Social Media.",
                "author": "Maria Krommyda, Anastasios Rigos, Kostas Bouklas, and Angelos Amditis. 2021b.",
                "venue": "Informatics, 8(1):19.",
                "url": "https://doi.org/10.3390/informatics8010019"
            }
        },
        {
            "49": {
                "title": "Mitigating Media Bias through Neutral Article Generation.",
                "author": "Nayeon Lee, Yejin Bang, Andrea Madotto, and Pascale Fung. 2021a.",
                "venue": "CoRR, abs/2104.00336.",
                "url": "https://doi.org/10.48550/arXiv.2104.00336"
            }
        },
        {
            "50": {
                "title": "On unifying misinformation detection.",
                "author": "Nayeon Lee, Belinda Z. Li, Sinong Wang, Pascale Fung, Hao Ma, Wen-tau Yih, and Madian Khabsa. 2021b.",
                "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5479\u20135485, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.naacl-main.432"
            }
        },
        {
            "51": {
                "title": "Annotating and Analyzing Biased Sentences in News Articles using Crowdsourcing.",
                "author": "Sora Lim, Adam Jatowt, Michael F\u00e4rber, and Masatoshi Yoshikawa. 2020.",
                "venue": "In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 1478\u20131484, Marseille, France. European Language Resources Association.",
                "url": "https://aclanthology.org/2020.lrec-1.184"
            }
        },
        {
            "52": {
                "title": "Sentiment Analysis and Opinion Mining.",
                "author": "Bing Liu. 2012.",
                "venue": "Synthesis Lectures on Human Language Technologies. Springer International Publishing, Cham.",
                "url": "https://doi.org/10.1007/978-3-031-02145-9"
            }
        },
        {
            "53": {
                "title": "Enhancing Zero-shot and Few-shot Stance Detection with Commonsense Knowledge Graph.",
                "author": "Rui Liu, Zheng Lin, Yutong Tan, and Weiping Wang. 2021.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3152\u20133157, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.findings-acl.278"
            }
        },
        {
            "54": {
                "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach.",
                "author": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.",
                "venue": "CoRR, abs/1907.11692.",
                "url": "http://arxiv.org/abs/1907.11692"
            }
        },
        {
            "55": {
                "title": "DeSMOG: Detecting Stance in Media On Global Warming.",
                "author": "Yiwei Luo, Dallas Card, and Dan Jurafsky. 2020.",
                "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3296\u20133315, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.findings-emnlp.296"
            }
        },
        {
            "56": {
                "title": "GradTS: A gradient-based automatic auxiliary task selection method based on transformer networks.",
                "author": "Weicheng Ma, Renze Lou, Kai Zhang, Lili Wang, and Soroush Vosoughi. 2021.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5621\u20135632, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.emnlp-main.455"
            }
        },
        {
            "57": {
                "title": "Learning Word Vectors for Sentiment Analysis.",
                "author": "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011a.",
                "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142\u2013150, Portland, Oregon, USA. Association for Computational Linguistics.",
                "url": "https://www.aclweb.org/anthology/P11-1015"
            }
        },
        {
            "58": {
                "title": "Learning Word Vectors for Sentiment Analysis.",
                "author": "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011b.",
                "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142\u2013150, Portland, Oregon, USA. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/P11-1015"
            }
        },
        {
            "59": {
                "title": "HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection.",
                "author": "Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee. 2021.",
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 35(17):14867\u201314875.",
                "url": "https://doi.org/10.1609/aaai.v35i17.17745"
            }
        },
        {
            "60": {
                "title": "ParlAI: A dialog research software platform.",
                "author": "Alexander Miller, Will Feng, Dhruv Batra, Antoine Bordes, Adam Fisch, Jiasen Lu, Devi Parikh, and Jason Weston. 2017.",
                "venue": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 79\u201384, Copenhagen, Denmark. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D17-2014"
            }
        },
        {
            "61": {
                "title": "The touch\u00e923-valueeval dataset for identifying human values behind arguments.",
                "author": "Nailia Mirzakhmedova, Johannes Kiesel, Milad Alshomary, Maximilian Heinrich, Nicolas Handke, Xiaoni Cai, Barriere Valentin, Doratossadat Dastgheib, Omid Ghahroodi, Mohammad Ali Sadraei, Ehsaneddin Asgari, Lea Kawaletz, Henning Wachsmuth, and Benno Stein. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2301.13771"
            }
        },
        {
            "62": {
                "title": "Semeval-2016 task 6: Detecting stance in tweets.",
                "author": "Saif Mohammad, Svetlana Kiritchenko, Parinaz Sobhani, Xiaodan Zhu, and Colin Cherry. 2016.",
                "venue": "In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 31\u201341.",
                "url": "https://doi.org/https://doi.org/10.18653/v1/s16-1003"
            }
        },
        {
            "63": {
                "title": "Stance and Sentiment in Tweets.",
                "author": "Saif M. Mohammad, Parinaz Sobhani, and Svetlana Kiritchenko. 2017.",
                "venue": "ACM Transactions on Internet Technology, 17(3):26:1\u201326:23.",
                "url": "https://doi.org/10.1145/3003433"
            }
        },
        {
            "64": {
                "title": "StereoSet: Measuring stereotypical bias in pretrained language models.",
                "author": "Moin Nadeem, Anna Bethke, and Siva Reddy. 2021a.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5356\u20135371, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.acl-long.416"
            }
        },
        {
            "65": {
                "title": "StereoSet: Measuring stereotypical bias in pretrained language models.",
                "author": "Moin Nadeem, Anna Bethke, and Siva Reddy. 2021b.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5356\u20135371. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.acl-long.416"
            }
        },
        {
            "66": {
                "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models.",
                "author": "Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020.",
                "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953\u20131967, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.emnlp-main.154"
            }
        },
        {
            "67": {
                "title": "A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts.",
                "author": "Bo Pang and Lillian Lee. 2004.",
                "venue": "In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL \u201904, pages 271\u2013es, USA. Association for Computational Linguistics.",
                "url": "https://doi.org/10.3115/1218955.1218990"
            }
        },
        {
            "68": {
                "title": "News categorization, framing and persuasion techniques: Annotation guidelines.",
                "author": "Jakub Piskorski, Nicolas Stefanovitch, Valerie-Anne Bausier, Nicolo Faggiani, Jens Linge, Sopho Kharazi, Nikolaos Nikolaidis, Giulia Teodori, Bertrand De Longueville, Brian Doherty, Jason Gonin, Camelia Ignat, Bonka Kotseva, Eleonora Mantica, Lorena Marcaletti, Enrico Rossi, Alessio Spadaro, Marco Verile, Giovanni Da San Martino, Firoj Alam, and Preslav Nakov. 2023.",
                "venue": "Technical report, European Commission Joint Research Centre, Ispra (Italy).",
                "url": "https://knowledge4policy.ec.europa.eu/text-mining/news-categorization-framing-persuasion-techniques-annotation-guidelines_en"
            }
        },
        {
            "69": {
                "title": "SemEval-2014 Task 4: Aspect Based Sentiment Analysis.",
                "author": "Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014.",
                "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27\u201335, Dublin, Ireland. Association for Computational Linguistics.",
                "url": "https://doi.org/10.3115/v1/S14-2004"
            }
        },
        {
            "70": {
                "title": "Automatically neutralizing subjective bias in text.",
                "author": "Reid Pryzant, Richard Diehl Martinez, Nathan Dass, Sadao Kurohashi, Dan Jurafsky, and Diyi Yang. 2020.",
                "venue": "In Proceedings of the aaai conference on artificial intelligence, volume 34, pages 480\u2013489.",
                "url": "https://doi.org/10.1609/aaai.v34i01.5385"
            }
        },
        {
            "71": {
                "title": "Reinforcement Guided Multi-Task Learning Framework for Low-Resource Stereotype Detection.",
                "author": "Rajkumar Pujari, Erik Oveson, Priyanka Kulkarni, and Elnaz Nouri. 2022.",
                "venue": "Technical report.",
                "url": "https://ui.adsabs.harvard.edu/abs/2022arXiv220314349P"
            }
        },
        {
            "72": {
                "title": "Effective use of Spearman\u2019s and Kendall\u2019s correlation coefficients for association between two measured traits.",
                "author": "Marie-Therese Puth, Markus Neuh\u00e4user, and Graeme D. Ruxton. 2015.",
                "venue": "Animal Behaviour, 102:77\u201384.",
                "url": "https://doi.org/10.1016/j.anbehav.2015.01.010"
            }
        },
        {
            "73": {
                "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.",
                "author": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.",
                "venue": "Journal of Machine Learning Research, 21(140):1\u201367.",
                "url": "http://jmlr.org/papers/v21/20-074.html"
            }
        },
        {
            "74": {
                "title": "Dbias: Detecting biases and ensuring fairness in news articles.",
                "author": "Shaina Raza, Deepak John Reji, and Chen Ding. 2022.",
                "venue": "International Journal of Data Science and Analytics.",
                "url": "https://doi.org/10.1007/s41060-022-00359-4"
            }
        },
        {
            "75": {
                "title": "BU-NEmo: an Affective Dataset of Gun Violence News.",
                "author": "Carley Reardon, Sejin Paik, Ge Gao, Meet Parekh, Yanling Zhao, Lei Guo, Margrit Betke, and Derry Wijaya. 2022.",
                "venue": "Proceedings of the 13th Conference on Language Resources and Evaluation (LREC 2022), pages 2507\u20132516.",
                "url": "http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.267.pdf"
            }
        },
        {
            "76": {
                "title": "Linguistic Models for Analyzing and Detecting Biased Language.",
                "author": "Marta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. 2013.",
                "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1650\u20131659, Sofia, Bulgaria. Association for Computational Linguistics.",
                "url": "https://www.aclweb.org/anthology/P13-1162"
            }
        },
        {
            "77": {
                "title": "Learning to select data for transfer learning with Bayesian optimization.",
                "author": "Sebastian Ruder and Barbara Plank. 2017.",
                "venue": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 372\u2013382, Copenhagen, Denmark. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D17-1038"
            }
        },
        {
            "78": {
                "title": "Aggression and Misogyny Detection using BERT: A Multi-Task Approach.",
                "author": "Niloofar Safi Samghabadi, Parth Patwa, Srinivas PYKL, Prerana Mukherjee, Amitava Das, and Thamar Solorio. 2020.",
                "venue": "In Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying, pages 126\u2013131, Marseille, France. European Language Resources Association (ELRA).",
                "url": "https://aclanthology.org/2020.trac-1.20"
            }
        },
        {
            "79": {
                "title": "\"Call me sexist, but\u2026\": Revisiting Sexism Detection Using Psychological Scales and Adversarial Samples.",
                "author": "Mattia Samory, Indira Sen, Julian Kohne, Fabian Floeck, and Claudia Wagner. 2020.",
                "venue": null,
                "url": "https://doi.org/10.48550/ARXIV.2004.12764"
            }
        },
        {
            "80": {
                "title": "FakeNewsNet: A Data Repository with News Content, Social Context, and Spatiotemporal Information for Studying Fake News on Social Media.",
                "author": "Kai Shu, Deepak Mahudeswaran, Suhang Wang, Dongwon Lee, and Huan Liu. 2020a.",
                "venue": "Big Data, 8(3):171\u2013188.",
                "url": "https://doi.org/10.1089/big.2020.0062"
            }
        },
        {
            "81": {
                "title": "FakeNewsNet: A Data Repository with News Content, Social Context, and Spatiotemporal Information for Studying Fake News on Social Media.",
                "author": "Kai Shu, Deepak Mahudeswaran, Suhang Wang, Dongwon Lee, and Huan Liu. 2020b.",
                "venue": "Big Data, 8(3):171\u2013188.",
                "url": "https://doi.org/10.1089/big.2020.0062"
            }
        },
        {
            "82": {
                "title": "A history of technology.",
                "author": "Charles Joseph Singer, E. J. Holmyard, and A. R. Hall, editors. 1954\u201358.",
                "venue": "Oxford University Press, London.",
                "url": null
            }
        },
        {
            "83": {
                "title": "Determining Subjective Bias in Text through Linguistically Informed Transformer based Multi-Task Network.",
                "author": "Manjira Sinha and Tirthankar Dasgupta. 2021.",
                "venue": "In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 3418\u20133422. ACM.",
                "url": "https://doi.org/10.1145/3459637.3482084"
            }
        },
        {
            "84": {
                "title": "A Dataset for Multi-Target Stance Detection.",
                "author": "Parinaz Sobhani, Diana Inkpen, and Xiaodan Zhu. 2017.",
                "venue": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 551\u2013557, Valencia, Spain. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/E17-2088"
            }
        },
        {
            "85": {
                "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank.",
                "author": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013a.",
                "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/D13-1170"
            }
        },
        {
            "86": {
                "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank.",
                "author": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013b.",
                "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/D13-1170"
            }
        },
        {
            "87": {
                "title": "The media bias taxonomy: A systematic literature review on the forms and automated detection of media bias.",
                "author": "Timo Spinde, Smi Hinterreiter, Fabian Haak, Terry Ruas, Helge Giese, Norman Meuschke, and Bela Gipp. 2023.",
                "venue": "CSUR.",
                "url": null
            }
        },
        {
            "88": {
                "title": "Do You Think It\u2019s Biased? How To Ask For The Perception Of Media Bias.",
                "author": "Timo Spinde, Christina Kreuter, Wolfgang Gaissmaier, Felix Hamborg, Bela Gipp, and Helge Giese. 2021a.",
                "venue": "In Proceedings of the ACM/IEEE Joint Conference on Digital Libraries (JCDL), pages 61\u201369.",
                "url": "https://doi.org/10.1109/JCDL52503.2021.00018"
            }
        },
        {
            "89": {
                "title": "Towards A Reliable Ground-Truth For Biased Language Detection.",
                "author": "Timo Spinde, David Krieger, Manu Plank, and Bela Gipp. 2021b.",
                "venue": "In Proceedings of the ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL), Virtual Event.",
                "url": "https://doi.org/10.1109/JCDL52503.2021.00053"
            }
        },
        {
            "90": {
                "title": "Exploiting transformer-based multitask learning for the detection of media bias in news articles.",
                "author": "Timo Spinde, Jan-David Krieger, Terry Ruas, Jelena Mitrovi\u0107, Franz G\u00f6tz-Hahn, Akiko Aizawa, and Bela Gipp. 2022.",
                "venue": "In Proceedings of the iConference 2022, Virtual event.",
                "url": "https://doi.org/10.1007/978-3-030-96957-8_20"
            }
        },
        {
            "91": {
                "title": "Neural Media Bias Detection Using Distant Supervision With BABE - Bias Annotations By Experts.",
                "author": "Timo Spinde, Manuel Plank, Jan-David Krieger, Terry Ruas, Bela Gipp, and Akiko Aizawa. 2021c.",
                "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1166\u20131177, Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.findings-emnlp.101"
            }
        },
        {
            "92": {
                "title": "Automated identification of bias inducing words in news articles using linguistic and context-oriented features.",
                "author": "Timo Spinde, Lada Rudnitckaia, Jelena Mitrovi\u0107, Felix Hamborg, Michael Granitzer, Bela Gipp, and Karsten Donnay. 2021d.",
                "venue": "Information Processing & Management, 58(3):102505.",
                "url": "https://doi.org/https://doi.org/10.1016/j.ipm.2021.102505"
            }
        },
        {
            "93": {
                "title": "Estimating causal effects of tone in online debates.",
                "author": "Dhanya Sridhar and Lise Getoor. 2019.",
                "venue": "In Proceedings of the 28th International Joint Conference on Artificial Intelligence, IJCAI\u201919, page 1872\u20131878. AAAI Press.",
                "url": "https://doi.org/10.5555/3367243.3367298"
            }
        },
        {
            "94": {
                "title": "Temporal tagging on different domains: Challenges, strategies, and gold standards.",
                "author": "Jannik Str\u00f6tgen and Michael Gertz. 2012.",
                "venue": "In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC\u201912), pages 3746\u20133753, Istanbul, Turkey. European Language Resource Association (ELRA).",
                "url": null
            }
        },
        {
            "95": {
                "title": "Superheroes experiences with books, 20th edition.",
                "author": "S. Superman, B. Batman, C. Catwoman, and S. Spiderman. 2000.",
                "venue": "The Phantom Editors Associates, Gotham City.",
                "url": null
            }
        },
        {
            "96": {
                "title": "RtGender: A Corpus for Studying Differential Responses to Gender.",
                "author": "Rob Voigt, David Jurgens, Vinodkumar Prabhakaran, Dan Jurafsky, and Yulia Tsvetkov. 2018a.",
                "venue": "In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).",
                "url": "https://aclanthology.org/L18-1445"
            }
        },
        {
            "97": {
                "title": "RtGender: A Corpus for Studying Differential Responses to Gender.",
                "author": "Rob Voigt, David Jurgens, Vinodkumar Prabhakaran, Dan Jurafsky, and Yulia Tsvetkov. 2018b.",
                "venue": "In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).",
                "url": "https://aclanthology.org/L18-1445"
            }
        },
        {
            "98": {
                "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems.",
                "author": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019.",
                "venue": "In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 3261\u20133275.",
                "url": "https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html"
            }
        },
        {
            "99": {
                "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding.",
                "author": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018.",
                "venue": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\u2013355, Brussels, Belgium. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/W18-5446"
            }
        },
        {
            "100": {
                "title": "\"Liar, Liar Pants on Fire\": A New Benchmark Dataset for Fake News Detection.",
                "author": "William Yang Wang. 2017a.",
                "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 422\u2013426, Vancouver, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P17-2067"
            }
        },
        {
            "101": {
                "title": "\"Liar, Liar Pants on Fire\": A New Benchmark Dataset for Fake News Detection.",
                "author": "William Yang Wang. 2017b.",
                "venue": null,
                "url": "https://doi.org/10.48550/ARXIV.1705.00648"
            }
        },
        {
            "102": {
                "title": "Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns.",
                "author": "Kellie Webster, Marta Recasens, Vera Axelrod, and Jason Baldridge. 2018.",
                "venue": "Transactions of the Association for Computational Linguistics, 6:605\u2013617.",
                "url": "https://doi.org/10.1162/tacl_a_00240"
            }
        },
        {
            "103": {
                "title": "VaccineLies: A Natural Language Resource for Learning to Recognize Misinformation about the COVID-19 and HPV Vaccines.",
                "author": "Maxwell Weinzierl and Sanda Harabagiu. 2022.",
                "venue": "In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 6967\u20136975, Marseille, France. European Language Resources Association.",
                "url": "https://aclanthology.org/2022.lrec-1.753"
            }
        },
        {
            "104": {
                "title": "Introducing MBIB - The First Media Bias Identification Benchmark Task and Dataset Collection.",
                "author": "Martin Wessel, Tomas Horych, Terry Ruas, Akiko Aizawa, Bela Gipp, and Timo Spinde. 2023.",
                "venue": "In Proceedings of 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR\u201923), New York, NY, USA. ACM.",
                "url": "https://doi.org/https://doi.org/10.1145/3539618.3591882"
            }
        },
        {
            "105": {
                "title": "Fine-Grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity, and Attitudes of Private States.",
                "author": "Theresa Ann Wilson. 2008.",
                "venue": "Ph.D. thesis, USA.",
                "url": "https://doi.org/10.5555/1559698"
            }
        },
        {
            "106": {
                "title": "Ex Machina: Personal Attacks Seen at Scale.",
                "author": "Ellery Wulczyn, Nithum Thain, and Lucas Dixon. 2017a.",
                "venue": "In Proceedings of the 26th International Conference on World Wide Web, WWW 2017, Perth, Australia, April 3-7, 2017, pages 1391\u20131399. ACM.",
                "url": "https://doi.org/10.1145/3038912.3052591"
            }
        },
        {
            "107": {
                "title": "Ex Machina: Personal Attacks Seen at Scale.",
                "author": "Ellery Wulczyn, Nithum Thain, and Lucas Dixon. 2017b.",
                "venue": "In Proceedings of the 26th International Conference on World Wide Web, WWW \u201917, pages 1391\u20131399, Republic and Canton of Geneva, CHE. International World Wide Web Conferences Steering Committee.",
                "url": "https://doi.org/10.1145/3038912.3052591"
            }
        },
        {
            "108": {
                "title": "Gradient surgery for multi-task learning.",
                "author": "Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. 2020.",
                "venue": "In Advances in Neural Information Processing Systems, volume 33, pages 5824\u20135836. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper/2020/file/3fe78a8acf5fda99de95303940a2420c-Paper.pdf"
            }
        },
        {
            "109": {
                "title": "Character-level convolutional networks for text classification.",
                "author": "Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015a.",
                "venue": "In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS\u201915, pages 649\u2013657, Cambridge, MA, USA. MIT Press.",
                "url": null
            }
        },
        {
            "110": {
                "title": "Character-level Convolutional Networks for Text Classification.",
                "author": "Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015b.",
                "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 649\u2013657.",
                "url": "https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html"
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.07910v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.3.1",
            "3.3.2",
            "3.3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1",
            "3.3.1",
            "3.3.2",
            "4",
            "4.1",
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2403.07910v2",
        "paper_title": "MAGPIE: Multi-Task Analysis of Media-Bias Generalization with Pre-Trained Identification of Expressions",
        "research_background": "### Motivation:\nThe motivation of the paper stems from the observed limitations in current media bias detection research, which often relies on training classifiers on small and narrowly focused datasets. Existing methods exhibit restricted generalizability across different domains and types of biases, such as political, gender, racial, and linguistic biases. The multifaceted nature of media bias further complicates its detection, necessitating a more robust multi-task approach. The potential of Multi-Task Learning (MTL) to address these challenges through cross-task knowledge transfer is well-noted, yet a comprehensive MTL solution for media bias detection has not been established.\n\n### Research Problem:\nThe research problem addresses the need for an effective and generalizable method to detect various types of media bias, leveraging the capabilities of multi-task learning. The goal is to overcome the limitations of existing media bias detection methodologies, which often exhibit poor performance, particularly when evaluated outside their training domains. This paper aims to develop a large-scale, multi-task pre-training approach that can accommodate diverse biases to improve detection accuracy and generalizability.\n\n### Relevant Prior Work:\n1. **Media Bias Detection Techniques**:\n   - **Recasens et al. (2013)** highlighted the complex nature of media bias, which impacts various facets of information dissemination.\n   - **Raza et al. (2022)** emphasized the intersecting and multi-dimensional characteristics of media bias, making its classification a complex task.\n   - **Krieger et al. (2022)** noted that most existing media bias detection research relies on small, in-domain datasets with limited applicability.\n   - **Wessel et al. (2023)** argued for the necessity of moving beyond isolated approaches to tackling media bias through multi-task methods.\n\n2. **Multi-Task Learning (MTL)**:\n   - **Aribandi et al. (2021a)**, **Chen et al. (2021)**, and **Kirstein et al. (2022)** have demonstrated the potential of MTL in enabling knowledge transfer across various tasks, thereby improving performance on related tasks.\n\n3. **Previous Media Bias MTL Method**:\n   - **Spinde et al. (2022)** attempted an MTL approach for media bias detection but faced performance issues due to its narrow task focus and inability to surpass baseline results.\n\nIn summary, the paper identifies a gap in the comprehensive application of MTL for media bias detection and aims to fill this gap by introducing a robust, large-scale multi-task pre-training approach named MAGPIE. This approach seeks to leverage diverse datasets and task compositions to significantly enhance the accuracy and generalizability of bias detection models.",
        "methodology": "The methodology outlined in the MAGPIE model proposes a sophisticated approach to analyzing media bias through multi-task learning (MTL) using pre-trained transformer models. Here's a detailed description of the model, its key components, and innovations:\n\n1. **Model Framework**: \n   - MAGPIE is designed as an **encoder-only MTL transformer model**.\n   - It employs a method known as **pre-finetuning**, as described in Aghajanyan et al. (2021).\n\n2. **Auxiliary Task Collection**: \n   - MAGPIE is **pre-finetuned on 59 media bias-related auxiliary tasks**. These are curated into a comprehensive task set called the **Large Bias Mixture (LBM)**, which is introduced in the paper.\n\n3. **Innovation in Training Approach**:\n   - One notable innovation is the **Head-Specific Early Stopping and Resurrection** strategy. This method is designed to manage tasks of various sizes more effectively during the training process.\n\n4. **Model Construction and Training**:\n   - The construction of LBM is the initial step.\n   - The model and MTL framework are then defined, including:\n     - **Optimization strategies**: Techniques tailored to improve the performance and efficiency of the MTL framework.\n     - **Task sampling**: Methods for selecting tasks from the LBM during training.\n     - **Auxiliary task selection**: Criteria and process for choosing which auxiliary tasks to include in the pre-finetuning process.\n\n5. **Evaluation**:\n   - The model's performance is evaluated using two primary datasets:\n     - **Media Bias Annotation by Experts (BABE)** dataset, which provides high-quality linguistic bias labels.\n     - **Media Bias Identification Benchmark (MBIB) collection**, noted as the most extensive media bias benchmark available.\n\nIn summary, MAGPIE leverages pre-finetuning on a wide array of bias-related tasks, introduces a novel training strategy to handle datasets of varying sizes effectively, and is evaluated using prominent benchmarks to demonstrate its generalizability and efficacy in identifying media bias.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Datasets**:\n- **BABE**: This dataset is used for evaluating the effectiveness of the model on the main biased-news detection task.\n- **MBIB Benchmark**: This benchmark is used to test the performance of the final model, MAGPIE, on media-bias detection.\n\n**Baselines**:\n- **Single-task baseline**: A model trained solely on the primary BABE task.\n- **Multiple MTL baselines**: Models trained using different multi-task learning (MTL) strategies.\n\n**Evaluation Metrics**:\n- Performance metrics for assessing model efficacy during finetuning on the BABE task.\n- Comparative performance of MAGPIE on the MBIB benchmark, though the specific metrics (accuracy, F1 score, etc.) used are not explicitly mentioned in the excerpt.\n\n**Main Experimental Results**:\n- **Comparison with Baselines**: The MTL approach outperformed the single-task baseline and other MTL baselines in terms of effectiveness on the BABE task.\n- **Performance on MBIB**: The best model, MAGPIE, demonstrated superior performance on the MBIB benchmark compared to other methods.\n\nThis overview highlights the success of the multi-task learning strategy in enhancing model performance on media-bias detection tasks."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To address the imbalanced optimization problem in multi-task learning when tasks have varied complexity and different convergence speeds through Head-Specific-Early-Stopping (HSES) and Resurrection methods.",
            "experiment_process": "The experiment involves uniform data distribution by sampling fixed-size sub-batches from each task per training step and early stopping for each task individually. HSES stops updating head parameters of a task once it stops, while still backpropagating its language model gradients. Resurrection allows a task to resume training if its validation loss increases after it stops. The effectiveness of these methods is evaluated through a preliminary study involving 20 randomly selected tasks from the Large Bias Mixture (LBM) collection.",
            "result_discussion": "Both HSES and Resurrection methods reduce the average evaluation loss by 5% and decrease the variance across different training setups by 85%. However, the authors hypothesize that random task selection might affect the evaluation and thus propose a robust examination of these methods in future work.",
            "ablation_id": "2403.07910v2.No1"
        },
        {
            "research_objective": "To address potential biases in task selection for Multi-Task Learning (MTL) and automate the process using the GradTS algorithm.",
            "experiment_process": "The GradTS algorithm trains all tasks individually, accumulates absolute gradients, and then forms a 12x12 importance matrix for each task. Tasks are sorted by correlation to the BABE task's matrix, and models are pre-finetuned on the top tasks from the sorted list. The BABE task is then evaluated on these models to find the optimal number of tasks based on evaluation loss.",
            "result_discussion": "The selected set of ten most correlated tasks, referred to as the GradTS set, demonstrates a strong semantic connection to media bias domains such as lexical bias, rumor detection, and fake news detection. The correlation helps in selecting suitable auxiliary tasks for pre-finetuning.",
            "ablation_id": "2403.07910v2.No2"
        }
    ]
}