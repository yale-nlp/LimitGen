{
    "title": "NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition",
    "abstract": "Available training data for named entity recognition (NER) often contains a significant percentage of incorrect labels for entity types and entity boundaries. Such label noise poses challenges for supervised learning and may significantly deteriorate model quality. To address this, prior work proposed various noise-robust learning approaches capable of learning from data with partially incorrect labels. These approaches are typically evaluated using simulated noise where the labels in a clean dataset are automatically corrupted. However, as we show in this paper, this leads to unrealistic noise that is far easier to handle than real noise caused by human error or semi-automatic annotation. To enable the study of the impact of various types of real noise, we introduce NoiseBench, an NER benchmark consisting of clean training data corrupted with 6 types of real noise, including expert errors, crowdsourcing errors, automatic annotation errors and LLM errors. We present an analysis that shows that real noise is significantly more challenging than simulated noise, and show that current state-of-the-art models for noise-robust learning fall far short of their theoretically achievable upper bound. We release NoiseBench to the research community111https://github.com/elenamer/NoiseBench.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Named entity recognition (NER) is the task of detecting and classifying named entities in text, such as the names of organizations or locations. All current state-of-the-art approaches for NER require supervision in the form of labeled training data, i.e. sentences in which named entities are marked and assigned their correct type.\nHowever, prior work found that available datasets for NER and other supervised tasks are affected by label noise, meaning that a certain percentage of entity labels are incorrect. For instance, the common NER dataset CoNLL-03 Tjong Kim Sang and\nDe Meulder (2003  ###reference_b23###) was estimated in various studies to have noise shares of between 5 and 7% Wang et al. (2019  ###reference_b26###); Reiss et al. (2020  ###reference_b16###); R\u00fccker and Akbik (2023  ###reference_b19###). Other NER datasets have also been found to contain a share of incorrect labels, with Ontonotes4 estimated around 8% and WNUT-17 around 18% Wang et al. (2019  ###reference_b26###); Huang et al. (2021  ###reference_b6###).\n###figure_1### ###figure_2### ###figure_3### Label noise introduces inconsistencies during training, which may significantly deteriorate model quality Zhang et al. (2021a  ###reference_b29###). To address this issue, prior work proposed approaches for noise-robust learning aimed at mitigating the negative effects of the noisy training signal Song et al. (2022  ###reference_b21###). However, the evaluation of these approaches has two main limitations.\nLimitation 1: Simulated noise is too easy. Most current research in noise-robust learning relies on experiments with simulated label noise T\u00e4nzer et al. (2022  ###reference_b22###); Klie et al. (2023  ###reference_b8###). While this allows for evaluation in a controlled setting, it has been shown that simulated noise, even though it can model noise well to some extent, is much easier for deep learning models to disregard than real label noise Jiang et al. (2020  ###reference_b7###); Zhu et al. (2022  ###reference_b32###).\nRefer to Figure 1  ###reference_### for an illustrative comparison between real and simulated noise for three example sentences, including different types of errors that occur in NER datasets. These examples demonstrate that simulated noise can introduce similar errors as real noise, however the choice of spans to mislabel is random and as a result often less plausible. This means that an approach shown to be robust to simulated noise may not in fact be robust to real noise in practice.\nLimitation 2: Distinct types of real noise. Additionally, there exist many possible sources of \"real\" noise. For instance, expert labelers may make different mistakes than crowd workers Frenay and Verleysen (2014  ###reference_b3###). Next to human labeling, there are widely-used automatic approaches to create NER-labeled datasets such as distant supervision from a knowledge base Mintz et al. (2009  ###reference_b13###); Hedderich et al. (2021  ###reference_b5###) and weak supervision using rules Zhang et al. (2021b  ###reference_b30###). Lastly, current research investigates the use of LLMs to label datasets Golde et al. (2023  ###reference_b4###); Wang et al. (2023  ###reference_b25###).\n](option1) choose one image of Figure 2, put the others in appendix\nWe postulate that these types of real noise differ in their characteristics, meaning that a noise-robust learning approach shown to perform well on one type of noise may not perform well on another. For this reason, we argue there is a need for evaluating noise-robustness across multiple label noise types.\nContributions. With this paper, we present NoiseBench, a new benchmark for measuring the impact of label noise in the training data on the prediction quality of trained NER models. In more detail, our contributions are:\nWe construct a noisy training dataset in 7 different variants, where each noisy variant contains the same sentences and is affected by one class of real errors, spanning errors made by experts, crowd workers, distant supervision, weak supervision and teacher LLMs.\nWe present a set of experiments that empirically show that real noise from NoiseBench is significantly more difficult for current approaches. We further find that during training, real noise is memorized immediately, whereas memorization of simulated noise is delayed.\nWe comparatively evaluate current state-of-the-art approaches for noise-robust learning on NoiseBench, and experimentally establish theoretical upper bounds.\nOur analysis finds that no single current approach works best for all types of real noise, and that all current approaches fall far short of their theoretical upper bound. To enable the research community to leverage our benchmark in their evaluations, we publicly release all data and implementation.\ninline]strengthen contributions. also make it clear that simulating noise is what everyone else is doing and creating an ideal NER noise model is also challenging."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "NoiseBench",
            "text": "Our benchmark is derived from a subset of the classic CoNLL-03 dataset for NER in English, annotated with entities belonging to four classes. We chose this dataset since it has been extensively studied in the field, allowing us to integrate various prior works.\nNoiseBench consists of the following parts: (1) A noise-free test split to evaluate trained models. (2) Seven variants of the training split, where six are annotated with different types of noise and one is without noise. The quality of the six noisy variants, in reference to the noise-free dataset, is presented in Table 2  ###reference_###.\nThe training split contains 5,885 sentences from 400 documents, covering 9,685 entity mentions. The test split\ncontains 3,427 sentences from 231 documents, covering 5,725 entity mentions.\n###table_1###"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Types of Noise",
            "text": "In the following, we discuss each training split and the type of noise it models."
        },
        {
            "section_id": "2.1.1",
            "parent_section_id": "2.1",
            "section_name": "2.1.1 Noise-Free Data",
            "text": "Our benchmark requires two splits without any label noise: A clean test split to evaluate models trained on noisy training data, and a Clean training split to measure the upper bound performance.\nSince the original annotations of CoNLL-03 have been shown to be noisy Wang et al. (2019  ###reference_b26###); Reiss et al. (2020  ###reference_b16###), we use the labels of CleanCoNLL (R\u00fccker and Akbik, 2023  ###reference_b19###), a recently released resource in which 7% of all original annotations were semi-automatically relabeled. In their evaluation, R\u00fccker and Akbik (2023  ###reference_b19###) find their resulting dataset to be of very high quality and largely improved consistency. The Clean Test split in our benchmark is the standard CoNLL-03 test split, with the CleanCoNLL labels."
        },
        {
            "section_id": "2.1.2",
            "parent_section_id": "2.1",
            "section_name": "2.1.2 Expert Errors",
            "text": "The largest portion of machine learning datasets relies on manual annotation by domain experts to provide high-quality labels. However, errors have been found to occur even in expert annotation, affecting even well-known benchmarks, though usually with relatively low noise shares of under 10% Northcutt et al. (2021b  ###reference_b15###); Song et al. (2022  ###reference_b21###).\nTo represent such noise, our benchmark includes a variant of the train split called Expert, which contains the original CoNLL-03 annotations. As Table 1  ###reference_### shows, this split has a noise share of 5.5% and is thus the split with lowest noise."
        },
        {
            "section_id": "2.1.3",
            "parent_section_id": "2.1",
            "section_name": "2.1.3 Crowdsourcing Errors",
            "text": "In addition to expert annotations, labeled datasets can be obtained via crowdsourcing. These annotations are heavily prone to human annotation errors, which can happen for different reasons (Frenay and Verleysen, 2014  ###reference_b3###). In order to create noisy variants of the train set in our benchmark representing real-world human errors, we utilize the crowdsourced labels by Rodrigues et al. (2014  ###reference_b17###). This study involves 47 crowd workers labelling a subset of the CoNLL-03 dataset, of around 400 news articles. They released their dataset and all annotations produced by each crowd worker. We selected only the sentences where the tokenization matched the Clean variant, resulting in 5,885 sentences.\nWe include two noisy training splits based on crowd annotations into our benchmark: (1) In the first, Crowd, we do a simple majority vote over all annotations provided for each token, i.e. the baseline method for aggregating crowdsourced annotations. (2) In the second, Crowd++, we use an oracle version of the majority vote, selected by either taking the correct label if it is provided by any of the annotators or, in the absence of a correct label, by choosing the label with the majority of votes. This version represents the upper bound of crowdsourced labels given a perfect label aggregation method. As Table 1  ###reference_### shows, the noise share of Crowd (36.6%) is considerably higher than Crowd++ (15.3%)."
        },
        {
            "section_id": "2.1.4",
            "parent_section_id": "2.1",
            "section_name": "2.1.4 Distant Supervision",
            "text": "One approach for labeling data without human participation is distant supervision Mintz et al. (2009  ###reference_b13###), where entity mentions in target datasets are matched to entity types in knowledge bases (KBs). The matching can be achieved by simple string matching, use of regular expressions or heuristics.\nWe include a Distant noisy training variant in our benchmark, adapted from the annotations by Liang et al. (2020  ###reference_b9###)222Available under Apache 2.0 license that use the Wikidata corpus and gazetteers collected from multiple online sources as external knowledge bases. After initial POS tagging, the unlabeled sentences were matched with the knowledge bases.\nThis process results in incomplete annotations due to limited coverage over entity types of KBs. This explains the rather high number of missing entities and the overall noise level (31.3%) of the Distant training variant, as shown in Table 1  ###reference_###."
        },
        {
            "section_id": "2.1.5",
            "parent_section_id": "2.1",
            "section_name": "2.1.5 Weak Supervision",
            "text": "Another approach aimed at reducing manual annotation efforts is weak supervision. Here, labels are obtained using a number of \u2019weak\u2019 supervision sources, such as heuristics or expression-based rules. Each weak source is typically specialised to detect only a subset of the correct labels.\nWe use the labels from the approach by Lison et al. (2020  ###reference_b10###)2  ###reference_te2### to create our Weak label set. This covers 16 weak labeling sources, those used in Zhang et al. (2021b  ###reference_b30###), including heuristics, gazetteers and predictions of NER models trained on other corpora. An example heuristic is detecting PER (person) entities using a pre-defined list of first names.\nWe aggregate the weak label sets with simple majority voting. We apply majority vote on every token with at least one entity label assigned to it, following Zhang et al. (2021b  ###reference_b30###). Due to the large number of labelling sources, majority voting yields a large number of entities, as shown in Table 1  ###reference_###, including many false positives. As a result, the Weak label set has a high noise share of 40.4%."
        },
        {
            "section_id": "2.1.6",
            "parent_section_id": "2.1",
            "section_name": "2.1.6 LLM Teacher Models",
            "text": "Our benchmark includes a noisy variant of the train split annotated by an LLM. This follows recent efforts that use LLMs for dataset generation Wang et al. (2023  ###reference_b25###). Here, the main idea is to pass a natural language description of the annotation task and target classes to an LLM, and provide sentences that the LLM should label. Current works find that LLMs are able to generate high quality labels for some tasks (e.g. sentiment classification) while for others (e.g. NER and question type categorization) the resulting labels are very noisy.\nWe created the LLM variant using the Fabricator toolkit Golde et al. (2023  ###reference_b4###) by prompting GPT3.5 for named entities in our training dataset. To use LLM outputs for annotation of NER datasets, a certain output format is required. To achieve this, we provide one example with the correct output format in each prompt. This example is the same for each sentence we wish to annotate, which we refer to as a static one-shot setting. The example sentence was selected from the remainder of the CoNLL-03 training split, which consists of all sentences not included in our benchmark.\nAs Table 1  ###reference_### shows, the LLM label set results in the highest noise share of 45.6%. This is mainly due to the large number of nouns incorrectly identified as entity mentions, which also makes this the label set with the largest number of entity annotations out of the variants in NoiseBench."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Statistics",
            "text": "An overview of NoiseBench is given in Table 1  ###reference_###.\nThe table shows token-level F1 score and entity-level F1 score expressed as percentages. We define the noise level (%Noise) in terms of the entity-level F1 score, as .\nThe noise levels of the noisy splits range from 5.5 to 45.6 percent.\nFurthermore, the table shows the total number of entities, the number of correct entities, as well as the share of different error types. The errors are categorized into 4 main categories: missing mentions, non-entity mentions (false positives), incorrect entity type (where the boundary is correct, but type incorrect)\nand partial matches. Partial matches are special cases where the type is correct, however the mention boundary is only partially correct. Refer to Figure 1  ###reference_### for examples.\nWe observe that the Crowd++, Crowd and Distant label sets have a lower total number of entity annotations than the Clean dataset, and the largest portion of errors are missing mentions. Conversely, the Weak and LLM label sets have more annotations than the Clean dataset, and most of the errors are either an incorrect mention or incorrect type. Most of the errors in the Expert label set are due to incorrect type. Regarding the number of partial matches, for almost all noise types, they make up between 10% and 15% of all errors."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Comparing Real and Simulated Noise",
            "text": "We first use NoiseBench to investigate how real label noise affects NER model performance in comparison to simulated noise. For this, we conduct two experiments: the first one addresses the impact of each type of training noise on the clean test set performance, and the second one compares training dynamics under real and simulated label noise to highlight the differences in noise memorization.\n###table_2###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Noise Simulation Methods",
            "text": "We consider two noise simulation methods, namely the simple uniform noise used in most prior work and a more involved oracle class-dependent noise method that we design to mirror each noisy variant in NoiseBench.\nUniform noise. Uniform noise corrupts samples into any other label with a uniform probability distribution, given a target noise share. Studies investigating simulated noise in the NER task commonly rely on variants of this method Mayhew et al. (2019  ###reference_b12###); T\u00e4nzer et al. (2022  ###reference_b22###).\nOracle class-dependent noise.\nClass-dependent noise is based on the knowledge that some pairs of classes are more likely to be mislabeled than others. It is defined by a noise transition matrix, which contains the mislabeling probabilities between all pairs of classes Hedderich et al. (2021  ###reference_b5###). We design an oracle version of class-dependent noise, where the per-class mislabeling probabilities of real noise are known. This allows us to investigate class-dependent noise in an ideal case, where it is able to mirror real noise closely, even though this is not possible in practice. This method mirrors real noise by utilizing the token-level mislabeling frequencies as probabilities to form a noise transition matrix.\nUsing each noise simulation method, we created 6 label sets, corresponding to each noise level in NoiseBench. It should be noted that the simulated labels replicate the token-level F1 scores of the real noisy labels, however the entity-level F1 and sentence-level accuracy can deviate."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Experimental Setup",
            "text": "In both experiments, we train a baseline approach for NER on each noisy variant of the training split, as well as on the additional simulated noise.\nValidation splits. We evaluate the setting in which all available data to train a model is noisy, including the validation set.\nTo obtain noisy validation sets for each of our 7 dataset variants, we split the noisy datasets into training and validation sets. All sentences from 66 news documents from 1996-08-24 comprise the validation set, which is around 17% of all sentences, and are left out from model training and used for hyperparameter tuning.\nBaseline. For NER, as a baseline approach, we fine-tune an xlm-roberta-large transformer using the FLERT approach Schweter and Akbik (2021  ###reference_b20###). It improves upon the regular fine-tuning setup by considering document-level features of a sentence to be tagged. We use a learning rate of 5e-6 and a batch size of 32, for a fixed number of 10 epochs. These parameters were obtained according to the performance on a noisy validation set, keeping in mind that larger batch sizes are more robust to noise Rolnick et al. (2017  ###reference_b18###).\nAs an evaluation metric we use entity-level micro F1-score.\nReal\n###figure_4### ###figure_5### ###figure_6### Simulated\n###figure_7### ###figure_8### ###figure_9###"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Experiment 1: Impact of Label Noise on Test Performance",
            "text": "In the first experiment, we compare how the clean test set performance is impacted by the 6 types of real label noise when present in the training set. In addition, we provide the same comparison for corresponding simulated noisy label sets."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 Results and Discussion",
            "text": "The results for uniform noise are shown in Appendix A  ###reference_###. We initially established that uniform noise is significantly less challenging for the model, so in the results from Experiments 1 and 2 we chose to focus our analysis solely on oracle class-dependent noise.\nThe main results from Experiment 1 for oracle class-dependent noise are shown in Table 2  ###reference_###. Additional analysis of the results can be found in Appendix B  ###reference_###. Following are our main observations.\nLabel noise degrades performance. When we compare the test F1 scores of the real noisy variants with the average score of 93.99 achieved when training on the Clean variant, we can see that model performance is affected by each noise type.\nIn all cases, the noisy training variants yield lower test scores, even when training with the low-noise Expert variant. As the noise levels increase, the impact on model performance becomes more pronounced. This shows that the baseline model lacks robustness to any of the real noise types. When we compare the test F1 scores of the simulated noisy variants, we can see that noise of 5.9 in the training set does not hurt model performance and results in a score comparable to training on the Clean variant. However, as simulated noise levels increase, they do degrade the prediction scores on the test set.\nReal noise is more difficult. Furthermore, when we compare the real noisy label sets with their equivalent simulated noisy variants, we can observe that the simulated training variants show a score of around 2.5 percentage points higher on average than the real label sets. This shows that for predictive NER models, real noise is more difficult to overcome than simulated noise. In other words, models are more likely to overfit to real noisy labels, rather than simulated ones."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Experiment 2: Memorization of Noise",
            "text": "Prior analysis has found that there are distinct phases of learning when training a model on data with label noise Arpit et al. (2017  ###reference_b1###). This has been referred to as a generalization phase, where models learn patterns that generalize well to clean data, followed by a memorization phase, where models overfit to the label noise and deteriorate in prediction quality T\u00e4nzer et al. (2022  ###reference_b22###).\nTo investigate this phenomenon for real and simulated noise, we extend the training stage to 100 epochs. At the end of each epoch, we measure the F1 score of the model on both the noisy training split it is being trained on, and separately on the clean training split. The difference between these two scores allows us to measure memorization."
        },
        {
            "section_id": "3.4.1",
            "parent_section_id": "3.4",
            "section_name": "3.4.1 Results and Discussion",
            "text": "In Table 2  ###reference_### we show training curves from training with real and simulated variants of NoiseBench  for 3 noise types: Expert, Crowd++ and Distant. We plot two scores: the F1-score on the respective noisy variant of the training set, and the F1 score on the Clean variant of the training set.\nIn all training curves, we can observe the memorization effect, with each model perfectly fitting the noisy data by the end of training and reaching an F1 score close to 1.\nDelayed memorization of simulated noise. However, we note that with simulated noise (see Table 4  ###reference_###, 4  ###reference_###, 4  ###reference_###) this happens much later in the training process than with real noise. In addition, the training curves of simulated noise show a stage during the early epochs where the score on the clean labels is consistently higher than the score on the noisy labels. This confirms previous findings that the model is able to learn general patterns first, before starting to memorize the noise.\nImmediate memorization of real noise. With real noise this does not happen and the model starts fitting the noisy labels from the beginning of training (see Table 4  ###reference_###, 4  ###reference_###, 4  ###reference_###). As a result, the score on the clean labels is consistently lower than the score on the noisy labels, during the entire training run.\nOur experiments find that real noise does not display distinct generalization/memorization phases during training, and rather immediately begins with memorization. This makes intuitive sense, as real noise has underlying patterns that may be extracted during learning. This lends further evidence to the increased challenges and the need to evaluate noise-robust learning with real noise."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Evaluating Noise-Robust Learning",
            "text": "Having established the difficulty of real noise, we now use NoiseBench to perform a comparative evaluation of widely-used noise-robust learning approaches. Our goal is to determine their effectiveness in the presence of real label noise, and to establish upper bounds of what noise-robust learning could ideally achieve."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Compared Approaches",
            "text": "We surveyed current state-of-the-art methods for noise-robust NER and found that many approaches rely on the same underlying ideas for handling label noise. In the following, we group approaches by the underlying idea, select a state-of-the-art representative for each group and, if possible, derive an upper bound method for each noise-robust method. For more details about the implementation of compared approaches refer to Appendix D  ###reference_###."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Learning from a Clean Subset",
            "text": "The first family of approaches relies on utilizing the subset of each noisy dataset in which all labels are correct. They either filter out all likely incorrect annotations and learn only from a clean subset Northcutt et al. (2021a  ###reference_b14###). Or they derive confidence weights for each annotation so that annotations judged to be of higher quality feature more during training, as in the CrossWeigh and COSINE approaches Wang et al. (2019  ###reference_b26###); Yu et al. (2021  ###reference_b28###).\nAs representative of this class of approaches, we chose confident learning Northcutt et al. (2021a  ###reference_b14###).\nUpper bound: Oracle subset. To obtain an upper bound for this family of approaches, we use an oracle to select the subset of clean sentences from each of the noisy training splits in NoiseBench. We then use a standard fine-tuning approach only on this subset. This setting illustrates the best-case scenario for training on a clean subset only."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Delaying Memorization",
            "text": "Upper bound: Oracle stopping. To obtain an upper bound, we use a simple stopping criterion based on the score on the clean test set at the end of each epoch. We use the epoch of best generalization to report the final score. This simulates an ideal stopping (albeit at the granularity of full epochs)."
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Combined Approaches",
            "text": "While the approach discussed so far each build on the individual ideas of identifying a clean subset or delaying memorization, many current approaches in fact combine multiple of such ideas in multi-stage pipelines Liang et al. (2020  ###reference_b9###); Yu et al. (2021  ###reference_b28###); Wang et al. (2022  ###reference_b24###). As representative of such approaches, we evaluate BOND Liang et al. (2020  ###reference_b9###), which combines pseudo-labeling in a student-teacher setup and sample selection.\nNo upper bound for pseudo-labeling. We cannot derive a separate upper bound for pseudo-labeling, as the best case scenario here would mean that all noisy labels are replaced by correct labels. This means that the upper bound for pseudo-labeling is the same as training on fully clean data."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "Table 3  ###reference_### summarizes the evaluation results. We make the following observations:\nClean subset has high potential. The upper bound of training only on clean examples is the best achievable result for all types of noise. This was to be expected, since in this scenario the labels are not contaminated by any noise. This indicates that noise-robust approaches focusing on identifying the clean subset have high potential. Oracle stopping, on the other hand, does not achieve the same level of performance as the oracle subset, while only slightly outperforming the FLERT baseline. This is in line with our findings in Experiment 2 that the early-learning generalization phase is skipped when training with real noise, indicating that early stopping approaches have less potential.\nSmall benefit of noise-robust approaches. Evidently, there is no single best approach for all noise types. While for all noise types some of the noise-robust approaches are able to outperform the baseline, they do so only slightly, with the exception of BOND, on the Distant and Crowd variants. Still, their performance is far below the upper bound. This raises the issue of trade-offs of existing noise-robust learning approaches, since they often require additional hyperparameter tuning or incur computational costs, but only lead to slight improvements over the baseline in the presence of real noise."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "There are a few benchmarks for learning with label noise and related topics. The WRENCH benchmark Zhang et al. (2021b  ###reference_b30###) focuses only on weak supervision labels for multiple tasks, with the emphasis on combining multiple weak labelling sources. Klie et al. (2023  ###reference_b8###) compare a large number of methods for the detection of annotation errors. Multiple tasks are included, including NER on CoNLL-03, where they evaluate the detection of expert errors, concluding that most approaches are not successful at this. Similarly, Chong et al. (2022  ###reference_b2###) evaluate annotation error detection on datasets with noise only from crowdsourced labels, for part-of-speech tagging and natural language inference tasks. Liu et al. (2022  ###reference_b11###) propose a benchmark for text classification under label noise, where they re-annotate an existing sentiment classification dataset and construct noisy label sets according to annotator disagreements; however, they do not publish these label sets. NoisyWikiHow, a benchmark for intention identification has also been presented Wu et al. (2023  ###reference_b27###), where the authors propose a method to simulate realistic noise that closely imitates human errors by producing heterogeneous and instance-dependent errors. For NER in Estonian, Hedderich et al. (2021  ###reference_b5###) introduce the NoisyNER, which includes multiple noise levels obtained from distant supervision approaches with varying quality."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we address the issue of label noise in the NER task. We introduce a new benchmark, based on the commonly used NER dataset CoNLL-03 in English, for evaluating the impact of 6 distinct types of real label noise on the same set of sentences, with varying degrees of difficulty.\nWe demonstrate that real noise causes transformer-based language models to immediately memorize the noise pattern, making real label noise a more challenging problem than simulated label noise, even in the case of oracle class-dependent noise informed by the characteristics of real noise.\nWe further investigate this by evaluating popular approaches aimed at mitigating this issue and showcasing their limitations. Our experiments indicate that current methods fall far short of what can be potentially achieved on almost all of the noisy datasets in NoiseBench, leaving room for improvement. We hope that NoiseBench aids other researchers in the further development of more effective noise-robust approaches and in characterizing the differences between types of real noise."
        }
    ],
    "url": "http://arxiv.org/html/2405.07609v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "5"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.1.1",
            "2.1.2",
            "2.1.3",
            "2.1.4",
            "2.1.5",
            "2.1.6",
            "2.2"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.3.1",
            "3.4",
            "3.4.1",
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.4.1",
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.07609v1",
        "paper_title": "NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition",
        "research_background": "### Introduction:\n\nNamed entity recognition (NER) is a task that involves detecting and classifying named entities in text, such as names of organizations or locations. For achieving state-of-the-art performance in NER, supervised learning approaches that rely on labeled training data\u2014sentences with marked and correctly typed named entities\u2014are utilized. However, prior research has identified label noise as a significant problem in these datasets. Label noise occurs when a certain percentage of entity labels within the dataset are incorrect. For example, the widely-used NER dataset CoNLL-03 has been estimated to contain noise shares between 5% and 7% according to various studies. Other datasets like Ontonotes4 and WNUT-17 have also been found to contain noise, estimated at 8% and 18% respectively.\n\nLabel noise can introduce inconsistencies during the training process, leading to a significant deterioration in model quality. Prior solutions have proposed noise-robust learning approaches to mitigate the negative impacts of noisy training data. However, the current evaluation methods for these approaches face two main limitations:\n\n1. **Simulated Noise is Too Easy:** Most research relies on experiments with simulated label noise. While simulation allows for controlled evaluation settings, it has been shown that simulated noise is easier for deep learning models to handle compared to real label noise.\n   \n2. **Distinct Types of Real Noise:** Real label noise can originate from various sources, such as mistakes made by expert labelers, crowd workers, distant supervision from knowledge bases, weak supervision using rules, and labeling using large language models (LLMs). These sources of noise differ in their characteristics and may not be uniformly addressed by a single noise-robust learning approach.\n\n### Contributions:\n\nWith this paper, we introduce **NoiseBench**, a new benchmark designed to measure the impact of label noise in training data on the prediction quality of NER models. Our contributions are summarized as follows:\n\n1. **Construction of Noise Variants:** We create a noisy training dataset in seven variants, each affected by one class of real noise, including errors made by experts, crowd workers, distant supervision, weak supervision, and teacher LLMs.\n   \n2. **Empirical Study on Noise Impact:** We conduct experiments demonstrating that real noise from NoiseBench is significantly more challenging for current approaches compared to simulated noise. We find that real noise is memorized immediately during training, whereas simulated noise memorization is delayed.\n\n3. **Evaluation of Noise-Robust Learning Approaches:** We evaluate state-of-the-art noise-robust learning methods using NoiseBench and establish theoretical upper bounds for noise robustness. Our analysis reveals that no single approach performs best across all types of real noise and that all current methods fall short of their theoretical potential.\n\n4. **Public Release for Community:** To aid further research, we publicly release all data and implementation related to NoiseBench, allowing the research community to utilize our benchmark for their evaluations.\n\n---\n\n### Relevant Prior Work:\n\n1. **Detection of Label Noise in NER Datasets:** Studies have estimated noise shares between 5% and 7% in the CoNLL-03 dataset (Wang et al. 2019; Reiss et al. 2020; R\u00fccker and Akbik 2023), around 8% in Ontonotes4, and 18% in WNUT-17 (Wang et al. 2019; Huang et al. 2021).\n\n2. **Noise-Robust Learning Approaches:** Various methods have been proposed to mitigate the effects of noisy training data (Song et al. 2022).\n\n3. **Challenges with Simulated Noise:** Prior work indicates that simulated label noise is less challenging for models compared to real noise (Jiang et al. 2020; Zhu et al. 2022).\n\n4. **Sources of Real Noise in NER:** Common sources of real noise include errors from expert labelers and crowd workers (Frenay and Verleysen 2014), distant supervision from a knowledge base (Mintz et al. 2009; Hedderich et al. 2021), weak supervision using rules (Zhang et al. 2021b), and labeling with LLMs (Golde et al. 2023; Wang et al. 2023). \n\nOverall, NoiseBench aims to address the limitations of existing evaluation methods by providing a more realistic and challenging benchmark for assessing the impact of real label noise on NER systems.",
        "methodology": "NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition \n\nMethodology: The benchmark developed in this study uses a subset of the well-known CoNLL-03 dataset focused on Named Entity Recognition (NER) in the English language, which includes entities classified into four distinct categories. We selected the CoNLL-03 dataset due to its established prominence and the substantial amount of prior research conducted using it, enabling us to incorporate various previous works seamlessly.\n \nNoiseBench is composed of the following components: \n1. **A noise-free test split**: This is used to assess the performance of trained models without the interference of label noise, providing a clear benchmark for comparison.\n\nDataset Details:\n- The training split consists of 5,885 sentences extracted from 400 documents, featuring a total of 9,685 entity mentions.\n- The test split encompasses 3,427 sentences from 231 documents, covering 5,725 entity mentions.\n\nThis structured approach ensures a comprehensive evaluation of the robustness and performance of NER models, thereby providing valuable insights into the resilience of these models when faced with noisy data.",
        "main_experiment_and_results": "## Main Experiment Setup and Results\n\nWe utilize NoiseBench to study the impact of real label noise on Named Entity Recognition (NER) model performance, comparing it to simulated noise. For this study, we conduct two primary experiments:\n\n1. **Impact of Different Types of Training Noise on Clean Test Set Performance**: \n    - **Datasets**: We employ commonly used NER datasets to ensure our findings are broadly applicable. \n    - **Baselines**: The NER models are trained with various types of noise (both real and simulated).\n    - **Evaluation Metrics**: The models are evaluated on a clean test set using standard NER evaluation metrics such as F1-score, precision, and recall.\n\n2. **Comparison of Training Dynamics Under Real and Simulated Label Noise**:\n    - **Objective**: This is to analyze the differences in how models memorize noisy data compared to clean data during training.\n    - **Techniques**: Observing metrics such as training loss and accuracy over time for both real and simulated noise scenarios.\n\n### Main Experimental Results \n- **Performance Impact**: The comparison of real and simulated noise types reveals that real noise often has a more detrimental effect on the clean test set performance, with lower F1-scores, precision, and recall compared to simulated noise scenarios.\n- **Training Dynamics**: When comparing training dynamics, it is evident that real noise leads to different patterns of noise memorization. Models tend to memorize real noisy labels differently compared to simulated noisy labels, as seen in the divergence of training loss and accuracy trajectories.\n\nThe results underscore the need for handling real noise differently than simulated noise and open up further research into noise-robust learning methods for real-world applications."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate how real label noise affects NER model performance in comparison to simulated noise.",
            "experiment_process": "Two experiments were conducted using NoiseBench. The first experiment addresses the impact of each type of training noise on clean test set performance. The second experiment compares training dynamics under real and simulated label noise to highlight differences in noise memorization. For both, simulated noise was created using uniform noise and an oracle class-dependent noise method to mirror NoiseBench's noisy variants. Models were trained on both real and simulated noisy training splits, and evaluated using entity-level micro F1-score. The baseline model was an xlm-roberta-large transformer fine-tuned with the FLERT approach, evaluated for up to 100 epochs.",
            "result_discussion": "Results showed real noise causes immediate memorization, with models fitting noisy labels from the start of training, and consistently scoring lower on clean labels compared to noisy ones. In contrast, simulated noise exhibited a generalization phase where models learned useful patterns before memorization. This confirms that real noise is more challenging, necessitating robust evaluation with real noise.",
            "ablation_id": "2405.07609v1.No1"
        },
        {
            "research_objective": "Determine the effectiveness of widely-used noise-robust learning approaches in the presence of real label noise.",
            "experiment_process": "Various state-of-the-art noise-robust NER methods were evaluated using NoiseBench, grouped by their underlying idea: learning from clean subsets, delaying memorization, and combined approaches. Representative methods were chosen for each group, including confident learning, co-regularization, and BOND. Upper bound scenarios were also considered, such as using an oracle for clean subset selection and ideal stopping based on clean test set performance. The evaluation compared these approaches against a standard FLERT baseline.",
            "result_discussion": "The clean subset approach showed the highest potential, achieving the best results across noise types. Oracle stopping outperformed the baseline slightly but did not match the clean subset's performance. Noise-robust approaches generally outperformed the baseline but only marginally, with BOND performing notably better for Distant and Crowd noise. These findings highlight the limited benefits and trade-offs of current noise-robust methods, requiring further innovation for significant improvements.",
            "ablation_id": "2405.07609v1.No2"
        }
    ]
}