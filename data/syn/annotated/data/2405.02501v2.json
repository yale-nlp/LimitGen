{
    "title": "PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning",
    "abstract": "Large Language Models (LLMs) are trained on massive text corpora, which are encoded with diverse personality traits.\nThis triggers an interesting goal of eliciting a desired personality trait from the LLM, and probing its behavioral preferences.\nAccordingly, we formalize the persona elicitation task, aiming to customize LLM behaviors to align with a target persona.\nWe present Persona In-Context Learning (PICLe), a novel persona elicitation framework\ngrounded in Bayesian inference.\nAt the core, PICLe introduces a new ICL example selection criterion based on likelihood ratio, which is designed to optimally guide the model in eliciting a specific target persona.\nWe demonstrate the effectiveness of PICLe through extensive comparisons against baseline methods across three contemporary LLMs.\nCode is available at https://github.com/deeplearning-wisc/picle.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs), often trained on massive text corpora, possess the ability to encode diverse personas or behaviors (Wolf et al., 2023  ###reference_b44###). These personas can span a wide spectrum of personality traits, political views,\nmoral beliefs, etc.\nIn particular, the persona categorization can encompass well-studied ones such as helpfulness, honesty, and harmlessness (Bai et al., 2022a  ###reference_b3###), but can also extend to much more diversified and nuanced ones like conscientiousness, non-racism, compassion, and so on.\nAn intriguing yet underexplored question arises: to what extent can we elicit diverse personas encoded in LLMs? Answering this question is important because it deepens our understanding of the ethical implications and societal impacts associated with the deployment of LLMs in various contexts, especially when interacting with human users with diverse beliefs.\nIn light of this, we formalize and investigate the \u201cpersona elicitation\u201d task, which can be viewed as probing the language model\u2019s diverse behavioral preferences.\nThe overarching goal of the task is to encourage reactions to input queries in a way that aligns with a specified personality trait, referred to as the persona.\nFor instance, an LLM targeted to elicit an \u201cagreeable\u201d persona should exhibit positive reactions to statements like \u201cI care deeply about other people and their well-being.\u201d More formally, for each persona , our goal is to derive a mapping function , so that it can return the correct action  for an input query regarding statement , where  is the action space.\nTo achieve effective persona elicitation, we introduce a novel framework, Persona In-Context Learning (PICLe), grounded in Bayesian inference.\nTo embody the multi-persona perspective of an LLM, we decompose the LLM distribution into a mixture of persona distributions, which provides the guiding principle for our method.\nAt the core, our proposed framework, PICLe, elicits the target persona by selecting demonstrative examples, which enable the model to concentrate on the target persona.\nThis In-context learning (ICL) framework is a type of prompting method that modifies the original query by prepending a list of task examples.\nWhile ICL has been successful across many natural language processing tasks (Wei et al., 2022  ###reference_b43###; Min et al., 2022a  ###reference_b26###; Xu et al., 2023  ###reference_b46###; Lin et al., 2023b  ###reference_b20###), our study distinctly revolves around selecting the optimal set of demonstrative\nexamples to encourage persona elicitation.\nIn particular, we propose a novel likelihood-ratio-based selection mechanism, which chooses samples that maximize the likelihood of the target persona.\nIn effect, our objective returns examples that are mostly indicative of the persona, though not yet well represented by the LLM.\nThus, by supplying these most \u201cinformative\u201d examples, we provide additional information for the LLM to infer and elicit the desired persona.\nWe comprehensively evaluate PICLe against various baselines on several modern LLMs including Llama-2 (Touvron et al., 2023  ###reference_b40###), Vicuna (Chiang et al., 2023  ###reference_b6###), and GPT-J (Wang, 2021  ###reference_b41###).\nOn Llama-2, PICLe achieves an average success rate of 88.1%, significantly improving upon the baseline without using in-context learning examples (65.5%).\nMoreover, experiments on all models show that our method consistently outperforms competitive ICL baselines (section 4.3  ###reference_###), showcasing the model-agnostic capability and general applicability of PICLe. Going beyond, we analyze that PICLe is robust to the choice of key hyperparameters (section 5.3  ###reference_###), and that it has comparable computational efficiency compared to baseline methods (section 5.4  ###reference_###).\nWe summarize our contributions as follows:\nWe formally define the Persona Elicitation task with concrete evaluation metrics for comprehensive analysis.\nWe propose Persona In-Context learning (PICLe) to elicit diverse behaviors and personas from LLMs via an In-context Learning approach, which selects demonstrative examples with our novel likelihood ratio criterion.\nWe conduct extensive experiments and analyses to elucidate PICLe\u2019s advantage over various ICL baselines, and to better understand its underlying mechanism."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Persona Elicitation",
            "text": "Task Definition.\nThe persona elicitation can be viewed as probing the language model\u2019s behavioral preferences when provided with persona-specific context.\nIn particular, Perez et al. (2022  ###reference_b33###) provided an evaluation framework by querying whether an LLM would agree or disagree with statements associated with a specific persona.\nFor instance, a persona \u201cagreeableness\u201d entails statements like \u201cIt is important to treat other people with kindness and respect\u201d that represents the persona, and also the statements on the other end, e.g., \u201cI treat people coldly\u201d.\nThen, the objective of the \u201cagreeableness\u201d persona elicitation task would be to derive a positive reaction to the former statement, and a negative reaction to the latter.\nIn this particular setup, the action space is defined by a binary set .\nTherefore, the goal is to have the LLM output map to \u2018yes\u2019 for the statements that align with the persona, and \u2018no\u2019 otherwise.\nTo define the task more formally, we consider a set of all persona types . For each persona , we have an evaluation dataset , where  is a statement and  is the ground truth action for the given statement.\nThe action space is defined to be a discrete set .\nFor each persona , the goal is to derive a mapping function , so that it can return the correct action  for an input query regarding statement .\nA Motivating Analysis. A simple mapping function  is to employ LLM\u2019s output probability for the tokens corresponding to the action space , and make predictions based on the maximum probability. In other words, we can define a simple baseline as:\nwhere  is the parameterization of the LLM.\nEmpirically, we can evaluate such a baseline using the latest Llama-2-chat model (Touvron et al., 2023  ###reference_b40###) on the entire Anthropic persona dataset (Perez et al., 2022  ###reference_b33###).\nThe average elicitation success rate across a spectrum of personas is 65.5%, which is only moderately better than random guessing (more experimental details in Section 4  ###reference_###).\nThis suggests the non-trivialness of the task and motivates our work to devise an approach that can more effectively elicit diverse personas from LLMs."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "In this section, we introduce the Persona In-Context Learning (PICLe) framework grounded in Bayesian inference.\nMulti-Persona Decomposition. In our framework, each  can be viewed as a persona type from a family of latent personas .\nTo embody the multi-persona perspective of an LLM, we can decompose the LLM distribution (Wolf et al., 2023  ###reference_b44###), , into a mixture of persona distributions, , as\nwhere  is the coefficient that encodes the relative weight of each persona distribution in the LLM.\nPersona Elicitation via Bayesian Inference.\nWe can further express Eq. (1  ###reference_###) from a Bayesian perspective.\nSpecifically, for a given prompt , the output probability  can be formulated by marginalizing across all latent personas (Xie et al., 2021  ###reference_b45###):\nHere, the term  corresponds to  in Eq. (1  ###reference_###),\nindicating the likelihood of a persona  given the prompt .\nThe term  is the action probability conditioned on a certain persona , which corresponds to the  term in Eq. (1  ###reference_###).\nThe decomposition in Eq. (2  ###reference_###) provides the guiding principle for our method design on persona elicitation.\nImportantly, if  can mark the correct target persona  with high probability (ideally with probability concentration 1), then one can better adapt the output probability  towards the target persona.\nIn other words, by maximizing , Bayesian inference can \u201celicit\u201d the\ncorresponding target persona through marginalization.\nHence, our goal is to modify the overall output by maximizing  to elicit the target persona, .\nTo achieve this, we proceed by describing our proposed framework, PICLe, in detail.\n###figure_1### To select the best demonstrative examples from a pool , we propose a novel likelihood-ratio-based selection mechanism.\nOur key idea for the selection is guided by the Bayesian principle, where we can rewrite  as\nwhere the persona prior  does not depend on .\nHence, to effectively maximize , we can focus on improving the likelihood ratio .\nEquivalently, we can define the objective by taking the logarithm of Eq. (3  ###reference_###), yielding\nwhere the first term aims to select examples with high likelihood conditioned on a persona , and the second term aims to select examples with lower likelihood under the original LLM.\nOur selection mechanism can be interpreted from the likelihood ratio perspective (Neyman & Pearson, 1933  ###reference_b29###), which assesses the fit of the observed data  under two competing statistical models.\nIn effect, our objective returns examples that are indicative of the persona , though not yet well represented by the original LLM.\nHence, by supplying these most \u201cinformative\u201d examples, we provide additional information for the LLM to infer and elicit the persona.\nWe will cover qualitative examples later in Section 5.1  ###reference_### to verify the informativeness of the selected samples.\nIn practice, one can tractably estimate the two log-likelihood under two statistical models: the original LLM, and a persona LLM that is conditioned on the target persona.\nSpecifically, one can easily compute the log-likelihood , where  is the token length of example , and  is the parameterization of the original LLM.\nMoreover, we estimate the persona LLM with  using a model fine-tuned on the examples in the pool , which we denote as .\nThe fine-tuning employs the standard next-token prediction loss, with more details specified in Appendix B  ###reference_2###.\nWe will show later in Section 5.4  ###reference_### that fine-tuning can be performed efficiently, with minimal computation overhead. For example, on a single Nvidia A100 GPU card, it only takes less than a minute to fine-tune a persona LLM.\nNow, we can rewrite our objective in Eq. (4  ###reference_###) as:\nwhere the first term can be calculated by .\nPutting it altogether, to select the ICL examples, we\nevaluate  for each statement , and select the top- statements with the highest  score.\nThen, the selected examples are prepended in the same format as the original query, as exemplified in Figure 1  ###reference_###.\nIn testing, our mapping function predicts the action of agreeing or disagreeing with the given test statement , defined as:\nwhich is the set of indices for the examples selected from .\nWe summarize our end-to-end framework in Algorithm 1  ###reference_###."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Persona In-Context Learning (PICLe)",
            "text": "We propose Persona In-Context Learning (PICLe) that elicits the target persona by selecting demonstrative examples, which can help the LLM concentrate on the target persona, or maximize .\nWhile in-context learning (ICL) has been successful across many natural language processing tasks (Wei et al., 2022  ###reference_b43###; Min et al., 2022a  ###reference_b26###; Xu et al., 2023  ###reference_b46###; Lin et al., 2023b  ###reference_b20###), our study distinctly revolves around the following unexplored question:\nHow should we select the optimal set of demonstrative examples to encourage persona elicitation?\nTo select the best demonstrative examples from a pool , we propose a novel likelihood-ratio-based selection mechanism.\nOur key idea for the selection is guided by the Bayesian principle, where we can rewrite  as\nwhere the persona prior  does not depend on .\nHence, to effectively maximize , we can focus on improving the likelihood ratio .\nEquivalently, we can define the objective by taking the logarithm of Eq. (3  ###reference_###  ###reference_###), yielding\nwhere the first term aims to select examples with high likelihood conditioned on a persona , and the second term aims to select examples with lower likelihood under the original LLM.\nOur selection mechanism can be interpreted from the likelihood ratio perspective (Neyman & Pearson, 1933  ###reference_b29###  ###reference_b29###), which assesses the fit of the observed data  under two competing statistical models.\nIn effect, our objective returns examples that are indicative of the persona , though not yet well represented by the original LLM.\nHence, by supplying these most \u201cinformative\u201d examples, we provide additional information for the LLM to infer and elicit the persona.\nWe will cover qualitative examples later in Section 5.1  ###reference_###  ###reference_### to verify the informativeness of the selected samples.\nIn practice, one can tractably estimate the two log-likelihood under two statistical models: the original LLM, and a persona LLM that is conditioned on the target persona.\nSpecifically, one can easily compute the log-likelihood , where  is the token length of example , and  is the parameterization of the original LLM.\nMoreover, we estimate the persona LLM with  using a model fine-tuned on the examples in the pool , which we denote as .\nThe fine-tuning employs the standard next-token prediction loss, with more details specified in Appendix B  ###reference_2###  ###reference_2###.\nWe will show later in Section 5.4  ###reference_###  ###reference_### that fine-tuning can be performed efficiently, with minimal computation overhead. For example, on a single Nvidia A100 GPU card, it only takes less than a minute to fine-tune a persona LLM.\nNow, we can rewrite our objective in Eq. (4  ###reference_###  ###reference_###) as:\nwhere the first term can be calculated by .\nPutting it altogether, to select the ICL examples, we\nevaluate  for each statement , and select the top- statements with the highest  score.\nThen, the selected examples are prepended in the same format as the original query, as exemplified in Figure 1  ###reference_###  ###reference_###.\nIn testing, our mapping function predicts the action of agreeing or disagreeing with the given test statement , defined as:\nwhich is the set of indices for the examples selected from .\nWe summarize our end-to-end framework in Algorithm 1  ###reference_###  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Evaluating Persona Elicitation",
            "text": "In this subsection, we systematically define four evaluation metrics for the general Persona Elicitation task: (1) Action Consistency, (2) Action Confidence, (3) Action Uncertainty, and (4) Degree of Alteration.\nEach metric is computed over a set of test input statements .\nDefinition 1. (Action Consistency) is defined as the fraction of predicted actions  matching ground truth :\nwhere  is an indicator function.\nDefinition 2. (Action Confidence) is defined as the average posterior probability of the selected action :\nDefinition 3. (Action Uncertainty) is defined as the average entropy over the actions in :\nwhere  is the probability corresponding to the -th action in , and is rescaled and normalized across actions so that .\nIn addition to the action-level entropy, we also consider the token-level uncertainty computed over all the tokens in the vocabulary set :\nwhere  corresponds to the -th token in .\nDefinition 4. (Degree of Alteration) is defined as the Kullback-Leibler divergence between the output probability  after persona elicitation and the base model\u2019s output probability :"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "For evaluation, we leverage Anthropic\u2019s Persona dataset (Perez et al., 2022  ###reference_b33###), which encompasses diverse types of personas.\nWe use the Huggingface version which consists of 99 different personas111https://huggingface.co/datasets/Anthropic/model-written-evals  ###reference_model-written-evals###, each entailing 500 statements that align and 500 statements that disagree with the persona trait.\nWe split these 1,000 statements into 700 train samples and 300 test samples randomly, while preserving the label proportion.\nThe train set is used as the sampling pool for ICL baselines and as the training data for our persona fine-tuning phase.\nThe test set is preserved only for evaluation.\nThe list of 99 personas, dataset split indices, and other dataset details are further provided in Appendix A  ###reference_1###.\nWe comprehensively evaluate PICLe on three different LLMs: Llama-2 (Touvron et al., 2023  ###reference_b40###), Vicuna (Chiang et al., 2023  ###reference_b6###), and GPT-J (Wang, 2021  ###reference_b41###).\nSpecifically, we use the \u2018llama-2-7b-chat-hf\u2019 version for Llama-2, which is a model aligned to human preferences using RLHF.\nFor Vicuna, we use the \u2018vicuna-7b-v1.5\u2019 version, which is a model fine-tuned from Llama-2-base without RLHF.\nFinally, we utilize the \u2018gpt-j-6b\u2019 version for GPT-J.\nPersona SFT is performed via a next-token prediction objective, wherein every input sample is a fusion of three statements selected from the persona statement pool .\nFor Persona SFT, we use LoRA (Hu et al., 2021  ###reference_b11###) with rank  and , and train for 4 epochs.\nFor the number of in-context examples, we default to  and perform ablations by varying  in Section 5.1  ###reference_###.\nFor the ICL phase, we map semantically equivalent outputs to an action in the binary action set, .\nMoreover, to encourage the model to respond in a desired way, we also append a system prompt \u201cAnswer with Yes or No only\u201d at the end of each input query, for our method and all baselines.\nFurther implementation details are in Appendix B  ###reference_2###.\nWhile baselines have no consensus on which approach works best in all three models, our PICLe achieves the highest Action Consistency overall. On Llama-2, PICLe achieves an average action consistency of 88.1%, outperforming the current strongest baseline similarity (84.6%) using the same number of in-context examples ().\nMoreover, PICLe demonstrates generally high confidence and low uncertainty in its responses, especially when applied to Llama-2.\nAlso see Appendix F  ###reference_6### for experiment on a bigger Llama-2 model.\nWe verify the performance of PICLe on the non-RLHF models, Vicuna and GPT-J.\nIn particular, without ICL, GPT-J completely fails to follow instructions of responding \u2018yes\u2019 or \u2018no\u2019, making it impossible to report any meaningful performances.\nVicuna, on the other hand, consistently outputs the same response across different statements, with high confidence.\nThis behavior accounts for Vicuna\u2019s Action Consistency of around 50 with near-zero standard deviations.\nWe conjecture that GPT-J and Vicuna being non-RLHF base models contributes to these phenomena.\nHowever, when ICL-based methods are applied, these models too show signs of persona elicitation, with significantly increased action consistency values.\nNotably, PICLe improves the performance from 50.1% (base) to 78.6%, with only three in-context examples.\nIn the original experimental settings (Table 1  ###reference_###), none of the ICL methods have access to the labels of examples in the pool; they select examples in a label-agnostic manner and persona SFT is done on all persona statements disregarding the labels.\nHere, we extend the experimental setting to a label-aware setting.\nSpecifically, the ICL baseline methods now select examples from the positive-labeled statements that align with the persona.\nIn Table 2  ###reference_###, we observe that this selection pool refinement significantly improves the performance of all ICL methods, when evaluated on Llama-2.\nFor instance, the Action Consistency of the Similarity-based ICL improves from 84.6% to 92.4%.\nWe also demonstrate PICLe+, a variant that only uses the positive-labeled statements for Persona SFT and ICL example selection.\nThe table shows that PICLe+ improves PICLe by 5.0% points, and outperforms the similarity baseline, achieving the best performance overall (93.1%)."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Settings",
            "text": "For evaluation, we leverage Anthropic\u2019s Persona dataset (Perez et al., 2022  ###reference_b33###  ###reference_b33###), which encompasses diverse types of personas.\nWe use the Huggingface version which consists of 99 different personas111https://huggingface.co/datasets/Anthropic/model-written-evals  ###reference_model-written-evals###  ###reference_model-written-evals###, each entailing 500 statements that align and 500 statements that disagree with the persona trait.\nWe split these 1,000 statements into 700 train samples and 300 test samples randomly, while preserving the label proportion.\nThe train set is used as the sampling pool for ICL baselines and as the training data for our persona fine-tuning phase.\nThe test set is preserved only for evaluation.\nThe list of 99 personas, dataset split indices, and other dataset details are further provided in Appendix A  ###reference_1###  ###reference_1###.\nWe comprehensively evaluate PICLe on three different LLMs: Llama-2 (Touvron et al., 2023  ###reference_b40###  ###reference_b40###), Vicuna (Chiang et al., 2023  ###reference_b6###  ###reference_b6###), and GPT-J (Wang, 2021  ###reference_b41###  ###reference_b41###).\nSpecifically, we use the \u2018llama-2-7b-chat-hf\u2019 version for Llama-2, which is a model aligned to human preferences using RLHF.\nFor Vicuna, we use the \u2018vicuna-7b-v1.5\u2019 version, which is a model fine-tuned from Llama-2-base without RLHF.\nFinally, we utilize the \u2018gpt-j-6b\u2019 version for GPT-J.\nPersona SFT is performed via a next-token prediction objective, wherein every input sample is a fusion of three statements selected from the persona statement pool .\nFor Persona SFT, we use LoRA (Hu et al., 2021  ###reference_b11###  ###reference_b11###) with rank  and , and train for 4 epochs.\nFor the number of in-context examples, we default to  and perform ablations by varying  in Section 5.1  ###reference_###  ###reference_###.\nFor the ICL phase, we map semantically equivalent outputs to an action in the binary action set, .\nMoreover, to encourage the model to respond in a desired way, we also append a system prompt \u201cAnswer with Yes or No only\u201d at the end of each input query, for our method and all baselines.\nFurther implementation details are in Appendix B  ###reference_2###  ###reference_2###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Baselines",
            "text": ""
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Non-ICL baselines",
            "text": "Base is the most basic approach that directly queries the model with a prompt form, \u201cIs the following statement something you would say? [STATEMENT].\u201d\nInstructive prompting explicitly instructs the model to adhere to the persona by prompting as \u201cAssume that you have or agree with the persona called [PERSONA]. Is the following statement something you would say? [STATEMENT].\u201d\nDescriptive prompting improves the instructive prompting approach by providing a short self-generated description of the target persona.\nThe algorithm for this baseline is:\n(1) Generate a description of persona with \u201cHow would you describe a persona called [PERSONA] in one sentence?\u201d.\n(2) Use the generated description as \u201cThe persona called [PERSONA] can be described as: [DESCRIPTION]. Now assume that you have or agree with this persona. Is the following statement something you would say? [STATEMENT].\u201d\nSee Appendix C  ###reference_3### for sample descriptions."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 ICL baselines",
            "text": "Random selects  ICL examples randomly from pool  for each test query .\nSimilarity selects ICL examples whose sentence embeddings have the highest dot product similarity with respect to the statement in the query.\nWe use the last token embedding extracted from the final layer of the causal language model.\nSee Appendix C  ###reference_3### for a comparison of the embeddings extracted from different layers.\nUncertainty selects ICL examples with high entropy values defined as\n, where . The probability is calculated using the prompt \u201cIs the following statement something you would say? [STATEMENT].\u201d\nWe also consider the token-level entropy by computing the entropy of the probability distribution across the entire vocabulary set, similar to our definition in Eq. (9  ###reference_###).\nThis approach will be denoted Uncertainty-token hereafter.\nCertainty is the opposite of \u201cuncertainty\u201d baseline.\nThe ICL examples with the lowest entropy values are selected.\nSimilarly, the token-level entropy is considered as well, which will be denoted Certainty-token hereafter.\nDiversity selects by maximizing diversity.\n-means clustering is applied to the sentence embeddings to select  samples that are closest to their respective centroid.\nLikelihood selects ICL examples with high log-likelihood values.\nThe log-likelihood of a statement  is evaluated on the base model as , where  is the token length of ."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "Here, we present the main experimental results of Persona Elicitation (Table 1 ###reference_###), evalauated on the test datasets. While baselines have no consensus on which approach works best in all three models, our PICLe achieves the highest Action Consistency overall. On Llama-2, PICLe achieves an average action consistency of 88.1%, outperforming the current strongest baseline similarity (84.6%) using the same number of in-context examples (). Moreover, PICLe demonstrates generally high confidence and low uncertainty in its responses, especially when applied to Llama-2. Also see Appendix F ###reference_6### ###reference_6### for experiment on a bigger Llama-2 model. We verify the performance of PICLe on the non-RLHF models, Vicuna and GPT-J. In particular, without ICL, GPT-J completely fails to follow instructions of responding \u2018yes\u2019 or \u2018no\u2019, making it impossible to report any meaningful performances. Vicuna, on the other hand, consistently outputs the same response across different statements, with high confidence. This behavior accounts for Vicuna\u2019s Action Consistency of around 50 with near-zero standard deviations. We conjecture that GPT-J and Vicuna being non-RLHF base models contributes to these phenomena. However, when ICL-based methods are applied, these models too show signs of persona elicitation, with significantly increased action consistency values. Notably, PICLe improves the performance from 50.1% (base) to 78.6%, with only three in-context examples. In the original experimental settings (Table 1 ###reference_### ###reference_###), none of the ICL methods have access to the labels of examples in the pool; they select examples in a label-agnostic manner and persona SFT is done on all persona statements disregarding the labels. Specifically, the ICL baseline methods now select examples from the positive-labeled statements that align with the persona. In Table 2 ###reference_### ###reference_###, we observe that this selection pool refinement significantly improves the performance of all ICL methods, when evaluated on Llama-2. For instance, the Action Consistency of the Similarity-based ICL improves from 84.6% to 92.4%. We also demonstrate PICLe+, a variant that only uses the positive-labeled statements for Persona SFT and ICL example selection. The table shows that PICLe+ improves PICLe by 5.0% points, and outperforms the similarity baseline, achieving the best performance overall (93.1%)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Analyses",
            "text": "In this section, we provide in-depth analyses of PICLe to better understand its mechanisms and emphasize its advantages.\nWe answer the following research questions:\nRQ1. What is the advantage of PICLe and how does it affect model inference? (See section 5.1  ###reference_### and section 5.2  ###reference_###)\nRQ2. How do different hyperparameter values affect PICLe performance? (See section 5.3  ###reference_###) \nRQ3. How does the efficiency of PICLe compare with other methods? (See section 5.4  ###reference_###)\nThe analyses are done on Llama-2 unless stated otherwise.\nThe effectiveness of utilizing the likelihood ratio is evident in the results presented in Table 3  ###reference_###.\nPICLe and PICLe+ exhibit a notable advantage over either SFT-likelihood or Original-likelihood.\nThen, it follows that the advantage comes from the log-likelihood difference we are taking to evaluate a persona statement.\nThis difference quantifies the level of likelihood change required for the Persona LLM to understand a persona statement.\nA higher degree of change indicates that the statement captures the target persona more thoroughly.\nConsequently, providing the model with statements that better represent the persona leads to enhanced performance in ICL.\nMoreover, as the approach prioritizes the relative change in likelihood values, PICLe emphasizes the importance of the shift in distribution rather than the absolute value learned during the Persona SFT phase.\nHence, only a single epoch of Persona SFT may be enough to render meaningful differences, making PICLe robust to variations in the number of epochs. Relevant experiment is in section 5.3  ###reference_###.\nHere, we provide qualitative examples of the statements selected for ICL.\nWe list below the three statements selected by PICLe+, SFT-likelihood+, and Original-likelihood+, for the target persona called \u201cInterest-in-literature\u201d.\nInterestingly, PICLe+ tends to select long and descriptive statements that better represent the target persona.\nThe two baselines, on the other hand, favor shorter statements that sometimes do not seem to properly embody the persona traits.\nWe provide more examples in Appendix D  ###reference_4###, along with examples extracted from the original experiment setting of Table 1  ###reference_###.\nPICLe+  Interest-in-literature\n1. People get a different experience from reading imaginative books (fiction stories and novels) than from non-fiction and popular non-fiction books, and such fiction helps stimulate their imaginations.\n2. I see great pleasure, and sometimes deep meaning, in reading for many hours at a time.\n3. I enjoy discussing literature, talking about what books and poems mean, speculating on the biographical factors in writers\u2019 work, and related sorts of conversation.\nSFT-likelihood+\n1. Reading is an important part of my life.\n2. Books are an important part of my life.\n3. Reading books is one of my greatest joys.\nOriginal-likelihood+\n1. Books are an important part of my life.\n2. Time spent reading books is time well spent.\n3. Reading is an important part of my life.\nComparing the overall DoA scale of the two persona classes, the \u201cless favorable\u201d personas have strictly higher values than \u201cfavorable\u201d ones.\nThis is intuitive considering that the base model, Llama-2 (llama-2-7b-chat-hf), is already aligned to the Helpfulness and Harmlessness objective.\nThat is, more distribution change is required to understand the \u201cless favorable\u201d persona statements.\nWhen comparing the DoA values between three ICL baselines (Random, Similarity, and PICLe), PICLe mostly renders the smallest values for the \u201cfavorable\u201d personas.\nThat is, our method shifts the distribution minimally for personas that might already be elicited by the Helpfulness and Harmlessness objective.\nThis suggests that our method preserves the original model distribution by avoiding unnecessary distributional changes with respect to familiar persona types.\nFor \u201cless favorable\u201d personas, on the other hand, the Similarity baseline and our PICLe demonstrates similar DoA values overall.\nA key hyperparameter is the number of ICL examples.\nIn Figure 2  ###reference_###, we illustrate the correlation between the number of ICL examples and Action Consistency trends (refer to Appendix E  ###reference_5### for full tables).\nPICLe is compared with the best comparable method from the Llama-2 experiment setting, the Similarity baseline.\nNotably, PICLe consistently outperforms the baseline across various numbers of examples.\nWe can also observe that performance generally improves with more ICL examples for both methods.\nHere, the number of ICL examples is typically proportional to the number of input tokens, which impacts inference latency.\nTherefore, it is important to balance their tradeoff.\nTo further demonstrate PICLe\u2019s effectiveness with larger number of examples, we show the results on Llama-2 evaluated with 10 ICL examples, in Table 5  ###reference_###.\nWhile most baselines\u2019 Consistency improves with more examples, PICLe yet stands out as the best among them achieving 92.3 action consistency.\nIn Table 6  ###reference_###, we reveal how the persona elicitation performances change as the number of Persona SFT epochs is tuned.\nIt shows that the performance does not change significantly with different number of epochs, which is an advantage in terms of hyperparameter tuning.\nNotably, 1 epoch of Persona SFT is enough to outperform the best baseline method on Llama-2 in Table 1  ###reference_###, i.e., the Similarity baseline with 84.6 Action Consistency.\nAnother aspect of efficiency is data efficiency.\nIn Table 8  ###reference_###, we demonstrate how our PICLe performs with smaller amount of data.\nTo elaborate, we use only 70 and 40 of the data to train the persona SFT model and select examples for in-context learning.\nSurprisingly, PICLe retains a high performance of 87.0 consistency even with only 40 of the samples."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Ablation Study",
            "text": "PICLe adopts two models, the original LLM and the persona LLM (i.e., the model after Persona SFT), to compute the log-likelihood difference  (Eq. (5  ###reference_###)).\nIn Table 3  ###reference_###, we compare PICLe with baselines that use only one of the two models for likelihood calculation.\n\u2018SFT-likelihood\u2019 uses only the persona LLM to compute the likelihood of a statement , whereas \u2018Original-likelihood\u2019 is equivalent to the \u2018likelihood\u2019 baseline in Table 1  ###reference_### that uses the original model parameter  to compute the likelihood .\nWe also study PICLe+, whose SFT model is trained only on positive-labeled statements.\nThe effectiveness of utilizing the likelihood ratio is evident in the results presented in Table 3  ###reference_###  ###reference_###.\nPICLe and PICLe+ exhibit a notable advantage over either SFT-likelihood or Original-likelihood.\nThen, it follows that the advantage comes from the log-likelihood difference we are taking to evaluate a persona statement.\nThis difference quantifies the level of likelihood change required for the Persona LLM to understand a persona statement.\nA higher degree of change indicates that the statement captures the target persona more thoroughly.\nConsequently, providing the model with statements that better represent the persona leads to enhanced performance in ICL.\nMoreover, as the approach prioritizes the relative change in likelihood values, PICLe emphasizes the importance of the shift in distribution rather than the absolute value learned during the Persona SFT phase.\nHence, only a single epoch of Persona SFT may be enough to render meaningful differences, making PICLe robust to variations in the number of epochs. Relevant experiment is in section 5.3  ###reference_###  ###reference_###.\nHere, we provide qualitative examples of the statements selected for ICL.\nWe list below the three statements selected by PICLe+, SFT-likelihood+, and Original-likelihood+, for the target persona called \u201cInterest-in-literature\u201d.\nInterestingly, PICLe+ tends to select long and descriptive statements that better represent the target persona.\nThe two baselines, on the other hand, favor shorter statements that sometimes do not seem to properly embody the persona traits.\nWe provide more examples in Appendix D  ###reference_4###  ###reference_4###, along with examples extracted from the original experiment setting of Table 1  ###reference_###  ###reference_###.\nPICLe+  Interest-in-literature\n1. People get a different experience from reading imaginative books (fiction stories and novels) than from non-fiction and popular non-fiction books, and such fiction helps stimulate their imaginations.\n2. I see great pleasure, and sometimes deep meaning, in reading for many hours at a time.\n3. I enjoy discussing literature, talking about what books and poems mean, speculating on the biographical factors in writers\u2019 work, and related sorts of conversation.\nSFT-likelihood+\n1. Reading is an important part of my life.\n2. Books are an important part of my life.\n3. Reading books is one of my greatest joys.\nOriginal-likelihood+\n1. Books are an important part of my life.\n2. Time spent reading books is time well spent.\n3. Reading is an important part of my life."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Degree of Alteration",
            "text": "To understand the persona-wise effect of PICLe, we evaluate Degree of Alteration (Eq. (10  ###reference_###)).\nIn particular, we measure DoA on a subset of 5 personas that can be generally regarded \u201cfavorable\u201d to have in an AI assistant (Table 4  ###reference_### upper), and 6 personas that are \u201cless favorable\u201d (Table 4  ###reference_### lower).\nComparing the overall DoA scale of the two persona classes, the \u201cless favorable\u201d personas have strictly higher values than \u201cfavorable\u201d ones.\nThis is intuitive considering that the base model, Llama-2 (llama-2-7b-chat-hf), is already aligned to the Helpfulness and Harmlessness objective.\nThat is, more distribution change is required to understand the \u201cless favorable\u201d persona statements.\nWhen comparing the DoA values between three ICL baselines (Random, Similarity, and PICLe), PICLe mostly renders the smallest values for the \u201cfavorable\u201d personas.\nThat is, our method shifts the distribution minimally for personas that might already be elicited by the Helpfulness and Harmlessness objective.\nThis suggests that our method preserves the original model distribution by avoiding unnecessary distributional changes with respect to familiar persona types.\nFor \u201cless favorable\u201d personas, on the other hand, the Similarity baseline and our PICLe demonstrates similar DoA values overall."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Impact of Hyperparameters",
            "text": "###figure_2### PICLe has two major hyperparameters: (1) the number of selected ICL examples , and (2) the number of Persona SFT epochs.\nThe impact of these hyperparameters is analyzed.\nA key hyperparameter is the number of ICL examples.\nIn Figure 2  ###reference_###  ###reference_###, we illustrate the correlation between the number of ICL examples and Action Consistency trends (refer to Appendix E  ###reference_5###  ###reference_5### for full tables).\nPICLe is compared with the best comparable method from the Llama-2 experiment setting, the Similarity baseline.\nNotably, PICLe consistently outperforms the baseline across various numbers of examples.\nWe can also observe that performance generally improves with more ICL examples for both methods.\nHere, the number of ICL examples is typically proportional to the number of input tokens, which impacts inference latency.\nTherefore, it is important to balance their tradeoff.\nTo further demonstrate PICLe\u2019s effectiveness with larger number of examples, we show the results on Llama-2 evaluated with 10 ICL examples, in Table 5  ###reference_###  ###reference_###.\nWhile most baselines\u2019 Consistency improves with more examples, PICLe yet stands out as the best among them achieving 92.3 action consistency.\nIn Table 6  ###reference_###  ###reference_###, we reveal how the persona elicitation performances change as the number of Persona SFT epochs is tuned.\nIt shows that the performance does not change significantly with different number of epochs, which is an advantage in terms of hyperparameter tuning.\nNotably, 1 epoch of Persona SFT is enough to outperform the best baseline method on Llama-2 in Table 1  ###reference_###  ###reference_###, i.e., the Similarity baseline with 84.6 Action Consistency."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Computational Efficiency Analysis",
            "text": "PICLe necessitates the computation of two models and leverages an additional Persona SFT phase.\nThus, it becomes imperative to scrutinize the surplus latency incurred during computation.\nIn Table 7  ###reference_###, we report the latency for each phase in seconds, comparing base, similarity, uncertainty, likelihood, and PICLe.\nNote, the latency of certainty baselines is anticipated to be similar to the uncertainty method.\nMost ICL baselines exhibit comparable inference latency of around 30 seconds, although Example Selection latency may vary.\nFor instance, the similarity-based method requires more computation than the uncertainty and likelihood baselines due to an additional dot-product computation for each test query.\nPICLe also introduces more latency as it involves the utilization of two model parameters for  computation (Eq. (5  ###reference_###)).\nOverall, PICLe incurs a relative 22.6 increase compared to the similarity baseline.\nThe Persona SFT step within PICLe, on the other hand, takes 54.8 seconds to fine-tune the model over 4 epochs, translating to approximately 13.7 seconds per epoch. Furthermore, the Persona SFT is a one-time phase that can be conducted prior to inference.\nAnother aspect of efficiency is data efficiency.\nIn Table 8  ###reference_###  ###reference_###, we demonstrate how our PICLe performs with smaller amount of data.\nTo elaborate, we use only 70 and 40 of the data to train the persona SFT model and select examples for in-context learning.\nSurprisingly, PICLe retains a high performance of 87.0 consistency even with only 40 of the samples."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": "With the advent of Large Language Models (LLMs), various natural language processing tasks can be addressed with a single model (OpenAI, 2023  ###reference_b30###; Touvron et al., 2023  ###reference_b40###; Chiang et al., 2023  ###reference_b6###; Zhang et al., 2022  ###reference_b48###; Chung et al., 2022  ###reference_b8###).\nAside from the outstanding performances of LLMs, there has been a growing concern regarding AI safety.\nTo resolve relevant concerns, many works have tried to align the model with human preferences via learnable approaches (Ouyang et al., 2022  ###reference_b31###; Menick et al., 2022  ###reference_b25###; Bai et al., 2022b  ###reference_b4###; Glaese et al., 2022  ###reference_b9###; Mitchell et al., 2022  ###reference_b28###; Korbak et al., 2023  ###reference_b16###; OpenAI, 2023  ###reference_b30###; Hu & Sadigh, 2023  ###reference_b12###; An et al., 2023  ###reference_b1###; Rafailov et al., 2023  ###reference_b34###; Khanov et al., 2024  ###reference_b14###),\nwhile some tried to reveal the vulnerability of LLMs with attack methods (Liu et al., 2020  ###reference_b22###; Shen et al., 2023  ###reference_b39###; Anonymous, 2023  ###reference_b2###; Zou et al., 2023  ###reference_b49###; Wang et al., 2023  ###reference_b42###).\nOn the other hand, an interesting aspect of LLMs is that it can embody different personality traits.\nWolf et al. (2023  ###reference_b44###) analyzed this multi-persona property of LLMs, while many works attempted to elicit a certain persona or behavior with different approaches.\nSome adopted a learnable approach to adapt the model parameters (Mitchell et al., 2022  ###reference_b28###; Yang et al., 2023  ###reference_b47###),\nwhile others tried to design the input prompt in a way that can encourage a certain personality trait (Choi et al., 2022  ###reference_b7###; Mao et al., 2023  ###reference_b23###; Anonymous, 2023  ###reference_b2###; Salewski et al., 2023  ###reference_b37###; Shen et al., 2023  ###reference_b39###; Lin et al., 2023b  ###reference_b20###).\nMoreover, some works also focused on persona evaluation methods for the LLM (Jiang et al., 2023  ###reference_b13###; Scherrer et al., 2023  ###reference_b38###; Safdari et al., 2023  ###reference_b36###; Cheng et al., 2023  ###reference_b5###; Pan & Zeng, 2023  ###reference_b32###).\nDifferent from prior work, PICLe introduces a novel ICL framework to elicit a certain persona from a Bayesian inference perspective.\nICL emerged as an effective means for LLMs to \u201clearn\u201d via demonstrative examples without being fine-tuned to a specific task.\nSeveral works have tried to interpret the underlying mechanism of ICL (Xie et al., 2021  ###reference_b45###; Min et al., 2022b  ###reference_b27###; Kossen et al., 2023  ###reference_b17###),\nwhile some works applied this framework to different natural language processing tasks (Wei et al., 2022  ###reference_b43###; Min et al., 2022a  ###reference_b26###; Xu et al., 2023  ###reference_b46###; Lin et al., 2023b  ###reference_b20###, a  ###reference_b19###).\nAmong ICL literature, many addressed the example selection method via the similarity (Liu et al., 2022  ###reference_b21###), uncertainty and diversity metric (Mavromatis et al., 2023  ###reference_b24###), or with other sophisticated methods (Rubin et al., 2022  ###reference_b35###; Kim et al., 2022  ###reference_b15###; Hongjin et al., 2022  ###reference_b10###; Li et al., 2023  ###reference_b18###). Our work complements existing literature by introducing a novel likelihood-ratio-based selection mechanism."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In the realm of LLM research, the extent to which we can elicit diverse behaviors or personality traits from LLMs stands out as an intriguing question.\nWe accordingly propose PICLe, an In-Context Learning (ICL) method designed to carefully curate demonstrative examples that guide the model in eliciting a specific target persona.\nOur example selection criterion is grounded in the Bayesian inference framework, which demonstrates effectiveness across three distinct LLMs and a wide range of personas.\nFurthermore, to evaluate persona elicitation, we introduce comprehensive evaluation metrics, providing a systematic understanding of the model\u2019s performance.\nThrough extensive analyses, we underscore the effectiveness of PICLe and elucidate the potential of ICL for persona elicitation."
        }
    ],
    "url": "http://arxiv.org/html/2405.02501v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.3",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.3",
            "5.1",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.02501v2",
        "paper_title": "PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning",
        "research_background": "### Paper Motivation:\nThe motivation behind this paper is rooted in the intriguing and underexplored question regarding the extent to which diverse personas encoded within large language models (LLMs) can be elicited. This exploration is crucial for understanding the ethical implications and societal impacts of deploying LLMs, particularly when interacting with human users who possess varied beliefs and values.\n\n### Research Problem:\nThe central research problem addressed in this paper is the formalization and investigation of the \"persona elicitation\" task. This task involves probing LLMs to elicit responses that align with specified personality traits (referred to as personas). The paper aims to develop a framework that can effectively map inputs to the correct actions that reflect a desired persona.\n\n### Relevant Prior Work:\nThe paper situates its work within a context of several strands of prior research:\n\n1. **Diverse Persona Encoding**: LLMs have been acknowledged for their ability to encode diverse personas (Wolf et al., 2023), including a range of personality traits and behaviors like helpfulness, honesty, and harmlessness (Bai et al., 2022a), as well as more nuanced traits like conscientiousness, non-racism, and compassion.\n\n2. **In-Context Learning (ICL)**: The study draws from the success of In-context Learning (ICL) methods across various natural language processing tasks, where queries are modified by prepending task examples (Wei et al., 2022; Min et al., 2022a; Xu et al., 2023; Lin et al., 2023b).\n\n3. **Specific ICL Techniques**: The paper proposes a new framework, Persona In-Context Learning (PICLe), which introduces a likelihood-ratio-based selection mechanism for demonstrative examples to maximize the LLM's likelihood of eliciting the target persona.\n\n4. **Evaluation of LLMs**: The effectiveness of the proposed framework is evaluated on several modern LLMs, including Llama-2 (Touvron et al., 2023), Vicuna (Chiang et al., 2023), and GPT-J (Wang, 2021).\n\nBy leveraging these aspects of prior research, the paper builds upon and advances existing methodologies to address the unique challenge of persona elicitation within LLMs.",
        "methodology": "The methodology section outlines the Persona In-Context Learning (PICLe) framework, designed to elicit diverse behaviors from Large Language Models (LLMs) through the lens of Bayesian inference. Here\u2019s an overview of the proposed method and key components:\n\n### Multi-Persona Decomposition\n\n1. **Latent Personas**: Each LLM response is viewed as stemming from a family of latent personas.\n\n2. **LLM Distribution as Mixture**: The distribution of LLM outputs is decomposed into a mixture of persona distributions. Each persona has a relative weight:\n   \\\n   p(y|x) = \\sum_{k} w_k \\cdot p_k(y|x)\n   \\\n   Here, \\( w_k \\) encodes the relative weight of each persona distribution \\( p_k \\) within the LLM.\n\n### Persona Elicitation via Bayesian Inference\n\n1. **Marginalization Across Personas**: For a prompt \\( x \\), the probability of an output \\( y \\) is marginalized over all personas:\n   \\\n   p(y|x) = \\sum_{k} p(y|x, \\theta_k) \\cdot p(\\theta_k | x)\n   \\\n   - \\( p(\\theta_k | x) \\) represents the likelihood of a persona given the prompt.\n   - \\( p(y|x, \\theta_k) \\) is the action probability conditioned on a persona.\n\n2. **Maximizing Persona Probability**: The objective is to maximize \\( p(\\theta_k | x) \\), thereby adapting the output probability towards the target persona \\( \\theta_k \\).\n\n### Likelihood-Ratio-Based Selection Mechanism\n\n1. **Selection Strategy**: Examples are selected based on a novel likelihood-ratio-based mechanism:\n   \\\n   \\log \\frac{p(D | \\theta_k, \\mathcal{M}_k)}{p(D | \\mathcal{M})}\n   \\\n   - Focus is on increasing the likelihood ratio, which equates to selecting examples with high likelihood under the persona-conditioned model but not well-represented in the original LLM.\n\n2. **Efficient Calculation**: Log-likelihood for example \\( i \\) under the original LLM can be efficiently computed:\n   \\\n   \\log p(y_i | \\mathcal{M})\n   \\\n   - Persona LLM log-likelihood \\( \\log p(y_i | \\mathcal{M}_k) \\) is estimated using a fine-tuned model on examples from a pool \\( \\mathcal{P} \\).\n\n3. **Objective Simplification**:\n   \\\n   \\log p(D | \\theta_k, \\mathcal{M}_k) - \\log p(D | \\mathcal{M})\n   \\\n   - Select top- \\( t \\) statements with the highest score.\n\n### Implementation Steps\n\n1. **Example Selection**: Evaluate and select examples indicative of the target persona, then prepend these examples to the original query in the same format.\n\n2. **Prediction**: During testing, predict the agreement or disagreement with the test statement using the selected examples:\n   \\\n   a_{\\text{test}} = f({\\theta_k^*}, \\mathcal{X}_{\\text{test}})\n   \\\n   - \\( \\theta_k^* \\) represents the indices of selected examples.\n\n### Efficiency\n\n- Fine-tuning a persona LLM is efficient with minimal computational overhead (e.g., less than a minute on an Nvidia A100 GPU).\n\n### Summary Framework\n\nIn summary, the PICLe framework innovates by leveraging Bayesian inference to decompose LLM outputs into persona distributions. It utilizes a likelihood-ratio-based mechanism to select the most informative examples, resulting in efficient and effective persona elicitation from large language models.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiment Setup\n1. **Dataset:**\n   - **Anthropic\u2019s Persona Dataset:** Contains 99 different personas, each with 500 statements that align and 500 statements that disagree with the persona trait.\n   - **Data Split:** The 1,000 statements per persona are split into 700 training samples and 300 test samples, while preserving the label proportion.\n\n2. **Models Evaluated:**\n   - **Llama-2:** \u2018llama-2-7b-chat-hf\u2019 version, aligned to human preferences using RLHF.\n   - **Vicuna:** \u2018vicuna-7b-v1.5\u2019 version, fine-tuned from Llama-2-base without RLHF.\n   - **GPT-J:** \u2018gpt-j-6b\u2019 version.\n\n3. **Training and Evaluation:**\n   - **Persona SFT:** Next-token prediction objective using three statements from the persona pool; employs LoRA for fine-tuning.\n   - **In-Context Learning (ICL):** Averages action responses of \u2018Yes\u2019 or \u2018No\u2019, appending the system prompt \u201cAnswer with Yes or No only\u201d to every query.\n\n4. **Evaluation Metrics:**\n   - **Action Consistency:** Measures alignment with persona traits by averaging performance over multiple queries.\n\n#### Results\n1. **Llama-2 Performance:**\n   - PICLe achieves an average action consistency of **88.1%**, outperforming the strongest baseline, which achieved **84.6%** using the same number of in-context examples.\n   - Exhibits high confidence and low uncertainty in responses.\n\n2. **Vicuna and GPT-J:**\n   - **Vicuna:** Without ICL, Vicuna consistently outputs the same response with high confidence, resulting in an action consistency of around **50%**.\n   - **GPT-J:** Fails to follow instructions without ICL, showing no meaningful performance.\n\nOverall, PICLe shows the highest action consistency, especially on Llama-2, and demonstrates improved performance across various experimental settings."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To assess the impact of using two models for likelihood calculation in PICLe, and to evaluate how focusing on log-likelihood differences enhances Performance.",
            "experiment_process": "The study compares PICLe, which uses both the original LLM and the persona LLM to compute log-likelihood differences, with two baselines: 'SFT-likelihood' (using only the persona LLM's likelihood) and 'Original-likelihood' (using only the original LLM's likelihood). The experiments are conducted using various configurations with the results summarized in Table 3. Additionally, a variant of PICLe called PICLe+ is analyzed, where the SFT model is trained exclusively on positive-labeled statements.",
            "result_discussion": "The results establish that PICLe and its variant PICLe+ have a notable advantage over 'SFT-likelihood' and 'Original-likelihood.' This advantage stems from the log-likelihood difference metric, which efficiently quantifies the degree to which a statement represents the target persona. PICLe+ generally selects longer and more descriptive statements that embody persona traits better than the baselines, which favor shorter, more generic statements.",
            "ablation_id": "2405.02501v2.No1"
        },
        {
            "research_objective": "To investigate the impact of hyperparameters, specifically the number of ICL examples and the number of Persona SFT epochs, on the performance of PICLe.",
            "experiment_process": "The study measures the correlation between the number of ICL examples and Action Consistency using Figure 2 and corresponding tables in the appendix. PICLe is compared against the Similarity baseline from the Llama-2 experimental setting under various numbers of ICL examples. Additionally, experiments in Table 6 assess the impact of different numbers of Persona SFT epochs on persona elicitation performance.",
            "result_discussion": "The experiments reveal that PICLe consistently outperforms the baseline across various numbers of ICL examples, with performance generally improving as the number of examples increases. However, a tradeoff is noted between the number of input tokens and inference latency. The results also show that varying the number of Persona SFT epochs has minimal impact on performance, with just one epoch being sufficient to outperform the best baseline method on Llama-2.",
            "ablation_id": "2405.02501v2.No2"
        }
    ]
}