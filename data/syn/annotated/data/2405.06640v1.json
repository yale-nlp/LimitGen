{
    "title": "Linearizing Large Language Models",
    "abstract": "Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost.\nHowever, their original formulation suffers from poor scaling and underperforms compute-matched transformers. Recent linear models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments. Thus, the search for subquadratic architectures is limited by the availability of compute and quality pre-training datasets.\nAs a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA).111We borrow the term \u201cuptraining\u201d from Ainslie et al. (2023) to refer to continued training with a modified architecture, as opposed to fine-tuning, which usually refers to continued training on a different dataset. We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget. This allows us to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost.\nWe find that our linearization technique leads to competitive performance on standard benchmarks, but we identify persistent in-context learning and long-context modeling shortfalls for even the largest linear models. Our code and models can be found at https://github.com/TRI-ML/linear_open_lm.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Over the last few years, Transformers (Vaswani et al., 2017  ###reference_b41###) have displaced Recurrent Neural Networks (RNNs) in sequence modeling tasks, owing to their highly parallel training efficiency and unmatched scaling performance (Kaplan et al., 2020  ###reference_b18###). However, this training efficiency comes at the cost of inference cost that scales linearly with the number of tokens, compared to the fixed-cost inference of RNNs. The memory-intensive nature of transformers has led to renewed interest in recurrence\u2014the fixed-size hidden state remains an attractive modeling proposition to reduce the cost of inference for language and multimodal models.\nSeveral recent works, starting with Linear Transformers (Katharopoulos et al., 2020  ###reference_b20###), have observed a relationship between a linearized form of attention and recurrence, leading to a duality between transformers and RNNs: models can be trained with sequence parallelism (i.e. as transformers, avoiding backpropagation through time), but can operate as RNNs at inference time. Although this architecture allows efficient training of RNNs, softmax transformers continue to outperform linear transformers across natural language understanding benchmarks.\nA number of novel RNN architectures have attempted to bridge this performance gap. These include RWKV (Peng et al., 2023a  ###reference_b27###), Retentive Networks (Sun et al., 2023  ###reference_b38###), TransNormer (Qin et al., 2022a  ###reference_b32###),\nand more recently, Griffin (De et al., 2024  ###reference_b9###) and RecurrentGemma (Griffin Team et al., 2024  ###reference_b12###). These models are pre-trained on the same pre-training datasets as transformers and show promising results.\nState-space models (Gu et al., 2021  ###reference_b14###) (SSMs) are another recurrent alternative to softmax transformers, combining RNNs and convolutional networks to efficiently model long sequences. The Mamba (Gu & Dao, 2023  ###reference_b13###) architecture is a SSM that shows impressive performance at smaller scales, matching or exceeding the performance of softmax transformers on a number of natural language understanding (NLU) benchmarks. However, a gap remains for long-context NLU tasks, showing a persistent advantage of softmax attention.\nArchitecture search at the scale of large language models is expensive.\nRather than pre-training linear models, another approach is to convert an existing transformer into an RNN; Kasai et al. (2021  ###reference_b19###) proposed to uptrain encoder-decoder transformers into RNNs by introducing an approximating MLP attention module. Zhang et al. (2024  ###reference_b45###) improved on this method by adding a loss to match softmax attention to approximate more closely the base transformer.\nWhile approximating attention is an intriguing approach to re-using pre-trained transformers, it leads to instability and poor performance when uptraining large-scale models. We instead take a different approach: rather than approximate softmax attention, we replace it with a linear kernel and a normalization strategy to uptrain the most performant LLMs into RNNs (see Figure 2  ###reference_###). We take advantage of models trained on high-quality, proprietary datasets\nfor trillions of tokens (e.g. Mistral (Jiang et al., 2023  ###reference_b17###) and Llama2 (Touvron et al., 2023  ###reference_b40###)). Fine-tuning these models on publicly available data for a small fraction of pre-training tokens (see Figure 1  ###reference_###), we obtain linear models that are competitive with the best linear transformers for a fraction of the compute. We call our approach Scalable UPtraining for Recurrent Attention (SUPRA).\n###figure_1### Our contributions are as follows:\nWe propose Scalable UPtraining for Recurrent Attention (SUPRA), a linearization strategy to uptrain state-of-the-art LLMs into performant RNNs.\nWe show that this simple uptraining technique is competitive with the strongest pre-trained recurrent LLMs.\nWe investigate the limitations of recurrent LLMs, comparing pre-trained and uptrained RNNs to transformers, revealing a persistent gap for in-context learning and long-context tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "In this section we review linear transformers, and describe the linearization technique of  Kasai et al. (2021  ###reference_b19###) as it lays the groundwork for our approach. Finally, we present SUPRA, our method for uptraining large transformers into RNNs.\nLinear attention can be expressed as an RNN that updates a state  and a normalization factor  at each time step. Katharopoulos et al. (2020  ###reference_b20###) call these terms the attention memory and normalized memory. This RNN formulation is mathematically equivalent to linear attention, allowing the user to choose the most efficient one for a given task and hardware.\nConsider a stream of tokens we want to generate\n. At inference time, we use the following update rule, where subscripts denote timestep in the recurrence (calling , etc):\nThe state  acts as a constant-size KV cache. Instead of appending new values to the cache, the state is updated. This allows for inference cost that is constant in the number of generated tokens."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Background: Linear Attention",
            "text": "Linear Transformers (Katharopoulos et al., 2020  ###reference_b20###) establish a connection between transformers and RNNs, generalizing the definition of attention by replacing the softmax dot-product attention  with a more generic similarity function  between the queries  and keys :\nStandard softmax attention is a special case, using .\nThe authors explore several alternative functions for , including a linear kernel. Their main architecture uses the similarity function  with a fixed exponential linear unit kernel . They show the computational benefits of linear attention and, more importantly for this work, they demonstrate how such a model can be expressed as an RNN in the case of attention with causal masking.\nLinear attention can be expressed as an RNN that updates a state  and a normalization factor  at each time step. Katharopoulos et al. (2020  ###reference_b20###  ###reference_b20###) call these terms the attention memory and normalized memory. This RNN formulation is mathematically equivalent to linear attention, allowing the user to choose the most efficient one for a given task and hardware.\nConsider a stream of tokens we want to generate\n. At inference time, we use the following update rule, where subscripts denote timestep in the recurrence (calling , etc):\nThe state  acts as a constant-size KV cache. Instead of appending new values to the cache, the state is updated. This allows for inference cost that is constant in the number of generated tokens."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Finetuning a Transformer into an RNN",
            "text": "Kasai et al. (2021  ###reference_b19###) introduced a linear transformer uptraining procedure that converts a pre-trained softmax transformer into an RNN by approximating the attention computation with multi-layer perceptrons (MLPs). The method (T2R) starts with a softmax attention model, and linearizes the softmax operation. Recall the kernel linear attention similarity function:\nInstead of choosing  as a simple non-linearity, the authors use a trainable layer:\nThe weights are shared between keys and queries for a given attention head. By using  and rearranging the operations, attention can be written as:\nThis allows the recurrent inference described in Section 2.1  ###reference_###.\nHowever, this formulation has a number of drawbacks. First, it requires a significant re-training of the model, using approximately  of pre-training tokens for conversion, while suffering a  drop in performance on language benchmarks. Furthermore, this approach was tested on relatively small models ( scale). Because it mimics the attention formulation closely, it suffers from stability issues at larger scales. To address these issues, we modify the approach to adapt it to large-scale model uptraining."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "SUPRA: Scalable UPtraining for Recurrent Attention",
            "text": "Rather than pre-training linear models from scratch, we choose to instead uptrain state-of-the-art transformers. Leveraging models that take advantage of high-quality (but proprietary) pre-training datasets, we linearize them using a modest fraction of pre-training data (see Figure 1). We build on T2R, identifying two major issues and proposing SUPRA, an approach to fine-tuning very large transformers into RNNs.\n\nNext we note that linear attention suffers more with absolute positional encoding than softmax attention, and a modern relative positional encoding scheme like RoPE (Su et al., 2021) is crucial for competitive performance. Rather than training a linear transformer from scratch incorporating these findings (RetNet, TransNormer) we use MLP kernels to convert large language models into RNNs.\n\nStarting with the pre-trained model, we add weights shared between keys and queries, and use the rotary positional embedding (RoPE (Su et al., 2021)) such that the similarity function becomes\n\nWe use a fixed decay vector, with heads, as in Sun et al. (2023).\n\nThis leads to the following attention formulation (see Figure 2 for a graphical representation): These new parameters are trained jointly with the rest of the network; at test time, we use the recurrent formulation for inference."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We uptrain a variety of models from the 1B to 7B range into RNNs (Llama2 (Touvron et al., 2023  ###reference_b40###) and Mistral (Jiang et al., 2023  ###reference_b17###)), and evaluate our models in two settings: standard language understanding benchmarks and long-context evaluations. We compare the results of different architectural choices and training strategies, and then show the limitations of linear models on various benchmarks, describing the persistent gap between vanilla attention and recurrence. We choose Llama2-7B and Mistral-7B as our base models for uptraining, but our recipe is general to any transformer model.\nWe compare our procedure to a variety of pre-trained recurrent models. Given that the largest available state-space models are at the 2.8B scale, we also train a Mamba model on the RefinedWeb (Penedo et al., 2023  ###reference_b26###) dataset from scratch for 1.2T tokens, to serve as a strong baseline for a pre-trained recurrent model 222The Mistral-SUPRA  ###reference_a### and Mamba-7B  ###reference_### models are released along with the code  ###reference_###..\nWe use a fork of OpenLM  ###reference_### (Gururangan et al., 2023  ###reference_b15###) for all training and fine-tuning. Please see Section 7  ###reference_### for hyperparameters and further details on reproducibility.\n###table_1### In Table 1  ###reference_### we report results on standard NLU evaluations using the Eleuther evaluation harness (Gao et al., 2023  ###reference_b11###). We primarily compare to transformers and linear models at the 7B scale, and we train a Mamba model at 7B for comparison with RWKV-5.\nAs our model is initialized from strong pre-trained transformers (Llama2 and Mistral-7B), it preserves performance on most benchmarks (except MMLU; see Section 4  ###reference_### for a discussion below).\nOur technique outperforms RWKV-5 with minimal uptraining and is competitive with our 7B Mamba trained from scratch on 1.2T tokens.\nRecurrent models were thought to perform well on long-context tasks because of their ability to preserve performance beyond their training sequence size. However, their downstream performance on long-context tasks has not been well-documented.\nPrior studies either do not conduct long-context evaluations (Katharopoulos et al., 2020  ###reference_b20###; Kasai et al., 2021  ###reference_b19###), evaluate only on perplexity (Sun et al., 2023  ###reference_b38###; De et al., 2024  ###reference_b9###; Gu & Dao, 2023  ###reference_b13###), or evaluate on datasets which require task-specific training (Peng et al., 2023a  ###reference_b27###).\nInstead, we consider downstream natural language tasks from the SCROLLS benchmark (Shaham et al., 2022a  ###reference_b35###). Specifically, in Table 2  ###reference_### we present two tasks \u2013 Qasper (Dasigi et al., 2021  ###reference_b8###) and NarrativeQA (Ko\u010disk\u00fd et al., 2018  ###reference_b21###) \u2013 from the set of tasks evaluated in the Llama2-Long report (Xiong et al., 2023  ###reference_b43###).\nWe evaluate both tasks with an input context cut-off at different lengths. A strong long-context model should perform better given more context. However, the training context lengths for these models do not go beyond 8k tokens. Transformer models show the strongest results up to the context length they were trained for but degrade beyond that. Interestingly, applying the YaRN trick (Peng et al., 2023b  ###reference_b28###) enables transformers to scale beyond their training context quite well. RWKV shows a strong ability to handle much longer context than its training. Our Mamba model on the contrary is not able to generalize beyond its training context length. Surprisingly, the RecurrentGemma model (Griffin Team et al., 2024  ###reference_b12###) shows degrading performances even within its training context length.\nFinally, our Mistral-SUPRA  model preserves some performance at larger context lengths but we believe it to result from the decay along the context length that shortens the effective context. This is discussed in more details below.\nWe find a significant gap in performance between transformers and available linear models, including models uptrained from strong long-context transformers. We speculate that more sophisticated recurrent state update rules may be required to perform well at this task. Ideas such as gating strategies (De et al., 2024  ###reference_b9###), higher order linear attention (Mercat, 2020  ###reference_b24###), or associative binding (Munkhdalai et al., 2024  ###reference_b25###) could be explored.\nThe default decay factors proposed in Qin et al. (2024  ###reference_b34###) gives better results than no decay on short context benchmarks but at a long range, the decay cancels out the influence of the context (). This can be related to a smooth version of window attention (Beltagy et al., 2020  ###reference_b4###). However, as more context is given to the model, the long-range evaluation performances plateau. When using the values proposed in Sun et al. (2023  ###reference_b38###), that allow longer range attention, we observe a performance drop on short-context benchmarks and no substantial improvement on long-context evaluation.\nTable 3  ###reference_### compares transformers pre-trained on 100B tokens to Mamba (Gu & Dao, 2023  ###reference_b13###), T2R (Kasai et al., 2021  ###reference_b19###), and our approach. At this scale, with 100B tokens of training, the Mamba model performs best and other models show similar performance.\nThe second half of Table 3  ###reference_### shows results for uptraining from a pre-trained transformer. TheT2R (Kasai et al., 2021  ###reference_b19###) uptraining was unstable, yielding poor results compapred to SUPRA. This confirms that normalization is key for maintaining performance of the base LLM when uptraining.\nTo test the hypothesis that linear attention approximates the softmax attention, we experimented with a 2-step approach. The first step trains only the new parameters such that the model could learn to approximate the softmax. The second step fine-tunes all the weights. The results show no benefit from the two steps approach and indicates that the softmax is not approximated. See Appendix A  ###reference_### for a different approach to compare softmax attention and linear attention.\nFinally, we compare the results of SUPRA uptrainings from two pre-trained softmax models. It appears that pre-training a linear model for a 100B token yields better results than fine-tuning a softmax model that was trained with the same budget.\nThese results also shows, along with the comparison of LLama2-SUPRA and Mistral-SUPRA in Table 1  ###reference_###, that SUPRA benefits significantly from a stronger pre-trained transformer. Thus, given a limited training budget, using SUPRA from a strong pre-trained model is the best option."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Comparison to pre-training SSMs/RNNs. With only 20B tokens of training, which represents  of RWKV and RetNet training cost, we obtain a model that outperforms both on HellaSwag and that is competitive on other benchmarks (see Table 1  ###reference_###).\nGiven the existing performance gap between the strongest transformer models and the most performant linear models, SUPRA is a simple recipe for conversion, allowing the study of strong RNNs with limited uptraining.\nComparison to Transformers on Short-Context Tasks. Our approach does not explicitly approximate attention from the base transformer model (see Appendix A  ###reference_###), we do see a modest drop in performance across all benchmarks compared to softmax transformers. This could be partially explained by the lower quality of our data compared to the pre-training mix used to train models like Mistral-7B. It is also likely that linear transformers are inherently less expressive. However, the performance drop is relatively modest on most benchmarks, and significantly smaller than the drop from T2R uptraining, which shows the relevance of our approach.\nLong Context Comparisons.\nPrior work on linear attention showcased similar or better validation set perplexity to transformer models over long context (e.g. Sun et al. (2023  ###reference_b38###)) but did not evaluate linear models on natural language long-context evaluations like SCROLLS (Shaham et al., 2022b  ###reference_b36###). The results in Table 2  ###reference_### show that recurrent models generally maintain performance beyond their training context (except for Mamba-7b) while transformers (without modification) do not. However, Table 2  ###reference_### also demonstrates that simple linear scaling of the rotary positional embedding (Peng et al., 2023b  ###reference_b28###; emozilla, 2023  ###reference_b10###) can allow for context scaling beyond the window used for training a given transformer model, effectively nullifying the performance edge of these linear models. Furthermore, transformers generally outperform linear models at their maximum training context length.\nFurther research is needed into extending linear models to long-context inference to take full advantage of the lower inference cost relative to vanilla transformers.\nLimitations.\nSince our method relies on initializing with strong pre-trained transformers, our models inherit any of the biases and weaknesses of their base models. Additionally, models that are already instruct-tuned do not linearize as well as base models. Our models suffer from poor performance on MMLU which requires in-context learning (5-shot), a weakness of linear models (Aky\u00fcrek et al., 2024  ###reference_b2###). We leave the investigation of these weaknesses of linear models to future work and hope that our proposed uptraining approach can help facilitate and accelerate the research in this area."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "The linear transformers introduced in Katharopoulos et al. (2020  ###reference_b20###) lagged behind vanilla transformers in downstream performance, and subsequent architectures such as TransNormer (Qin et al., 2022a  ###reference_b32###) and RetNet (Sun et al., 2023  ###reference_b38###) narrow the gap, but do not demonstrate competitive results with modern transformers at scale.\nRWKV (Peng et al., 2023a  ###reference_b27###), a linear transformer that takes inspiration from LSTM (Hochreiter & Schmidhuber, 1997  ###reference_b16###), is competitive with compute-matched transformer-based models, but lags behind on a number of NLU benchmarks.\nGriffin (De et al., 2024  ###reference_b9###) is a concurrent model that takes a hybrid approach, combining a sliding window with linear attention shows impressive performance relative to vanilla transformers, but is trained on a high-quality proprietary dataset.\nAnother thread in the literature focuses on efficient attention alternatives (Performers (Choromanski et al., 2020  ###reference_b6###), Cosformer (Qin et al., 2022b  ###reference_b33###), LUNA (Ma et al., 2021  ###reference_b23###), RFA (Peng et al., 2021  ###reference_b29###), Attention-free Transformer (Zhai et al., 2021  ###reference_b44###)). All of these approaches sacrifice performances for efficiency. Efficiency improvements for vanilla transformers have narrowed the capabilities gap between vanilla and linear transformers. The KV cache Pope et al. (2023  ###reference_b31###)greatly narrows the inference efficiency gap between linear and vanilla transformers. RingAttention Liu et al. (2023  ###reference_b22###) allows for very long context scaling of vanilla attention without approximation.\nState-space models (SSMs) such as H3 (Dao et al., 2022  ###reference_b7###), Hyena (Poli et al., 2023  ###reference_b30###), and Mamba (Gu & Dao, 2023  ###reference_b13###) are recent alternatives to vanilla transformers, combining the strengths of convolutional and recurrent models with efficient hardware implementations.\nInstead of parallelizing training over the sequence, they produce an efficient way to train the sequential RNN. While these models are competitive with vanilla transformers on some tasks, we show that SSMs share the limitations of linear transformers on several in-context learning and long-context tasks.\nHedgehog (Zhang et al., 2024  ###reference_b45###) builds on the work of Kasai et al. (2021  ###reference_b19###), identifying three different ways of training linear transformers \u2013 from scratch, uptraining quadratic transformers for a specific task, and uptraining generally.\nThe authors focus on the first two, and we focus on the third. Moreover, they aim at approximating the softmax attention matrices with linear alternatives. In this work, we do not aim to approximate softmax attention, we replace it with a linear alternative (see ablation above and appendix A  ###reference_###).\nTheir method is only tested for smaller scale models and with parameter-efficient fine-tuning for larger models, but presents challenges for scaling for two reasons: (1) their training strategy involves comparing full attention matrices which is computationally expensive, and not feasible for full fine-tuning of large models with long sequences and (2) their method also inherits the gradient instabilities of linear transformers studied in Sun et al. (2023  ###reference_b38###), while our normalization setup leads to stable uptraining of large models."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduced SUPRA, a technique for converting large-scale pre-trained softmax transformers into recurrent neural networks, enabling the study of the strengths and limitations of recurrent models at scale with minimal compute cost. Compared to pre-training linear models from scratch, the SUPRA strategy produces competitive models comparable to the best available recurrent LLMs (RWKV and Mamba) at the 7B scale.\nWe identify the strengths of linear models on standard NLU benchmarks but also the enduring limitations on in-context (i.e. MMLU) and long-context (NarrativeQA, Qasper) tasks, showing that linearized models do not inherit these capabilities from the base softmax transformers.\nOne possible path to rectifying these limitations is explicitly training for in-context learning (Aky\u00fcrek et al., 2024  ###reference_b2###). We leave explorations of specialized and instruct data in the context of linear transformers to future work. More sophisticated gating mechanisms as in in Peng et al. (2023a  ###reference_b27###) and De et al. (2024  ###reference_b9###) are promising alternatives to our simple linearization.\nUsing our uptraining method would greatly reduce the necessary time and cost of such experimentation."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Reproducibility",
            "text": "We train our linear models using our fork  ###reference_### of OpenLM (Gururangan et al., 2023  ###reference_b15###) that we modify to include a linear attention function (printed below). We use Lightning Attention 2 (Qin et al., 2024  ###reference_b34###) that offers a fast Triton (Tillet et al., 2019  ###reference_b39###) kernel for linear attention computation. Evaluations are done with the Eleuther evaluation harness (Gao et al., 2023  ###reference_b11###).\nWe train and uptrain models on RefinedWeb (Penedo et al., 2023  ###reference_b26###)(with 2 epochs for our Mamba training), which we tokenize with the pre-trained model\u2019s tokenizers. When training from scratch, we used the GPT-NeoX-20B (Black et al., 2022  ###reference_b5###) tokenizer. We tokenize with sequence packing and use a default sequence length of 2048.\nWe use square matrices with biases for the linear layers in the kernel  functions to keep the same feature dimension in the queries and keys. We use the same kernel, with the same weights for both queries and keys and apply a ReLU activation.\nWe use 1000 steps of linear learning rate warmup and a cosine learning rate decay from  to  for our 7B uptrainings and from  to  for our 1B uptrainings and for trainings from scratch. We use the Adam optimizer with  and .\nWe trained our models with mini-batches totaling 2M tokens each.\nOur default RoPE frequency uses the Llama value of . For longer sequence lengths, we use a RoPE frequency of .\nDepending on the model size and the availability, we use from 4 to 32 nodes of 8 GPUs Nvidia H100 with Pytorch FSDP. We use a mixed precision strategy from OpenLM that automatically selects between bfloat 16 and float 32 for different operations. A linear 7B parameter model uptraining throughput is around  tokens per second per GPU.\nOur results can be reproduced by following the same training recipe or using the model weights that we release: Mistral-SUPRA  ###reference_a### and Mamba-7b  ###reference_###."
        }
    ],
    "url": "http://arxiv.org/html/2405.06640v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "4",
            "5"
        ],
        "methodology_sections": [
            "2",
            "2.3"
        ],
        "main_experiment_and_results_sections": [
            "3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "4"
        ]
    },
    "research_context": {
        "paper_id": "2405.06640v1",
        "paper_title": "Linearizing Large Language Models",
        "research_background": "### Motivation\nTransformers have revolutionized sequence modeling tasks due to their highly parallel training efficiency and scalability. However, their inference cost, which scales linearly with the number of tokens, and their memory-intensive nature present significant challenges. Despite the impressive efficiency in training, the high inference cost creates a need for models with fixed-size hidden states, similar to Recurrent Neural Networks (RNNs), which offer reduced inference costs. This motivation is driven by the need to develop models that combine the efficient training of transformers with the fixed-cost inference of RNNs.\n\n### Research Problem\nThe research problem addressed by this paper is the high cost of inference associated with transformers in large language models (LLMs) and the performance gap that exists between softmax transformers and their recurrent alternatives, such as linear transformers and state-space models (SSMs). More specifically, the paper aims to explore novel methods to transform the training advantages of transformers into efficient, high-performance recurrent models that remain competitive even on long-context natural language understanding (NLU) tasks.\n\n### Relevant Prior Work\n1. **Transformers vs. RNNs**: Transformers have outperformed RNNs due to their parallel efficiency and scaling capabilities (Vaswani et al., 2017), but remain memory-intensive and have linear inference costs.\n2. **Linear Transformers**: Works such as Katharopoulos et al. (2020) have introduced linearized forms of attention that exhibit relationships with recurrence, but softmax transformers still outperform linear transformers on NLU benchmarks.\n3. **Novel RNN Architectures**: Several novel RNN architectures like RWKV (Peng et al., 2023a), Retentive Networks (Sun et al., 2023), TransNormer (Qin et al., 2022a), Griffin (De et al., 2024), and RecurrentGemma (Griffin Team et al., 2024) have been pre-trained on similar datasets as transformers, showing promising results but still not closing the performance gap.\n4. **State-Space Models (SSMs)**: SSMs (Gu et al., 2021) combine RNNs and convolutional networks. The Mamba architecture (Gu & Dao, 2023) shows impressive results but falls short on long-context tasks.\n5. **Uptraining Pre-trained Transformers**: Kasai et al. (2021) and Zhang et al. (2024) proposed methods to convert transformers into RNNs by approximating attention mechanisms but faced instability and performance issues in large-scale models.\n\n### Contribution\nThe paper introduces the \"Scalable UPtraining for Recurrent Attention (SUPRA)\" method, which aims to linearize and uptrain state-of-the-art LLMs into efficient RNNs. This approach replaces softmax attention with a linear kernel and normalization strategy to produce competitive linear models without incurring the high computational costs. By fine-tuning pre-trained models on a fraction of the pre-training dataset, SUPRA aims to deliver high-performance RNNs, shedding light on the limitations and potential of recurrent LLMs for in-context and long-context tasks.",
        "methodology": "In the Methodology section, the paper outlines the process of linearizing large language models and introduces SUPRA, a novel method for uptraining large transformers into Recurrent Neural Networks (RNNs). This section is divided into a review of linear transformers, a description of the linearization technique by Kasai et al. (2021), and the introduction of SUPRA.\n\n### Linear Attention and its RNN Equivalence\nLinear attention can be implemented as an RNN that updates two key components at each time step:\n\n1. **Attention memory (state \\( S_t \\))**\n2. **Normalized memory (normalization factor \\(\\phi_t\\))**\n\nKatharopoulos et al. (2020) refer to these updates in their work, which provides a mathematical equivalence between linear attention and this RNN formulation. This equivalence allows users to choose the more efficient implementation based on their task and hardware requirements.\n\n### Update Rule for Generating Tokens\nFor a stream of tokens \\( \\{x_t\\} \\) to be generated, the state \\( S_t \\) and normalization factor \\(\\phi_t\\) are updated at each timestep \\( t \\). The subscripts in the equations denote the current timestep in the recurrence, using variables like \\( S_t \\), \\(\\phi_t\\), etc.:\n\n```\n(timestep t) <- (update rule here)\n```\n\n### Constant-Size KV Cache\nIn this approach, the state \\( S_t \\) functions as a constant-size Key-Value (KV) cache. This cache is updated with new values rather than appending them, resulting in an inference cost that remains constant regardless of the number of generated tokens.\n\n### Proposed Method: SUPRA\nSUPRA is introduced as the proposed method for uptraining large transformers into RNNs. This method leverages the aforementioned linear attention-based RNN formulation to create a system where inference efficiency is significantly enhanced. By maintaining a constant-size KV cache and updating it at each timestep, SUPRA provides a scalable solution for generating tokens with large language models. \n\nOverall, the key innovations in this methodology include:\n- The reformation of linear attention into an RNN-like structure.\n- The use of constant-size KV caches to maintain efficiency during inference.\n- The introduction of SUPRA to adapt large transformers effectively into RNNs for better performance.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Experiment Setup:**\n\n1. **Models and Datasets:**\n   - **Models:** The study focused on Llama2 (Touvron et al., 2023) and Mistral (Jiang et al., 2023) in the parameter range of 1B to 7B. The uptraining process converted these models into RNN architectures.\n   - **Datasets:** The models were evaluated using standard language understanding benchmarks and long-context evaluations. Specifically, for long-context tasks, the SCROLLS benchmark was used, which includes the Qasper and NarrativeQA tasks.\n\n2. **Baselines:**\n   - **Pre-trained Recurrent Models:** Comparisons were made with existing recurrent models such as RWKV-5 and Mamba.\n\n3. **Training Tools and Strategies:**\n   - **Training Framework:** OpenLM framework was utilized for all training and fine-tuning activities.\n   - **Uptraining Procedure:** The models Llama2-7B and Mistral-7B were chosen as base models for uptraining into recurrent models.\n\n**Evaluation Metrics:**\n- **Standard NLU Evaluations:** Utilized the Eleuther evaluation harness to measure performance on language understanding tasks.\n- **Long-context Evaluations:** Performance was gauged on long-context tasks from the SCROLLS benchmark, with specific attention to input context cut-offs at various lengths.\n\n**Main Experimental Results:**\n\n1. **Standard NLU Results:**\n   - The uptrained models preserved performance on most benchmarks relative to their transformer counterparts, except for MMLU.\n   - The proposed technique outperformed RWKV-5 with minimal uptraining and proved competitive with the 7B Mamba trained from scratch on 1.2T tokens, demonstrating effectiveness in standard NLU settings.\n\n2. **Long-context Results:**\n   - Tested on tasks like Qasper and NarrativeQA from the SCROLLS benchmark, performance was evaluated with different context lengths.\n   - Strong long-context models are expected to improve with more context. Transformer models performed well up to their training context length but degraded beyond that.\n   - Applying the YaRN trick allowed transformers to scale beyond their training context effectively, while RWKV handled much longer contexts well.\n\nThese results underline the significant gap in performance between transformers and linear models, suggesting the need for more advanced recurrent state-update rules and strategies to improve long-context performance."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the effectiveness of uptraining existing large pre-trained transformers into Recurrent Neural Networks (RNNs) and compare their performance to both standard transformers and pre-trained recurrent models.",
            "experiment_process": "Models from the 1B to 7B range, specifically Llama2-7B and Mistral-7B, were uptrained into RNNs. The training and fine-tuning were executed using a fork of OpenLM. The experiments were conducted on standard language understanding benchmarks using the Eleuther evaluation harness. Long-context evaluations included tasks from the SCROLLS benchmark, specifically Qasper and NarrativeQA, to assess performance with varying context lengths. Additionally, a Mamba model was trained from scratch on the RefinedWeb dataset for comparison.",
            "result_discussion": "The uptrained models preserved performance on most benchmarks and outperformed the RWKV-5 model with minimal uptraining. However, they did not achieve the same performance as models trained from scratch on substantial data. In long-context tasks, the recurrent models generally maintained performance beyond their training context, while transformers needed modifications like the YaRN trick to scale beyond their training context. There is a significant performance gap between transformers and available linear models, indicating a need for more sophisticated recurrent state update rules for better performance.",
            "ablation_id": "2405.06640v1.No1"
        },
        {
            "research_objective": "Investigate the performance of linear models uptrained from pre-trained transformers on both short-context and long-context tasks.",
            "experiment_process": "The performance on standard benchmarks was compared to transformers and linear models at the 7B scale, including a 7B Mamba model trained from scratch. The analysis included downstream natural language tasks from the SCROLLS benchmark, with evaluations on both short and extended context lengths. This included testing the preservation of downstream performance by the models uptrained from strong pre-trained transformers like Llama2-Long and the implications of different decay factors on context length performance.",
            "result_discussion": "On short-context tasks, the uptrained models showed modest performance drops compared to softmax transformers, attributed to lower data quality and intrinsic differences in expressiveness. Long-context task evaluations demonstrated that recurrent models often preserve performance past their training context length, unlike transformers. However, transformers with modifications like rotary positional embedding can effectively nullify recurrent models\u2019 performance edge at longer contexts. The results underscore the strengths and current limitations of linear models in handling extended contexts efficiently.",
            "ablation_id": "2405.06640v1.No2"
        },
        {
            "research_objective": "Compare uptraining methods and their influence on maintaining performance of base models, and explore the hypothesis that linear attention can approximate softmax attention.",
            "experiment_process": "Models were subjected to a two-step training approach to examine if training new parameters before fine-tuning all weights could approximate softmax attention. Performance comparisons included models pre-trained on 100B tokens and uptraining from pre-trained softmax models, with experiments designed to contrast performance stability and drops across different approaches (SUPRA versus methods like T2R).",
            "result_discussion": "The two-step approach did not demonstrate any benefit in approximating softmax attention. SUPRA showed better results in retaining performance for models pre-trained on a significant token budget. T2R uptraining was less stable, confirming the importance of normalization in maintaining performance. The study observed that uptraining from a strong pre-trained transformer model benefits SUPRA, indicating the best approach under limited training budgets.",
            "ablation_id": "2405.06640v1.No3"
        }
    ]
}