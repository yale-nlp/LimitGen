{
    "title": "Can Small Language Models be Good Reasoners for Sequential Recommendation?",
    "abstract": "Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities. However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses.\nSecondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a \u201cslim\u201d (i.e., resource-efficient) manner. We introduce CoT prompting based on user behavior sequences for the larger teacher model. The rationales generated by the teacher model are then utilized as labels to distill the downstream smaller student model (e.g., LLaMA2-7B). In this way, the student model acquires the step-by-step reasoning capabilities in recommendation tasks. We encode the generated rationales from the student model into a dense vector, which empowers recommendation in both ID-based and ID-agnostic scenarios. Extensive experiments demonstrate the effectiveness of SLIM over state-of-the-art baselines, and further analysis showcasing its ability to generate meaningful recommendation reasoning at affordable costs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "Sequential recommendation is extensively utilized in a variety of internet applications due to its prominent performance in uncovering a user\u2019s evolving and dynamic interests from his/her chronological interactions (Quadrana et al., 2018  ###reference_b24###).\nDespite the effectiveness, existing models are often trained on a closed-loop user-item interaction dataset, inevitably suffering from severe exposure bias and popularity bias.\nTherefore, beyond narrow information present in the original datasets, it is crucial to incorporate open-world knowledge to foster a more comprehensive and generalized understanding of historical behaviors.\nDue to the impressive reasoning capability, the recent emergence of Large Language Models (LLMs), such as GPT 3.5/4, has brought a significant breakthrough in various NLP tasks (Qin et al., 2023  ###reference_b23###; Zhang et al., 2023b  ###reference_b42###; Wei et al., 2023  ###reference_b35###; Pan et al., 2023  ###reference_b22###), showing substantial potential in overcoming the isolated nature of real-world sequential recommenders that rely on closed data sources for training (Sun et al., 2019  ###reference_b26###; Chen et al., 2018  ###reference_b3###). These LLMs are trained on massive corpora, granting them to exhibit the remarkable capability of human-like thinking as well as seamless reasoning.\nRoughly speaking, current LLM empowered recommenders mainly fall into the following two groups:\n(1)  LLM as a ranker, which typically involves prompting the frozen LLM to offer a reasonable ranked list that satisfies the user interests (Hou et al., 2023  ###reference_b11###).\nHowever, solely relying on the zero-shot or few-shot learning capability of LLMs is still inferior compared to traditional sequential recommendations that utilize in-domain collaborative knowledge.\nTo address this limitation, (2) LLM as a knowledge enhancer has been proposed, typically following a cascading architecture: the LLM is first instructed to generate rich knowledge (e.g., user preference and\nfactual knowledge on items), followed by a classical recommendation backbone for harvesting in-domain knowledge and collaborative signals. Generally, the bridging of both worlds tends to elicits a more promising performance (Xi et al., 2023  ###reference_b38###).\nWhile LLMs for recommendation hold promise, they also face significant challenges that cannot be ignored.\nOne is the exceptional reasoning capability of LLM within the context of recommendation has not been fully explored. There is a gap between the open-world nature and recommender systems, which means that the recommendation knowledge generated by LLMs may be incorrect or task-irrelevant.\nFortunately, with the chain-of-thought (CoT) prompting strategy (Wei et al., 2022  ###reference_b34###; Magister et al., 2022  ###reference_b21###; Hsieh et al., 2023  ###reference_b12###), LLMs can break down complex tasks into a series of intermediate reasoning steps, which can improve the ability to understand behavior patterns and explore user interests.\nConsequently, there is a strong motivation to leverage the CoT reasoning capability of LLMs in sequential recommender systems, enabling the generation of targeted recommendation-related rationales. For instance, guiding LLMs to reason progressively, similar to a human salesperson, to deduce user interests, narrow down the categories of items that align with their interests, and ultimately recommend specific items within these categories that the user is likely to interact with.\nAnother significant challenge is the prohibitively high resources are far beyond affordable for real-world recommender systems. The immense size of LLMs demands a considerable amount of memory and computational power, which necessitates specialized infrastructure. For instance, the deployment of the open-source LLaMA2-70B requires eight Nvidia A100 servers.\nOn the other hand, working with closed-source LLMs also involves significant costs. For instance, using ChatGPT as an example, the current approach requires calling its API, which comes with substantial monetary expenses. For instance, in gpt-3.5-turbo, the costs are approximately $0.0015 per 1,000 tokens for input and $0.002 per 1,000 tokens for output.\nTherefore, a natural question arises:\nCan a language model with affordable costs still serve as \nan effective reasoning engine for sequential recommendation?\nTo answer this question, in this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), which enables sequential recommendations to enjoy the significant reasoning capabilities of LLMs in a \u201cslim\u201d (i.e., resource-efficient) manner.\nSpecifically, we develop a step-by-step knowledge distillation strategy for sequential recommendations to transfer the reasoning capabilities of LLMs (i.e., teacher) to a \u201csmall\u201d language model (i.e., student). This strategy guides the larger teacher model to engage in macro-to-micro thinking for complex recommendation task through CoT prompting.\nThrough the process of distillation, the small student model with only 4% parameters of the large teacher model acquires step-by-step thinking capabilities and evolves into a good reasoner. Subsequently, we directly deploy the small language model as a knowledge generator for sequential recommendation, which can derive high-quality reasoning knowledge highly relevant to recommendation. These knowledge reflect user preferences for categories, brands, and specific items, which can be flexibly integrated with any sequential recommendation backbone, including ID-based and ID-agnostic scenarios.\nOur key contributions can be summarized as follows:\nTo the best of our knowledge, it is the first knowledge distillation framework of LLMs tailored for sequential recommendation.\nWe propose SLIM, a novel step-by-step knowledge distillation framework, empowering sequential recommenders with the CoT reasoning capabilities of LLMs in a resource-efficient manner.\nExtensive experiments on three datasets demonstrates the effectiveness of our proposed SLIM. Further analysis reveals that SLIM generates meaningful reasoning at affordable costs."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. The Proposed Framework",
            "text": "###figure_1### In this section, we propose SLIM, a novel knowledge distillation framework tailored for recommendation, which incorporates the reasoning capabilities of LLMs into recommender systems in a resource-efficient manner. The overview is illustrated in Figure 1  ###reference_###."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. Sequential Recommendation Backbone",
            "text": "Sequential recommendation aims at the accurate prediction of users\u2019 next behavior by capturing evolved and dynamic preferences over historical behavior sequences, which has occupied a critical position in various modern information systems (Quadrana et al., 2018  ###reference_b24###).\nIn general, the success of sequential recommendation typically hinges on the meaningful representation of items and effectively encoding behavior patterns.\nItem Representation. \nFor neural sequential recommendations, the item encoder is the key component which transfers the items to representations.\nFormally, given an item set , each item  may be associated with several optional attributes , such as title, category and brand. The encoder can generate the item representations for each item based on their ID (i.e., ) and attributes (i.e., ):\nwhere  is the representation of item .\nGenerally,  is implemented as a hybrid architecture where a embedding layer aims at tackling ID-like features (e.g., item id), coupled with a text encoder (e.g., BERT (Devlin et al., 2018  ###reference_b6###)) for context embedding based on the item description (e.g., title, category).\nSequential Encoding. \nTo capture the sequential characteristics of user behaviors, the action sequence of user  can be organized in chronological order , where  represents the -th item that the user  interacted with.\nNext, each item in  is firstly fed into  (denoted as ), followed by the a sequential encoding.\nwhere  denotes the representation of sequence .  is sequence encoder, which can be implemented with the Attention (Vaswani et al., 2017  ###reference_b30###) or other neural architectures (Zhang et al., 2019  ###reference_b41###; Sherstinsky, 2020  ###reference_b25###).\nBased on the sequence , our objective is to predict the next item  that the user  is likely to interact with at the -th step.\nPrediction and Optimization.\nAfter generating the above representations, we can obtain the final prediction  at time  with dot product or MLP layer followed by a sigmoid activation function (He et al., 2017  ###reference_b8###), where each element  indicates how likely the item  should be recommended to the target user .\nFinally, the model is trained with binary cross-entropy (De Boer et al., 2005  ###reference_b5###) loss as follows:\nNote that these classical sequencical models typically perform recommendation based on the user action sequences and item attributes (e.g., title, category and brand), lacking the reasoning power that have recently emerged in LLMs."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. Step-by-Step Knowledge Distillation for Recommendation",
            "text": "Despite the remarkable reasoning ability of LLMs, it is non-trivial to adapt LLMs to empower the traditional recommender systems.\nThe challenge arises from two aspects: (1) Complex behavior patterns of users are difficult to understand directly by LLMs. (2) The large size and high inference latency of LLMs exacerbates resource-consuming.\nTherefore, we propose step-by-step knowledge distillation to transfer the reasoning capabilities of LLMs to a smaller LLaMA2-7B (Touvron et al., 2023  ###reference_b29###) model specialized for the recommendation tasks.\nIn detail, our distillation strategy consists of two straightforward steps:\nFirstly, we employ CoT prompting related to user behavior to guide the LLM (i.e., teacher) in thinking step-by-step and generating natural language rationales that support its predictions in the recommendation scenario.\nSecondly, these rationales are subsequently utilized as labels to fine-tune the downstream smaller language model (i.e., student), enabling it to approach the reasoning capabilities of the larger model in the recommendation domain. Finally, the fine-tuned smaller model acts as the ultimate knowledge generator, offering reasoning knowledge to the recommender systems."
        },
        {
            "section_id": "2.2.2",
            "parent_section_id": "2.2",
            "section_name": "2.2.2. Fune-tuning Smaller Models with Recommendation Rationales.",
            "text": "By guiding the thought process of LLMs step-by-step, we can comprehend complex behavior patterns of users and generate high-quality recommendation rationales.\nHowever, their large scale and computational overhead make them unsuitable for recommendation scenarios that require low latency. For instance, serving a single 175 billion LLM necessitates a minimum of 350GB of GPU memory (Zheng et al., 2022  ###reference_b44###).\nDespite recent study (Xi et al., 2023  ###reference_b38###) attempting to mitigate this issue by offline inference, it\u2019s still unaffordable to generate recommendation rationales for all users in the real-world scenario.\nTo this end, we leverage knowledge distillation to transfer the recommendation reasoning capabilities of larger teacher models to smaller student models, thereby reducing the computational overhead.\nConsidering that complex prompts can improve the reasoning quality of large models but greatly increase the understanding difficulty of small models, we design simplified template  based on the template , as showed in Figure 3  ###reference_###.\nSubsequently, we generate simplified prompts  as input, and collect the rationales  generated by teacher LLMs as the expected output labels to fine-tune the smaller student model.\nAs a result, for a given input instruction , we train the smaller model with parameters  to generate the corresponding recommendation rationale . Formally, we optimize the negative log-likelihood of conditional language modeling objective as follows:\nwhere  is the - token of the ,  represents the tokens before .\nTo conserve resources, we employ the LoRA (Hu et al., 2021  ###reference_b13###) for parameter-efficient model fine-tuning.\nThis approach involves training only a small set of additional parameters instead of the entire model.\nThrough experimental validation, we demonstrate that the generated rationales maintain a comparable quality to models with 25 times the model size, despite using a limited number of training samples and a smaller model size. As illustrated in Figure 3  ###reference_###, the student model responses showed a step-by-step reasoning ability similar to that of the teacher model.\nFor instance, the student model initiates by logically inferring the user\u2019s intent by leveraging its recommendation-related CoT. Subsequently, it offers potential game genres that align with the user\u2019s interests. Ultimately, several specific games are recommended to the user.\nOverall, by utilizing recommendation rationales as labels instead of generating pseudo-labels for recommended results from LLMs, we enhance the smaller language model with step-by-step reasoning capabilities similar to the reasoning process of the larger model.\n###figure_3###"
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "2.3. Empowering Recommender with Reasoning Knowledge",
            "text": "With the help of step-by-step knowledge distillation, small language models can become efficient reasoners. However, traditional sequential recommendation models cannot directly utilize the rationales of natural language forms.\nThus, in this section, we explore how to apply the recommendation rationales generated by small language models to the sequence recommendation model, enabling it to efficiently combine the reasoning ability of LLMs in a resource-efficient manner.\nSpecifically, we introduce two application approaches.\nThe first approach is ID-based, where we treat the rationales text as supplementary knowledge and combine them with ID-based recommendation backbone to improve the traditional closed-loop learning dependent on user-item interactions.\nThe second approach is ID-agnostic, where we encode the rationale text of user behaviors and the description text of candidate items as the representations of user and item, respectively. This allows us to make recommendations based on text similarity."
        },
        {
            "section_id": "2.3.1",
            "parent_section_id": "2.3",
            "section_name": "2.3.1. Encoding Recommendation Rationales",
            "text": "Owing to the efficient reasoning power of small language model, each user behavior sequence  can be associated with corresponding CoT rationales , while each item  can be associated with attribute descriptions  (e.g., title, category, brand).\nThen we leverage pre-trained language models (PLMs) to learn text representations, enabling the measurement of semantic distance in vector space. Concretely, we adopt the text encoder to map the text on both the item side and the sequence side into a unified semantic space:\nwhere  and  represent the text representations of item descriptions and recommendation rationales, respectively. The encoder  can be flexibly configured as any frozen or trainable text encoding model, which we instantiate with BERT (Devlin et al., 2018  ###reference_b6###) in this work.\nDue to the step-by-step thinking process of language model, the representation  encodes rich reasoning knowledge from open-world at both macro-level (i.e., general user preference) and micro-level (i.e., specific recommended item)."
        },
        {
            "section_id": "2.3.2",
            "parent_section_id": "2.3",
            "section_name": "2.3.2. Utilizing Recommendation Rationales",
            "text": "Traditional sequential recommendation methods learn the user sequence representation based only on the user-item interaction history, resulting in an information-enclosed model.\nTo alleviate this issue, we enhance the traditional recommender systems with the rationale representations obtained from Eq. (5  ###reference_###), which is derived from open-world knowledge and deep reasoning about the user\u2019s behavior patterns. Specifically, we leverage it through the following two approaches.\nEmpowering ID-Based Recommendation.\nTo disrupt the closed systems of sequential recommendation, we integrate the rationale representations into traditional recommendation backbone, effectively combining the open-world reasoning knowledge with the collaborative signal of traditional recommendations.\nSpecifically, we propose an information fusion layer to combine the meaningful text representations (i.e.,  and ) with the original embeddings in the backbone model\nas follows:\nwhere  is the ID embedding of item,\n is the sequence representation obtained from SeqEncoder (i.e., Eq (2  ###reference_###)) in backbone model,  denotes the concatenation operation,  transforms the text representations to the same dimension with the ID embeddings, and  is an fusion layer that enables the model to learn and incorporate flowing information from both sources. Without loss of generality, we implement  and  with linear layers.\nEmpowering ID-Agnostic Recommendation.\nRecent studies have revealed that sequential models that focus on text modeling exhibit superior generalization abilities and are more effective in handling cold-start items (Li et al., 2023  ###reference_b16###; Hou et al., 2022  ###reference_b10###).\nTherefore, we explore a direct utilization of rationale representations in ID-agnostic recommendation scenarios. In this case, the representations of item text and rationale are directly transformed into a unified space as follows:\nwhere  denotes the transformation layer, which we implement using linear layers.\nSince  contains step-by-step reasoning knowledge about user preferences, the model can recommend item with matching item-side information  to the user and provide explainable recommendation rationales of natural language form."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Experiments",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Experimental Settings",
            "text": ""
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1. Datasets.",
            "text": "We conduct our experiments on three categories from the Amazon Review dataset: Video Games (Games), Grocery and Gourmet Food (Food), and Home and Kitchen (Home). More information\nabout these datasets are presented in Appendix  A.1  ###reference_###."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2. Baselines.",
            "text": "We adopt three widely used sequencial recommendation models as the backbone, i.e., GRU4Rec, SASRec, SRGNN. More information\nabout these backbones are shown in Appendix A.2  ###reference_###.\nFor each backbone, we examine the performance of its Item Feature Extensions: denoted as , , and . These extensions concatenating the item ID vector and item description text vector as the input, resulting in enhanced item representations.\nWe also introduce another ChatGPT Feature Extension of each backbone: , which directly input the rationales generated by the teacher model into Eq (5  ###reference_###) without distillation.\nThe implementation details of each methods shown in Appendix  A.3  ###reference_###."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3. Evaluation Metrics",
            "text": "The details of evaluation shown in Appendix  A.3  ###reference_###.\nWe utilize three widely-adopted metrics for evaluation: NDCG@10, Hit Rate@10, and Hit Rate@20. The average scores of 5 runs and the standard deviation are reported. Following the strategy in (Kang and McAuley, 2018  ###reference_b14###),\nwe randomly sample 100 negative items for each user  and rank these items alongside the ground-truth item. The rankings of these 101 items are then used to evaluate."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Overall Performance",
            "text": ""
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1. Improvement over Backbone Models in ID-based scenarios.",
            "text": "Our SLIM is highly flexible and can be integrated with any type of sequential recommendation backbone. Firstly, we evaluate the performance of the SLIM across various backbones. The results of these comparisons are presented in Table 1  ###reference_###. We have the following observations:\n(1) Compared to all backbones and their item feature extensions, the proposed SLIM achieves state-of-the-art (SOTA) performance across all datasets. This further substantiates the effectiveness of our model in enhancing traditional recommendations. Notably, SLIM achieves a relative improvement of  over the  in terms of Hit Rate@10 on the Home dataset. These improvements are attributed to the meaningful rationales generated from our distilled student model, which contains a wealth of knowledge that benefits recommendations as a valuable supplement to closed collaborative signals.\n(2) Surprisingly, in most cases (22 / 27), SLIM in each backbone outperforms the ChatGPT feature extensions , achieving a relative improvement of  in terms of Hit Rate@10 on the Home dataset with the GRU4Rec backbone. While SLIM\u2019s knowledge is distilled from the teacher model ChatGPT, the lack of control over closed-source models may result in the generation of correct but irrelevant responses to recommendations. This indicates that our smaller model can further prioritize the information relevant to recommendations after distillation. Despite being smaller in scale, it greater effectiveness in recommendations."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2. Performance in ID-agnostic scenarios.",
            "text": "To establish a more efficient and generalizable model, we evaluate the performance of SLIM in ID-agnostic scenarios, i.e., we solely based on matching CoT-based sequence embeddings and text-based item embeddings as Eq (7  ###reference_###), named Text Matching. The results are shown in Table 2  ###reference_###. We also obtained interesting findings:\n(1) In comparison to models that generate rationales based on the teacher model (), SLIM outperforms it in  of cases, despite having only  of the parameters compared to ChatGPT. This demonstrates that even with limit training samples (1000-2000) and a smaller model size, SLIM can generate high-quality recommendation rationales that are highly competitive with ChatGPT.\n(2) Additionally, this straightforward matching approach exhibits superior performance compared to all ID-based backbones listed in Table 1  ###reference_###. This indicates that high-quality text from both the sequence and item side can lead to promising recommendations, even without meticulous design of the text encoder.\n(3) To verify the effectiveness of each step in the rationales, i.e., the user interest of Step1, the item category of Step2, and the specific product of Step3, we evaluate them separately in Text Matching. It is worth noting that the ranking of recommendation performance consistently follows the pattern of Step3 \u00bf Step2 \u00bf Step1 in all cases.\nSurprisingly, on the Home dataset, Step3 even surpasses the performance achieved using the entire Rationale.\nThese results suggest that the smaller model trained with CoT prompting is capable of step-by-step thinking, similar to human reasoning. As the chain of thought evolves, the information relevant to recommendations will be inferred.\nHowever, the performance of the Step1 is not satisfactory, possibly because the macroscopic information in this step fails to align well with the microscopic information on the item side, such as titles and categories. Nevertheless, the first step still plays a crucial role as the foundation for subsequent reasoning processes and ensures the interpretability of the model."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3. Merits of SLIM",
            "text": "###figure_4### Potentially Good Interpretability for the Recommendation Results.\nFigure 4  ###reference_### illustrates a sample where SLIM successfully recommends the ground-truth, while SASRec fails.\nThe target next item in this sample is a long-tail item that only appears once in the training set. As a result, traditional ID-based models struggle to capture adequate collaborative signals. However, SLIM\u2019s generated rationales are able to deduce the user\u2019s preferences, which align closely with the characteristics of the target item \u201cMilk Protein Rich Nutrition Bar\u201d, such as the categories of \u201cHealthy snacks\u201d.\nMore significantly, SLIM showcases its remarkable reasoning capabilities and extensive domain knowledge by accurately inferring that users are likely to purchase \u201cProtein Bars\u201d.\nIn this manner, the textual information from both the sequence side and item side aligns well, leading to a high similarity in the vector space. Moreover, SLIM generates rationales in human-understandable natural language. The rationales provided in Step 1 and Step 2 offer justifications for the recommendation of \u201cProtein Bars\u201d by SLIM. For each recommended item, SLIM can provide a natural language explanation, enhancing the interpretability of the recommendation process.\nConsistent Improvement for User with Different Sparsity.\nTo investigate the impact of interaction data sparsity, we group users based on the sparsity level of their interactions and evaluate the performance of SLIM separately on different user groups.\nSpecifically, we sorted users based on their interaction frequency, and then divided them equally into five user groups. Subsequently, SLIM and SASRec are trained separately on the interaction data of each user group, and compare their recommendation performance on different user groups.\nThe results, as depicted in Figure 5  ###reference_###, that SLIM consistently outperforms SASRec across all user groups, and SLIM exhibits greater improvement on the relatively sparse user group  compared to the dense user group . This suggests that our method\u2019s improvement is stable and robust, effectively mitigating the issue of sparsity in sequential recommendation.\n###figure_5### ###figure_6### ###figure_7### Impressive Capability of Alleviating Popularity Bias.\nIn the field of recommender systems, popularity bias means that popular items are recommended even more frequently than their popularity would warrant. This bias intensifies the long-tail effects in real-world recommendation domains.\nTo analyze the impact of our proposed SLIM on popularity bias, we count the frequency of items in the training data and recommendation results.\nAs depicted in Figure 6  ###reference_### (the results of two additional datasets are presented in Figure 7  ###reference_### in the Appendix), our method effectively recommends tail items compared to the traditional method SASRec, which focuses on recommending popular head items.\nExperimental results confirm that our method significantly mitigate the popularity bias.\n###figure_8### Significantly More Affordable Compared to SOTAs. SLIM has demonstrated promising performance. In this part, we will analyze the efficiency of the model through a comprehensive cost analysis across multiple dimensions, including time cost, model size, deployment difficulty and API monetary cost.\nSpecifically, we compare two representative recommendation models based on the generation capabilities of LLM.\n(1) LLM as a ranker (Hou et al., 2023  ###reference_b11###). (2) LLM as a knowledge enhancer (Xi et al., 2023  ###reference_b38###).\nWe do not take into account the costs associated with backbones, as the backbone model is typically variable and the cost is generally negligible compared to LLM.\nDue to the two-stage nature of the LLM as a knowledge enhancer approach, it involves offline knowledge generation based on LLM and online inference. The term \u201cOffline/Online Time\u201d refers to the average response time of the closed-source ChatGPT API for the compared methods. Conversely, our method corresponds to the average inference time on a single Nvidia A100 GPU. It is worth mentioning that despite deploying SLIM on only one GPU, we achieve a comparable time cost compared to the API call duration of ChatGPT, which requires significant resource consumption for deployment.\nFrom Table 3  ###reference_###, it can be concluded that SLIM is a highly efficient model compared to existing LLM-based recommendations. It possesses acceptable inference latency, minimal model size, and can be deployed on limited resources. Additionally, being based on an open-source model, it does not incur any financial cost.\nSpecifically, we study the data efficiency of fine-tuning the student models, and we conclude that only 1000 samples is enough for a promising performance, the details are shown in Appendix B.1  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Related Work",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. Sequential Recommender Systems",
            "text": "The core idea of existing sequential recommendations lies in initially formalizing user behavior as a chronologically-ordered interaction sequence with items, followed by designing diverse behavior encoders to learn behavior patterns that accurately depict user interests (Wang et al., 2019  ###reference_b31###).\nGRU4Rec (Hidasi et al., 2015  ###reference_b9###) is one of the earliest attempts to learn evolving patterns for user behaviors using Gated Recurrent Units (GRU).\nWith the rapid development of deep learning (Wang et al., 2017  ###reference_b32###), there have also been emerging many neural networks as behavior encoders, including Convolutional Neural Networks-based methods (Tang and Wang, 2018  ###reference_b28###), Attention-based methods (Kang and McAuley, 2018  ###reference_b14###), and Graph Neural Networks-based methods (Wu et al., 2019  ###reference_b37###; Sun et al., 2024  ###reference_b27###; Wang et al., 2022  ###reference_b33###).\nTo enhance the transferability of the sequence modeling, recent studies have begun exploring ID-agnostic text-based modeling approaches (Li et al., 2023  ###reference_b16###; Hou et al., 2022  ###reference_b10###), such as Recformer (Li et al., 2023  ###reference_b16###), which proposes formulating each item as a \u201csentence\u201d and designing a Transformer-based language model to learn user preference representations.\nHowever, these methods rely on limited textual information provided by the recommendation dataset, which restricts the model\u2019s capabilities due to its isolation from rich open-world knowledge. Recently, the emergence of LLM that utilize massive training corpora and large model sizes has disrupted the traditional closed-loop of user-item interaction in recommendations (Liu et al., 2023a  ###reference_b20###, b  ###reference_b19###)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. LLM Enhanced Recommender Systems",
            "text": "The utilization of LLMs, with their human-like understanding and generation capabilities, introduces new knowledge spaces in recommendations (Lin et al., 2023a  ###reference_b17###; Fan et al., 2023  ###reference_b7###; Wu et al., 2023  ###reference_b36###).\nTo integrate LLM\u2019s generation capabilities into recommendations, the current methods can be primarily categorized into two mainstream trends based on the different roles LLM plays in the recommendation pipeline.\nThe first trend involves utilizing LLM as a ranker or scorer (Zhang et al., 2023a  ###reference_b39###; Liu et al., 2023b  ###reference_b19###; Dai et al., 2023  ###reference_b4###; Hou et al., 2023  ###reference_b11###; Zhang et al., 2023c  ###reference_b40###; Kang et al., 2023  ###reference_b15###).\nFor instance,  (Hou et al., 2023  ###reference_b11###) explores the zero-shot ranking capabilities of LLM in recommendation. This requires careful design of prompts that involve a predefined list of candidate items for the limited re-ranking stage.\n (Zhang et al., 2023c  ###reference_b40###) proposes to view recommendation as instruction following by LLMs. In this approach, 39 instruction templates are manually designed for LLMs.\nHowever, these methods often exhibit limited performance because the frozen LLMs are typically trained on open-world corpora that lack domain-specific collaborative signals from recommendations.\nTo incorporate collaborative information, recent studies have started exploring another trend, which involves utilizing LLM as a knowledge enhancer to complement traditional recommendations (Liu et al., 2023a  ###reference_b20###; Xi et al., 2023  ###reference_b38###).\nFor example,  (Xi et al., 2023  ###reference_b38###) explores the acquisition of user preferences and item factual knowledge from ChatGPT, and utilizes them to enhance traditional Click-Through Rate (CTR) prediction.\n (Liu et al., 2023a  ###reference_b20###) proposes to employ open-source LLM as content encoders and utilize closed-source ChatGPT to enrich the training data.\n (Lin et al., 2023b  ###reference_b18###) proposes a LLM-based augmentation technique to enrich training samples.\nWhile promising, existing work has not fully leveraged the step-by-step reasoning capabilities of LLM in the recommendation scenario. Furthermore, current approaches often rely on the use of large model sizes to achieve improved reasoning capabilities. Although techniques like prestoring can be used to deploy only the inference model, these models still require larger model sizes in either offline or online stages, which may not be feasible in real-world recommender systems."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Conclusion",
            "text": "In this paper, we propose SLIM, a method that enables sequential recommender systems to leverage the substantial reasoning capabilities of LLMs in a resource-efficient manner. We design a step-by-step knowledge distillation module to transfer the step-by-step reasoning capabilities in recommendation from a larger teacher model to a smaller student model (with approximately 4% of the parameters of the teacher model).\nThis smaller model evolves into a proficient reasoner, which can be directly deployed as a \u201cslim\u201d knowledge generator for sequential recommendation. Consequently, this knowledge can be flexibly integrated with any sequential recommendation backbone and utilized in both ID-based and ID-agnostic scenarios.\nThe experimental results demonstrate that SLIM significantly improves the performance of sequential recommendation backbones. It also achieves promising results in ID-agnostic scenarios without relying on any backbone. Furthermore, additional analysis experiments highlight that the costs associated with SLIM are affordable and have the potential to enhance the interpretability of recommendations. A possible future direction is to design customized knowledge encoders to further capture the information from smaller models."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Details of Experimental Settings",
            "text": "###figure_9###"
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B More Experimental Results",
            "text": ""
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 1. </span>ID-based scenarios. Comparison of recommendation performance among different backbones. The best results are highlighted in bold. \u201dImprov.\u201d indicates the relative improvement of SLIM compared to the best performance in backbones (original backbone and ).</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.35\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.35.34.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T1.35.34.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.35.34.1.1.1\" style=\"font-size:90%;\">Methods</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S3.T1.35.34.1.2\"><span class=\"ltx_text\" id=\"S3.T1.35.34.1.2.1\" style=\"font-size:90%;\">Games</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S3.T1.35.34.1.3\"><span class=\"ltx_text\" id=\"S3.T1.35.34.1.3.1\" style=\"font-size:90%;\">Food</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S3.T1.35.34.1.4\"><span class=\"ltx_text\" id=\"S3.T1.35.34.1.4.1\" style=\"font-size:90%;\">Home</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.35.35.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.35.35.2.1\"><span class=\"ltx_text\" id=\"S3.T1.35.35.2.1.1\" style=\"font-size:90%;\">NDCG@10</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.35.35.2.2\"><span class=\"ltx_text\" id=\"S3.T1.35.35.2.2.1\" style=\"font-size:90%;\">Hit @10</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.35.35.2.3\"><span class=\"ltx_text\" id=\"S3.T1.35.35.2.3.1\" style=\"font-size:90%;\">Hit @20</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.35.35.2.4\"><span class=\"ltx_text\" id=\"S3.T1.35.35.2.4.1\" style=\"font-size:90%;\">NDCG@10</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.35.35.2.5\"><span class=\"ltx_text\" id=\"S3.T1.35.35.2.5.1\" style=\"font-size:90%;\">Hit @10</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.35.35.2.6\"><span class=\"ltx_text\" id=\"S3.T1.35.35.2.6.1\" style=\"font-size:90%;\">Hit @20</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.35.35.2.7\"><span class=\"ltx_text\" id=\"S3.T1.35.35.2.7.1\" style=\"font-size:90%;\">NDCG@10</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.35.35.2.8\"><span class=\"ltx_text\" id=\"S3.T1.35.35.2.8.1\" style=\"font-size:90%;\">Hit @10</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.35.35.2.9\"><span class=\"ltx_text\" id=\"S3.T1.35.35.2.9.1\" style=\"font-size:90%;\">Hit @20</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.35.36.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.35.36.1.1\"><span class=\"ltx_text\" id=\"S3.T1.35.36.1.1.1\" style=\"font-size:90%;\">GRU4Rec</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.36.1.2\"><span class=\"ltx_text\" id=\"S3.T1.35.36.1.2.1\" style=\"font-size:90%;\">17.61 \u00b1 0.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.36.1.3\"><span class=\"ltx_text\" id=\"S3.T1.35.36.1.3.1\" style=\"font-size:90%;\">30.87 \u00b1 0.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.36.1.4\"><span class=\"ltx_text\" id=\"S3.T1.35.36.1.4.1\" style=\"font-size:90%;\">42.39 \u00b1 0.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.36.1.5\"><span class=\"ltx_text\" id=\"S3.T1.35.36.1.5.1\" style=\"font-size:90%;\">9.10 \u00b1 0.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.36.1.6\"><span class=\"ltx_text\" id=\"S3.T1.35.36.1.6.1\" style=\"font-size:90%;\">15.27 \u00b1 0.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.36.1.7\"><span class=\"ltx_text\" id=\"S3.T1.35.36.1.7.1\" style=\"font-size:90%;\">19.51\u00b1 0.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.36.1.8\"><span class=\"ltx_text\" id=\"S3.T1.35.36.1.8.1\" style=\"font-size:90%;\">2.19 \u00b1 0.21</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.36.1.9\"><span class=\"ltx_text\" id=\"S3.T1.35.36.1.9.1\" style=\"font-size:90%;\">4.17 \u00b1 0.39</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.36.1.10\"><span class=\"ltx_text\" id=\"S3.T1.35.36.1.10.1\" style=\"font-size:90%;\">7.53 \u00b1 0.64</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.3.1.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.2\"><span class=\"ltx_text\" id=\"S3.T1.3.1.2.1\" style=\"font-size:90%;\">27.33 \u00b1 0.53</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.3\"><span class=\"ltx_text\" id=\"S3.T1.3.1.3.1\" style=\"font-size:90%;\">44.06 \u00b1 0.79</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.4\"><span class=\"ltx_text\" id=\"S3.T1.3.1.4.1\" style=\"font-size:90%;\">56.53 \u00b1 1.24</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.5\"><span class=\"ltx_text\" id=\"S3.T1.3.1.5.1\" style=\"font-size:90%;\">17.75\u00b1 0.78</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.6\"><span class=\"ltx_text\" id=\"S3.T1.3.1.6.1\" style=\"font-size:90%;\">31.10 \u00b1 1.09</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.7\"><span class=\"ltx_text\" id=\"S3.T1.3.1.7.1\" style=\"font-size:90%;\">45.01 \u00b1 1.46</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.8\"><span class=\"ltx_text\" id=\"S3.T1.3.1.8.1\" style=\"font-size:90%;\">12.19 \u00b1 1.02</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.9\"><span class=\"ltx_text\" id=\"S3.T1.3.1.9.1\" style=\"font-size:90%;\">26.76 \u00b1 2.58</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.10\"><span class=\"ltx_text\" id=\"S3.T1.3.1.10.1\" style=\"font-size:90%;\">49.10 \u00b1 3.82</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.4.2.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.4.2.2\"><span class=\"ltx_text\" id=\"S3.T1.4.2.2.1\" style=\"font-size:90%;\">27.70 \u00b1 0.47</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.4.2.3\"><span class=\"ltx_text\" id=\"S3.T1.4.2.3.1\" style=\"font-size:90%;\">45.13\u00b1 0.56</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.4.2.4\"><span class=\"ltx_text\" id=\"S3.T1.4.2.4.1\" style=\"font-size:90%;\">57.70 \u00b1 0.37</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.4.2.5\"><span class=\"ltx_text\" id=\"S3.T1.4.2.5.1\" style=\"font-size:90%;\">17.97 \u00b1 0.70</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.4.2.6\"><span class=\"ltx_text\" id=\"S3.T1.4.2.6.1\" style=\"font-size:90%;\">31.78 \u00b1 1.47</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.4.2.7\"><span class=\"ltx_text\" id=\"S3.T1.4.2.7.1\" style=\"font-size:90%;\">46.88 \u00b1 2.10</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.4.2.8\"><span class=\"ltx_text\" id=\"S3.T1.4.2.8.1\" style=\"font-size:90%;\">13.59 \u00b1 1.05</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.4.2.9\"><span class=\"ltx_text\" id=\"S3.T1.4.2.9.1\" style=\"font-size:90%;\">30.30 \u00b1 2.01</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.4.2.10\"><span class=\"ltx_text\" id=\"S3.T1.4.2.10.1\" style=\"font-size:90%;\">55.85 \u00b1 3.55</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.35.37.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.35.37.2.1\"><span class=\"ltx_text\" id=\"S3.T1.35.37.2.1.1\" style=\"font-size:90%;\">SLIM</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.37.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.37.2.2.1\" style=\"font-size:90%;\">28.37 \u00b1 0.41</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.37.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.37.2.3.1\" style=\"font-size:90%;\">45.68 \u00b1 0.53</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.37.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.37.2.4.1\" style=\"font-size:90%;\">58.09 \u00b1 0.58</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.37.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.37.2.5.1\" style=\"font-size:90%;\">18.32 \u00b1 0.53</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.37.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.37.2.6.1\" style=\"font-size:90%;\">32.56 \u00b1 1.30</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.37.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.37.2.7.1\" style=\"font-size:90%;\">46.92 \u00b1 1.82</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.37.2.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.37.2.8.1\" style=\"font-size:90%;\">15.64 \u00b1 0.51</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.37.2.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.37.2.9.1\" style=\"font-size:90%;\">34.33 \u00b1 1.53</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.37.2.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.37.2.10.1\" style=\"font-size:90%;\">62.93 \u00b1 3.46</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.13.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.13.11.10\"><span class=\"ltx_text\" id=\"S3.T1.13.11.10.1\" style=\"font-size:90%;\">Improv.</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.3.1\">\n<span class=\"ltx_text\" id=\"S3.T1.5.3.1.1\" style=\"font-size:90%;\">3.81</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.6.4.2\">\n<span class=\"ltx_text\" id=\"S3.T1.6.4.2.1\" style=\"font-size:90%;\">3.68</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.5.3\">\n<span class=\"ltx_text\" id=\"S3.T1.7.5.3.1\" style=\"font-size:90%;\">2.76</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.8.6.4\">\n<span class=\"ltx_text\" id=\"S3.T1.8.6.4.1\" style=\"font-size:90%;\">3.21</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.9.7.5\">\n<span class=\"ltx_text\" id=\"S3.T1.9.7.5.1\" style=\"font-size:90%;\">4.69</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.10.8.6\">\n<span class=\"ltx_text\" id=\"S3.T1.10.8.6.1\" style=\"font-size:90%;\">4.24</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.11.9.7\">\n<span class=\"ltx_text\" id=\"S3.T1.11.9.7.1\" style=\"font-size:90%;\">28.3</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.12.10.8\">\n<span class=\"ltx_text\" id=\"S3.T1.12.10.8.1\" style=\"font-size:90%;\">28.29</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.13.11.9\">\n<span class=\"ltx_text\" id=\"S3.T1.13.11.9.1\" style=\"font-size:90%;\">28.17</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.35.38.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.35.38.3.1\"><span class=\"ltx_text\" id=\"S3.T1.35.38.3.1.1\" style=\"font-size:90%;\">SASRec</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.38.3.2\"><span class=\"ltx_text\" id=\"S3.T1.35.38.3.2.1\" style=\"font-size:90%;\">22.73 \u00b1 0.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.38.3.3\"><span class=\"ltx_text\" id=\"S3.T1.35.38.3.3.1\" style=\"font-size:90%;\">37.77 \u00b1 0.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.38.3.4\"><span class=\"ltx_text\" id=\"S3.T1.35.38.3.4.1\" style=\"font-size:90%;\">51.53 \u00b1 0.39</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.38.3.5\"><span class=\"ltx_text\" id=\"S3.T1.35.38.3.5.1\" style=\"font-size:90%;\">26.78 \u00b1 0.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.38.3.6\"><span class=\"ltx_text\" id=\"S3.T1.35.38.3.6.1\" style=\"font-size:90%;\">35.78 \u00b1 0.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.38.3.7\"><span class=\"ltx_text\" id=\"S3.T1.35.38.3.7.1\" style=\"font-size:90%;\">43.32 \u00b1 0.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.38.3.8\"><span class=\"ltx_text\" id=\"S3.T1.35.38.3.8.1\" style=\"font-size:90%;\">2.66 \u00b1 0.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.38.3.9\"><span class=\"ltx_text\" id=\"S3.T1.35.38.3.9.1\" style=\"font-size:90%;\">5.56 \u00b1 0.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.38.3.10\"><span class=\"ltx_text\" id=\"S3.T1.35.38.3.10.1\" style=\"font-size:90%;\">14.93 \u00b1 1.53</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.14.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.14.12.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.14.12.2\"><span class=\"ltx_text\" id=\"S3.T1.14.12.2.1\" style=\"font-size:90%;\">27.46 \u00b1 0.19</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.14.12.3\"><span class=\"ltx_text\" id=\"S3.T1.14.12.3.1\" style=\"font-size:90%;\">44.88 \u00b1 0.63</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.14.12.4\"><span class=\"ltx_text\" id=\"S3.T1.14.12.4.1\" style=\"font-size:90%;\">58.90 \u00b1 0.38</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.14.12.5\"><span class=\"ltx_text\" id=\"S3.T1.14.12.5.1\" style=\"font-size:90%;\">30.95 \u00b1 0.38</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.14.12.6\"><span class=\"ltx_text\" id=\"S3.T1.14.12.6.1\" style=\"font-size:90%;\">44.98 \u00b1 0.53</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.14.12.7\"><span class=\"ltx_text\" id=\"S3.T1.14.12.7.1\" style=\"font-size:90%;\">55.61 \u00b1 1.12</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.14.12.8\"><span class=\"ltx_text\" id=\"S3.T1.14.12.8.1\" style=\"font-size:90%;\">5.58 \u00b1 0.10</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.14.12.9\"><span class=\"ltx_text\" id=\"S3.T1.14.12.9.1\" style=\"font-size:90%;\">11.09 \u00b1 0.16</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.14.12.10\"><span class=\"ltx_text\" id=\"S3.T1.14.12.10.1\" style=\"font-size:90%;\">20.69 \u00b1 0.77</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.15.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.15.13.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.15.13.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.15.13.2.1\" style=\"font-size:90%;\">31.58 \u00b1 0.35</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.15.13.3\"><span class=\"ltx_text\" id=\"S3.T1.15.13.3.1\" style=\"font-size:90%;\">50.83 \u00b1 0.62</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.15.13.4\"><span class=\"ltx_text\" id=\"S3.T1.15.13.4.1\" style=\"font-size:90%;\">63.45 \u00b1 0.71</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.15.13.5\"><span class=\"ltx_text\" id=\"S3.T1.15.13.5.1\" style=\"font-size:90%;\">32.65 \u00b1 0.15</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.15.13.6\"><span class=\"ltx_text\" id=\"S3.T1.15.13.6.1\" style=\"font-size:90%;\">48.01 \u00b1 0.48</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.15.13.7\"><span class=\"ltx_text\" id=\"S3.T1.15.13.7.1\" style=\"font-size:90%;\">59.25 \u00b1 0.56</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.15.13.8\"><span class=\"ltx_text\" id=\"S3.T1.15.13.8.1\" style=\"font-size:90%;\">5.95 \u00b1 0.32</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.15.13.9\"><span class=\"ltx_text\" id=\"S3.T1.15.13.9.1\" style=\"font-size:90%;\">11.83 \u00b1 0.62</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.15.13.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.15.13.10.1\" style=\"font-size:90%;\">22.43 \u00b1 0.54</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.35.39.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.35.39.4.1\"><span class=\"ltx_text\" id=\"S3.T1.35.39.4.1.1\" style=\"font-size:90%;\">SLIM</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.39.4.2\"><span class=\"ltx_text\" id=\"S3.T1.35.39.4.2.1\" style=\"font-size:90%;\">31.43 \u00b1 0.39</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.39.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.39.4.3.1\" style=\"font-size:90%;\">51.11 \u00b1 0.82</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.39.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.39.4.4.1\" style=\"font-size:90%;\">64.10 \u00b1 0.26</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.39.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.39.4.5.1\" style=\"font-size:90%;\">32.80 \u00b1 0.40</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.39.4.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.39.4.6.1\" style=\"font-size:90%;\">48.27 \u00b1 0.64</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.39.4.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.39.4.7.1\" style=\"font-size:90%;\">59.30 \u00b1 0.89</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.39.4.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.39.4.8.1\" style=\"font-size:90%;\">6.01 \u00b1 0.19</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.39.4.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.39.4.9.1\" style=\"font-size:90%;\">12.01 \u00b1 0.38</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.39.4.10\"><span class=\"ltx_text\" id=\"S3.T1.35.39.4.10.1\" style=\"font-size:90%;\">22.29 \u00b1 0.85</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.24.22\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.24.22.10\"><span class=\"ltx_text\" id=\"S3.T1.24.22.10.1\" style=\"font-size:90%;\">Improv.</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.16.14.1\">\n<span class=\"ltx_text\" id=\"S3.T1.16.14.1.1\" style=\"font-size:90%;\">14.46</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.17.15.2\">\n<span class=\"ltx_text\" id=\"S3.T1.17.15.2.1\" style=\"font-size:90%;\">13.88</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.18.16.3\">\n<span class=\"ltx_text\" id=\"S3.T1.18.16.3.1\" style=\"font-size:90%;\">8.83</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.19.17.4\">\n<span class=\"ltx_text\" id=\"S3.T1.19.17.4.1\" style=\"font-size:90%;\">5.98</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.20.18.5\">\n<span class=\"ltx_text\" id=\"S3.T1.20.18.5.1\" style=\"font-size:90%;\">7.31</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.21.19.6\">\n<span class=\"ltx_text\" id=\"S3.T1.21.19.6.1\" style=\"font-size:90%;\">6.64</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.22.20.7\">\n<span class=\"ltx_text\" id=\"S3.T1.22.20.7.1\" style=\"font-size:90%;\">7.71</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.23.21.8\">\n<span class=\"ltx_text\" id=\"S3.T1.23.21.8.1\" style=\"font-size:90%;\">8.3</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.24.22.9\">\n<span class=\"ltx_text\" id=\"S3.T1.24.22.9.1\" style=\"font-size:90%;\">7.73</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.35.40.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.35.40.5.1\"><span class=\"ltx_text\" id=\"S3.T1.35.40.5.1.1\" style=\"font-size:90%;\">SRGNN</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.40.5.2\"><span class=\"ltx_text\" id=\"S3.T1.35.40.5.2.1\" style=\"font-size:90%;\">16.45 \u00b1 0.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.40.5.3\"><span class=\"ltx_text\" id=\"S3.T1.35.40.5.3.1\" style=\"font-size:90%;\">29.29 \u00b1 0.14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.40.5.4\"><span class=\"ltx_text\" id=\"S3.T1.35.40.5.4.1\" style=\"font-size:90%;\">40.99 \u00b1 0.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.40.5.5\"><span class=\"ltx_text\" id=\"S3.T1.35.40.5.5.1\" style=\"font-size:90%;\">10.99 \u00b1 2.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.40.5.6\"><span class=\"ltx_text\" id=\"S3.T1.35.40.5.6.1\" style=\"font-size:90%;\">20.32 \u00b1 4.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.40.5.7\"><span class=\"ltx_text\" id=\"S3.T1.35.40.5.7.1\" style=\"font-size:90%;\">32.14 \u00b1 6.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.40.5.8\"><span class=\"ltx_text\" id=\"S3.T1.35.40.5.8.1\" style=\"font-size:90%;\">5.04 \u00b1 0.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.40.5.9\"><span class=\"ltx_text\" id=\"S3.T1.35.40.5.9.1\" style=\"font-size:90%;\">13.48 \u00b1 2.23</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.35.40.5.10\"><span class=\"ltx_text\" id=\"S3.T1.35.40.5.10.1\" style=\"font-size:90%;\">37.22 \u00b1 3.85</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.25.23\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.25.23.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.25.23.2\"><span class=\"ltx_text\" id=\"S3.T1.25.23.2.1\" style=\"font-size:90%;\">21.54 \u00b1 0.64</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.25.23.3\"><span class=\"ltx_text\" id=\"S3.T1.25.23.3.1\" style=\"font-size:90%;\">36.77 \u00b1 1.05</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.25.23.4\"><span class=\"ltx_text\" id=\"S3.T1.25.23.4.1\" style=\"font-size:90%;\">49.11 \u00b1 1.54</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.25.23.5\"><span class=\"ltx_text\" id=\"S3.T1.25.23.5.1\" style=\"font-size:90%;\">11.91 \u00b1 0.71</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.25.23.6\"><span class=\"ltx_text\" id=\"S3.T1.25.23.6.1\" style=\"font-size:90%;\">21.39 \u00b1 1.91</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.25.23.7\"><span class=\"ltx_text\" id=\"S3.T1.25.23.7.1\" style=\"font-size:90%;\">33.63 \u00b1 3.41</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.25.23.8\"><span class=\"ltx_text\" id=\"S3.T1.25.23.8.1\" style=\"font-size:90%;\">11.61 \u00b1 1.14</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.25.23.9\"><span class=\"ltx_text\" id=\"S3.T1.25.23.9.1\" style=\"font-size:90%;\">25.22 \u00b1 2.46</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.25.23.10\"><span class=\"ltx_text\" id=\"S3.T1.25.23.10.1\" style=\"font-size:90%;\">43.85 \u00b1 3.58</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.26.24\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.26.24.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.26.24.2\"><span class=\"ltx_text\" id=\"S3.T1.26.24.2.1\" style=\"font-size:90%;\">22.35 \u00b1 1.48</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.26.24.3\"><span class=\"ltx_text\" id=\"S3.T1.26.24.3.1\" style=\"font-size:90%;\">37.69 \u00b1 1.53</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.26.24.4\"><span class=\"ltx_text\" id=\"S3.T1.26.24.4.1\" style=\"font-size:90%;\">51.29 \u00b1 0.59</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.26.24.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.26.24.5.1\" style=\"font-size:90%;\">12.92 \u00b1 0.78</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.26.24.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.26.24.6.1\" style=\"font-size:90%;\">23.80 \u00b1 1.60</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.26.24.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.26.24.7.1\" style=\"font-size:90%;\">37.22 \u00b1 2.75</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.26.24.8\"><span class=\"ltx_text\" id=\"S3.T1.26.24.8.1\" style=\"font-size:90%;\">11.25 \u00b1 1.42</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.26.24.9\"><span class=\"ltx_text\" id=\"S3.T1.26.24.9.1\" style=\"font-size:90%;\">24.28 \u00b1 3.19</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.26.24.10\"><span class=\"ltx_text\" id=\"S3.T1.26.24.10.1\" style=\"font-size:90%;\">44.05 \u00b1 5.97</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.35.41.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.35.41.6.1\"><span class=\"ltx_text\" id=\"S3.T1.35.41.6.1.1\" style=\"font-size:90%;\">SLIM</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.41.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.41.6.2.1\" style=\"font-size:90%;\">23.77 \u00b1 0.20</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.41.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.41.6.3.1\" style=\"font-size:90%;\">39.81 \u00b1 0.52</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.41.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.41.6.4.1\" style=\"font-size:90%;\">52.34 \u00b1 0.63</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.41.6.5\"><span class=\"ltx_text\" id=\"S3.T1.35.41.6.5.1\" style=\"font-size:90%;\">12.38 \u00b1 0.51</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.41.6.6\"><span class=\"ltx_text\" id=\"S3.T1.35.41.6.6.1\" style=\"font-size:90%;\">22.98 \u00b1 1.30</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.41.6.7\"><span class=\"ltx_text\" id=\"S3.T1.35.41.6.7.1\" style=\"font-size:90%;\">36.44 \u00b1 1.72</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.41.6.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.41.6.8.1\" style=\"font-size:90%;\">12.29 \u00b1 1.39</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.41.6.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.41.6.9.1\" style=\"font-size:90%;\">26.51 \u00b1 2.71</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.35.41.6.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.35.41.6.10.1\" style=\"font-size:90%;\">47.01 \u00b1 3.61</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.35.33\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S3.T1.35.33.10\"><span class=\"ltx_text\" id=\"S3.T1.35.33.10.1\" style=\"font-size:90%;\">Improv.</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T1.27.25.1\">\n<span class=\"ltx_text\" id=\"S3.T1.27.25.1.1\" style=\"font-size:90%;\">10.35</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T1.28.26.2\">\n<span class=\"ltx_text\" id=\"S3.T1.28.26.2.1\" style=\"font-size:90%;\">8.27</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T1.29.27.3\">\n<span class=\"ltx_text\" id=\"S3.T1.29.27.3.1\" style=\"font-size:90%;\">6.58</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T1.30.28.4\">\n<span class=\"ltx_text\" id=\"S3.T1.30.28.4.1\" style=\"font-size:90%;\">3.95</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T1.31.29.5\">\n<span class=\"ltx_text\" id=\"S3.T1.31.29.5.1\" style=\"font-size:90%;\">7.43</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T1.32.30.6\">\n<span class=\"ltx_text\" id=\"S3.T1.32.30.6.1\" style=\"font-size:90%;\">8.36</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T1.33.31.7\">\n<span class=\"ltx_text\" id=\"S3.T1.33.31.7.1\" style=\"font-size:90%;\">5.86</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T1.34.32.8\">\n<span class=\"ltx_text\" id=\"S3.T1.34.32.8.1\" style=\"font-size:90%;\">5.11</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T1.35.33.9\">\n<span class=\"ltx_text\" id=\"S3.T1.35.33.9.1\" style=\"font-size:90%;\">7.21</span>\n</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 1. ID-based scenarios. Comparison of recommendation performance among different backbones. The best results are highlighted in bold. \u201dImprov.\u201d indicates the relative improvement of SLIM compared to the best performance in backbones (original backbone and )."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 2. </span>ID-agnostic Text Matching model. Comparison of recommendation performance without relying on any backbone models.  indicates only using the - step rationales generated by SLIM. </figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.7\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.7.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S3.T2.7.2.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T2.7.2.1.1.1\" style=\"font-size:90%;\">Methods</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S3.T2.7.2.1.2\"><span class=\"ltx_text\" id=\"S3.T2.7.2.1.2.1\" style=\"font-size:90%;\">Games</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S3.T2.7.2.1.3\"><span class=\"ltx_text\" id=\"S3.T2.7.2.1.3.1\" style=\"font-size:90%;\">Food</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S3.T2.7.2.1.4\"><span class=\"ltx_text\" id=\"S3.T2.7.2.1.4.1\" style=\"font-size:90%;\">Home</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.7.3.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.3.2.1\"><span class=\"ltx_text\" id=\"S3.T2.7.3.2.1.1\" style=\"font-size:90%;\">NDCG@10</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.3.2.2\"><span class=\"ltx_text\" id=\"S3.T2.7.3.2.2.1\" style=\"font-size:90%;\">Hit @10</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.3.2.3\"><span class=\"ltx_text\" id=\"S3.T2.7.3.2.3.1\" style=\"font-size:90%;\">Hit @20</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.3.2.4\"><span class=\"ltx_text\" id=\"S3.T2.7.3.2.4.1\" style=\"font-size:90%;\">NDCG@10</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.3.2.5\"><span class=\"ltx_text\" id=\"S3.T2.7.3.2.5.1\" style=\"font-size:90%;\">Hit @10</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.3.2.6\"><span class=\"ltx_text\" id=\"S3.T2.7.3.2.6.1\" style=\"font-size:90%;\">Hit @20</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.3.2.7\"><span class=\"ltx_text\" id=\"S3.T2.7.3.2.7.1\" style=\"font-size:90%;\">NDCG@10</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.3.2.8\"><span class=\"ltx_text\" id=\"S3.T2.7.3.2.8.1\" style=\"font-size:90%;\">Hit @10</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.3.2.9\"><span class=\"ltx_text\" id=\"S3.T2.7.3.2.9.1\" style=\"font-size:90%;\">Hit @20</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.7.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.7.4.3.1\"><span class=\"ltx_text\" id=\"S3.T2.7.4.3.1.1\" style=\"font-size:90%;\">SLIM-Step1</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.4.3.2\"><span class=\"ltx_text\" id=\"S3.T2.7.4.3.2.1\" style=\"font-size:90%;\">13.78 \u00b1 0.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.4.3.3\"><span class=\"ltx_text\" id=\"S3.T2.7.4.3.3.1\" style=\"font-size:90%;\">26.08 \u00b1 0.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.4.3.4\"><span class=\"ltx_text\" id=\"S3.T2.7.4.3.4.1\" style=\"font-size:90%;\">41.71 \u00b1 0.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.4.3.5\"><span class=\"ltx_text\" id=\"S3.T2.7.4.3.5.1\" style=\"font-size:90%;\">13.62 \u00b1 0.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.4.3.6\"><span class=\"ltx_text\" id=\"S3.T2.7.4.3.6.1\" style=\"font-size:90%;\">24.90 \u00b1 0.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.4.3.7\"><span class=\"ltx_text\" id=\"S3.T2.7.4.3.7.1\" style=\"font-size:90%;\">38.15 \u00b1 0.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.4.3.8\"><span class=\"ltx_text\" id=\"S3.T2.7.4.3.8.1\" style=\"font-size:90%;\">4.25 \u00b1 0.06</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.4.3.9\"><span class=\"ltx_text\" id=\"S3.T2.7.4.3.9.1\" style=\"font-size:90%;\">9.53 \u00b1 0.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.4.3.10\"><span class=\"ltx_text\" id=\"S3.T2.7.4.3.10.1\" style=\"font-size:90%;\">18.91 \u00b1 0.77</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.7.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.7.5.4.1\"><span class=\"ltx_text\" id=\"S3.T2.7.5.4.1.1\" style=\"font-size:90%;\">SLIM-Step2</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.5.4.2\"><span class=\"ltx_text\" id=\"S3.T2.7.5.4.2.1\" style=\"font-size:90%;\">16.78 \u00b1 0.66</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.5.4.3\"><span class=\"ltx_text\" id=\"S3.T2.7.5.4.3.1\" style=\"font-size:90%;\">30.09 \u00b1 0.89</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.5.4.4\"><span class=\"ltx_text\" id=\"S3.T2.7.5.4.4.1\" style=\"font-size:90%;\">45.75 \u00b1 0.59</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.5.4.5\"><span class=\"ltx_text\" id=\"S3.T2.7.5.4.5.1\" style=\"font-size:90%;\">13.71 \u00b1 0.48</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.5.4.6\"><span class=\"ltx_text\" id=\"S3.T2.7.5.4.6.1\" style=\"font-size:90%;\">24.23 \u00b1 0.78</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.5.4.7\"><span class=\"ltx_text\" id=\"S3.T2.7.5.4.7.1\" style=\"font-size:90%;\">36.89 \u00b1 0.77</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.5.4.8\"><span class=\"ltx_text\" id=\"S3.T2.7.5.4.8.1\" style=\"font-size:90%;\">4.75 \u00b1 0.29</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.5.4.9\"><span class=\"ltx_text\" id=\"S3.T2.7.5.4.9.1\" style=\"font-size:90%;\">10.30 \u00b1 0.55</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.5.4.10\"><span class=\"ltx_text\" id=\"S3.T2.7.5.4.10.1\" style=\"font-size:90%;\">19.99 \u00b1 0.61</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.7.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.7.6.5.1\"><span class=\"ltx_text\" id=\"S3.T2.7.6.5.1.1\" style=\"font-size:90%;\">SLIM-Step3</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.6.5.2\"><span class=\"ltx_text\" id=\"S3.T2.7.6.5.2.1\" style=\"font-size:90%;\">20.20 \u00b1 0.31</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.6.5.3\"><span class=\"ltx_text\" id=\"S3.T2.7.6.5.3.1\" style=\"font-size:90%;\">35.57 \u00b1 0.54</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.6.5.4\"><span class=\"ltx_text\" id=\"S3.T2.7.6.5.4.1\" style=\"font-size:90%;\">50.04 \u00b1 0.47</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.6.5.5\"><span class=\"ltx_text\" id=\"S3.T2.7.6.5.5.1\" style=\"font-size:90%;\">15.69 \u00b1 0.26</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.6.5.6\"><span class=\"ltx_text\" id=\"S3.T2.7.6.5.6.1\" style=\"font-size:90%;\">26.69 \u00b1 0.68</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.6.5.7\"><span class=\"ltx_text\" id=\"S3.T2.7.6.5.7.1\" style=\"font-size:90%;\">39.19 \u00b1 0.98</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.6.5.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.7.6.5.8.1\" style=\"font-size:90%;\">4.83 \u00b1 0.27</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.6.5.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.7.6.5.9.1\" style=\"font-size:90%;\">10.39 \u00b1 0.38</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.7.6.5.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.7.6.5.10.1\" style=\"font-size:90%;\">21.05 \u00b1 0.43</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.7.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.7.1.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.1.2\"><span class=\"ltx_text\" id=\"S3.T2.7.1.2.1\" style=\"font-size:90%;\">21.75 \u00b1 0.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.1.3\"><span class=\"ltx_text\" id=\"S3.T2.7.1.3.1\" style=\"font-size:90%;\">38.05 \u00b1 0.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.7.1.4.1\" style=\"font-size:90%;\">53.73 \u00b1 0.79</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.7.1.5.1\" style=\"font-size:90%;\">19.08 \u00b1 0.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.7.1.6.1\" style=\"font-size:90%;\">32.34 \u00b1 0.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.7.1.7.1\" style=\"font-size:90%;\">44.63 \u00b1 0.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.1.8\"><span class=\"ltx_text\" id=\"S3.T2.7.1.8.1\" style=\"font-size:90%;\">4.63 \u00b1 0.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.1.9\"><span class=\"ltx_text\" id=\"S3.T2.7.1.9.1\" style=\"font-size:90%;\">10.15 \u00b1 0.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.1.10\"><span class=\"ltx_text\" id=\"S3.T2.7.1.10.1\" style=\"font-size:90%;\">20.11 \u00b1 1.01</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.7.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T2.7.7.6.1\"><span class=\"ltx_text\" id=\"S3.T2.7.7.6.1.1\" style=\"font-size:90%;\">SLIM</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.7.7.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.7.7.6.2.1\" style=\"font-size:90%;\">21.99 \u00b1 0.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.7.7.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.7.7.6.3.1\" style=\"font-size:90%;\">38.33 \u00b1 0.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.7.7.6.4\"><span class=\"ltx_text\" id=\"S3.T2.7.7.6.4.1\" style=\"font-size:90%;\">53.59 \u00b1 0.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.7.7.6.5\"><span class=\"ltx_text\" id=\"S3.T2.7.7.6.5.1\" style=\"font-size:90%;\">18.13 \u00b1 0.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.7.7.6.6\"><span class=\"ltx_text\" id=\"S3.T2.7.7.6.6.1\" style=\"font-size:90%;\">30.84 \u00b1 0.54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.7.7.6.7\"><span class=\"ltx_text\" id=\"S3.T2.7.7.6.7.1\" style=\"font-size:90%;\">44.04 \u00b1 0.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.7.7.6.8\"><span class=\"ltx_text\" id=\"S3.T2.7.7.6.8.1\" style=\"font-size:90%;\">4.49 \u00b1 0.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.7.7.6.9\"><span class=\"ltx_text\" id=\"S3.T2.7.7.6.9.1\" style=\"font-size:90%;\">9.96 \u00b1 0.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.7.7.6.10\"><span class=\"ltx_text\" id=\"S3.T2.7.7.6.10.1\" style=\"font-size:90%;\">19.67 \u00b1 0.59</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 2. ID-agnostic Text Matching model. Comparison of recommendation performance without relying on any backbone models.  indicates only using the - step rationales generated by SLIM. "
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T3\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 3. </span>The comparison of LLM costs. \u201cOffline Time\u201d represents the time it takes for LLM to generate one piece of knowledge offline. \u201cOnline Time\u201d represents the time it takes for LLM to perform inference for each ranking online.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T3.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T3.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.4.1.1.1\"><span class=\"ltx_text\" id=\"S3.T3.4.1.1.1.1\" style=\"font-size:90%;\">Costs</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.4.1.1.2\"><span class=\"ltx_text\" id=\"S3.T3.4.1.1.2.1\" style=\"font-size:90%;\">LLM as Ranker</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.4.1.1.3\"><span class=\"ltx_text\" id=\"S3.T3.4.1.1.3.1\" style=\"font-size:90%;\">LLM as Enhancer</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.4.1.1.4\"><span class=\"ltx_text\" id=\"S3.T3.4.1.1.4.1\" style=\"font-size:90%;\">SLIM</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T3.4.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.4.2.1.1\"><span class=\"ltx_text\" id=\"S3.T3.4.2.1.1.1\" style=\"font-size:90%;\">Offline Time (s)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.4.2.1.2\"><span class=\"ltx_text\" id=\"S3.T3.4.2.1.2.1\" style=\"font-size:90%;\">\u2717</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.4.2.1.3\"><span class=\"ltx_text\" id=\"S3.T3.4.2.1.3.1\" style=\"font-size:90%;\">5.54 (API call)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.4.2.1.4\"><span class=\"ltx_text\" id=\"S3.T3.4.2.1.4.1\" style=\"font-size:90%;\">6.11</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.4.3.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.3.2.1\"><span class=\"ltx_text\" id=\"S3.T3.4.3.2.1.1\" style=\"font-size:90%;\">Online Time (s)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.3.2.2\"><span class=\"ltx_text\" id=\"S3.T3.4.3.2.2.1\" style=\"font-size:90%;\">5.54 (API call)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.3.2.3\"><span class=\"ltx_text\" id=\"S3.T3.4.3.2.3.1\" style=\"font-size:90%;\">\u2717</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.3.2.4\"><span class=\"ltx_text\" id=\"S3.T3.4.3.2.4.1\" style=\"font-size:90%;\">\u2717</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.4.4.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.4.3.1\"><span class=\"ltx_text\" id=\"S3.T3.4.4.3.1.1\" style=\"font-size:90%;\">Model Size</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.4.3.2\"><span class=\"ltx_text\" id=\"S3.T3.4.4.3.2.1\" style=\"font-size:90%;\">175B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.4.3.3\"><span class=\"ltx_text\" id=\"S3.T3.4.4.3.3.1\" style=\"font-size:90%;\">175B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.4.3.4\"><span class=\"ltx_text\" id=\"S3.T3.4.4.3.4.1\" style=\"font-size:90%;\">7B</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.4.5.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.5.4.1\"><span class=\"ltx_text\" id=\"S3.T3.4.5.4.1.1\" style=\"font-size:90%;\">Deployment</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.5.4.2\"><span class=\"ltx_text\" id=\"S3.T3.4.5.4.2.1\" style=\"font-size:90%;\">Hard</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.5.4.3\"><span class=\"ltx_text\" id=\"S3.T3.4.5.4.3.1\" style=\"font-size:90%;\">Hard</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.5.4.4\"><span class=\"ltx_text\" id=\"S3.T3.4.5.4.4.1\" style=\"font-size:90%;\">1 A100</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.4.6.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.6.5.1\"><span class=\"ltx_text\" id=\"S3.T3.4.6.5.1.1\" style=\"font-size:90%;\">API Costs/Input</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.6.5.2\"><span class=\"ltx_text\" id=\"S3.T3.4.6.5.2.1\" style=\"font-size:90%;\">$0.0015/1K tokens</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.6.5.3\"><span class=\"ltx_text\" id=\"S3.T3.4.6.5.3.1\" style=\"font-size:90%;\">$0.0015/1K tokens</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.6.5.4\"><span class=\"ltx_text\" id=\"S3.T3.4.6.5.4.1\" style=\"font-size:90%;\">\u2717</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.4.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.4.7.6.1\"><span class=\"ltx_text\" id=\"S3.T3.4.7.6.1.1\" style=\"font-size:90%;\">API Costs/Output</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.4.7.6.2\"><span class=\"ltx_text\" id=\"S3.T3.4.7.6.2.1\" style=\"font-size:90%;\">$0.002/1K tokens</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.4.7.6.3\"><span class=\"ltx_text\" id=\"S3.T3.4.7.6.3.1\" style=\"font-size:90%;\">$0.002/1K tokens</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.4.7.6.4\"><span class=\"ltx_text\" id=\"S3.T3.4.7.6.4.1\" style=\"font-size:90%;\">\u2717</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 3. The comparison of LLM costs. \u201cOffline Time\u201d represents the time it takes for LLM to generate one piece of knowledge offline. \u201cOnline Time\u201d represents the time it takes for LLM to perform inference for each ranking online."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T4\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"A2.T4.5.1.1\" style=\"font-size:90%;\">Table 4</span>. </span><span class=\"ltx_text\" id=\"A2.T4.6.2\" style=\"font-size:90%;\">ID-based scenarios. Comparison of recommendation performance across different scales of student LLMs. </span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A2.T4.3\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T4.3.4.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"A2.T4.3.4.1.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"A2.T4.3.4.1.2\">Games</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.3.5.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T4.3.5.2.1\">Methods</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.5.2.2\">NDCG@10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.5.2.3\">Hit @10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.5.2.4\">Hit @20</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.3.6.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A2.T4.3.6.3.1\">GRU4Rec</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.3.6.3.2\">17.61 \u00b1 0.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.3.6.3.3\">30.87 \u00b1 0.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.3.6.3.4\">42.39 \u00b1 0.62</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T4.1.1.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.1.1.2\">27.33 \u00b1 0.53</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.1.1.3\">44.06 \u00b1 0.79</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.1.1.4\">56.53 \u00b1 1.24</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.3.7.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T4.3.7.4.1\">Bloomz-560M</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.7.4.2\">27.80 \u00b1 0.55</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.7.4.3\">44.79 \u00b1 0.58</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.7.4.4\">57.23 \u00b1 0.40</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.3.8.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T4.3.8.5.1\">Bloomz-1B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.8.5.2\">28.22 \u00b1 0.33</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.8.5.3\">45.54 \u00b1 0.87</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.8.5.4\">57.48 \u00b1 0.76</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.3.9.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T4.3.9.6.1\">LLaMA2-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.9.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.3.9.6.2.1\">28.37 \u00b1 0.41</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.9.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.3.9.6.3.1\">45.68 \u00b1 0.53</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.9.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.3.9.6.4.1\">58.09 \u00b1 0.58</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.3.10.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A2.T4.3.10.7.1\">SASRec</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.3.10.7.2\">22.73 \u00b1 0.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.3.10.7.3\">37.77 \u00b1 0.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.3.10.7.4\">51.53 \u00b1 0.39</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T4.2.2.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.2.2.2\">27.46 \u00b1 0.19</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.2.2.3\">44.88 \u00b1 0.63</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.2.2.4\">58.90 \u00b1 0.38</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.3.11.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T4.3.11.8.1\">Bloomz-560M</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.11.8.2\">30.50 \u00b1 0.33</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.11.8.3\">49.96 \u00b1 0.59</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.11.8.4\">63.08 \u00b1 0.26</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.3.12.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T4.3.12.9.1\">Bloomz-1B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.12.9.2\">30.93 \u00b1 0.29</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.12.9.3\">50.67 \u00b1 0.33</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.12.9.4\">63.35 \u00b1 0.48</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.3.13.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T4.3.13.10.1\">LLaMA2-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.13.10.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.3.13.10.2.1\">31.43 \u00b1 0.39</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.13.10.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.3.13.10.3.1\">51.11 \u00b1 0.82</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.13.10.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.3.13.10.4.1\">64.10 \u00b1 0.26</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.3.14.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A2.T4.3.14.11.1\">SRGNN</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.3.14.11.2\">16.45 \u00b1 0.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.3.14.11.3\">29.29 \u00b1 0.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.3.14.11.4\">40.99 \u00b1 0.52</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T4.3.3.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.3.2\">21.54 \u00b1 0.64</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.3.3\">36.77 \u00b1 1.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.3.4\">49.11 \u00b1 1.54</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.3.15.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T4.3.15.12.1\">Bloomz-560M</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.15.12.2\">22.36 \u00b1 0.90</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.15.12.3\">37.48 \u00b1 1.26</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.15.12.4\">49.65 \u00b1 0.87</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.3.16.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T4.3.16.13.1\">Bloomz-1B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.16.13.2\">23.05 \u00b1 0.66</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.16.13.3\">38.47 \u00b1 0.81</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.3.16.13.4\">50.34 \u00b1 0.72</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.3.17.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A2.T4.3.17.14.1\">LLaMA2-7B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T4.3.17.14.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.3.17.14.2.1\">23.77 \u00b1 0.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T4.3.17.14.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.3.17.14.3.1\">39.81 \u00b1 0.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T4.3.17.14.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.3.17.14.4.1\">52.34 \u00b1 0.63</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 4. ID-based scenarios. Comparison of recommendation performance across different scales of student LLMs. "
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T5\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"A2.T5.2.1.1\" style=\"font-size:90%;\">Table 5</span>. </span><span class=\"ltx_text\" id=\"A2.T5.3.2\" style=\"font-size:90%;\">ID-agnostic Text Matching model. Comparison of recommendation performance across different scales of student LLMs.</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A2.T5.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A2.T5.4.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"A2.T5.4.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"A2.T5.4.1.1.2\">Games</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T5.4.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" id=\"A2.T5.4.2.2.1\">Methods</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"A2.T5.4.2.2.2\">NDCG@10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"A2.T5.4.2.2.3\">Hit @10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"A2.T5.4.2.2.4\">Hit @20</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T5.4.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A2.T5.4.3.1.1\">Bloomz-560M</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.4.3.1.2\">19.31 \u00b1 0.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.4.3.1.3\">34.15 \u00b1 0.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.4.3.1.4\">49.55 \u00b1 0.88</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T5.4.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T5.4.4.2.1\">Bloomz-1B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T5.4.4.2.2\">20.92 \u00b1 0.28</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T5.4.4.2.3\">36.81 \u00b1 0.37</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T5.4.4.2.4\">51.77 \u00b1 0.77</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T5.4.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A2.T5.4.5.3.1\">LLaMA2-7B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.4.5.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T5.4.5.3.2.1\">21.99 \u00b1 0.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.4.5.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T5.4.5.3.3.1\">38.33 \u00b1 0.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.4.5.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T5.4.5.3.4.1\">53.59 \u00b1 0.72</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 5. ID-agnostic Text Matching model. Comparison of recommendation performance across different scales of student LLMs."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T6\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"A2.T6.7.2.1\" style=\"font-size:90%;\">Table 6</span>. </span><span class=\"ltx_text\" id=\"A2.T6.2.1\" style=\"font-size:90%;\">ID-based scenarios. Distilling on the Home dataset and deploying it on Games and Food (). </span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A2.T6.5\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A2.T6.5.4.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"A2.T6.5.4.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"A2.T6.5.4.1.2\">Games</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"A2.T6.5.4.1.3\">Home</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.5.5.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" id=\"A2.T6.5.5.2.1\">Methods</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"A2.T6.5.5.2.2\">NDCG@10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"A2.T6.5.5.2.3\">Hit @10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"A2.T6.5.5.2.4\">Hit @20</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"A2.T6.5.5.2.5\">NDCG@10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"A2.T6.5.5.2.6\">Hit @10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"A2.T6.5.5.2.7\">Hit @20</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T6.5.6.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A2.T6.5.6.1.1\">GRU4Rec</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.5.6.1.2\">17.61 \u00b1 0.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.5.6.1.3\">30.87 \u00b1 0.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.5.6.1.4\">42.39 \u00b1 0.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.5.6.1.5\">9.10 \u00b1 0.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.5.6.1.6\">15.27 \u00b1 0.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.5.6.1.7\">19.51\u00b1 0.24</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T6.3.1.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.3.1.2\">28.17 \u00b1 0.46</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.3.1.3\">45.22 \u00b1 0.43</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.3.1.4\">57.39 \u00b1 0.58</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.3.1.5\">17.80 \u00b1 0.45</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.3.1.6\">31.70 \u00b1 0.71</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.3.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.3.1.7.1\">47.16 \u00b1 1.29</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.5.7.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T6.5.7.2.1\">SLIM</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.5.7.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.5.7.2.2.1\">28.37 \u00b1 0.41</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.5.7.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.5.7.2.3.1\">45.68 \u00b1 0.53</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.5.7.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.5.7.2.4.1\">58.09 \u00b1 0.58</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.5.7.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.5.7.2.5.1\">18.32 \u00b1 0.53</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.5.7.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.5.7.2.6.1\">32.56 \u00b1 1.30</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.5.7.2.7\">46.92 \u00b1 1.82</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.5.8.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A2.T6.5.8.3.1\">SASRec</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.5.8.3.2\">22.73 \u00b1 0.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.5.8.3.3\">37.77 \u00b1 0.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.5.8.3.4\">51.53 \u00b1 0.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.5.8.3.5\">26.78 \u00b1 0.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.5.8.3.6\">35.78 \u00b1 0.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.5.8.3.7\">43.32 \u00b1 0.55</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T6.4.2.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.4.2.2\">31.39 \u00b1 0.23</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.4.2.3\">50.83 \u00b1 0.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.4.2.4\">63.37 \u00b1 0.51</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.4.2.5\">32.68 \u00b1 0.36</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.4.2.6\">48.20 \u00b1 0.83</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.4.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.4.2.7.1\">59.74 \u00b1 0.98</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.5.9.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T6.5.9.4.1\">SLIM</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.5.9.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.5.9.4.2.1\">31.43 \u00b1 0.39</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.5.9.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.5.9.4.3.1\">51.11 \u00b1 0.82</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.5.9.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.5.9.4.4.1\">64.10 \u00b1 0.26</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.5.9.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.5.9.4.5.1\">32.80 \u00b1 0.40</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.5.9.4.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.5.9.4.6.1\">48.27 \u00b1 0.64</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.5.9.4.7\">59.30 \u00b1 0.89</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.5.10.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A2.T6.5.10.5.1\">SRGNN</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.5.10.5.2\">16.45 \u00b1 0.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.5.10.5.3\">29.29 \u00b1 0.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.5.10.5.4\">40.99 \u00b1 0.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.5.10.5.5\">10.99 \u00b1 2.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.5.10.5.6\">20.32 \u00b1 4.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.5.10.5.7\">32.14 \u00b1 6.55</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T6.5.3.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.5.3.2\">23.64 \u00b1 0.35</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.5.3.3\">39.74 \u00b1 0.65</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.5.3.4\">51.63 \u00b1 0.62</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.5.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.5.3.5.1\">12.83 \u00b1 0.48</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.5.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.5.3.6.1\">23.85 \u00b1 1.27</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.5.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.5.3.7.1\">36.91 \u00b1 1.60</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.5.11.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A2.T6.5.11.6.1\">SLIM</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.5.11.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.5.11.6.2.1\">23.77 \u00b1 0.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.5.11.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.5.11.6.3.1\">39.81 \u00b1 0.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.5.11.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.5.11.6.4.1\">52.34 \u00b1 0.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.5.11.6.5\">12.38 \u00b1 0.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.5.11.6.6\">22.98 \u00b1 1.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.5.11.6.7\">36.44 \u00b1 1.72</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 6. ID-based scenarios. Distilling on the Home dataset and deploying it on Games and Food (). "
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T7\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"A2.T7.5.2.1\" style=\"font-size:90%;\">Table 7</span>. </span><span class=\"ltx_text\" id=\"A2.T7.2.1\" style=\"font-size:90%;\"> ID-agnostic Text Matching model. Distilling on the Home dataset and deploying it on Games and Food (). </span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A2.T7.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A2.T7.3.2.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"A2.T7.3.2.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"A2.T7.3.2.1.2\">Games</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"A2.T7.3.2.1.3\">Home</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T7.3.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T7.3.3.1.1\">Methods</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.3.3.1.2\">NDCG@10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.3.3.1.3\">Hit @10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.3.3.1.4\">Hit @20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.3.3.1.5\">NDCG@10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.3.3.1.6\">Hit @10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.3.3.1.7\">Hit @20</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"A2.T7.3.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.3.1.2\">21.15 \u00b1 0.46</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.3.1.3\">37.33 \u00b1 0.45</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.3.1.4\">53.15 \u00b1 0.44</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.3.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.3.1.5.1\">18.70 \u00b1 0.39</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.3.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.3.1.6.1\">31.07 \u00b1 0.71</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.3.1.7\">43.92 \u00b1 0.55</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.3.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A2.T7.3.4.2.1\">SLIM</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.3.4.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.3.4.2.2.1\">21.99 \u00b1 0.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.3.4.2.3\">3<span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.3.4.2.3.1\">8.33 \u00b1 0.32</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.3.4.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.3.4.2.4.1\">53.59 \u00b1 0.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.3.4.2.5\">18.13 \u00b1 0.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.3.4.2.6\">30.84 \u00b1 0.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.3.4.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.3.4.2.7.1\">44.04 \u00b1 0.66</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 7.  ID-agnostic Text Matching model. Distilling on the Home dataset and deploying it on Games and Food (). "
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T8\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 8. </span>Popularity bias metrics are shown on SLIM and SASRec over three datasets.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A2.T8.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A2.T8.4.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"A2.T8.4.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"A2.T8.4.1.1.2\"><span class=\"ltx_text\" id=\"A2.T8.4.1.1.2.1\" style=\"font-size:90%;\">EFD@10</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"A2.T8.4.1.1.3\"><span class=\"ltx_text\" id=\"A2.T8.4.1.1.3.1\" style=\"font-size:90%;\">EPC@10</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T8.4.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T8.4.2.1.1\"><span class=\"ltx_text\" id=\"A2.T8.4.2.1.1.1\" style=\"font-size:90%;\">Methods</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T8.4.2.1.2\"><span class=\"ltx_text\" id=\"A2.T8.4.2.1.2.1\" style=\"font-size:90%;\">Games</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T8.4.2.1.3\"><span class=\"ltx_text\" id=\"A2.T8.4.2.1.3.1\" style=\"font-size:90%;\">Food</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T8.4.2.1.4\"><span class=\"ltx_text\" id=\"A2.T8.4.2.1.4.1\" style=\"font-size:90%;\">Home</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T8.4.2.1.5\"><span class=\"ltx_text\" id=\"A2.T8.4.2.1.5.1\" style=\"font-size:90%;\">Games</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T8.4.2.1.6\"><span class=\"ltx_text\" id=\"A2.T8.4.2.1.6.1\" style=\"font-size:90%;\">Food</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T8.4.2.1.7\"><span class=\"ltx_text\" id=\"A2.T8.4.2.1.7.1\" style=\"font-size:90%;\">Home</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T8.4.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"A2.T8.4.3.2.1\"><span class=\"ltx_text\" id=\"A2.T8.4.3.2.1.1\" style=\"font-size:90%;\">SASRec</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T8.4.3.2.2\"><span class=\"ltx_text\" id=\"A2.T8.4.3.2.2.1\" style=\"font-size:90%;\">0.5810</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T8.4.3.2.3\"><span class=\"ltx_text\" id=\"A2.T8.4.3.2.3.1\" style=\"font-size:90%;\">0.6550</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T8.4.3.2.4\"><span class=\"ltx_text\" id=\"A2.T8.4.3.2.4.1\" style=\"font-size:90%;\">0.0765</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T8.4.3.2.5\"><span class=\"ltx_text\" id=\"A2.T8.4.3.2.5.1\" style=\"font-size:90%;\">0.0511</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T8.4.3.2.6\"><span class=\"ltx_text\" id=\"A2.T8.4.3.2.6.1\" style=\"font-size:90%;\">0.0531</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T8.4.3.2.7\"><span class=\"ltx_text\" id=\"A2.T8.4.3.2.7.1\" style=\"font-size:90%;\">0.0059</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T8.4.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A2.T8.4.4.3.1\"><span class=\"ltx_text\" id=\"A2.T8.4.4.3.1.1\" style=\"font-size:90%;\">SLIM</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.4.4.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.4.4.3.2.1\" style=\"font-size:90%;\">0.8171</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.4.4.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.4.4.3.3.1\" style=\"font-size:90%;\">0.9197</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.4.4.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.4.4.3.4.1\" style=\"font-size:90%;\">0.1835</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.4.4.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.4.4.3.5.1\" style=\"font-size:90%;\">0.0690</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.4.4.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.4.4.3.6.1\" style=\"font-size:90%;\">0.0732</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.4.4.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.4.4.3.7.1\" style=\"font-size:90%;\">0.0135</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 8. Popularity bias metrics are shown on SLIM and SASRec over three datasets."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.04260v2_figure_1.png",
            "caption": "Figure 1. The overview of the proposed framework."
        },
        "2": {
            "figure_path": "2403.04260v2_figure_2.png",
            "caption": "Figure 2. Zero-shot CoT prompting for larger teacher model. Eliciting LLMs to reason in a step-by-step manner."
        },
        "3": {
            "figure_path": "2403.04260v2_figure_3.png",
            "caption": "Figure 3. Prompting for fine-tuned student model. Eliciting smaller model to reason in a step-by-step manner."
        },
        "4": {
            "figure_path": "2403.04260v2_figure_4.png",
            "caption": "Figure 4. Case study. Comparison of the predictions for the next item obtained from SASRec and SLIM."
        },
        "5": {
            "figure_path": "2403.04260v2_figure_5.png",
            "caption": "(a) Games"
        },
        "6": {
            "figure_path": "2403.04260v2_figure_6.png",
            "caption": "(b) Food"
        },
        "7": {
            "figure_path": "2403.04260v2_figure_7.png",
            "caption": "(c) Home"
        },
        "8": {
            "figure_path": "2403.04260v2_figure_8.png",
            "caption": "Figure 6. Analysis of popularity bias. We sort the items based on their frequency in the training set (i.e., popularity) and draw line plots based on each item\u2019s frequency in the recommendation results of SASRec and SLIM, respectively."
        },
        "9": {
            "figure_path": "2403.04260v2_figure_9.png",
            "caption": "Figure 7. Supplementary material for Figure 6."
        },
        "10": {
            "figure_path": "2403.04260v2_figure_10.png",
            "caption": "Figure 7. Supplementary material for Figure 6."
        },
        "11": {
            "figure_path": "2403.04260v2_figure_11.png",
            "caption": "Figure 8. Compare the recommendation performance of fine-tuning student models with different data sizes using various backbones on Games dataset."
        },
        "12": {
            "figure_path": "2403.04260v2_figure_12.png",
            "caption": "Figure 9. Case study. Comparison of the response from teacher and student model."
        }
    },
    "references": [
        {
            "1": {
                "title": "Elliot: A comprehensive and rigorous framework for reproducible recommender systems evaluation. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval. 2405\u20132414.",
                "author": "Vito Walter Anelli, Alejandro Bellog\u00edn, Antonio Ferrara, Daniele Malitesta, Felice Antonio Merra, Claudio Pomo, Francesco Maria Donini, and Tommaso Di Noia. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "2": {
                "title": "Sequential recommendation with user memory networks. In Proceedings of the eleventh ACM international conference on web search and data mining. 108\u2013116.",
                "author": "Xu Chen, Hongteng Xu, Yongfeng Zhang, Jiaxi Tang, Yixin Cao, Zheng Qin, and Hongyuan Zha. 2018.",
                "venue": "",
                "url": null
            }
        },
        {
            "3": {
                "title": "Uncovering ChatGPT\u2019s Capabilities in Recommender Systems.",
                "author": "Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023.",
                "venue": "arXiv preprint arXiv:2305.02182 (2023).",
                "url": null
            }
        },
        {
            "4": {
                "title": "A tutorial on the cross-entropy method.",
                "author": "Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. 2005.",
                "venue": "Annals of operations research 134 (2005), 19\u201367.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.",
                "venue": "arXiv preprint arXiv:1810.04805 (2018).",
                "url": null
            }
        },
        {
            "6": {
                "title": "Recommender systems in the era of large language models (llms).",
                "author": "Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, and Qing Li. 2023.",
                "venue": "arXiv preprint arXiv:2307.02046 (2023).",
                "url": null
            }
        },
        {
            "7": {
                "title": "Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web. 173\u2013182.",
                "author": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017.",
                "venue": "",
                "url": null
            }
        },
        {
            "8": {
                "title": "Session-based recommendations with recurrent neural networks.",
                "author": "Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015.",
                "venue": "arXiv preprint arXiv:1511.06939 (2015).",
                "url": null
            }
        },
        {
            "9": {
                "title": "Towards universal sequence representation learning for recommender systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 585\u2013593.",
                "author": "Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022.",
                "venue": "",
                "url": null
            }
        },
        {
            "10": {
                "title": "Large language models are zero-shot rankers for recommender systems.",
                "author": "Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023.",
                "venue": "arXiv preprint arXiv:2305.08845 (2023).",
                "url": null
            }
        },
        {
            "11": {
                "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes.",
                "author": "Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "12": {
                "title": "Lora: Low-rank adaptation of large language models.",
                "author": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021.",
                "venue": "arXiv preprint arXiv:2106.09685 (2021).",
                "url": null
            }
        },
        {
            "13": {
                "title": "Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM). IEEE, 197\u2013206.",
                "author": "Wang-Cheng Kang and Julian McAuley. 2018.",
                "venue": "",
                "url": null
            }
        },
        {
            "14": {
                "title": "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction.",
                "author": "Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, and Derek Zhiyuan Cheng. 2023.",
                "venue": "arXiv preprint arXiv:2305.06474 (2023).",
                "url": null
            }
        },
        {
            "15": {
                "title": "Text Is All You Need: Learning Language Representations for Sequential Recommendation.",
                "author": "Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian McAuley. 2023.",
                "venue": "arXiv preprint arXiv:2305.13731 (2023).",
                "url": null
            }
        },
        {
            "16": {
                "title": "How Can Recommender Systems Benefit from Large Language Models: A Survey.",
                "author": "Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, et al. 2023a.",
                "venue": "arXiv preprint arXiv:2306.05817 (2023).",
                "url": null
            }
        },
        {
            "17": {
                "title": "Rella: Retrieval-enhanced large language models for lifelong sequential behavior comprehension in recommendation.",
                "author": "Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang. 2023b.",
                "venue": "arXiv preprint arXiv:2308.11131 (2023).",
                "url": null
            }
        },
        {
            "18": {
                "title": "Is chatgpt a good recommender? a preliminary study.",
                "author": "Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023b.",
                "venue": "arXiv preprint arXiv:2304.10149 (2023).",
                "url": null
            }
        },
        {
            "19": {
                "title": "ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models.",
                "author": "Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2023a.",
                "venue": "",
                "url": null
            }
        },
        {
            "20": {
                "title": "Teaching small language models to reason.",
                "author": "Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. 2022.",
                "venue": "arXiv preprint arXiv:2212.08410 (2022).",
                "url": null
            }
        },
        {
            "21": {
                "title": "A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding.",
                "author": "Wenbo Pan, Qiguang Chen, Xiao Xu, Wanxiang Che, and Libo Qin. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "22": {
                "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
                "author": "Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "23": {
                "title": "Sequence-Aware Recommender Systems.",
                "author": "Massimo Quadrana, Paolo Cremonesi, and Dietmar Jannach. 2018.",
                "venue": "",
                "url": null
            }
        },
        {
            "24": {
                "title": "Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network.",
                "author": "Alex Sherstinsky. 2020.",
                "venue": "Physica D: Nonlinear Phenomena 404 (2020), 132306.",
                "url": null
            }
        },
        {
            "25": {
                "title": "BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management. 1441\u20131450.",
                "author": "Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "26": {
                "title": "Motif-aware Riemannian Graph Neural Network with Generative-Contrastive Learning.",
                "author": "Li Sun, Zhenhao Huang, Zixi Wang, Feiyang Wang, Hao Peng, and Philip Yu. 2024.",
                "venue": "arXiv preprint arXiv:2401.01232 (2024).",
                "url": null
            }
        },
        {
            "27": {
                "title": "Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the eleventh ACM international conference on web search and data mining. 565\u2013573.",
                "author": "Jiaxi Tang and Ke Wang. 2018.",
                "venue": "",
                "url": null
            }
        },
        {
            "28": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023.",
                "venue": "arXiv preprint arXiv:2307.09288 (2023).",
                "url": null
            }
        },
        {
            "29": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017.",
                "venue": "Advances in neural information processing systems 30 (2017).",
                "url": null
            }
        },
        {
            "30": {
                "title": "Sequential recommender systems: challenges, progress and prospects.",
                "author": "Shoujin Wang, Liang Hu, Yan Wang, Longbing Cao, Quan Z Sheng, and Mehmet Orgun. 2019.",
                "venue": "arXiv preprint arXiv:2001.04830 (2019).",
                "url": null
            }
        },
        {
            "31": {
                "title": "Community preserving network embedding. In Proceedings of the AAAI conference on artificial intelligence, Vol. 31.",
                "author": "Xiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, and Shiqiang Yang. 2017.",
                "venue": "",
                "url": null
            }
        },
        {
            "32": {
                "title": "Ensemble multi-relational graph neural networks.",
                "author": "Yuling Wang, Hao Xu, Yanhua Yu, Mengdi Zhang, Zhenhao Li, Yuji Yang, and Wei Wu. 2022.",
                "venue": "arXiv preprint arXiv:2205.12076 (2022).",
                "url": null
            }
        },
        {
            "33": {
                "title": "Chain-of-thought prompting elicits reasoning in large language models.",
                "author": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022.",
                "venue": "Advances in Neural Information Processing Systems 35 (2022), 24824\u201324837.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Zero-Shot Information Extraction via Chatting with ChatGPT.",
                "author": "Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, and Wenjuan Han. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "35": {
                "title": "A Survey on Large Language Models for Recommendation.",
                "author": "Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2023.",
                "venue": "arXiv preprint arXiv:2305.19860 (2023).",
                "url": null
            }
        },
        {
            "36": {
                "title": "Session-based recommendation with graph neural networks. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 346\u2013353.",
                "author": "Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. 2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "37": {
                "title": "Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models.",
                "author": "Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. 2023.",
                "venue": "arXiv preprint arXiv:2306.10933 (2023).",
                "url": null
            }
        },
        {
            "38": {
                "title": "Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation.",
                "author": "Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023a.",
                "venue": "arXiv preprint arXiv:2305.07609 (2023).",
                "url": null
            }
        },
        {
            "39": {
                "title": "Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach.",
                "author": "Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023c.",
                "venue": "",
                "url": null
            }
        },
        {
            "40": {
                "title": "Graph convolutional networks: a comprehensive review.",
                "author": "Si Zhang, Hanghang Tong, Jiejun Xu, and Ross Maciejewski. 2019.",
                "venue": "Computational Social Networks 6, 1 (2019), 1\u201323.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Sentiment Analysis in the Era of Large Language Models: A Reality Check.",
                "author": "Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong Bing. 2023b.",
                "venue": "arXiv preprint arXiv:2305.15005 (2023).",
                "url": null
            }
        },
        {
            "42": {
                "title": "Recbole: Towards a unified, comprehensive and efficient framework for recommendation algorithms. In proceedings of the 30th acm international conference on information & knowledge management. 4653\u20134664.",
                "author": "Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Yushuo Chen, Xingyu Pan, Kaiyuan Li, Yujie Lu, Hui Wang, Changxin Tian, et al. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "43": {
                "title": "Alpa: Automating inter-and Intra-Operator parallelism for distributed deep learning. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). 559\u2013578.",
                "author": "Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al. 2022.",
                "venue": "",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.04260v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "4.1",
            "4.2"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.2.1",
            "2.2.2",
            "2.3",
            "2.3.1",
            "2.3.2"
        ],
        "main_experiment_and_results_sections": [
            "3.1.1",
            "3.1.2",
            "3.1.3",
            "3.2.1",
            "3.2.2",
            "3.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.2.1",
            "3.2.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.04260v2",
        "paper_title": "Can Small Language Models be Good Reasoners for Sequential Recommendation?",
        "research_background": "### Motivation\n\nThe paper aims to address critical shortcomings in existing sequential recommendation models, such as exposure bias and popularity bias, which stem from training on closed-loop user-item interaction datasets. These systems fail to integrate open-world knowledge, which is crucial for a comprehensive understanding of user behaviors. While large language models (LLMs) like GPT 3.5/4 have demonstrated impressive reasoning capabilities in various NLP tasks, their application in sequential recommendation has not been fully explored. Additionally, the immense computational resources required for deploying LLMs make them impractical for real-world recommender systems. This research seeks to ascertain whether small language models can be effective reasoners for sequential recommendation, thus offering a more practical and resource-efficient solution.\n\n### Research Problem\n\nThe central research problem this paper addresses is whether a cost-effective, small language model can serve as an effective reasoning engine for sequential recommendation tasks. The focus is on leveraging the reasoning capabilities of LLMs, specifically through the chain-of-thought (CoT) prompting strategy, to improve sequential recommendations without the prohibitive resource requirements associated with large models. \n\n### Relevant Prior Work\n\n1. **Sequential Recommendation Models**: As noted by Quadrana et al. (2018) and others, existing sequential recommendation systems excel at identifying dynamic user interests through chronological interactions but are constrained by exposure and popularity biases.\n   \n2. **LLMs in NLP Tasks**: Recent advancements in large language models, such as GPT 3.5/4, have shown remarkable performance in various NLP applications by utilizing massive corpora to mimic human-like reasoning (Qin et al., 2023; Zhang et al., 2023b; Wei et al., 2023; Pan et al., 2023).\n\n3. **LLMs in Recommendations**: Current methodologies can be broadly classified into two types:\n   - **LLM as a Ranker**: Using frozen LLMs to generate a ranked list of items based on user interests through zero-shot or few-shot learning (Hou et al., 2023).\n   - **LLM as a Knowledge Enhancer**: Employing LLMs to generate rich contextual knowledge that is then used alongside traditional recommendation algorithms (Xi et al., 2023).\n\n4. **Challenges with LLMs**: Despite their potential, LLMs face challenges in sequential recommendation due to their resource-intensive nature and the risk of generating incorrect or irrelevant recommendation knowledge.\n\n5. **Chain-of-Thought Prompting**: Prior work has suggested that chain-of-thought prompting can help LLMs break down complex tasks into intermediate reasoning steps, thereby enhancing their understanding of user behavior patterns (Wei et al., 2022; Magister et al., 2022; Hsieh et al., 2023).\n\nThis paper builds on these prior studies by developing a novel knowledge distillation framework called SLIM, which enables small language models to inherit the CoT reasoning capabilities of larger models, thereby making sequential recommendations both effective and resource-efficient.",
        "methodology": "In this section, we propose SLIM, a novel knowledge distillation framework tailored for recommendation, which incorporates the reasoning capabilities of Large Language Models (LLMs) into recommender systems in a resource-efficient manner.\n\nKey Components and Innovations:\n1. **Knowledge Distillation Framework**: SLIM leverages a knowledge distillation process to transfer the reasoning abilities of LLMs to smaller, more resource-efficient models, making the technology feasible for widespread use in recommendation systems.\n2. **Reasoning Capabilities of LLMs**: The methodology focuses on integrating the advanced reasoning and understanding that LLMs possess, which enables more intelligent and context-aware recommendations.\n3. **Resource Efficiency**: A primary innovation of SLIM is its emphasis on maintaining high performance while reducing the computational resources required by traditional LLMs, thus making it more practical for real-world applications.\n\nBy distilling the knowledge from large models into smaller ones, SLIM aims to enhance the reasoning prowess of recommender systems without the typically high computational costs associated with LLMs.",
        "main_experiment_and_results": "### Main Experiment Setup ###\n\n**Datasets**: The main experiments are conducted on three categories from the Amazon Review dataset:\n1. Video Games (Games)\n2. Grocery and Gourmet Food (Food)\n3. Home and Kitchen (Home)\n\n**Baselines**: The performance of the small language models is compared against standard sequential recommendation models and more complex large language models.\n\n**Evaluation Metrics**: The evaluation of the models is done using traditional recommendation metrics, although the specific metrics used (e.g., precision, recall, F1 score, etc.) are unspecified here but can be examined in more detail in the supplementary materials.\n\n### Main Experimental Results ###\nThe paper reports that, overall, the small language models perform competitively when compared to their larger counterparts and traditional sequential recommendation models. The specific results and metrics demonstrate that proper training and fine-tuning enable smaller models to exhibit strong reasoning capabilities in sequential recommendation tasks across differing categories of the Amazon Review dataset."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the performance improvements of the SLIM model over various backbone models in ID-based sequential recommendation scenarios.",
            "experiment_process": "The performance of SLIM was evaluated across various backbone models and compared with their item feature extensions. The evaluation metrics included Hit Rate@10 on multiple datasets, including the Home dataset. The comparison aimed to determine SLIM's effectiveness relative to traditional recommendations and ChatGPT feature extensions.",
            "result_discussion": "SLIM achieved state-of-the-art performance across all datasets and demonstrated a relative improvement in Hit Rate@10 on the Home dataset. Compared to ChatGPT feature extensions, SLIM outperformed in 22 out of 27 cases, indicating that the distilled smaller model can prioritize relevant information more effectively than the larger, less controlled ChatGPT model. These results affirm SLIM's ability to enhance recommendations with meaningful rationales at a smaller scale.",
            "ablation_id": "2403.04260v2.No1"
        },
        {
            "research_objective": "Assess the performance of SLIM in ID-agnostic scenarios to establish a more efficient and generalizable recommendation model.",
            "experiment_process": "SLIM's performance was evaluated in ID-agnostic scenarios using CoT-based sequence embeddings and text-based item embeddings (Text Matching). The experiment compared SLIM's outcomes with those of models generating rationales based on the teacher model (ChatGPT). Each step in the reasoning process (user interest, item category, specific product) was separately evaluated to verify their individual contributions to recommendation performance.",
            "result_discussion": "SLIM outperformed the teacher model in a significant number of cases, despite its smaller size and limited training samples. The Text Matching approach showed superior performance compared to ID-based backbones. Each step's evaluation revealed that the ranking of recommendation performance consistently followed Step3 > Step2 > Step1, indicating the model's step-by-step reasoning capability. Despite lower performance at Step1, it remains crucial as a foundation for subsequent reasoning processes, highlighting the model's interpretability and efficiency.",
            "ablation_id": "2403.04260v2.No2"
        }
    ]
}