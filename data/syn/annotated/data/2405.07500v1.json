{
    "title": "PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking",
    "abstract": "Linking (aligning) biomedical concepts across diverse data sources enables various integrative analyses, but it is challenging due to the discrepancies in concept naming conventions.\nVarious strategies have been developed to overcome this challenge, such as those based on string-matching rules, manually crafted thesauri, and machine learning models. However, these methods are constrained by limited prior biomedical knowledge and can hardly generalize beyond the limited amounts of rules, thesauri, or training samples. Recently, large language models (LLMs) have exhibited impressive results in diverse biomedical NLP tasks due to their unprecedentedly rich prior knowledge and strong zero-shot prediction abilities. However, LLMs suffer from issues including high costs, limited context length, and unreliable predictions. In this research, we propose PromptLink, a novel biomedical concept linking framework that leverages LLMs. It first employs a biomedical-specialized pre-trained language model to generate candidate concepts that can fit in the LLM context windows. Then it utilizes an LLM to link concepts through two-stage prompts, where the first-stage prompt aims to elicit the biomedical prior knowledge from the LLM for the concept linking task and the second-stage prompt enforces the LLM to reflect on its own predictions to further enhance their reliability. Empirical results on the concept linking task between two EHR datasets and an external biomedical KG demonstrate the effectiveness of PromptLink. Furthermore, PromptLink is a generic framework without reliance on additional prior knowledge, context, or training data, making it well-suited for concept linking across various types of data sources. The source code of this study is available at https://github.com/constantjxyz/PromptLink.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "Biomedical concept linking studies the intricate task of linking closely related concepts across different data sources by leveraging their semantic meanings and underlying biomedical knowledge, as exemplified in Figure 1  ###reference_### (Sevgili et al., 2022  ###reference_b30###).\nThis linking process is crucial for enabling integrative analyses, as biomedical concepts obtained from diverse sources offer multifaceted views of biomedical knowledge and data (Su et al., 2023  ###reference_b33###; Lu et al., 2023  ###reference_b20###). For example, the electronic health record (EHR), which is regarded as a valuable asset for comprehensive patient health analysis, contains various digital medical information including tabular data, clinical notes, and other types of patient data (Abul-Husn and Kenny, 2019  ###reference_b2###; Sun et al., 2018  ###reference_b34###; Xu et al., 2022  ###reference_b40###). Similarly, the knowledge graph (KG), playing an important role in biomedical research, provides structured knowledge, such as definitions of concepts and their interrelationships (Ma et al., 2018  ###reference_b22###).\nHowever, the cross-source biomedical linking task is challenging due to discrepancies in the biomedical naming conventions used in different systems (Kohane et al., 2021  ###reference_b16###). For example, a KG may mention a disease as \u201cEllis-Van Creveld syndrome\u201d, while an EHR may refer to the same disease as \u201cChondroectodermal dysplasia\u201d. This inconsistency presents a strong barrier to cohesive data analysis.\n###figure_1### The challenge of biomedical concept linking has motivated the development of various methods. Conventional methods focus on setting string-matching rules (D\u2019Souza and Ng, 2015  ###reference_b8###; Kang et al., 2013  ###reference_b14###) and leveraging constructed thesauri (Aronson and Lang, 2010  ###reference_b4###; Savova et al., 2010  ###reference_b28###; Friedman et al., 2001  ###reference_b10###). However, their reliance on fixed rules and crafted thesauri limits coverage and generalizability in real-world scenarios (Shi et al., 2023  ###reference_b31###).\nAddressing these limitations, machine learning-based methods have been widely explored, avoiding the manual design of rules or thesauri. These methods essentially transform biomedical concepts from raw text into embeddings (latent vector representations), which are then used to compute similarity scores via distance functions (e.g. cosine similarity) or learning-based scoring functions (e.g. bilinear attention (Kim et al., 2018  ###reference_b15###)).\nVarious models have been used to obtain biomedical concept embeddings, including pre-trained language models (PLMs) (Wang et al., 2023b  ###reference_b35###) that capture fine-grained semantic relations through extensive training on biomedical corpora  (Xu et al., 2020  ###reference_b39###; Lee et al., 2020  ###reference_b17###; Alsentzer et al., 2019  ###reference_b3###; Liu et al., 2021  ###reference_b18###), and graph neural networks (GNNs) (Zhou et al., 2020  ###reference_b43###) that capture both semantics and relations of biomedical concepts (Bordes et al., 2013  ###reference_b5###; Grover and Leskovec, 2016  ###reference_b11###; Liu et al., 2022  ###reference_b19###).\nDespite the notable achievements of these ML-based linking methods, they are data-hungry and require significant supervision signals when adapted into novel downstream applications. They face challenges due to the costly data annotation and model training processes.\nRecently, large language models (LLMs) have exhibited impressive performances in various NLP tasks, due to their unprecedentedly rich prior knowledge and language capabilities (Zhou et al., 2023  ###reference_b44###; Singhal et al., 2023  ###reference_b32###; Wang et al., 2023a  ###reference_b36###), enabling various applications in a zero-shot learning setting (Lu et al., 2023  ###reference_b20###).\nTherefore, LLMs provide a promising solution for linking related concepts across different systems.\nMeanwhile, LLMs also face challenges including the design of effective and cost-efficient prompts within the context length limits (Zhang et al., 2023  ###reference_b41###), and the NIL prediction capability of reliably rejecting all candidates when correct concepts are absent, instead of returning relatively close but incorrect ones (Peters et al., 2019  ###reference_b24###).\nIn this paper, we propose PromptLink, leveraging LLMs for the cross-source biomedical concept linking task. Considering LLMs\u2019 high cost and context length constraints, we first employ a pre-trained SAPBERT language model to generate biomedical-aware concept embeddings and retrieve top candidates based on the cosine similarities of these embeddings. We then design a novel two-stage prompting mechanism for the GPT-4 model to derive reliable linking predictions. The first stage efficiently filters out irrelevant candidates, thereby minimizing the response token numbers required in the subsequent stage. The second stage generates the final linking results and incorporates a self-verification prompt to address the NIL prediction challenge, effectively rejecting all candidates when none are relevant. In the experiments, PromptLink demonstrates exceptional performance, surpassing various existing concept linking methods by over 5% in two scenarios of biomedical concept linking between EHR and external biomedical KG, which could be attributed to LLM\u2019s intrinsic strong biomedical knowledge. Moreover, PromptLink works as a zero-shot framework due to the utilization of pre-trained language models, eliminating the need for a training process. It is also a versatile framework that performs well even when only concept names, without concept context or topological structure, are provided. Given its various advantages, PromptLink boasts strong generalization capabilities, making it suitable for a wide range of biomedical research and applications."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Biomedical Concept Linking",
            "text": "###figure_2###"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. Problem Definition",
            "text": "The biomedical concept linking task aims to link biomedical concepts across sources/systems based on semantic meanings and biomedical knowledge. It solely relies on concept names and can thus cover much broader real-world applications. This task differs from existing tasks such as entity linking (Shi et al., 2023  ###reference_b31###), entity alignment (Lu et al., 2023  ###reference_b20###), and ontology matching (Jim\u00e9nez-Ruiz and Cuenca Grau, 2011  ###reference_b12###), which depend on extra contextual or topological information.\nIn this study, we link the concepts in EHR to corresponding concepts in a biomedical KG.\nWe define an EHR database , a biomedical KG , and the linking task as follows:\nAn EHR database  is a relational database , with  being patient identifiers,  patient attributes,  the values of these attributes. Additionally,  represents multi-token biomedical concepts associated with patient attributes.\nA biomedical KG is a multi-relation graph , where  are concepts,  are relation names, and  are the relational triples among them.\nLink identified biomedical concepts from an EHR  to a biomedical KG  based on semantic meanings and biomedical knowledge, forming linkages . If a concept  from  is not in , link it to a special \u201cNIL\u201d entity, indicating it is unlinkable."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. PromptLink",
            "text": "We propose PromptLink, a novel LLM-based solution for cross-source biomedical concept linking, as illustrated in Figure 2  ###reference_###.\nAddressing LLMs\u2019 high cost and limited input text length, we first employ a biomedical-specialized pre-trained language model to generate concept embeddings and retrieve top candidates via cosine similarities. Subsequently, we employ a two-stage prompting mechanism with GPT-4 to generate the final linking predictions.\nConcept representation and candidate generation.\nAfter preprocessing text by lowercasing and removing punctuation, we use a pre-trained LM (specifically SapBERT (Liu et al., 2021  ###reference_b18###)),\nto create embeddings  for EHR concepts  and KG concepts , represented as\nFor concepts that span multiple tokens, the token-level embeddings are averaged to create the concept embedding.\nThis model helps to project the semantic meanings and prior biomedical knowledge into the embedding space.\nFor candidate generation, we compute cosine similarity  between pairs of EHR concept embedding  and KG concept embedding , represented as\nGiven each input query EHR concept , We select the top- (=10) KG concepts  with the highest similarities as candidates for further GPT-based linking prediction.\nLinking prediction using two-stage prompts.\nThe next step of our framework is generating linking predictions of query  over the top- candidate  using GPT-4 model, leveraging its text comprehension ability, logical reasoning ability, and prior biomedical knowledge (Bubeck et al., 2023  ###reference_b6###; Singhal et al., 2023  ###reference_b32###). In this step, we design a novel two-stage prompt for our task, as can be seen in Figure 2  ###reference_###.\nCombining the two prompts utilizes their strengths and mitigates weaknesses. The first stage focuses on concept pairs to filter out unrelated candidates. The second stage evaluates all candidates in a broader context to identify the closest match or reject all unmatch candidates.\nIn the first stage, the LLM is prompted to check if a concept pair  should be linked. By defining the response structure, the LLM can return answers in specified formats. To improve the prompt response quality, we adopt the self-consistency (Wang et al., 2022  ###reference_b37###) prompting strategy that repeatedly prompts the same question to the LLM multiple times.\nSpecifically, we prompt each concept pair  for  times, thus obtaining the belief score  by:\nConsidering the belief scores across different candidates, we derive a comprehensive filter strategy to exclude irrelevant candidates, using parameter  (set as ). This approach ensures that irrelevant candidates are not considered in the next stage, optimizing both efficiency and effectiveness. The approach is described as follows:\nIf , this indicates some candidates closely align with the query concept. In such cases, candidates with belief scores of zero will be filtered out as they are deemed irrelevant to the query concept and there are closely aligning alternatives. This filtering strategy effectively removes many irrelevant candidates, thereby optimizing both efficiency and effectiveness for the subsequent stage.\nOtherwise, the range of different candidates\u2019 belief scores is not wide enough to justify filtering. Thus, all  candidates will be subjected to double-checking by the second-stage prompt.\nIn the second stage, the LLM evaluates the  candidates retained from the first stage\u2019s filtering process , where  , using a compositional prompt that consists of two consecutive questions to perform complex reasoning.\nSpecifically, the LLM is asked to (1) label the relationship between the query concept and all candidate concepts as \u201cexact match\u201d, \u201crelated to\u201d, or \u201cdifferent from\u201d; (2) use self-verification prompts to either identify the closest candidate or dismiss all candidates if none are close, thus the final concept linking result of this prompt is  (usually ) item from .\nIn this stage, we also use the self-consistency strategy that prompts one question for the same  times. Subsequently, we calculate the occurrence frequency  for answers in  and retrieve the final linking result for query EHR concept  as follows:\nIf , this indicates a high probability that none of the candidates are appropriate. Thus, \u201cNIL\u201d is chosen as the final linking prediction.\nOtherwise, the candidate  with the highest frequency  is decided as the final linking result. If two candidates tie for the highest frequency, the one  with higher embedding similarity  to the query concept  is chosen."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Experiments & Discussions",
            "text": ""
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Concept Linking Experiment Results",
            "text": "###table_1### Table 1  ###reference_### shows the accuracy of our proposed PromptLink along with baseline methods, when every method links a query EHR concept  with their predicted top-1 KG concept .\nAs can be seen, PromptLink outperforms competing approaches across both datasets in terms of zero-shot accuracy, underscoring the superiority of our LLM-based concept linking methodology.\nAmong the compared methods, SAPBERT, a SOTA biomedical entity linking method, achieves the second-highest performance.\nMoreover, conventional methods based on string similarity lag behind machine learning techniques, which leverage embeddings from pre-trained language models to effectively match conceptually similar but lexically distinct entities like \u201cEllis-Van Creveld syndrome\u201d and \u201cChondroectodermal dysplasia\u201d."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3. Ablation Studies",
            "text": "Prompt Effectiveness and Efficiency.\nWe conduct ablation studies to reveal the effectiveness and cost-efficiency of the prompt used in our approach, as shown in Table 2  ###reference_###. This comparison uses the same input data and 10 linking candidates across various prompts. In the table, the \u201cBefore prompting\u201d denotes the performance of using only embedding similarity obtained from the pre-trained LM, while other methods use LLM to predict linking results based on LM-generated candidates.\nFrom Table 2  ###reference_###, the \u201cBefore Prompting\u201d method achieves the worst accuracy, demonstrating that linking performance could be improved by using LLM.\nNotably, PromptLinkwith both two-stage prompts achieves the best accuracy with the second-highest cost (M total tokens, costing approximately $66.25), indicating that the combined effect of the prompts substantially enhances accuracy, with the costs being moderated by the first stage\u2019s proficiency in eliminating unrelated candidates.\nNIL Prediction.\nAnother ablation study examines PromptLink\u2019s NIL prediction ability.\nIn our built MIID and CISE datasets, each query EHR concept  is designed to have a ground-truth linking KG concept . To reflect the real-world unlinkable scenario, we extend our MIID dataset into \u201cMIID-NIL\u201d which contains a proportion () of unlikable EHR concept .\nIn Figure 3  ###reference_###, the overall accuracy of PromptLink in the MIID-NIL dataset is 0.8145.\nSpecifically for the unlikable concepts, PromptLink outputs the expected \u201cNIL\u201d with 0.9290 accuracy, which validates the NIL prediction ability of our proposed method.\nExisting methods highly rely on the hard-coded threshold. For example, we could threshold SAPBERT\u2019s generated embeddings\u2019 cosine similarity, then output the KG concept with the highest similarity above the threshold or \u201cNIL\u201d when none are above. However, this straightforward idea, requiring a manually set threshold, is less effective than PromptLink. As shown in Figure 3  ###reference_###, SAPBERT achieves lower accuracy (maximum value 0.7920) no matter what the threshold value is, which corresponds to our assumptions. When the threshold value is low, SAPBERT generates many wrong predictions to unlinkable query concepts; otherwise, SAPBERT continues to output \u201cNIL\u201d for many linkable concepts.\n###figure_3###"
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4. Case Studies",
            "text": "In case studies on linking EHR concepts to MIID\u2019s KG disease concepts, three scenarios are presented: (1) concepts assessed by both ground-truth labels and a clinician; (2) concepts evaluated by a clinician due to missing ground-truth labels; (3) irrelevant concepts judged by a clinician. The linking results of PromptLink and SAPBERT are presented in Table 3  ###reference_###.\nOverall, PromptLink could link biomedical concepts more accurately and appropriately.\nFor casse I-V, PromptLink\u2019s linking results are justified by the ground-truth label and clinician. Specifically, for cases I and II, PromptLink accurately links the EHR concepts to conceptually similar\nbut lexically distinct KG concepts, while SAPBERT links to lexically similar but conceptually different KG concepts. This difference showcases the effective use of LLM\u2019s biomedical knowledge. SAPBERT also shows inaccuracies in cases III-IV, and provides a broader prediction in case V, whereas PromptLink\u2019s predictions are more accurate and specific.\nFor cases VI-IX, where linking ground truth labels are lacking, PromptLink\u2019s predictions also align more accurately with EHR concepts than SAPBERT\u2019s, according to a clinician\u2019s review. In cases VI and VII, PromptLink closely matches the EHR concepts, while SAPBERT\u2019s predictions are overly specific. In cases VIII and IX, PromptLink correctly and automatically identifies no matching KG disease concepts, while SAPBERT fails to resolve that NIL prediction challenge unless manual thresholds are set and adjusted.\n###figure_4### ###figure_5### ###figure_6### ###figure_7### ###figure_8### ###figure_9### ###figure_10### ###figure_11### ###figure_12### ###figure_13### ###figure_14### ###figure_15### ###figure_16### ###figure_17### Note: \u201c\u201d indicates this prediction is justified by the clinician. \u201c\u201d indicates this prediction is justified by the ground-truth label.\n###figure_18### ###figure_19###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Conclusion",
            "text": "In this study, we introduce PromptLink, a novel framework leveraging LLMs and multi-stage prompts for effective biomedical concept linking.\nCompared with previous concept linking methods, PromptLink achieves better linking accuracy, attributed to LLM\u2019s intrinsic strong biomedical knowledge. PromptLink further employs multi-stage prompts to maintain cost-efficiency and handle the NIL prediction problem. Moreover, PromptLink functions as a zero-shot framework, requiring no training and demonstrating strong flexibility and generalizability across biomedical systems.\nPromising future work can focus on further enhancing the prompt effectiveness, reducing costs, and minimizing manual efforts, aiming to extend PromptLink\u2019s application to broader systems."
        }
    ],
    "url": "http://arxiv.org/html/2405.07500v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2.2"
        ],
        "main_experiment_and_results_sections": [
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.07500v1",
        "paper_title": "PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking",
        "research_background": "The paper \"PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking\" addresses the intricate and essential task of linking related biomedical concepts across different data sources. Here is a detailed breakdown of the paper\u2019s motivation, research problem, and relevant prior work:\n\n### Motivation\n\nThe primary motivation behind the paper is rooted in the necessity of enabling integrative analyses by linking biomedical concepts originating from diverse sources. These sources often provide multifaceted views of biomedical knowledge and data, such as Electronic Health Records (EHRs) and Knowledge Graphs (KGs). By effectively linking these concepts, researchers can harness comprehensive patient health analyses and structured biomedical knowledge, ultimately facilitating advanced biomedical research.\n\n### Research Problem\n\nThe research problem tackled in the paper revolves around the challenge of cross-source biomedical concept linking. Discrepancies in biomedical naming conventions across different systems pose a significant hurdle. For instance, the same disease might be termed differently in a KG and an EHR, creating a barrier to cohesive data analysis. Traditional methods relying on string-matching rules and thesauri have limitations related to coverage and generalizability. Machine learning-based methods, while promising, require extensive data and supervision signals, making them resource-intensive and less practical for novel downstream applications. Large Language Models (LLMs), with their rich prior knowledge and zero-shot learning capabilities, present a promising yet challenging solution due to their cost, context length constraints, and NIL prediction capability.\n\n### Relevant Prior Work\n\nThe paper builds upon various strands of prior research:\n1. **String-Matching and Thesauri-Based Methods**: Early approaches focused on string-matching rules and constructed thesauri (e.g., Aronson and Lang, 2010; Savova et al., 2010; Friedman et al., 2001). These methods, however, suffer from fixed rules and crafted thesauri, limiting their application in diverse real-world scenarios.\n   \n2. **Machine Learning-Based Methods**: The transition towards embedding-based methods involves transforming biomedical concepts into embeddings, leveraged for computing similarity scores. Notable advancements include the use of pre-trained language models (PLMs) (e.g., Wang et al., 2023b; Xu et al., 2020; Lee et al., 2020; Alsentzer et al., 2019) and graph neural networks (GNNs) (Zhou et al., 2020; Bordes et al., 2013; Grover and Leskovec, 2016). These models capture both semantics and relationships but are data-hungry and require significant supervision.\n   \n3. **Large Language Models (LLMs)**: Recent LLMs such as GPT-4 have shown impressive performance in NLP tasks, offering potential for zero-shot learning applications (e.g., Zhou et al., 2023; Singhal et al., 2023). However, they face challenges like context length limits and NIL prediction capability (e.g., Zhang et al., 2023; Peters et al., 2019).\n\nBy leveraging LLMs and introducing a novel two-stage prompting mechanism, the paper aims to overcome these challenges and enhance the cross-source biomedical concept linking task's effectiveness and efficiency.",
        "methodology": "### Methodology Section: Description of Proposed Method or Model\n\n**Proposed Method: PromptLink**\n\n*PromptLink* is a novel approach to cross-source biomedical concept linking that leverages large language models (LLMs). The method addresses two primary constraints of LLMs: high computational costs and limitations in input text length, by incorporating a two-stage process which is both efficient and effective.\n\n**Key Components and Innovations:**\n\n1. **Concept Representation and Candidate Generation**:\n    - **Preprocessing**: The input text undergoes preprocessing which includes lowercasing and removing punctuation.\n    - **Embedding Generation**: SapBERT, a biomedical-specialized pre-trained language model, is used to create embeddings for concepts extracted from both Electronic Health Records (EHR) and Knowledge Graphs (KG). For multi-token concepts, token-level embeddings are averaged.\n    - **Candidate Retrieval**: Cosine similarity is computed between EHR concept embeddings and KG concept embeddings. For each EHR concept, the top \\( k = 10 \\) KG concepts with the highest cosine similarities are selected as candidates for further linking.\n\n2. **Linking Prediction Using Two-Stage Prompts**:\n    - **First Stage**:\n        - This stage aims at filtering out unrelated candidates. GPT-4 is prompted to determine if each concept pair should be linked. The responses are obtained through a self-consistency strategy, where the same question is prompted multiple times (\\( t \\)). The belief score for each pair is computed based on the frequency of affirmative responses.\n        - A filtering strategy using parameter \\( \\tau = 0.5 \\) (threshold) is employed to discard candidates with zero belief scores, provided there are other closely aligned candidates. This reduces the number of irrelevant candidates and optimizes efficiency.\n    - **Second Stage**:\n        - The retained candidates are evaluated in a broader context using a compositional prompt with GPT-4. This consists of two questions:\n            1. Label the relationship between the query concept and candidates as \u201cexact match\u201d, \u201crelated to\u201d, or \u201cdifferent from\u201d.\n            2. Use self-verification prompts to either identify the closest match or dismiss all candidates if none are sufficiently close.\n        - A final linking result for the query EHR concept is decided based on occurrence frequency (\\( f \\)) of the outcomes and additional considerations such as embedding similarity for resolving ties.\n\nIn summary, *PromptLink* introduces an innovative two-stage prompting mechanism utilizing a specialized biomedical language model and GPT-4, significantly improving the accuracy and efficiency of cross-source biomedical concept linking.",
        "main_experiment_and_results": "### Main Experiment Setup and Results: PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking\n\n#### Datasets\nThe main datasets used in the experiments are two biomedical concept linking benchmark datasets, MIID (MIMIC-III-iBKH-Disease) and CISE (CRADLE-iBKH-Side-Effect):\n\n1. **MIID (MIMIC-III-iBKH-Disease)**:\n    - Contains 1,493 diagnosis concepts from the MIMIC-III dataset and 18,697 disease concepts from the iBKH knowledge graph.\n    - MIMIC-III is an extensive EHR corpus with over 53,423 hospital patient records.\n    - iBKH comprises 2,384,501 entities.\n    - For MIID construction, exact matches between diagnosis and disease concepts are excluded, and the remaining concepts are linked via ICD-9 and UMLS CUI codes to form ground-truth pairs.\n\n2. **CISE (CRADLE-iBKH-Side-Effect)**:\n    - Includes 1,500 diagnosis concepts from the CRADLE dataset and 4,251 drug side-effect concepts from iBKH.\n    - Uses CUI and SNOMED CT codes for constructing ground-truth pairs.\n\n#### Experimental Settings\n- **Zero-Shot Setting**: Given the limited supervision in the biomedical domain, the focus is on zero-shot biomedical concept linking.\n- **Concept Names**: The task relies solely on concept names which rules out the applicability of graph-based and thesauri-based linking methods needing topological information and predefined vocabulary, respectively.\n\n#### Baseline Methods\nThe following baseline methods are categorized into conventional and machine learning-based approaches:\n\n- **Conventional Methods**:\n    - Cosine Distance\n    - Jaccard Distance\n    - Levenshtein Distance\n    - Jaro-Winkler Distance\n    - BM25 \n\n- **Machine Learning-Based Methods**:\n    - Various pre-trained language models (PLMs) are used to generate embeddings and predict linking using embedding cosine similarity. The selected PLMs include:\n        - BioBERT\n        - BioGPT\n        - BioClinicalBERT\n        - BioDistilBERT\n        - KRISSBERT\n        - ada002\n        - SAPBERT\n\nAlthough the detailed results of each method are not provided in the query, the performance of each baseline would typically be evaluated on the two datasets using appropriate metrics such as precision, recall, and F1 score. The expected outcome would summarize the effectiveness of large language models in zero-shot cross-source biomedical concept linking, comparing them to conventional string similarity measures and highlighting their advantages in terms of accuracy and robustness."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the effectiveness and cost-efficiency of the prompts used in the PromptLink approach.",
            "experiment_process": "The experiment compares the performance of different prompting strategies using the same input data and 10 linking candidates. Specifically, the 'Before prompting' method, which uses only embedding similarity from a pre-trained language model, is compared against other methods that involve LLM-driven predictions based on LM-generated candidates. The analysis includes evaluating the accuracy and cost (measured in total tokens) of these methods, with costs approximated for LLM usage.",
            "result_discussion": "The 'Before prompting' method achieves the worst accuracy, affirming that LLM usage can improve linking performance. The PromptLink approach utilizing both two-stage prompts shows the best accuracy with the second-highest cost, indicating the significant combined effect of these prompts on enhancing accuracy. The cost is moderated by the first stage's efficiency in eliminating unrelated candidates.",
            "ablation_id": "2405.07500v1.No1"
        },
        {
            "research_objective": "To test PromptLink's ability to predict NIL (no link) outcomes for unlinkable concepts.",
            "experiment_process": "The study examines PromptLink's performance on the MIID-NIL dataset, an extension of the original MIID dataset that includes a proportion of unlinkable EHR concepts. The accuracy of PromptLink is assessed in this real-world scenario. The performance is further compared with the SAPBERT method, which uses a hard-coded cosine similarity threshold for NIL prediction.",
            "result_discussion": "PromptLink demonstrates an overall accuracy of 0.8145 on the MIID-NIL dataset. For unlinkable concepts, it outputs the expected 'NIL' with 0.9290 accuracy, validating its NIL prediction capability. In contrast, SAPBERT achieves lower accuracy (maximum 0.7920) due to its reliance on a manually set threshold, which either produces many wrong predictions (for low thresholds) or fails to link many concepts (for high thresholds).",
            "ablation_id": "2405.07500v1.No2"
        }
    ]
}