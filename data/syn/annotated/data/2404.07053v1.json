{
    "title": "Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and Interpretation",
    "abstract": "Metaphors, although occasionally unperceived, are ubiquitous in our everyday language. Thus, it is crucial for Language Models to be able to grasp the underlying meaning of this kind of figurative language. In this work, we present Meta4XNLI, a novel parallel dataset for the tasks of metaphor detection and interpretation that contains metaphor annotations in both Spanish and English. We investigate language models\u2019 metaphor identification and understanding abilities through a series of monolingual and cross-lingual experiments by leveraging our proposed corpus. In order to comprehend how these non-literal expressions affect models\u2019 performance, we look over the results and perform an error analysis. Additionally, parallel data offers many potential opportunities to investigate metaphor transferability between these languages and the impact of translation on the development of multilingual annotated resources.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Metaphor is commonly characterized as the understanding of an abstract concept in terms of another from a more concrete domain. According to Lakoff and Johnson (1980  ###reference_b34###), we can establish a distinction between conceptual metaphors, cognitive mappings that arise from the association between source and target domains, and linguistic metaphors, the expression of these mappings through language. The pervasiveness of metaphors in our daily speech makes it fundamental for language models to be able to process them accordingly, in order to achieve a satisfactory interaction between users and these tools. In addition, metaphor processing may have implications for other Natural Language Processing (NLP) tasks such as Machine Translation Mao, Lin, and Guerin (2018  ###reference_b45###); Sch\u00e4ffner (2004  ###reference_b62###); Shutova, Teufel, and Korhonen (2013  ###reference_b70###), political discourse analysis Charteris-Black (2011  ###reference_b18###); Prabhakaran, Rei, and Shutova (2021  ###reference_b56###); Rodr\u00edguez et al. (2023  ###reference_b59###) or hate speech Lemmens, Markov, and Daelemans (2021  ###reference_b35###), among others. Since in this work we study metaphor occurrence in natural language sentences, we will focus on linguistic metaphors only.\nThe most explored task so far is metaphor detection or identification, approached as a sequence labeling task grounded on different theoretical proposals Wilks (1975  ###reference_b80###, 1978  ###reference_b81###); Searle (1979  ###reference_b64###); Black (1962  ###reference_b12###). The methodology of most widespread use currently are the MIPVU guidelines Steen et al. (2010  ###reference_b72###), which rely on the mismatch between the basic and contextual meaning of a potential metaphor. The application of this procedure resulted in the publication of the referential dataset VUAM. As usual, most published work is English-centered, although multilingual and cross-lingual approaches are increasingly attaining more popularity. Yet available resources for other languages are still scarce, of reduced size or automatically labeled and non-parallel.\nAlthough metaphor interpretation has been less explored than detection, in the recent years it has arisen a growing interest that is reflected in the celebration of the FigLang 2022 Shared Task on Understanding Figurative Language Saakyan et al. (2022  ###reference_b60###). Previous work commonly approached it as a paraphrasing task Shutova (2010  ###reference_b66###); Shutova, Cruys, and Korhonen (2012  ###reference_b68###); Shutova, Teufel, and Korhonen (2013  ###reference_b70###); Shutova (2013  ###reference_b67###); Bizzoni and Lappin (2018  ###reference_b11###). Most recent works frame it within the task of Natural Language Inference (NLI), which consists in determining the relationship between a premise and a hypothesis, generally entailment, neutral or contradiction. In this way, a series of new datasets were published to explore this line of research.\nMost of them are limited to one language, typically English, or consist of premises and hypotheses sentences that are almost identical except for the metaphorical expressions, replaced by their literal or their antonym to construct entailment and contradiction pairs Agerri (2008  ###reference_b2###); Mohler, Tomlinson, and Bracewell (2013  ###reference_b50###); Chakrabarty et al. (2021  ###reference_b15###); Stowe, Utama, and Gurevych (2022  ###reference_b74###); Chakrabarty et al. (2022  ###reference_b16###); Kabra et al. (2023  ###reference_b31###). This mechanism to develop datasets might appear fruitful. Nonetheless, these artifacts do not constitute uses of metaphorical expressions in natural language and might yield to inconclusive results when evaluating the performance of the models.\nFollowing the state of the art and to compensate the mentioned shortcomings, in this work we will explore metaphor detection as a sequence labeling task and metaphor interpretation via NLI from a cross-lingual perspective with the aid of our proposed cross-lingual and parallel dataset with metaphor annotations for both tasks. Therefore, our contributions can be listed as follows:\nWe provide Meta4XNLI (M4X), a collection of existing NLI datasets, XNLI Conneau et al. (2018  ###reference_b23###) and esXNLI Artetxe, Labaka, and Agirre (2020  ###reference_b5###), enriched with metaphor annotations for the tasks of detection and interpretation in Spanish (ES) and English (EN). To the best of our knowledge, this is the first multilingual parallel dataset with metaphor annotations in natural language sentences. Meta4XNLI\u2019s characteristics include:\nParallel data and metaphorical annotations at token and premise-hypothesis pair levels.\nEnables cross-lingual analysis of metaphor.\nMetaphor annotations on texts of multiple domains and natural language sentences.\nContains texts translated in both directions: EN -> ES, ES -> EN, which enables analysis of translation impact on metaphor.\nMonolingual and cross-lingual experiments in various setups by leveraging Meta4XNLI. For metaphor detection, we evaluated with Meta4XNLI models trained on other corpora; we fine-tuned and evaluated Masked Language Models (MLM, encoders only) with Meta4XNLI for both languages and also in zero-shot scenarios. Through the usage of different datasets and a cross-lingual approach, our aim is to explore the generalization capabilities of MLMs and the extent of knowledge transference when it comes to metaphor processing. With respect to metaphor interpretation, we framed the task within NLI. In order to study whether MLMs struggle with metaphorical expressions understanding, on one hand, we tested MLMs fine-tuned for the task of NLI with pairs containing and lacking metaphors from Meta4XNLI. On the other, we trained and evaluated the models on pairs without metaphor instances and the whole dataset.\nCode and data publicly available in https://github.com/elisanchez-beep/meta4xnli  ###reference_li###\nIn this section, we will give an overview of most significant works that focused on metaphor processing. In the first place, we analyse metaphor detection and interpretation developed in EN, since most publications and datasets are English-centered. Afterwards, we will explore multi- and cross-lingual approaches for both tasks.\nInitially, most work for the task of metaphor detection was corpus-based Charteris-Black (2004  ###reference_b17###); Semino (2017  ###reference_b65###). However, the growing interest throughout the last years led to the celebration of FigLang shared tasks Leong, Beigman Klebanov, and\nShutova (2018  ###reference_b37###); Leong et al. (2020  ###reference_b36###), which promoted multiple approaches to address it as a sequence labeling task via deep learning techniques. The most used methodology consisted in training a model with specific features, either linguistics-related Stowe and Palmer (2018  ###reference_b73###) or of other nature such as abstractness, visual or emotion-related information Tsvetkov et al. (2014  ###reference_b76###); Bizzoni and Ghanimifard (2018  ###reference_b10###); Tong, Shutova, and Lewis (2021  ###reference_b75###); Neidlein, Wiesenbach, and\nMarkert (2020  ###reference_b52###).\nThe arrival of Transformer-based models Devlin et al. (2019  ###reference_b25###) led to an increase in the overall performance of the task. Most fine-tuned models are founded on linguistic theories, such as MIP (Steen et al., 2010  ###reference_b72###) or Selectional Preference (SP) Wilks (1975  ###reference_b80###); Percy (1958  ###reference_b54###), which, generally speaking, address metaphor as a contrast between basic and contextual meaning. The MIP approach was extended to MIPVU to develop the referential corpus in EN: the VUAM dataset Steen et al. (2010  ###reference_b72###), which covers texts of multiple domains and annotations at token level, and was used for the Shared Tasks along with the release of TOEFL dataset Leong et al. (2020  ###reference_b36###). Other well-known datasets are TroFi Birke and Sarkar (2006  ###reference_b9###), of considerable smaller size and restrained to verbs; or MOH-X dataset Mohammad, Shutova, and Turney (2016  ###reference_b48###), also focusing on metaphorical and literal examples of verbs. Other available corpora covers texts from a single domain, such as tweets Zayed, McCrae, and Buitelaar (2019  ###reference_b83###) or news headlines, in NewsMet dataset Joseph et al. (2023  ###reference_b30###).\nThe combination of pre-trained language models and these available resources brought forth multiple models fine-tuned for metaphor detection with ad hoc architectures. For instance, Song et al. (2021  ###reference_b71###) propose Mr-BERT, a model able to extract the grammatical and semantic relations of a metaphorical verb and its context. RoPPT Wang et al. (2023  ###reference_b79###) takes into account dependency trees information to extract the terms most relevant to the target word. The purpose of other published models is to identify the metaphoric span of the sentence, namely MelBERT Choi et al. (2021  ###reference_b19###), based on MIP and SP theories, as well as BasicBERT Li et al. (2023a  ###reference_b40###). To alleviate scarcity of metaphor annotated data, CATE Lin et al. (2021  ###reference_b42###) is a ContrAstive Pre-Trained ModEl that uses semi-supervised learning and self-training.\nOthers use additional linguistic resources besides datasets, like Babieno et al. (2022  ###reference_b6###), that take advantage of Wiktionary definitions to build their MLM MIss RoBERTa WiLDe; FrameBERT Li et al. (2023b  ###reference_b41###) makes use of FrameNet Fillmore, Baker, and Sato (2002  ###reference_b27###) to extract the concept of the detected metaphor. MisNET Zhang and Liu (2022  ###reference_b85###) exploits dictionary resources and is based on linguistic theories to predict metaphors at word level. The model of Wan et al. (2021  ###reference_b78###) learns from glosses of the definition of the contextual meaning of metaphors. Maudslay and Teufel (2022  ###reference_b47###) present what they call a Metaphorical Polysemy Detection model by exploiting WordNet and Word Sense Disambiguation (WSD) to perform the detection. Another approach is to frame metaphor detection within another NLP task: such as Zhang and Liu (2023  ###reference_b86###), who adopt a multi-task learning framework where knowledge from WSD is leveraged to identify metaphors; Feng and Ma (2022  ###reference_b26###) develop An Auto-Augmented Sturcture-aware generative model that approaches metaphor detection as a keywords-extraction task; and the work of Dankin, Bar, and Dershowitz (2022  ###reference_b24###), that explores few-shot scenarios from a Yes-No Question-Answering perspective. Badathala et al. (2023  ###reference_b7###) also propose a multi-task approach to detect metaphor and hyperbole altogether although at sentence level.\nAmong these examples, the state of the art on the task approached as sequence labelling and evaluated on VUA-20, our experimental setup, is the following: DeBERTa-large (73.79 F1) fine-tuned and evaluated on VUAM Sanchez-Bayona and\nAgerri (2022  ###reference_b61###), BasicBERT (73.3 F1), FrameBERT (73.0), RoPPT (72.8 F1) and MelBERT (72.3 F1). These scores show the complexity of the task and how there is still room for improvement to achieve competitive performance.\nThe evaluation of metaphor interpretation remains a difficult issue, thus most works frame it within other NLU tasks, namely paraphrasing or NLI.\nAmong the paraphrase current, we can find both supervised Shutova, Teufel, and Korhonen (2013  ###reference_b70###); Shutova (2013  ###reference_b67###) and unsupervised Shutova (2010  ###reference_b66###); Shutova, Cruys, and Korhonen (2012  ###reference_b68###) approaches. The work of Bollegala and Shutova (2013  ###reference_b14###) explores the generation of literal paraphrases for metaphorical verbs unsupervisedly. Bizzoni and Lappin (2018  ###reference_b11###) developed a corpus with sentences that contain metaphorical expressions and a set of literal paraphrases ranked according to their acceptability. They exploit their resource to test deep learning systems that approach metaphor interpretation as a classification and ranking task. Mao, Lin, and Guerin (2021  ###reference_b46###) focused on the paraphrase of verb metaphors as well. They take advantage of MOH-X Mohammad, Shutova, and Turney (2016  ###reference_b48###) and VUAM to test BERT\u2019s capability to generate the most probable literal substitute. Pedinotti et al. (2021  ###reference_b53###) develop an evaluation dataset in EN with 300 instances that include conventional and novel metaphors, as well as literal and nonsense sentences. They exploit it to test BERT\u2019s ability to interpret metaphors and discriminate among the different types of sentences, in addition to examine how MLMs encode this knowledge.\nSome recent works address metaphor interpretation as a Question-Answering problem. They reformulate metaphor expressions as questions or prompts to test LLMs. Com\\textcommabelowsa, Eisenschlos, and\nNarayanan (2022  ###reference_b21###) propose MiQA, a dataset of 300 items that gather literal and metaphorical premises, paired to implication sentences to evaluate LLMs\u2019 metaphors understanding by asking if the implications are true or false. Liu et al. (2022  ###reference_b43###) develop Fig-QA dataset also for EN but of a considerable larger size. It comprises 10,256 instances of creative metaphors paired with their literal implication sentences. They exploit their resource to evaluate state-of-the-art models ability to understand metaphor, framed as an inference task. Nevertheless, these pairs does not consist of natural utterances, but human-generated examples that present a fixed sentence structure. In addition, premises and their implications hold lexical overlapping, which could induce to bias in the models evaluation results, e.g.: Money is a helpful stranger - > Money is good; Money is a murderer - > Money is bad.\nRakshit and Flanigan (2023  ###reference_b58###) introduce FigurativeQA, which gathers 1000 yes/no questions in EN from the reviews domain that include metaphorical and literal context examples in addition to other figurative language phenomena, such as simile, hyperbole, idiom and sarcasm to probe models. Other works like Wachowiak and Gromann (2023  ###reference_b77###); Pitarch, Bernad, and Gracia (2023  ###reference_b55###) center their research on examining whether generative models understand conceptual metaphors and their reasoning capabilities, the former makes use of existing datasets in EN and ES and the latter proposes a new resource only in EN.\nA popular approach that continues being explored is the study of metaphor interpretation framed in the NLI or Recognizing Textual Entailment (RTE) task Agerri (2008  ###reference_b2###); Mohler, Tomlinson, and Bracewell (2013  ###reference_b50###). For example, Chakrabarty et al. (2021  ###reference_b15###) propose 12500 instances in EN collected from already existing datasets for RTE, which cover simile, metaphor and irony. Out of the total dataset, only 600 pairs are metaphor related. In addition, their data presents some bias that does not truthfully represent the use of metaphors in natural language. For instance, entailment pairs hypotheses are generated by literal substitution of the metaphorical expression from the premise. This also occurs with non-entailment. In this case, the metaphor from the premise is replaced by an antonym to generate the hypothesis.\nThe work of Zayed, McCrae, and Buitelaar (2020  ###reference_b84###) aimed at creating a gold standard for metaphor interpretation. Therefore, they developed a dataset of 2500 tweets with definitions of verb-noun metaphorical expressions with the aid of lexical resources and word embeddings. However, they do not present any experiments on models to evaluate their dataset.\nStowe, Utama, and Gurevych (2022  ###reference_b74###) introduce IMPLI, a semi-automatic constructed dataset for EN to evaluate RoBERTa-like models performance on figurative language, specifically idioms and metaphors, within the NLI task. Their resource is considerably larger in size (24,029 silver and 1,831 gold sentence pairs) and covers entailment and non-entailment (a merge of neutral and contradiction) relations. Similarly to the previous work, some biases are introduced throughout the generation of this dataset. Entailed pairs only consist of sentences that include a metaphorical expression and the entailed hypothesis is simply its literal paraphrase. On the other hand, non-entailment pairs were developed by \u201cinventing\u201d a literal but not true meaning of the figurative expression.\nChakrabarty et al. (2022  ###reference_b16###) propose FLUTE, an explanation-based dataset in EN of 9000 NLI pairs that include sarcasm, simile, metaphor and idioms that they later exploit to evaluate T5 Raffel et al. (2023  ###reference_b57###). FLUTE differs from the other resources in that each entailed/contradicted pair is accompanied by an explanation. However, premises and hypotheses are mainly based on lexical substitution: e.g. His dark clothes had a negative effect in the shadows -> His dark clothes were a plus in the shadows.\nAll the work we just mentioned is centered in English only, thus most relevant resources developed for metaphor processing comprehend data in this language, both for detection and interpretation tasks. In the case of metaphor detection, in order to compensate the lack of language diversity, some monolingual datasets were published in other languages, although they are commonly of reduced size and still are monolingual, e.g. KOMET corpus Antloga (2020  ###reference_b4###) in Slovene, CoMeta Sanchez-Bayona and\nAgerri (2022  ###reference_b61###) in Spanish, Estonian Aedmaa, K\u00f6per, and Schulte im\nWalde (2018  ###reference_b1###), German K\u00f6per and Schulte im\nWalde (2016  ###reference_b32###) or Polish adjective-noun metaphors Mykowiecka, Marciniak, and\nWawer (2018  ###reference_b51###).\nAmong the datasets that cover more than one language, we can find LCC Mohammad, Shutova, and Turney (2016  ###reference_b48###): it comprises texts of English, Spanish, Russian and Farsi and provides source/target, metaphoricity degree and metaphor/literal annotations on sentences. However, there are not labels at token level, annotations were extracted automatically, only a subset was manually validated and it is not parallel corpora. The work of Levin et al. (2014  ###reference_b38###) proposes the CCM corpora, which also includes sentences in the same four languages as the LCC dataset. Nevertheless, it is composed of source and target mappings, thus it is centered on conceptual metaphors.\nSchuster and Markert (2023  ###reference_b63###) explore static embeddings to generate a cross-lingual dataset out of existing resources in German, English and Polish but focused on adjective-noun metaphor pairs and non-parallel texts. Similarly, Berger (2022  ###reference_b8###) explores transfer learning techniques, such as neural machine translation, cross-lingual word embeddings or multilingual pre-trained models to obtain a dataset for metaphor detection in German from English corpora.\nIn addition to data generation, experimental settings of many recent publications shifted to multi- and/or cross-lingual approaches, which aim at analysing metaphor transferability among different languages. Previous to pre-trained language models, Tsvetkov et al. (2014  ###reference_b76###) already explored unsupervised methods to evaluate metaphor identification of specific syntactic constructions in English, Spanish, Russian and Farsi. Shutova et al. (2017  ###reference_b69###) as well experimented with semi- and unsupervised learning, specifically with clustering techniques for metaphor detection in English, Spanish and Russian.\nAghazadeh, Fayyaz, and\nYaghoobzadeh (2022  ###reference_b3###) investigate whether pre-trained language models are able to encode metaphorical meaning through the task of metaphor detection in English, Farsi, Russian and Spanish. Not only the trend is to mix languages but also types of figurative language.\nThe work of Lai, Toral, and Nissim (2023  ###reference_b33###) presents a combination of joint-models that are able to detect if a sentence has hyperbole, idioms or metaphors in these same four languages, nevertheless, it does not identify the figurative language span.\nWith respect to metaphor interpretation, the range of available datasets in multiple languages is more limited than that for detection.\nMost corpora just mentioned only concerns EN: FigurativeQA Rakshit and Flanigan (2023  ###reference_b58###), Fig-QA Liu et al. (2022  ###reference_b43###), IMPLI Stowe, Utama, and Gurevych (2022  ###reference_b74###) or FLUTE Chakrabarty et al. (2022  ###reference_b16###). The exception is the work of Kabra et al. (2023  ###reference_b31###), who delve into figurative language understanding from a multi-lingual and multi-cultural perspective. They present MABL, a dataset for seven languages with a high number of speakers, such as Hindi, Swahili, or Sudanese. They demonstrate that socio-cultural features pose an essential impact on the conceptual mappings that are later materialized in linguistic metaphors. They exploit it to evaluate language models and provide an insight of how English- or Western-centered the training process of these models can be.\nThis overview gives an account of the prevalence of EN in the research of metaphor processing. Monolingual datasets for detection in other languages are of a very reduced size in comparison to those in EN. Other limitations of metaphor detection corpora is that datasets with a larger amount of samples do not include annotations at token level or are limited to metaphorical expressions of specific POS. Namely LCC dataset, which is the most utilised multilingual dataset for the task, however, its original annotations are developed at sentence level and it does not contain parallel text.\nMetaphor interpretation datasets are more scarce and the lack of variety of languages is more striking than in metaphor detection corpora. The only multilingual dataset is MABL, however, data and annotations are not parallel between languages. We can find different datasets according to the tasks used to approach interpretation. Nevertheless, these corpora tend to bring along biases and artifacts as a result of synthetic annotation processes Boisson, Espinosa-Anke, and\nCamacho-Collados (2023  ###reference_b13###). In datasets developed for paraphrase, tuples are commonly composed of a sentence with a metaphorical expression that is replaced for its literal meaning. In the case of NLI datasets, premise-hypothesis pairs are typically constructed by lexical substitution: the entailment is based on the literal paraphrase of the metaphor and contradiction on the replacement of the metaphorical terms by their antonym. Thus, we reckon these instances are not representative of spontaneous occurrences of metaphor in natural language utterances.\nThese shortcomings when developing datasets for figurative language can induce misleading results and conclusions. To bridge this gap, we bring forward the first parallel corpus that contains metaphorical annotations for metaphor detection at token level that cover all POS, and for metaphor interpretation via NLI in ES and EN. In addition, this resource includes a sufficient number of instances and contains data from natural language utterances that were not specifically generated for metaphor processing tasks. We summarised the distinct features of most popular available datasets in Table 1  ###reference_###.\nWe believe our proposed dataset is a valuable resource to continue with this line of research of multi- and cross-lingual approaches, which can allow to explore whether metaphors are transferable among languages.\nLang\nDE, EN\nEN, ES, FA, RU\nHI, ID, JV, KN, SU, SW, YO\nES, EN\nTo address these shortcomings, we introduce Meta4XNLI, a parallel dataset in ES and EN with metaphorical annotations for detection and interpretation via NLI in texts of natural language utterances and multiple domains. In the following subsections, we describe the collection of the dataset (Subsection 3.1  ###reference_###), the methodology we employed to annotate metaphors for each task and language (Subsection 3.2  ###reference_###) and, as a result, we will provide details of the outcoming dataset of this process (Subsection 3.3  ###reference_###).\nThe annotation process in this language comprehends two phases, as depicted in Figure 2  ###reference_###. First, we automatically label Meta4XNLIES by leveraging mDeBERTa He, Gao, and Chen (2021  ###reference_b29###) fine-tuned for metaphor detection in ES with CoMeta. We choose mDeBERTa since it is the multilingual model that achieved highest F1 score in the experimental setup of Sanchez-Bayona and\nAgerri (2022  ###reference_b61###). This choice aims at reducing the heavy work load and time investment that manual annotation from scratch requires. Afterwards, we manually inspect and correct the predictions in the whole dataset. From the first phase of automatic labeling, 748 tokens were predicted as metaphor in the premises and 724 in the hypotheses. After a complete and manual revision of all sentences, we removed 74 tokens and added 481 undetected metaphors in the premises; whereas in the hypotheses, we deleted 118 false positives and labeled 533 false negatives.\n###figure_1### The main sources of ambiguity in ES emerge from multi-word expressions (MWE) and polysemy.\nThe main issue when labeling MWE is to decide whether to treat the MWE as a single lexical unit, a fixed expression where the meaning of each word is not transparent anymore, or if it should be regarded as a collocation. Collocations are expressions where the constituent words tend to co-occur with high frequency but are not fixed, since each of its elements can be replaced by others with similar meaning.\nDijo que era hora de entrar en p\u00e1nico (lit. \u201cThey said it was time to enter into panic\u201d).\nFor instance, in Example 2  ###reference_###, the expression entrar en p\u00e1nico could be initially considered as a MWE with a single lexical unit, therefore, the three tokens would be labeled as metaphorical. However, this expression specifically means \u201cto panic\u201d, in which the verb entrar (lit. \u201cto enter\u201d) holds metaphorical meaning, since \u201cpanic\u201d is not a physical place you can get into. In this case, we do not think of the expression as a fixed MWE, since the verb is also used metaphorically with other terms that are not places, like entrar en c\u00f3lera (\u201cto get angry\u201d, lit. \u201cto enter into wrath\u201d) or entrar en calor (\u201cto feel hot\u201d, lit. \u201cto enter into heat\u201d). In all of these expressions the verb is conveying the sense of starting to feel the noun it complements, as if by entering to a place it transformed our sensitivity. The association of concepts arises from understanding emotions or sensations as physical locations.\nRegarding polysemy, the existence of multiple and very nuanced senses associated to the same token can lead to confusion. It can be challenging to determine whether the basic meaning of the lexical unit is currently and generally known and used by native speakers or if they directly associate the lexical unit to the figurative meaning, not identifying the basic meaning at all.\n[\u2026] ha mostrado su apoyo a la candidatura para ser sede [\u2026] (lit. \u201cThey showed their support to the candidacy to be head office\u201d).\nFor instance, in Example 2  ###reference_### we label apoyo as metaphorical, since the most basic meaning of the verb apoyar in the dictionary defines it as \u201cto make something rest upon another thing\u201d. In this sentence, the contextual meaning refers to someone in favour of someone else\u2019s goal. Figuratively, the goal can be understood in terms of a physical object so heavy that requires more than one anchor point to distribute its weight. Doubts in this examples come from the fact that the figurative sense can be more used that its basic one, thus speakers might not identify the metaphorical meaning as such. We make use of the Diccionario de la Real Academia Espa\u00f1ola (DRAE) as a tool to help us clarify these ambiguous cases. Nonetheless, metaphor identification remains a highly subjective task.\nWe develop Meta4XNLIEN annotations semi-automatically and based on ES annotations, since our purpose is to publish a parallel resource to counterbalance monolingual English labeled data. These annotations serve as a starting point for further refinement and enable potential analysis of metaphors shared across these two languages. The whole process comprises mainly four phases, as depicted in Figure 3  ###reference_###.\nThe first step involves the projection of ES labels onto EN sentences. For this end, we utilised Easy-Label-Projection Garc\u00eda-Ferrero, Agerri, and\nRigau (2022  ###reference_b28###), a tool developed for cross-lingual sequence labeling that makes use of word alignments and data- and model-transfer to project the labels from a source language (ES) to an untagged target language (EN). This mechanism is suitable when the labeled entity is certainly appearing in both source and target sentences, however, metaphors present in a source sentence are not necessarily in its translation. It depends on a series of multiple factors, namely the type of translation, the knowledge of the translator, if it is human-translated, or socio-cultural knowledge, among others. The next step comprises the manual revision of the projections.\nIn order to alleviate this issue, we made use of XLM-RoBERTa Conneau et al. (2020  ###reference_b22###) trained on the VUAM dataset, as it is the multilingual model that showed best performance in the experiments in EN from the work of Sanchez-Bayona and\nAgerri (2022  ###reference_b61###). Since projected sentences were manually reviewed (13% of the total), we only predicted automatically those sentences that did not receive a projection in the first place (87%). We manually reviewed the outcoming predictions from the MLM to correct errors and undetected metaphorical expressions following the MIPVU procedure. The number of metaphorical tokens extracted in each annotation phase is detailed in Table 3  ###reference_###.\nThere are some issues that emerged during the whole annotation process. Firstly, the projections phase entails that for a metaphorical expression to be annotated in EN, it has to have been annotated in ES in advance. Hence, metaphors in EN sentences that were not expressed figuratively in their ES counterpart will not be spotted, as in Example 3  ###reference_###. In the ES sentence there is not a metaphorical expression annotated as such, however, in the EN version, the adjective heavy is holding metaphorical meaning, as a synonym of \u201cdemanding\u201d, which is expressed literally in ES with the adjective exigentes. In this kind of cases where the metaphor is \u201cgained in translation\u201d, the lack of annotation in the source language implies no label will be projected onto the target sentence, missing a metaphorical instance. These examples give an account of how translation and the singularities of metaphors according to languages may affect to the annotation process of this task.\n###figure_2### .\nA los usuarios m\u00e1s exigentes se les deber\u00eda cobrar m\u00e1s. \nThe heavy users should be charged the most.\nAnother issue that should be noted are false positives: all ES sentences with one or more labeled metaphors will transfer those tags to the EN sentence, regardless the translated tokens hold metaphoric meaning or not. To solve this question, we manually reviewed all sentences that received a projected tag. In this revision we had to remove metaphors that were \u201clost in translation\u201d and adjust some spans projected to the target language, e.g. some annotated verbs in ES were projected in EN to the subject pronoun and verb, since some ES verbs forms are synthetic and include the person information in a single morpheme. Therefore, we eliminated the label from the pronoun and maintained only that of the verb. In Example 3  ###reference_### we can see how the verb peleaban (lit. \u201cthey fought\u201d) labeled as metaphorical in the ES sentence (Example 3  ###reference_###) was projected to the subject and verb in EN (Example 3  ###reference_###). Example 3  ###reference_### represents the definitive version of the annotations after manual revision.\nPeleaban por lo ricos que eran los directores ejecutivos.\nThey fought about how rich CEOs were.\nThey fought about how rich CEOs were.\nRegarding the subset of sentences that were automatically labeled by XLM-RoBERTa, some concerns emerged with respect to annotations present in VUAM dataset during the phase of manual revision. Especially when it comes to phrasal verbs and abstract and polysemous terms that are lexicalised. For instance, due to Examples like 3  ###reference_### or 3  ###reference_### labeled as metaphorical in the training set VUAM, we observed a tendency to overannotate verbs that conform phrasal verbs as metaphorical, such as get, look, made or go. In most cases, these verbs appear in a context where they do not add any strong semantic information due to their lexicalization. Hence, we unmarked these instances that were predicted as metaphorical. Following this line of thought, we also discarded abstract and vague terms that are common-places of spontaneous discourse, like the word thing in Example 3  ###reference_###, since it can refer to any kind of entity, either concrete or abstract and might not be directly matched to a more broadly used basic meaning.\nHis lack of humbug about political balance has always made him more honest than all the employees [\u2026].\n\\enumsentenceTake what you want and leave the rest , your mother \u2019ll get rid of it . \n\\enumsentenceOne thing always linked to another thing.\nThe parallel data for this task comprises a total of 13320 sentences annotated at token level, since we approached metaphor detection as a sequence labeling task, following the criteria of the cited previous work in Section 2  ###reference_###.\nWith respect to Spanish annotations, there is a total of 1155 metaphorical tokens in premises and 1139 in hypotheses. Out of the 13320 sentences, 1873 contain at least one metaphorical expression, which constitutes the 14% of the whole dataset. This information is detailed in Table 4  ###reference_### according to the source datasets partitions. A higher proportion of metaphors in premises can be noticed. This might be due to the fact that premises are of larger length than hypotheses, as indicated in Conneau et al. (2018  ###reference_b23###). In addition, premises were collected from existing utterances, while hypotheses were generated from crowd-source workers in response to a given set of premises. Therefore, hypotheses sentences might tend to present shorter length and lower complexity.\n###table_1### Regarding English annotations, we can observe a similar trend to that of ES in Table 5  ###reference_###. A total of 3330 tokens were labeled as metaphor and 2736 sentences contain at least one metaphorical instance out of the total 13320 sentences. Premises show a higher metaphor ratio than hypotheses as in ES annotations. Additionally, we observe a larger amount of labeled metaphors in EN, caused by the different annotations processes specified in Subsection 3.2  ###reference_###. Since VUAM contains a significantly higher amount of labeled metaphorical expressions, the MLM fine-tuned with this dataset predicted many ambiguous metaphors, which we subsequently removed in manual revision. These discrepancies are not only noticeable in annotations but also in experiments results. Moreover, this gives an account of how guidelines for metaphor identification labeling are open to discussion and clarification, due to the subjective and nuanced nature of this cognitive-linguistic phenomenon.\n###table_2### ###table_3### Annotations for this task were developed at premise-hypothesis level. As shown in Table 6  ###reference_###, the average percentage of pairs with metaphors relevant to the inference relationship is 12%. This figure remains steady throughout each source dataset and inference labels. esXNLI shows a higher number of metaphor occurrence that might be caused by the difference of text domains and sentence characteristics with respect to XNLI data. Regarding non-relevant cases, we do not exploit them in the experiments in order to be able to analyse more clearly whether metaphor presence impacts models performance. We keep the same sample distribution for both languages to develop the experiments.\nIn this section we present the experimental setup designed with the objective of testing the capabilities of multilingual MLMs on metaphor detection in cross-domain, crosslingual and multilingual settings. Furthermore, we also experiment with their ability to perform NLI when the correct inference requires understanding of metaphorical language.\nTaking advantage of previous available resources and the corpus we present in this work, we conducted a series of experiments to evaluate and fine-tune MLMs.\nThe configuration of cross-domain experiments is specified in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###). For the other three setups, monolingual, multilingual and zero-shot cross-lingual, we performed hyperparameter tuning for batch size (8, 16, 36), linear decay (0.1, 0.01), learning rate (in the [1e-5-5e-5] interval), sequence length of 128 and epochs from 4 to 10. A warm-up of 6% is specified. The results of the hyperparameter tuning showed that after 4 epochs development loss started to increase, so results reported here are obtained with 4 epochs, batch size of 8, weight decay of 0.1 and learning rate of 5e-5.\nCross-domain: the aim of this set of experiments is to evaluate the performance of MLMs fine-tuned with CoMeta and VUAM datasets on Meta4XNLIES and Meta4XNLIEN, respectively, since each dataset contains texts from different domains. The motivation is to explore the impact of text features and genres on the performance, as well as annotation criteria Aghazadeh, Fayyaz, and\nYaghoobzadeh (2022  ###reference_b3###); Lai, Toral, and Nissim (2023  ###reference_b33###). To do so, we chose the models with best performance from monolingual experiments developed in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###). We conducted the evaluation on various data splits: within each source dataset, XNLIdev, XNLItest and esXNLI, we evaluated premises and hypotheses separated and combined. This is due to the dissimilarities and the unequal distribution of metaphorical expressions between premises and hypotheses sentences mentioned in Subsection 3.3  ###reference_###.\nMonolingual: this scenario comprises the fine-tuning and evaluation of the MLM on Meta4XNLIES and Meta4XNLIEN separately. To accomplish this task, we split Meta4XNLI into train, development and test sets (0.6-0.2-0.2). We equally distributed the data to ensure each partition is balanced in terms of source datasets sentences and metaphor occurrence. Data statistics are detailed in Appendix A  ###reference_###. These partitions will be used as well in subsequent multilingual and zero-shot cross-lingual scenarios. In addition to fine-tuning and evaluation on Meta4XNLIES and Meta4XNLIEN, we evaluated each trained monolingual model with the test sets of CoMeta and VUAM, following the same reasoning as in cross-domain experiments.\nMultilingual: the purpose of these experiments is to explore whether MLMs benefit from being trained on data in multiple languages. In this case, we combined Meta4XNLIES and Meta4XNLIEN train splits to fine-tune the models. Subsequently, we evaluated the trained models on each language test set, in order to analyse the impact on the performance for each language. The data splits used correspond to those from monolingual experiments.\nZero-shot cross-lingual: in this scenario we explore to what extent MLMs are able to generalize knowledge and metaphor transfer between these two languages in question. Therefore, we fine-tune the models with Meta4XNLI data in one language and evaluate it on the test set of the other. Data partitions used for these experiments are the same as in monolingual an multilingual scenarios.\nWe carried out two sets of experiments to evaluate metaphor interpretation within the NLI task. We performed hyperparameter tuning, with the same range of parameters specified on the task of metaphor detection. We report best results obtained on the development set, after 4 epochs, batch size of 8, learning rate of 1e-5, weight decay of 0.1 and 512 sequence length.\nThe purpose of the first experiments is to examine if the presence of metaphorical expressions in premise-hypothesis pairs impacts the performance of models in the NLI task. To that end, we fine-tuned the MLMs for the task with the MultiNLI dataset. Then, we evaluated them with Meta4XNLI. Among each source dataset, we discriminated pairs with metaphors relevant to the inference relationship from those without metaphorical expressions. We developed the evaluation on each of these subsets and for each language separately, e.g. one evaluation on XNLIdev metaphor set and another on XNLIdev without metaphors set, in EN and then in ES.\nThe goal of the second set of experiments is to analyse the effect on models performance of not being \u201cexposed\u201d to instances with metaphorical expressions during training. With this aim in mind, we extracted pairs with and without metaphors from Meta4XNLI train, dev and set splits. On the first scenario, we fine-tuned the models only with pairs from the train set that did not contain any metaphorical expression. On the second scenario, we mixed pairs with and without metaphors and fine-tuned the models as well. In both cases, we evaluated on the test sets with and without metaphors for each language.\nIn addition to F1 score, we performed in-vocabluary (Inv) and out-of-vocabulary (Oov) evaluations to test the impact of the labels seen from training data during the learning process of the MLMs. Precision and Recall metrics are reported in Appendix B  ###reference_###. For in-vocabulary evaluation, we computed F1 score taking into account predicted words seen in training labeled as metaphorical. While for out-of-vocabulary evaluation we considered only words not seen during training to compute the F1 score. We included this evaluation in all experiments but for the zero-shot cross-lingual setup, since the data of train and test sets are in different languages, thus the match of the exact same metaphorical token in both partitions is highly unlikely.\nAlthough the purpose of our experiments is not to beat state-of-the-art results but to evaluate the performance of MLMs on the task from a cross-lingual approach, we added two indicative baselines: on one hand, the system BasicBERT Li et al. (2023a  ###reference_b40###), which obtained 73.3 F1 score; on the other, the result of DeBERTa reported in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###), with 73.79 F1. We selected these results for comparison purposes, since they both were evaluated on the VUAM-2020 version of the dataset in EN used in the Shared Task 2020 Leong et al. (2020  ###reference_b36###) and in the same experimental setup we propose.\nCross-domain: We evaluated models trained on CoMeta and VUAM datasets with XNLI and esXNLI in ES and EN, respectively. From results reported in Table 7  ###reference_###, we can observe that in all cases mDeBERTa outperforms XLM-RoBERTa for ES. In EN the best result is obtained by DeBERTa, which also outperforms XLM-RoBERTa in all scenarios. In all datasets, except for XNLIdev in ES, premises sentences achieve better results than hypotheses and the combination of both. This goes in line with annotation statistics in Tables 4  ###reference_### and 5  ###reference_###, which show that premises contain a greater ratio of metaphors per sentence than hypotheses in both languages. In ES, in-vocabulary evaluation outperforms the general F1 score while out-of-vocabulary evaluation results decrease. The small distance in points of this cross-domain evaluation in ES exhibit stability of the models when it comes to predicting metaphors and coherence in annotations between both datasets despite the difference of text domains. Nevertheless, EN results do not demonstrate such consistency, as out-of-vocabulary obtains higher results than the overall F1 score. This might be due to the different labeling criteria used in VUAM and Meta4XNLI, as we mentioned in 3.2  ###reference_###. These discrepancies are also reflected in a significant drop in performance with respect to ES experiments and in-domain evaluation with VUAM. The high recall scores from Table 16  ###reference_### in Appendix B  ###reference_### show that the model tends to predict many metaphors, however, low precision scores indicate that a small amount of these predictions are correct.\n.\nMonolingual: Results from this set of experiments are specified in Table 8  ###reference_###. After fine-tuning and evaluating the models with Meta4XNLIES and Meta4XNLIEN, the highest overall F1 score is obtained by mDeBERTa in ES. On the other hand, XLM-RoBERTa achieves better performance than mDeBERTa in EN but still lower than in ES. In both languages, in-vocabulary evaluation results are higher than the overall ones and out-of-vocabulary results, lower. This is not the case when we use VUAM corpus for testing the model fine-tuned with Meta4XNLIEN. In this setup, similarly to results from cross-domain experiments, performance drastically falls to 32.42 F1 score and out-of-vocabulary results are the highest. The reason behind might be the mismatches in the annotation process. In ES, when evaluating CoMeta, we also encounter a decrease in the performance, however, it is more subtle: from 67.17 to 56.24, obtained by XLM-RoBERTa. This might be caused by the variety of text genres and dissimilarities between sentences from each dataset.\nMultilingual: In this set of experiments we assembled Meta4XNLIEN and Meta4XNLIES to train the MLMs. The evaluation is conducted for each language separately and results detailed in Table 9  ###reference_###. Best results in ES are obtained by mDeBERTa, which are higher than the top result from monolingual experiments. In EN, mDeBERTa is the model that achieves better performance but very close to that of XLM-RoBERTa. The highest F1 score is 8 points lower than that of ES but outweighs EN monolingual results. This suggests that the combination of parallel multilingual data for training is beneficial for the performance of the models.\nZero-shot cross-lingual: In these experiments we perform evaluation of Meta4XNLI in the opposite language to that utilised for training. Results are reported in Table 10  ###reference_###. XLM-RoBERTa performance exceeds that of mDeBERTa in both languages. Nonetheless, F1 score for EN is almost 20 points lower than ES evaluation results. In addition to the differences in annotation criteria between languages and datasets, another aspect to bear in mind in this scenario could be the number of positive examples present in training sets. As we explained in Section 3.3  ###reference_###, Meta4XNLIEN contains a higher number of metaphorical instances, thus models are exposed to a greater variety of examples that can be transferred to ES. While a more reduced amount of instances in Meta4XNLIES seen during training might hinder model\u2019s generalization ability, as the low recall and high precision scores show in Table 19  ###reference_### in Appendix B  ###reference_###.\nIn the first setup, we fine-tuned the models for the NLI task with MultiNLI dataset Williams, Nangia, and Bowman (2018  ###reference_b82###). Then we conducted the evaluation with two different splits for each source dataset conforming Meta4XNLI: pairs with at least one metaphorical expression and pairs lacking metaphors. From accuracy scores reported in Table 11  ###reference_### we can observe certain variability in the results. In the majority of cases, XLM-RoBERTa achieves better performance than mDeBERTa. We do observe a tendency of higher results on the sets of pairs without metaphors than on the set with metaphors. The exceptions to this current are the results from XLM-RoBERTa for XNLIdev and XNLItest in ES. In these partitions, the subset of pairs with metaphorical expressions obtained better results, although the difference does not even reach one point. We hypothesize the original language of the dataset might be involved in this performance, since XNLI includes natural utterances of EN that were afterwards manually translated to ES, thus some artifacts might have been introduced during this process Artetxe, Labaka, and Agirre (2020  ###reference_b5###).\nThe second scenario consisted in fine-tuning the models on two setups: a) with pairs without metaphors and b) pairs with and without metaphors. We performed the evaluation on subsets split by the criterion of metaphor occurrence as well. Results reported in Table 12  ###reference_### show XLM-RoBERTa outperforming mDeBERTa in all contexts. Both models show best results for the NLI task on the examples without metaphors and the lowest performance with pairs that contain metaphorical expressions. This outcome replicates in both languages and experimental setups a) and b).\nIn this section, we manually inspected a subset of erroneous cases in order to provide a qualitative insight of results and with the intention of finding potential explanations of errors and models performance for both detection and interpretation tasks.\nWe selected the predictions from the model that obtained highest F1 score from the monolingual experiments in ES for both Meta4XNLI and CoMeta evaluations. We extracted false negatives and false positives and grouped the tokens by their number of occurrences. Within the false positives of CoMeta test set, we find tokens like\napoyo (lit. \u201csupport\u201d) or tensi\u00f3n (lit. \u201ctension\u201d) that appear more frequently used figuratively than with their literal sense or even only appear in the training set labeled as metaphor. Other wrong predictions are words that appeared in specific domains, such as texts that allude to the pandemic, e.g. ola (lit. \u201cwave\u201d), not detected as metaphorical due to its absence with metaphorical meaning in Meta4XNLI sentences. Something similar occurs with the misclassified tokens from Meta4XNLI test set. Most errors stem from conventional metaphors, namely gran, abrir, paso, claro (lit. \u201cgreat\u201d, \u201cto open\u201d, \u201cstep\u201d, \u201cclear\u201d) that occur regularly used with their metaphorical meaning. The lack of balanced examples might contribute to these predictions. However, we maintained the distribution as is, since our aim is to study the presence and prevalence of metaphor in natural language utterances.\nWe analysed a subset of 30 errors from each experimental setup, both EN and ES, and evaluation sets. We chose the predictions from XLM-RoBERTa since it is the model that performs better in this task in most scenarios. Although results show that models struggle more to identify the inference relation if there is a metaphorical expression involved in the pair sentences, we do not observe any particular feature within the errors. A remarkable aspect to highlight is the lower results of esXNLI with respect to XNLIdev and XNLItest in Table 11  ###reference_###. It could be motivated by the difference of the text domains, since XNLI is an extension of MultiNLI, which maintains the same set of textual domains. While esXNLI is a collection of texts from another set of genres and sources.\nRegarding EN, some of the errors might derive from the misclassification of some pairs, since annotations were developed on ES text. A metaphorical expression involved in the inference relationship in ES might not be present in its EN version and vice versa. Thus, samples from these two classes, pairs with and without metaphors, should be reexamined in EN and correctly classified for future experimentation.\nIn this work, we provided Meta4XNLI, a cross-lingual parallel dataset in ES and EN labeled for metaphor detection and interpretation framed within the task of NLI. With this new resource that contains parallel text and annotations, we developed a series of experiments to assess the capabilities of MLMs when dealing with this kind of figurative expressions present in natural language utterances.\nRegarding the task of metaphor detection, after the annotation process and experiments results, we can conclude the importance of establishing a unified criterion for annotation that is valid for different languages if the aim is to continue researching cross-lingual approaches. In addition, the semi-automatic process of annotation followed for EN shows that automatic labeling of cross-lingual metaphor is far from trivial. Metaphorical expressions are language and culture dependant. Moreover, the translation of the data introduces a new layer in which metaphorical expressions can either be lost from the translation of the source to the target language, or can be introduced in the target language by means of the translator, either human or automatic. Further work to explore automatic annotation methodologies would be of considerable value in order to reduce the demanding workload and effort of manual labelling in more than one language.\nWith respect to results, the purpose of our experiments is not to overcome the state-of-the-art results, but to evaluate models performance from a cross-lingual approach. Best results are obtained when Meta4XNLI in both languages is used for training. The augmentation of the training set size and the parallel annotations might boost this performance. Cross-domain and monolingual experiments show how the lack of consistency in the annotation criteria affects the performance of models. This can be observed as well in the zero-shot cross-lingual setup, although the scenario of training in EN and evaluating in ES shows competitive performance. It should be noted that the EN set contains a larger number of instances annotated as metaphorical in the training set. In addition, the in-vocabulary and out-of-vocabulary evaluation points to some kind of bias in the learning process. This could stem from the fact that the majority of the metaphor instances are conventional or due to lexical memorization Levy et al. (2015  ###reference_b39###); Boisson, Espinosa-Anke, and\nCamacho-Collados (2023  ###reference_b13###). Future research on this line of work should be carried out to clarify this issue.\nFor metaphor interpretation, we evaluated the ability of MLMs to understand metaphorical expressions framed within the task of NLI. We provide parallel annotations at premise-hypothesis pair level that mark if the presence of metaphorical expressions is relevant for the inference relationship. We exploited this information to conduct our experiments. From reported results, we can recognize a tendency of the models to show a lower performance with pairs that contain at least one metaphorical expression. However, this trend breaks with XLM-RoBERTa results when fine-tuned in ES and evaluated on the XNLIdev and XNLItest in the same language. Since these sets are sentences translated from EN, we presume the translation process might induce biases in metaphor occurrence and the \u201cnaturalness\u201d of the sentences. Similar to metaphor detection, future work to analyse the impact of translation in the development of metaphor parallel resources should be explored for the task of metaphor interpretation, as well as additional experimentation from a multilingual perspective.\nMetaphor annotation is an inherently subjective task. This variance in annotations is reflected on Meta4XNLIEN, due to the different criterion employed through the annotation process. Labels in this language should be updated and further revised to improve their quality. Disagreement and subjectivity could be counterbalanced by a larger number of annotators, in order to develop more consistent and reliable labeled data, however it requires a huge amount of time and workload. Data augmentation and semi-automatic methods could be exploited to create larger datasets with similar characteristics to the one we present and extend it to more languages, since most corpora available for metaphor processing is of reduced size and limited to a narrow set of languages. The existence of parallel resources in multiple languages others than EN that reflect cultural and real world knowledge nuances is of great importance to continue researching such a complex phenomenon as figurative language, specifically metaphors."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Description",
            "text": "Meta4XNLI is a compilation of XNLI Conneau et al. (2018  ###reference_b23###) and esXNLI Artetxe, Labaka, and Agirre (2020  ###reference_b5###). We decided to exploit these resources since we evaluate metaphor interpretation through NLI and these datasets were originally developed for this task. This enables both annotations at token level for detection and pair level for interpretation. In addition, it contains parallel text, from which we select data in ES and EN. Moreover, the combination of XNLI and esXNLI constitutes a dataset of large size compared to common available resources for metaphor processing and with natural language utterances and spontaneous usage of metaphors. The distribution of Meta4XNLI is detailed in Table 2  ###reference_###.\nXNLI dataset is a cross-lingual evaluation set for MultiNLI Williams, Nangia, and Bowman (2018  ###reference_b82###). It contains parallel data with original text in EN subsequently human-translated to other 14 languages, among which we exploited only EN and ES. It comprehends a total of 7500 premise-hypothesis pairs from 10 text genres, that is, 830 premises and 2490 hypotheses from XNLIdev, and 1670 premises and 5010 hypotheses from XNLItest. esXNLI comprises a total of 2490 pairs, as in XNLIdev, from 5 different genres. In this case, sentences were originally collected in ES and then human-translated to EN. The direction of translation (EN > ES in XNLI and ES > EN in esXNLI) is an interesting feature to explore how translation may affect metaphor cross-lingual transfer and whether it impacts models performance.\nThe collection methodology is shared between XNLI and esXNLI: a set of premise sentences were crawled from various sources in EN and ES respectively. Afterwards, crowd-source workers were asked to generate three hypotheses for each premise, one for each label. Both corpora are balanced in terms of inference tags and text domains."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Annotation Process",
            "text": "The methodology to label Meta4XNLI varies across tasks and languages. In the following subsections, we will delve into the annotation processes used for detection and interpretation in each language. First, we developed ES annotations, from which we transferred a subset of detection labels and interpretation annotations in EN. Manual labeling was developed by a linguist native speaker of Spanish with advanced knowledge of English.\nThe annotation process in this language comprehends two phases, as depicted in Figure 2  ###reference_###  ###reference_###. First, we automatically label Meta4XNLIES by leveraging mDeBERTa He, Gao, and Chen (2021  ###reference_b29###  ###reference_b29###) fine-tuned for metaphor detection in ES with CoMeta. We choose mDeBERTa since it is the multilingual model that achieved highest F1 score in the experimental setup of Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###). This choice aims at reducing the heavy work load and time investment that manual annotation from scratch requires. Afterwards, we manually inspect and correct the predictions in the whole dataset. From the first phase of automatic labeling, 748 tokens were predicted as metaphor in the premises and 724 in the hypotheses. After a complete and manual revision of all sentences, we removed 74 tokens and added 481 undetected metaphors in the premises; whereas in the hypotheses, we deleted 118 false positives and labeled 533 false negatives.\n###figure_3### The main sources of ambiguity in ES emerge from multi-word expressions (MWE) and polysemy.\nThe main issue when labeling MWE is to decide whether to treat the MWE as a single lexical unit, a fixed expression where the meaning of each word is not transparent anymore, or if it should be regarded as a collocation. Collocations are expressions where the constituent words tend to co-occur with high frequency but are not fixed, since each of its elements can be replaced by others with similar meaning.\nDijo que era hora de entrar en p\u00e1nico (lit. \u201cThey said it was time to enter into panic\u201d).\nFor instance, in Example 2  ###reference_###  ###reference_###, the expression entrar en p\u00e1nico could be initially considered as a MWE with a single lexical unit, therefore, the three tokens would be labeled as metaphorical. However, this expression specifically means \u201cto panic\u201d, in which the verb entrar (lit. \u201cto enter\u201d) holds metaphorical meaning, since \u201cpanic\u201d is not a physical place you can get into. In this case, we do not think of the expression as a fixed MWE, since the verb is also used metaphorically with other terms that are not places, like entrar en c\u00f3lera (\u201cto get angry\u201d, lit. \u201cto enter into wrath\u201d) or entrar en calor (\u201cto feel hot\u201d, lit. \u201cto enter into heat\u201d). In all of these expressions the verb is conveying the sense of starting to feel the noun it complements, as if by entering to a place it transformed our sensitivity. The association of concepts arises from understanding emotions or sensations as physical locations.\nRegarding polysemy, the existence of multiple and very nuanced senses associated to the same token can lead to confusion. It can be challenging to determine whether the basic meaning of the lexical unit is currently and generally known and used by native speakers or if they directly associate the lexical unit to the figurative meaning, not identifying the basic meaning at all.\n[\u2026] ha mostrado su apoyo a la candidatura para ser sede [\u2026] (lit. \u201cThey showed their support to the candidacy to be head office\u201d).\nFor instance, in Example 2  ###reference_###  ###reference_### we label apoyo as metaphorical, since the most basic meaning of the verb apoyar in the dictionary defines it as \u201cto make something rest upon another thing\u201d. In this sentence, the contextual meaning refers to someone in favour of someone else\u2019s goal. Figuratively, the goal can be understood in terms of a physical object so heavy that requires more than one anchor point to distribute its weight. Doubts in this examples come from the fact that the figurative sense can be more used that its basic one, thus speakers might not identify the metaphorical meaning as such. We make use of the Diccionario de la Real Academia Espa\u00f1ola (DRAE) as a tool to help us clarify these ambiguous cases. Nonetheless, metaphor identification remains a highly subjective task.\nWe develop Meta4XNLIEN annotations semi-automatically and based on ES annotations, since our purpose is to publish a parallel resource to counterbalance monolingual English labeled data. These annotations serve as a starting point for further refinement and enable potential analysis of metaphors shared across these two languages. The whole process comprises mainly four phases, as depicted in Figure 3  ###reference_###  ###reference_###.\nThe first step involves the projection of ES labels onto EN sentences. For this end, we utilised Easy-Label-Projection Garc\u00eda-Ferrero, Agerri, and\nRigau (2022  ###reference_b28###  ###reference_b28###), a tool developed for cross-lingual sequence labeling that makes use of word alignments and data- and model-transfer to project the labels from a source language (ES) to an untagged target language (EN). This mechanism is suitable when the labeled entity is certainly appearing in both source and target sentences, however, metaphors present in a source sentence are not necessarily in its translation. It depends on a series of multiple factors, namely the type of translation, the knowledge of the translator, if it is human-translated, or socio-cultural knowledge, among others. The next step comprises the manual revision of the projections.\nIn order to alleviate this issue, we made use of XLM-RoBERTa Conneau et al. (2020  ###reference_b22###  ###reference_b22###) trained on the VUAM dataset, as it is the multilingual model that showed best performance in the experiments in EN from the work of Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###). Since projected sentences were manually reviewed (13% of the total), we only predicted automatically those sentences that did not receive a projection in the first place (87%). We manually reviewed the outcoming predictions from the MLM to correct errors and undetected metaphorical expressions following the MIPVU procedure. The number of metaphorical tokens extracted in each annotation phase is detailed in Table 3  ###reference_###  ###reference_###.\nThere are some issues that emerged during the whole annotation process. Firstly, the projections phase entails that for a metaphorical expression to be annotated in EN, it has to have been annotated in ES in advance. Hence, metaphors in EN sentences that were not expressed figuratively in their ES counterpart will not be spotted, as in Example 3  ###reference_###  ###reference_###. In the ES sentence there is not a metaphorical expression annotated as such, however, in the EN version, the adjective heavy is holding metaphorical meaning, as a synonym of \u201cdemanding\u201d, which is expressed literally in ES with the adjective exigentes. In this kind of cases where the metaphor is \u201cgained in translation\u201d, the lack of annotation in the source language implies no label will be projected onto the target sentence, missing a metaphorical instance. These examples give an account of how translation and the singularities of metaphors according to languages may affect to the annotation process of this task.\n###figure_4### .\nA los usuarios m\u00e1s exigentes se les deber\u00eda cobrar m\u00e1s. \nThe heavy users should be charged the most.\nAnother issue that should be noted are false positives: all ES sentences with one or more labeled metaphors will transfer those tags to the EN sentence, regardless the translated tokens hold metaphoric meaning or not. To solve this question, we manually reviewed all sentences that received a projected tag. In this revision we had to remove metaphors that were \u201clost in translation\u201d and adjust some spans projected to the target language, e.g. some annotated verbs in ES were projected in EN to the subject pronoun and verb, since some ES verbs forms are synthetic and include the person information in a single morpheme. Therefore, we eliminated the label from the pronoun and maintained only that of the verb. In Example 3  ###reference_###  ###reference_### we can see how the verb peleaban (lit. \u201cthey fought\u201d) labeled as metaphorical in the ES sentence (Example 3  ###reference_###  ###reference_###) was projected to the subject and verb in EN (Example 3  ###reference_###  ###reference_###). Example 3  ###reference_###  ###reference_### represents the definitive version of the annotations after manual revision.\nPeleaban por lo ricos que eran los directores ejecutivos.\nThey fought about how rich CEOs were.\nThey fought about how rich CEOs were.\nRegarding the subset of sentences that were automatically labeled by XLM-RoBERTa, some concerns emerged with respect to annotations present in VUAM dataset during the phase of manual revision. Especially when it comes to phrasal verbs and abstract and polysemous terms that are lexicalised. For instance, due to Examples like 3  ###reference_###  ###reference_### or 3  ###reference_###  ###reference_### labeled as metaphorical in the training set VUAM, we observed a tendency to overannotate verbs that conform phrasal verbs as metaphorical, such as get, look, made or go. In most cases, these verbs appear in a context where they do not add any strong semantic information due to their lexicalization. Hence, we unmarked these instances that were predicted as metaphorical. Following this line of thought, we also discarded abstract and vague terms that are common-places of spontaneous discourse, like the word thing in Example 3  ###reference_###  ###reference_###, since it can refer to any kind of entity, either concrete or abstract and might not be directly matched to a more broadly used basic meaning.\nHis lack of humbug about political balance has always made him more honest than all the employees [\u2026].\n\\enumsentenceTake what you want and leave the rest , your mother \u2019ll get rid of it . \n\\enumsentenceOne thing always linked to another thing."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Detection",
            "text": "Annotations for this task are developed at token level, since we approach metaphor detection as a sequence labeling task. We extract premises and hypotheses sentences and annotate them separately. Therefore, we do not take into account the premise as context to annotate its corresponding hypotheses and vice versa. Due to this split, the total number of labeled sentences amounts to 13320. With respect to the type of metaphors, we consider as candidates the tokens belonging to a semantically significant part of speech (POS): nouns, verbs, adjectives and adverbs.\n\nWe adopt the MIPVU guidelines Steen et al. (2010  ###reference_b72###) throughout the whole annotation process, either manual revision or in automatic predictions, since models used were trained on data labeled accordingly to this procedure. It can be summarised in four main steps: Read the entire text\u2013discourse to establish a general understanding of the meaning. Determine the lexical units in the text\u2013discourse. For each lexical unit in the text, establish its meaning in context, that is, how it applies to an entity, relation, or attribute in the situation evoked by the text (contextual meaning). Take into account what comes before and after the lexical unit. For each lexical unit, determine if it has a more basic contemporary meaning in other contexts than the one in the given context. For our purposes, basic meanings tend to be: More concrete; what they evoke is easier to imagine, see, hear, feel, smell, and taste. Related to bodily action. More precise (as opposed to vague). Historically older. Basic meanings are not necessarily the most frequent meanings of the lexical unit. If the lexical unit has a more basic current\u2013contemporary meaning in other contexts than the given context, decide whether the contextual meaning contrasts with the basic meaning but can be understood in comparison with it. If yes, mark the lexical unit as metaphorical.\n\nThe main sources of ambiguity in ES emerge from multi-word expressions (MWE) and polysemy. The main issue when labeling MWE is to decide whether to treat the MWE as a single lexical unit, a fixed expression where the meaning of each word is not transparent anymore, or if it should be regarded as a collocation. Collocations are expressions where the constituent words tend to co-occur with high frequency but are not fixed, since each of its elements can be replaced by others with similar meaning. Dijo que era hora de entrar en p\u00e1nico (lit. \u201cThey said it was time to enter into panic\u201d). For instance, in Example 2  ###reference_###  ###reference_###  ###reference_###, the expression entrar en p\u00e1nico could be initially considered as a MWE with a single lexical unit, therefore, the three tokens would be labeled as metaphorical. However, this expression specifically means \u201cto panic\u201d, in which the verb entrar (lit. \u201cto enter\u201d) holds metaphorical meaning, since \u201cpanic\u201d is not a physical place you can get into. In this case, we do not think of the expression as a fixed MWE, since the verb is also used metaphorically with other terms that are not places, like entrar en c\u00f3lera (\u201cto get angry\u201d, lit. \u201cto enter into wrath\u201d) or entrar en calor (\u201cto feel hot\u201d, lit. \u201cto enter into heat\u201d). In all of these expressions the verb is conveying the sense of starting to feel the noun it complements, as if by entering to a place it transformed our sensitivity. The association of concepts arises from understanding emotions or sensations as physical locations.\n\nRegarding polysemy, the existence of multiple and very nuanced senses associated to the same token can lead to confusion. It can be challenging to determine whether the basic meaning of the lexical unit is currently and generally known and used by native speakers or if they directly associate the lexical unit to the figurative meaning, not identifying the basic meaning at all. [\u2026] ha mostrado su apoyo a la candidatura para ser sede [\u2026] (lit. \u201cThey showed their support to the candidacy to be head office\u201d). For instance, in Example 2  ###reference_###  ###reference_###  ###reference_### we label apoyo as metaphorical, since the most basic meaning of the verb apoyar in the dictionary defines it as \u201cto make something rest upon another thing\u201d. In this sentence, the contextual meaning refers to someone in favour of someone else\u2019s goal. Figuratively, the goal can be understood in terms of a physical object so heavy that requires more than one anchor point to distribute its weight. Doubts in this examples come from the fact that the figurative sense can be more used that its basic one, thus speakers might not identify the metaphorical meaning as such. We make use of the Diccionario de la Real Academia Espa\u00f1ola (DRAE) as a tool to help us clarify these ambiguous cases. Nonetheless, metaphor identification remains a highly subjective task.\n\nWe develop Meta4XNLIEN annotations semi-autom"
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Interpretation",
            "text": "Similar to other works cited in Section 2  ###reference_###, we framed metaphor interpretation within the task of NLI Agerri (2008  ###reference_b2###); Mohler, Tomlinson, and Bracewell (2013  ###reference_b50###); Chakrabarty et al. (2021  ###reference_b15###); Stowe, Utama, and Gurevych (2022  ###reference_b74###); Kabra et al. (2023  ###reference_b31###). Our approach aims at evaluating whether MLMs struggle to identify the relationship of inference when there is a metaphorical expression either in the premise, hypothesis or both sentences. To do so, we labeled premise-hypothesis pairs with metaphorical expressions where the understanding of the figurative expression is crucial to determine the inference label. We summarised this annotation process in the following steps:\nRead the premise sentence and determine its general meaning.\nIdentify potential metaphorical expressions according to metaphor detection guidelines.\nRepeat the previous two instructions with the hypothesis sentence.\nEstablish the inference relation betweeen premise and hypothesis if not previously labeled.\nIf there is any metaphorical expression either in premise or hypothesis:\nIs it required to understand the literal meaning of the metaphorical expression to label the inference relationship between premise and hypothesis?\nYes: mark the pair.\nNo: mark the pair as an non-relevant case.\nRepeat the process with non-relevant cases until clarification. Otherwise, if they are intrinsically ambiguous or lack context to either identify the metaphors or determine if they are relevant to the inference, discard the pair.\nIn Example 3.2.2  ###reference_.SSS2###, we encounter the metaphor saltar (lit. \u201cto skip\u201d) with the meaning of omitting some information and not the literal sense of physically jumping. In the hypothesis, the sentence refers to the intention of telling the other interlocutor all the information, comprehensively, without ignoring any part, which contradicts the overall meaning of the premise. The understanding of this metaphorical expression, thus, is required to infer that they contradict each other.\nPremise: Hay tanto que se puede decir sobre eso, que sencillamente me voy a saltar eso. (lit. \u201cThere is so much you can say about that, that I am simply going to skip that\u201d).\nHypothesis: \u00a1Quiero contarte todo lo que s\u00e9 sobre eso! (lit. \u201cI want to tell you everything I know about it!\u201d). \nInference label: contradiction\nPremise: No hay necesidad de hurgar en ese tema, a menos que quieras asegurarte de que nos hundimos. (lit. \u201cThere is no need to rummage in that topic, unless you want to make sure we will sink\u201d).\nHypothesis: Hay una forma de que se hundan. (lit. \u201cThere is one way to make them sink\u201d). \nInference label: entailment\nNon-relevant cases comprehend premise-hypothesis pairs where the understanding of the literal sense of the metaphor is not essential to establish a relationship of entailment, contradiction or neutral. As we can see in Example 3.2.2  ###reference_.SSS2###, the metaphorical expression hurgar (lit. \u201cto rummage\u201d) is used with a sense of exploring an unpleasant topic, while the literal meaning implies to physically dig into an inner space. The entailment in this example is inferred from the likelihood of a sinking, not focusing on the willingness to talk about the mentioned topic. Thus, the interpretation of the metaphorical expression is not relevant to extract the inference relation.\nWe set aside the latter group as non-relevant cases, since we are not certain as to which extent the role of metaphor is significant in these occurrences. As a result, we discriminate three classes: a) pairs with metaphors that are relevant to the inference relationship, b) pairs with metaphors that are not relevant to the inference and c) pairs without metaphors.\nAnnotations were manually developed on ES text and were transferred to EN. Hence, we provide Meta4XNLIEN annotations as a silver standard for further refinement. Data about the number of samples and labels will be resumed in the following Subsection 3.3  ###reference_###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Resulting Dataset",
            "text": "The outcome of this annotation process is Meta4XNLI, the first parallel dataset with labels for the tasks of metaphor detection and interpretation via NLI in ES and EN.\nThe parallel data for this task comprises a total of 13320 sentences annotated at token level, since we approached metaphor detection as a sequence labeling task, following the criteria of the cited previous work in Section 2  ###reference_###  ###reference_###.\nWith respect to Spanish annotations, there is a total of 1155 metaphorical tokens in premises and 1139 in hypotheses. Out of the 13320 sentences, 1873 contain at least one metaphorical expression, which constitutes the 14% of the whole dataset. This information is detailed in Table 4  ###reference_###  ###reference_### according to the source datasets partitions. A higher proportion of metaphors in premises can be noticed. This might be due to the fact that premises are of larger length than hypotheses, as indicated in Conneau et al. (2018  ###reference_b23###  ###reference_b23###). In addition, premises were collected from existing utterances, while hypotheses were generated from crowd-source workers in response to a given set of premises. Therefore, hypotheses sentences might tend to present shorter length and lower complexity.\n###table_4### Regarding English annotations, we can observe a similar trend to that of ES in Table 5  ###reference_###  ###reference_###. A total of 3330 tokens were labeled as metaphor and 2736 sentences contain at least one metaphorical instance out of the total 13320 sentences. Premises show a higher metaphor ratio than hypotheses as in ES annotations. Additionally, we observe a larger amount of labeled metaphors in EN, caused by the different annotations processes specified in Subsection 3.2  ###reference_###  ###reference_###. Since VUAM contains a significantly higher amount of labeled metaphorical expressions, the MLM fine-tuned with this dataset predicted many ambiguous metaphors, which we subsequently removed in manual revision. These discrepancies are not only noticeable in annotations but also in experiments results. Moreover, this gives an account of how guidelines for metaphor identification labeling are open to discussion and clarification, due to the subjective and nuanced nature of this cognitive-linguistic phenomenon.\n###table_5### ###table_6### Annotations for this task were developed at premise-hypothesis level. As shown in Table 6  ###reference_###  ###reference_###, the average percentage of pairs with metaphors relevant to the inference relationship is 12%. This figure remains steady throughout each source dataset and inference labels. esXNLI shows a higher number of metaphor occurrence that might be caused by the difference of text domains and sentence characteristics with respect to XNLI data. Regarding non-relevant cases, we do not exploit them in the experiments in order to be able to analyse more clearly whether metaphor presence impacts models performance. We keep the same sample distribution for both languages to develop the experiments."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Inter-annotator Agreement",
            "text": "We selected a subset of 1000 pairs to the assess annotation quality in ES. The subset was labeled by another Spanish native speaker with a linguistics background. This annotator labeled the dataset subset from scratch and according to MIPVU guidelines in the case of detection, and the annotation process specified in Subsection 3.2  ###reference_### for interpretation. We computed Cohen\u2019s Kappa score Cohen (1960  ###reference_b20###) and obtained 0.74 in premises and 0.77 in hypotheses sentences for detection; for interpretation via the NLI task the obtained score was 0.64. Thus, these scores show a substantial agreement and are comparable to those obtained in the annotation of other datasets for metaphor detection and/or interpretation Steen et al. (2010  ###reference_b72###); Sanchez-Bayona and\nAgerri (2022  ###reference_b61###); Maudslay and Teufel (2022  ###reference_b47###); Badathala et al. (2023  ###reference_b7###) thereby demonstrating the consistency of our annotations. Still, annotating metaphorical language remains a rather subjective task that requires exhaustive work and precise guidelines.\nWith the aim of developing cross-lingual experiments, we chose the multilingual language models and checkpoints that obtained best results in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###): mDeBERTa (base) and XLM-RoBERTa (large) Conneau et al. (2020  ###reference_b22###), the multilingual versions of DeBERTa He, Gao, and Chen (2021  ###reference_b29###) and RoBERTa Liu et al. (2019  ###reference_b44###) respectively.\nTaking advantage of previous available resources and the corpus we present in this work, we conducted a series of experiments to evaluate and fine-tune MLMs.\nThe configuration of cross-domain experiments is specified in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###). For the other three setups, monolingual, multilingual and zero-shot cross-lingual, we performed hyperparameter tuning for batch size (8, 16, 36), linear decay (0.1, 0.01), learning rate (in the [1e-5-5e-5] interval), sequence length of 128 and epochs from 4 to 10. A warm-up of 6% is specified. The results of the hyperparameter tuning showed that after 4 epochs development loss started to increase, so results reported here are obtained with 4 epochs, batch size of 8, weight decay of 0.1 and learning rate of 5e-5.\nCross-domain: the aim of this set of experiments is to evaluate the performance of MLMs fine-tuned with CoMeta and VUAM datasets on Meta4XNLIES and Meta4XNLIEN, respectively, since each dataset contains texts from different domains. The motivation is to explore the impact of text features and genres on the performance, as well as annotation criteria Aghazadeh, Fayyaz, and\nYaghoobzadeh (2022  ###reference_b3###  ###reference_b3###); Lai, Toral, and Nissim (2023  ###reference_b33###  ###reference_b33###). To do so, we chose the models with best performance from monolingual experiments developed in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###). We conducted the evaluation on various data splits: within each source dataset, XNLIdev, XNLItest and esXNLI, we evaluated premises and hypotheses separated and combined. This is due to the dissimilarities and the unequal distribution of metaphorical expressions between premises and hypotheses sentences mentioned in Subsection 3.3  ###reference_###  ###reference_###.\nMonolingual: this scenario comprises the fine-tuning and evaluation of the MLM on Meta4XNLIES and Meta4XNLIEN separately. To accomplish this task, we split Meta4XNLI into train, development and test sets (0.6-0.2-0.2). We equally distributed the data to ensure each partition is balanced in terms of source datasets sentences and metaphor occurrence. Data statistics are detailed in Appendix A  ###reference_###  ###reference_###. These partitions will be used as well in subsequent multilingual and zero-shot cross-lingual scenarios. In addition to fine-tuning and evaluation on Meta4XNLIES and Meta4XNLIEN, we evaluated each trained monolingual model with the test sets of CoMeta and VUAM, following the same reasoning as in cross-domain experiments.\nMultilingual: the purpose of these experiments is to explore whether MLMs benefit from being trained on data in multiple languages. In this case, we combined Meta4XNLIES and Meta4XNLIEN train splits to fine-tune the models. Subsequently, we evaluated the trained models on each language test set, in order to analyse the impact on the performance for each language. The data splits used correspond to those from monolingual experiments.\nZero-shot cross-lingual: in this scenario we explore to what extent MLMs are able to generalize knowledge and metaphor transfer between these two languages in question. Therefore, we fine-tune the models with Meta4XNLI data in one language and evaluate it on the test set of the other. Data partitions used for these experiments are the same as in monolingual an multilingual scenarios.\nWe carried out two sets of experiments to evaluate metaphor interpretation within the NLI task. We performed hyperparameter tuning, with the same range of parameters specified on the task of metaphor detection. We report best results obtained on the development set, after 4 epochs, batch size of 8, learning rate of 1e-5, weight decay of 0.1 and 512 sequence length.\nThe purpose of the first experiments is to examine if the presence of metaphorical expressions in premise-hypothesis pairs impacts the performance of models in the NLI task. To that end, we fine-tuned the MLMs for the task with the MultiNLI dataset. Then, we evaluated them with Meta4XNLI. Among each source dataset, we discriminated pairs with metaphors relevant to the inference relationship from those without metaphorical expressions. We developed the evaluation on each of these subsets and for each language separately, e.g. one evaluation on XNLIdev metaphor set and another on XNLIdev without metaphors set, in EN and then in ES.\nThe goal of the second set of experiments is to analyse the effect on models performance of not being \u201cexposed\u201d to instances with metaphorical expressions during training. With this aim in mind, we extracted pairs with and without metaphors from Meta4XNLI train, dev and set splits. On the first scenario, we fine-tuned the models only with pairs from the train set that did not contain any metaphorical expression. On the second scenario, we mixed pairs with and without metaphors and fine-tuned the models as well. In both cases, we evaluated on the test sets with and without metaphors for each language.\nIn addition to F1 score, we performed in-vocabluary (Inv) and out-of-vocabulary (Oov) evaluations to test the impact of the labels seen from training data during the learning process of the MLMs. Precision and Recall metrics are reported in Appendix B  ###reference_###  ###reference_###. For in-vocabulary evaluation, we computed F1 score taking into account predicted words seen in training labeled as metaphorical. While for out-of-vocabulary evaluation we considered only words not seen during training to compute the F1 score. We included this evaluation in all experiments but for the zero-shot cross-lingual setup, since the data of train and test sets are in different languages, thus the match of the exact same metaphorical token in both partitions is highly unlikely.\nAlthough the purpose of our experiments is not to beat state-of-the-art results but to evaluate the performance of MLMs on the task from a cross-lingual approach, we added two indicative baselines: on one hand, the system BasicBERT Li et al. (2023a  ###reference_b40###  ###reference_b40###), which obtained 73.3 F1 score; on the other, the result of DeBERTa reported in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###), with 73.79 F1. We selected these results for comparison purposes, since they both were evaluated on the VUAM-2020 version of the dataset in EN used in the Shared Task 2020 Leong et al. (2020  ###reference_b36###  ###reference_b36###) and in the same experimental setup we propose.\nCross-domain: We evaluated models trained on CoMeta and VUAM datasets with XNLI and esXNLI in ES and EN, respectively. From results reported in Table 7  ###reference_###  ###reference_###, we can observe that in all cases mDeBERTa outperforms XLM-RoBERTa for ES. In EN the best result is obtained by DeBERTa, which also outperforms XLM-RoBERTa in all scenarios. In all datasets, except for XNLIdev in ES, premises sentences achieve better results than hypotheses and the combination of both. This goes in line with annotation statistics in Tables 4  ###reference_###  ###reference_### and 5  ###reference_###  ###reference_###, which show that premises contain a greater ratio of metaphors per sentence than hypotheses in both languages. In ES, in-vocabulary evaluation outperforms the general F1 score while out-of-vocabulary evaluation results decrease. The small distance in points of this cross-domain evaluation in ES exhibit stability of the models when it comes to predicting metaphors and coherence in annotations between both datasets despite the difference of text domains. Nevertheless, EN results do not demonstrate such consistency, as out-of-vocabulary obtains higher results than the overall F1 score. This might be due to the different labeling criteria used in VUAM and Meta4XNLI, as we mentioned in 3.2  ###reference_###  ###reference_###. These discrepancies are also reflected in a significant drop in performance with respect to ES experiments and in-domain evaluation with VUAM. The high recall scores from Table 16  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_### show that the model tends to predict many metaphors, however, low precision scores indicate that a small amount of these predictions are correct.\n.\nMonolingual: Results from this set of experiments are specified in Table 8  ###reference_###  ###reference_###. After fine-tuning and evaluating the models with Meta4XNLIES and Meta4XNLIEN, the highest overall F1 score is obtained by mDeBERTa in ES. On the other hand, XLM-RoBERTa achieves better performance than mDeBERTa in EN but still lower than in ES. In both languages, in-vocabulary evaluation results are higher than the overall ones and out-of-vocabulary results, lower. This is not the case when we use VUAM corpus for testing the model fine-tuned with Meta4XNLIEN. In this setup, similarly to results from cross-domain experiments, performance drastically falls to 32.42 F1 score and out-of-vocabulary results are the highest. The reason behind might be the mismatches in the annotation process. In ES, when evaluating CoMeta, we also encounter a decrease in the performance, however, it is more subtle: from 67.17 to 56.24, obtained by XLM-RoBERTa. This might be caused by the variety of text genres and dissimilarities between sentences from each dataset.\nMultilingual: In this set of experiments we assembled Meta4XNLIEN and Meta4XNLIES to train the MLMs. The evaluation is conducted for each language separately and results detailed in Table 9  ###reference_###  ###reference_###. Best results in ES are obtained by mDeBERTa, which are higher than the top result from monolingual experiments. In EN, mDeBERTa is the model that achieves better performance but very close to that of XLM-RoBERTa. The highest F1 score is 8 points lower than that of ES but outweighs EN monolingual results. This suggests that the combination of parallel multilingual data for training is beneficial for the performance of the models.\nZero-shot cross-lingual: In these experiments we perform evaluation of Meta4XNLI in the opposite language to that utilised for training. Results are reported in Table 10  ###reference_###  ###reference_###. XLM-RoBERTa performance exceeds that of mDeBERTa in both languages. Nonetheless, F1 score for EN is almost 20 points lower than ES evaluation results. In addition to the differences in annotation criteria between languages and datasets, another aspect to bear in mind in this scenario could be the number of positive examples present in training sets. As we explained in Section 3.3  ###reference_###  ###reference_###, Meta4XNLIEN contains a higher number of metaphorical instances, thus models are exposed to a greater variety of examples that can be transferred to ES. While a more reduced amount of instances in Meta4XNLIES seen during training might hinder model\u2019s generalization ability, as the low recall and high precision scores show in Table 19  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###.\nIn the first setup, we fine-tuned the models for the NLI task with MultiNLI dataset Williams, Nangia, and Bowman (2018  ###reference_b82###  ###reference_b82###). Then we conducted the evaluation with two different splits for each source dataset conforming Meta4XNLI: pairs with at least one metaphorical expression and pairs lacking metaphors. From accuracy scores reported in Table 11  ###reference_###  ###reference_### we can observe certain variability in the results. In the majority of cases, XLM-RoBERTa achieves better performance than mDeBERTa. We do observe a tendency of higher results on the sets of pairs without metaphors than on the set with metaphors. The exceptions to this current are the results from XLM-RoBERTa for XNLIdev and XNLItest in ES. In these partitions, the subset of pairs with metaphorical expressions obtained better results, although the difference does not even reach one point. We hypothesize the original language of the dataset might be involved in this performance, since XNLI includes natural utterances of EN that were afterwards manually translated to ES, thus some artifacts might have been introduced during this process Artetxe, Labaka, and Agirre (2020  ###reference_b5###  ###reference_b5###).\nThe second scenario consisted in fine-tuning the models on two setups: a) with pairs without metaphors and b) pairs with and without metaphors. We performed the evaluation on subsets split by the criterion of metaphor occurrence as well. Results reported in Table 12  ###reference_###  ###reference_### show XLM-RoBERTa outperforming mDeBERTa in all contexts. Both models show best results for the NLI task on the examples without metaphors and the lowest performance with pairs that contain metaphorical expressions. This outcome replicates in both languages and experimental setups a) and b).\nWe selected the predictions from the model that obtained highest F1 score from the monolingual experiments in ES for both Meta4XNLI and CoMeta evaluations. We extracted false negatives and false positives and grouped the tokens by their number of occurrences. Within the false positives of CoMeta test set, we find tokens like\napoyo (lit. \u201csupport\u201d) or tensi\u00f3n (lit. \u201ctension\u201d) that appear more frequently used figuratively than with their literal sense or even only appear in the training set labeled as metaphor. Other wrong predictions are words that appeared in specific domains, such as texts that allude to the pandemic, e.g. ola (lit. \u201cwave\u201d), not detected as metaphorical due to its absence with metaphorical meaning in Meta4XNLI sentences. Something similar occurs with the misclassified tokens from Meta4XNLI test set. Most errors stem from conventional metaphors, namely gran, abrir, paso, claro (lit. \u201cgreat\u201d, \u201cto open\u201d, \u201cstep\u201d, \u201cclear\u201d) that occur regularly used with their metaphorical meaning. The lack of balanced examples might contribute to these predictions. However, we maintained the distribution as is, since our aim is to study the presence and prevalence of metaphor in natural language utterances.\nWe analysed a subset of 30 errors from each experimental setup, both EN and ES, and evaluation sets. We chose the predictions from XLM-RoBERTa since it is the model that performs better in this task in most scenarios. Although results show that models struggle more to identify the inference relation if there is a metaphorical expression involved in the pair sentences, we do not observe any particular feature within the errors. A remarkable aspect to highlight is the lower results of esXNLI with respect to XNLIdev and XNLItest in Table 11  ###reference_###  ###reference_###. It could be motivated by the difference of the text domains, since XNLI is an extension of MultiNLI, which maintains the same set of textual domains. While esXNLI is a collection of texts from another set of genres and sources.\nRegarding EN, some of the errors might derive from the misclassification of some pairs, since annotations were developed on ES text. A metaphorical expression involved in the inference relationship in ES might not be present in its EN version and vice versa. Thus, samples from these two classes, pairs with and without metaphors, should be reexamined in EN and correctly classified for future experimentation."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental setup",
            "text": "With the aim of developing cross-lingual experiments, we chose the multilingual language models and checkpoints that obtained best results in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###): mDeBERTa (base) and XLM-RoBERTa (large) Conneau et al. (2020  ###reference_b22###  ###reference_b22###), the multilingual versions of DeBERTa He, Gao, and Chen (2021  ###reference_b29###  ###reference_b29###) and RoBERTa Liu et al. (2019  ###reference_b44###  ###reference_b44###) respectively.\nTaking advantage of previous available resources and the corpus we present in this work, we conducted a series of experiments to evaluate and fine-tune MLMs.\nThe configuration of cross-domain experiments is specified in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###). For the other three setups, monolingual, multilingual and zero-shot cross-lingual, we performed hyperparameter tuning for batch size (8, 16, 36), linear decay (0.1, 0.01), learning rate (in the [1e-5-5e-5] interval), sequence length of 128 and epochs from 4 to 10. A warm-up of 6% is specified. The results of the hyperparameter tuning showed that after 4 epochs development loss started to increase, so results reported here are obtained with 4 epochs, batch size of 8, weight decay of 0.1 and learning rate of 5e-5.\nCross-domain: the aim of this set of experiments is to evaluate the performance of MLMs fine-tuned with CoMeta and VUAM datasets on Meta4XNLIES and Meta4XNLIEN, respectively, since each dataset contains texts from different domains. The motivation is to explore the impact of text features and genres on the performance, as well as annotation criteria Aghazadeh, Fayyaz, and\nYaghoobzadeh (2022  ###reference_b3###  ###reference_b3###  ###reference_b3###); Lai, Toral, and Nissim (2023  ###reference_b33###  ###reference_b33###  ###reference_b33###). To do so, we chose the models with best performance from monolingual experiments developed in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###). We conducted the evaluation on various data splits: within each source dataset, XNLIdev, XNLItest and esXNLI, we evaluated premises and hypotheses separated and combined. This is due to the dissimilarities and the unequal distribution of metaphorical expressions between premises and hypotheses sentences mentioned in Subsection 3.3  ###reference_###  ###reference_###  ###reference_###.\nMonolingual: this scenario comprises the fine-tuning and evaluation of the MLM on Meta4XNLIES and Meta4XNLIEN separately. To accomplish this task, we split Meta4XNLI into train, development and test sets (0.6-0.2-0.2). We equally distributed the data to ensure each partition is balanced in terms of source datasets sentences and metaphor occurrence. Data statistics are detailed in Appendix A  ###reference_###  ###reference_###  ###reference_###. These partitions will be used as well in subsequent multilingual and zero-shot cross-lingual scenarios. In addition to fine-tuning and evaluation on Meta4XNLIES and Meta4XNLIEN, we evaluated each trained monolingual model with the test sets of CoMeta and VUAM, following the same reasoning as in cross-domain experiments.\nMultilingual: the purpose of these experiments is to explore whether MLMs benefit from being trained on data in multiple languages. In this case, we combined Meta4XNLIES and Meta4XNLIEN train splits to fine-tune the models. Subsequently, we evaluated the trained models on each language test set, in order to analyse the impact on the performance for each language. The data splits used correspond to those from monolingual experiments.\nZero-shot cross-lingual: in this scenario we explore to what extent MLMs are able to generalize knowledge and metaphor transfer between these two languages in question. Therefore, we fine-tune the models with Meta4XNLI data in one language and evaluate it on the test set of the other. Data partitions used for these experiments are the same as in monolingual an multilingual scenarios.\nWe carried out two sets of experiments to evaluate metaphor interpretation within the NLI task. We performed hyperparameter tuning, with the same range of parameters specified on the task of metaphor detection. We report best results obtained on the development set, after 4 epochs, batch size of 8, learning rate of 1e-5, weight decay of 0.1 and 512 sequence length.\nThe purpose of the first experiments is to examine if the presence of metaphorical expressions in premise-hypothesis pairs impacts the performance of models in the NLI task. To that end, we fine-tuned the MLMs for the task with the MultiNLI dataset. Then, we evaluated them with Meta4XNLI. Among each source dataset, we discriminated pairs with metaphors relevant to the inference relationship from those without metaphorical expressions. We developed the evaluation on each of these subsets and for each language separately, e.g. one evaluation on XNLIdev metaphor set and another on XNLIdev without metaphors set, in EN and then in ES.\nThe goal of the second set of experiments is to analyse the effect on models performance of not being \u201cexposed\u201d to instances with metaphorical expressions during training. With this aim in mind, we extracted pairs with and without metaphors from Meta4XNLI train, dev and set splits. On the first scenario, we fine-tuned the models only with pairs from the train set that did not contain any metaphorical expression. On the second scenario, we mixed pairs with and without metaphors and fine-tuned the models as well. In both cases, we evaluated on the test sets with and without metaphors for each language.\nIn addition to F1 score, we performed in-vocabluary (Inv) and out-of-vocabulary (Oov) evaluations to test the impact of the labels seen from training data during the learning process of the MLMs. Precision and Recall metrics are reported in Appendix B  ###reference_###  ###reference_###  ###reference_###. For in-vocabulary evaluation, we computed F1 score taking into account predicted words seen in training labeled as metaphorical. While for out-of-vocabulary evaluation we considered only words not seen during training to compute the F1 score. We included this evaluation in all experiments but for the zero-shot cross-lingual setup, since the data of train and test sets are in different languages, thus the match of the exact same metaphorical token in both partitions is highly unlikely.\nAlthough the purpose of our experiments is not to beat state-of-the-art results but to evaluate the performance of MLMs on the task from a cross-lingual approach, we added two indicative baselines: on one hand, the system BasicBERT Li et al. (2023a  ###reference_b40###  ###reference_b40###  ###reference_b40###), which obtained 73.3 F1 score; on the other, the result of DeBERTa reported in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###), with 73.79 F1. We selected these results for comparison purposes, since they both were evaluated on the VUAM-2020 version of the dataset in EN used in the Shared Task 2020 Leong et al. (2020  ###reference_b36###  ###reference_b36###  ###reference_b36###) and in the same experimental setup we propose.\nCross-domain: We evaluated models trained on CoMeta and VUAM datasets with XNLI and esXNLI in ES and EN, respectively. From results reported in Table 7  ###reference_###  ###reference_###  ###reference_###, we can observe that in all cases mDeBERTa outperforms XLM-RoBERTa for ES. In EN the best result is obtained by DeBERTa, which also outperforms XLM-RoBERTa in all scenarios. In all datasets, except for XNLIdev in ES, premises sentences achieve better results than hypotheses and the combination of both. This goes in line with annotation statistics in Tables 4  ###reference_###  ###reference_###  ###reference_### and 5  ###reference_###  ###reference_###  ###reference_###, which show that premises contain a greater ratio of metaphors per sentence than hypotheses in both languages. In ES, in-vocabulary evaluation outperforms the general F1 score while out-of-vocabulary evaluation results decrease. The small distance in points of this cross-domain evaluation in ES exhibit stability of the models when it comes to predicting metaphors and coherence in annotations between both datasets despite the difference of text domains. Nevertheless, EN results do not demonstrate such consistency, as out-of-vocabulary obtains higher results than the overall F1 score. This might be due to the different labeling criteria used in VUAM and Meta4XNLI, as we mentioned in 3.2  ###reference_###  ###reference_###  ###reference_###. These discrepancies are also reflected in a significant drop in performance with respect to ES experiments and in-domain evaluation with VUAM. The high recall scores from Table 16  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_### show that the model tends to predict many metaphors, however, low precision scores indicate that a small amount of these predictions are correct.\n.\nMonolingual: Results from this set of experiments are specified in Table 8  ###reference_###  ###reference_###  ###reference_###. After fine-tuning and evaluating the models with Meta4XNLIES and Meta4XNLIEN, the highest overall F1 score is obtained by mDeBERTa in ES. On the other hand, XLM-RoBERTa achieves better performance than mDeBERTa in EN but still lower than in ES. In both languages, in-vocabulary evaluation results are higher than the overall ones and out-of-vocabulary results, lower. This is not the case when we use VUAM corpus for testing the model fine-tuned with Meta4XNLIEN. In this setup, similarly to results from cross-domain experiments, performance drastically falls to 32.42 F1 score and out-of-vocabulary results are the highest. The reason behind might be the mismatches in the annotation process. In ES, when evaluating CoMeta, we also encounter a decrease in the performance, however, it is more subtle: from 67.17 to 56.24, obtained by XLM-RoBERTa. This might be caused by the variety of text genres and dissimilarities between sentences from each dataset.\nMultilingual: In this set of experiments we assembled Meta4XNLIEN and Meta4XNLIES to train the MLMs. The evaluation is conducted for each language separately and results detailed in Table 9  ###reference_###  ###reference_###  ###reference_###. Best results in ES are obtained by mDeBERTa, which are higher than the top result from monolingual experiments. In EN, mDeBERTa is the model that achieves better performance but very close to that of XLM-RoBERTa. The highest F1 score is 8 points lower than that of ES but outweighs EN monolingual results. This suggests that the combination of parallel multilingual data for training is beneficial for the performance of the models.\nZero-shot cross-lingual: In these experiments we perform evaluation of Meta4XNLI in the opposite language to that utilised for training. Results are reported in Table 10  ###reference_###  ###reference_###  ###reference_###. XLM-RoBERTa performance exceeds that of mDeBERTa in both languages. Nonetheless, F1 score for EN is almost 20 points lower than ES evaluation results. In addition to the differences in annotation criteria between languages and datasets, another aspect to bear in mind in this scenario could be the number of positive examples present in training sets. As we explained in Section 3.3  ###reference_###  ###reference_###  ###reference_###, Meta4XNLIEN contains a higher number of metaphorical instances, thus models are exposed to a greater variety of examples that can be transferred to ES. While a more reduced amount of instances in Meta4XNLIES seen during training might hinder model\u2019s generalization ability, as the low recall and high precision scores show in Table 19  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###.\nIn the first setup, we fine-tuned the models for the NLI task with MultiNLI dataset Williams, Nangia, and Bowman (2018  ###reference_b82###  ###reference_b82###  ###reference_b82###). Then we conducted the evaluation with two different splits for each source dataset conforming Meta4XNLI: pairs with at least one metaphorical expression and pairs lacking metaphors. From accuracy scores reported in Table 11  ###reference_###  ###reference_###  ###reference_### we can observe certain variability in the results. In the majority of cases, XLM-RoBERTa achieves better performance than mDeBERTa. We do observe a tendency of higher results on the sets of pairs without metaphors than on the set with metaphors. The exceptions to this current are the results from XLM-RoBERTa for XNLIdev and XNLItest in ES. In these partitions, the subset of pairs with metaphorical expressions obtained better results, although the difference does not even reach one point. We hypothesize the original language of the dataset might be involved in this performance, since XNLI includes natural utterances of EN that were afterwards manually translated to ES, thus some artifacts might have been introduced during this process Artetxe, Labaka, and Agirre (2020  ###reference_b5###  ###reference_b5###  ###reference_b5###).\nThe second scenario consisted in fine-tuning the models on two setups: a) with pairs without metaphors and b) pairs with and without metaphors. We performed the evaluation on subsets split by the criterion of metaphor occurrence as well. Results reported in Table 12  ###reference_###  ###reference_###  ###reference_### show XLM-RoBERTa outperforming mDeBERTa in all contexts. Both models show best results for the NLI task on the examples without metaphors and the lowest performance with pairs that contain metaphorical expressions. This outcome replicates in both languages and experimental setups a) and b).\nWe selected the predictions from the model that obtained highest F1 score from the monolingual experiments in ES for both Meta4XNLI and CoMeta evaluations. We extracted false negatives and false positives and grouped the tokens by their number of occurrences. Within the false positives of CoMeta test set, we find tokens like\napoyo (lit. \u201csupport\u201d) or tensi\u00f3n (lit. \u201ctension\u201d) that appear more frequently used figuratively than with their literal sense or even only appear in the training set labeled as metaphor. Other wrong predictions are words that appeared in specific domains, such as texts that allude to the pandemic, e.g. ola (lit. \u201cwave\u201d), not detected as metaphorical due to its absence with metaphorical meaning in Meta4XNLI sentences. Something similar occurs with the misclassified tokens from Meta4XNLI test set. Most errors stem from conventional metaphors, namely gran, abrir, paso, claro (lit. \u201cgreat\u201d, \u201cto open\u201d, \u201cstep\u201d, \u201cclear\u201d) that occur regularly used with their metaphorical meaning. The lack of balanced examples might contribute to these predictions. However, we maintained the distribution as is, since our aim is to study the presence and prevalence of metaphor in natural language utterances.\nWe analysed a subset of 30 errors from each experimental setup, both EN and ES, and evaluation sets. We chose the predictions from XLM-RoBERTa since it is the model that performs better in this task in most scenarios. Although results show that models struggle more to identify the inference relation if there is a metaphorical expression involved in the pair sentences, we do not observe any particular feature within the errors. A remarkable aspect to highlight is the lower results of esXNLI with respect to XNLIdev and XNLItest in Table 11  ###reference_###  ###reference_###  ###reference_###. It could be motivated by the difference of the text domains, since XNLI is an extension of MultiNLI, which maintains the same set of textual domains. While esXNLI is a collection of texts from another set of genres and sources.\nRegarding EN, some of the errors might derive from the misclassification of some pairs, since annotations were developed on ES text. A metaphorical expression involved in the inference relationship in ES might not be present in its EN version and vice versa. Thus, samples from these two classes, pairs with and without metaphors, should be reexamined in EN and correctly classified for future experimentation."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "In this section, we will give an overview of most significant works that focused on metaphor processing. In the first place, we analyse metaphor detection and interpretation developed in EN, since most publications and datasets are English-centered. Afterwards, we will explore multi- and cross-lingual approaches for both tasks.\nInitially, most work for the task of metaphor detection was corpus-based Charteris-Black (2004  ###reference_b17###  ###reference_b17###); Semino (2017  ###reference_b65###  ###reference_b65###). However, the growing interest throughout the last years led to the celebration of FigLang shared tasks Leong, Beigman Klebanov, and\nShutova (2018  ###reference_b37###  ###reference_b37###); Leong et al. (2020  ###reference_b36###  ###reference_b36###), which promoted multiple approaches to address it as a sequence labeling task via deep learning techniques. The most used methodology consisted in training a model with specific features, either linguistics-related Stowe and Palmer (2018  ###reference_b73###  ###reference_b73###) or of other nature such as abstractness, visual or emotion-related information Tsvetkov et al. (2014  ###reference_b76###  ###reference_b76###); Bizzoni and Ghanimifard (2018  ###reference_b10###  ###reference_b10###); Tong, Shutova, and Lewis (2021  ###reference_b75###  ###reference_b75###); Neidlein, Wiesenbach, and\nMarkert (2020  ###reference_b52###  ###reference_b52###).\nThe arrival of Transformer-based models Devlin et al. (2019  ###reference_b25###  ###reference_b25###) led to an increase in the overall performance of the task. Most fine-tuned models are founded on linguistic theories, such as MIP (Steen et al., 2010  ###reference_b72###  ###reference_b72###) or Selectional Preference (SP) Wilks (1975  ###reference_b80###  ###reference_b80###); Percy (1958  ###reference_b54###  ###reference_b54###), which, generally speaking, address metaphor as a contrast between basic and contextual meaning. The MIP approach was extended to MIPVU to develop the referential corpus in EN: the VUAM dataset Steen et al. (2010  ###reference_b72###  ###reference_b72###), which covers texts of multiple domains and annotations at token level, and was used for the Shared Tasks along with the release of TOEFL dataset Leong et al. (2020  ###reference_b36###  ###reference_b36###). Other well-known datasets are TroFi Birke and Sarkar (2006  ###reference_b9###  ###reference_b9###), of considerable smaller size and restrained to verbs; or MOH-X dataset Mohammad, Shutova, and Turney (2016  ###reference_b48###  ###reference_b48###), also focusing on metaphorical and literal examples of verbs. Other available corpora covers texts from a single domain, such as tweets Zayed, McCrae, and Buitelaar (2019  ###reference_b83###  ###reference_b83###) or news headlines, in NewsMet dataset Joseph et al. (2023  ###reference_b30###  ###reference_b30###).\nThe combination of pre-trained language models and these available resources brought forth multiple models fine-tuned for metaphor detection with ad hoc architectures. For instance, Song et al. (2021  ###reference_b71###  ###reference_b71###) propose Mr-BERT, a model able to extract the grammatical and semantic relations of a metaphorical verb and its context. RoPPT Wang et al. (2023  ###reference_b79###  ###reference_b79###) takes into account dependency trees information to extract the terms most relevant to the target word. The purpose of other published models is to identify the metaphoric span of the sentence, namely MelBERT Choi et al. (2021  ###reference_b19###  ###reference_b19###), based on MIP and SP theories, as well as BasicBERT Li et al. (2023a  ###reference_b40###  ###reference_b40###). To alleviate scarcity of metaphor annotated data, CATE Lin et al. (2021  ###reference_b42###  ###reference_b42###) is a ContrAstive Pre-Trained ModEl that uses semi-supervised learning and self-training.\nOthers use additional linguistic resources besides datasets, like Babieno et al. (2022  ###reference_b6###  ###reference_b6###), that take advantage of Wiktionary definitions to build their MLM MIss RoBERTa WiLDe; FrameBERT Li et al. (2023b  ###reference_b41###  ###reference_b41###) makes use of FrameNet Fillmore, Baker, and Sato (2002  ###reference_b27###  ###reference_b27###) to extract the concept of the detected metaphor. MisNET Zhang and Liu (2022  ###reference_b85###  ###reference_b85###) exploits dictionary resources and is based on linguistic theories to predict metaphors at word level. The model of Wan et al. (2021  ###reference_b78###  ###reference_b78###) learns from glosses of the definition of the contextual meaning of metaphors. Maudslay and Teufel (2022  ###reference_b47###  ###reference_b47###) present what they call a Metaphorical Polysemy Detection model by exploiting WordNet and Word Sense Disambiguation (WSD) to perform the detection. Another approach is to frame metaphor detection within another NLP task: such as Zhang and Liu (2023  ###reference_b86###  ###reference_b86###), who adopt a multi-task learning framework where knowledge from WSD is leveraged to identify metaphors; Feng and Ma (2022  ###reference_b26###  ###reference_b26###) develop An Auto-Augmented Sturcture-aware generative model that approaches metaphor detection as a keywords-extraction task; and the work of Dankin, Bar, and Dershowitz (2022  ###reference_b24###  ###reference_b24###), that explores few-shot scenarios from a Yes-No Question-Answering perspective. Badathala et al. (2023  ###reference_b7###  ###reference_b7###) also propose a multi-task approach to detect metaphor and hyperbole altogether although at sentence level.\nAmong these examples, the state of the art on the task approached as sequence labelling and evaluated on VUA-20, our experimental setup, is the following: DeBERTa-large (73.79 F1) fine-tuned and evaluated on VUAM Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###), BasicBERT (73.3 F1), FrameBERT (73.0), RoPPT (72.8 F1) and MelBERT (72.3 F1). These scores show the complexity of the task and how there is still room for improvement to achieve competitive performance.\nThe evaluation of metaphor interpretation remains a difficult issue, thus most works frame it within other NLU tasks, namely paraphrasing or NLI.\nAmong the paraphrase current, we can find both supervised Shutova, Teufel, and Korhonen (2013  ###reference_b70###  ###reference_b70###); Shutova (2013  ###reference_b67###  ###reference_b67###) and unsupervised Shutova (2010  ###reference_b66###  ###reference_b66###); Shutova, Cruys, and Korhonen (2012  ###reference_b68###  ###reference_b68###) approaches. The work of Bollegala and Shutova (2013  ###reference_b14###  ###reference_b14###) explores the generation of literal paraphrases for metaphorical verbs unsupervisedly. Bizzoni and Lappin (2018  ###reference_b11###  ###reference_b11###) developed a corpus with sentences that contain metaphorical expressions and a set of literal paraphrases ranked according to their acceptability. They exploit their resource to test deep learning systems that approach metaphor interpretation as a classification and ranking task. Mao, Lin, and Guerin (2021  ###reference_b46###  ###reference_b46###) focused on the paraphrase of verb metaphors as well. They take advantage of MOH-X Mohammad, Shutova, and Turney (2016  ###reference_b48###  ###reference_b48###) and VUAM to test BERT\u2019s capability to generate the most probable literal substitute. Pedinotti et al. (2021  ###reference_b53###  ###reference_b53###) develop an evaluation dataset in EN with 300 instances that include conventional and novel metaphors, as well as literal and nonsense sentences. They exploit it to test BERT\u2019s ability to interpret metaphors and discriminate among the different types of sentences, in addition to examine how MLMs encode this knowledge.\nSome recent works address metaphor interpretation as a Question-Answering problem. They reformulate metaphor expressions as questions or prompts to test LLMs. Com\\textcommabelowsa, Eisenschlos, and\nNarayanan (2022  ###reference_b21###  ###reference_b21###) propose MiQA, a dataset of 300 items that gather literal and metaphorical premises, paired to implication sentences to evaluate LLMs\u2019 metaphors understanding by asking if the implications are true or false. Liu et al. (2022  ###reference_b43###  ###reference_b43###) develop Fig-QA dataset also for EN but of a considerable larger size. It comprises 10,256 instances of creative metaphors paired with their literal implication sentences. They exploit their resource to evaluate state-of-the-art models ability to understand metaphor, framed as an inference task. Nevertheless, these pairs does not consist of natural utterances, but human-generated examples that present a fixed sentence structure. In addition, premises and their implications hold lexical overlapping, which could induce to bias in the models evaluation results, e.g.: Money is a helpful stranger - > Money is good; Money is a murderer - > Money is bad.\nRakshit and Flanigan (2023  ###reference_b58###  ###reference_b58###) introduce FigurativeQA, which gathers 1000 yes/no questions in EN from the reviews domain that include metaphorical and literal context examples in addition to other figurative language phenomena, such as simile, hyperbole, idiom and sarcasm to probe models. Other works like Wachowiak and Gromann (2023  ###reference_b77###  ###reference_b77###); Pitarch, Bernad, and Gracia (2023  ###reference_b55###  ###reference_b55###) center their research on examining whether generative models understand conceptual metaphors and their reasoning capabilities, the former makes use of existing datasets in EN and ES and the latter proposes a new resource only in EN.\nA popular approach that continues being explored is the study of metaphor interpretation framed in the NLI or Recognizing Textual Entailment (RTE) task Agerri (2008  ###reference_b2###  ###reference_b2###); Mohler, Tomlinson, and Bracewell (2013  ###reference_b50###  ###reference_b50###). For example, Chakrabarty et al. (2021  ###reference_b15###  ###reference_b15###) propose 12500 instances in EN collected from already existing datasets for RTE, which cover simile, metaphor and irony. Out of the total dataset, only 600 pairs are metaphor related. In addition, their data presents some bias that does not truthfully represent the use of metaphors in natural language. For instance, entailment pairs hypotheses are generated by literal substitution of the metaphorical expression from the premise. This also occurs with non-entailment. In this case, the metaphor from the premise is replaced by an antonym to generate the hypothesis.\nThe work of Zayed, McCrae, and Buitelaar (2020  ###reference_b84###  ###reference_b84###) aimed at creating a gold standard for metaphor interpretation. Therefore, they developed a dataset of 2500 tweets with definitions of verb-noun metaphorical expressions with the aid of lexical resources and word embeddings. However, they do not present any experiments on models to evaluate their dataset.\nStowe, Utama, and Gurevych (2022  ###reference_b74###  ###reference_b74###) introduce IMPLI, a semi-automatic constructed dataset for EN to evaluate RoBERTa-like models performance on figurative language, specifically idioms and metaphors, within the NLI task. Their resource is considerably larger in size (24,029 silver and 1,831 gold sentence pairs) and covers entailment and non-entailment (a merge of neutral and contradiction) relations. Similarly to the previous work, some biases are introduced throughout the generation of this dataset. Entailed pairs only consist of sentences that include a metaphorical expression and the entailed hypothesis is simply its literal paraphrase. On the other hand, non-entailment pairs were developed by \u201cinventing\u201d a literal but not true meaning of the figurative expression.\nChakrabarty et al. (2022  ###reference_b16###  ###reference_b16###) propose FLUTE, an explanation-based dataset in EN of 9000 NLI pairs that include sarcasm, simile, metaphor and idioms that they later exploit to evaluate T5 Raffel et al. (2023  ###reference_b57###  ###reference_b57###). FLUTE differs from the other resources in that each entailed/contradicted pair is accompanied by an explanation. However, premises and hypotheses are mainly based on lexical substitution: e.g. His dark clothes had a negative effect in the shadows -> His dark clothes were a plus in the shadows.\nAll the work we just mentioned is centered in English only, thus most relevant resources developed for metaphor processing comprehend data in this language, both for detection and interpretation tasks. In the case of metaphor detection, in order to compensate the lack of language diversity, some monolingual datasets were published in other languages, although they are commonly of reduced size and still are monolingual, e.g. KOMET corpus Antloga (2020  ###reference_b4###  ###reference_b4###) in Slovene, CoMeta Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###) in Spanish, Estonian Aedmaa, K\u00f6per, and Schulte im\nWalde (2018  ###reference_b1###  ###reference_b1###), German K\u00f6per and Schulte im\nWalde (2016  ###reference_b32###  ###reference_b32###) or Polish adjective-noun metaphors Mykowiecka, Marciniak, and\nWawer (2018  ###reference_b51###  ###reference_b51###).\nAmong the datasets that cover more than one language, we can find LCC Mohammad, Shutova, and Turney (2016  ###reference_b48###  ###reference_b48###): it comprises texts of English, Spanish, Russian and Farsi and provides source/target, metaphoricity degree and metaphor/literal annotations on sentences. However, there are not labels at token level, annotations were extracted automatically, only a subset was manually validated and it is not parallel corpora. The work of Levin et al. (2014  ###reference_b38###  ###reference_b38###) proposes the CCM corpora, which also includes sentences in the same four languages as the LCC dataset. Nevertheless, it is composed of source and target mappings, thus it is centered on conceptual metaphors.\nSchuster and Markert (2023  ###reference_b63###  ###reference_b63###) explore static embeddings to generate a cross-lingual dataset out of existing resources in German, English and Polish but focused on adjective-noun metaphor pairs and non-parallel texts. Similarly, Berger (2022  ###reference_b8###  ###reference_b8###) explores transfer learning techniques, such as neural machine translation, cross-lingual word embeddings or multilingual pre-trained models to obtain a dataset for metaphor detection in German from English corpora.\nIn addition to data generation, experimental settings of many recent publications shifted to multi- and/or cross-lingual approaches, which aim at analysing metaphor transferability among different languages. Previous to pre-trained language models, Tsvetkov et al. (2014  ###reference_b76###  ###reference_b76###) already explored unsupervised methods to evaluate metaphor identification of specific syntactic constructions in English, Spanish, Russian and Farsi. Shutova et al. (2017  ###reference_b69###  ###reference_b69###) as well experimented with semi- and unsupervised learning, specifically with clustering techniques for metaphor detection in English, Spanish and Russian.\nAghazadeh, Fayyaz, and\nYaghoobzadeh (2022  ###reference_b3###  ###reference_b3###) investigate whether pre-trained language models are able to encode metaphorical meaning through the task of metaphor detection in English, Farsi, Russian and Spanish. Not only the trend is to mix languages but also types of figurative language.\nThe work of Lai, Toral, and Nissim (2023  ###reference_b33###  ###reference_b33###) presents a combination of joint-models that are able to detect if a sentence has hyperbole, idioms or metaphors in these same four languages, nevertheless, it does not identify the figurative language span.\nWith respect to metaphor interpretation, the range of available datasets in multiple languages is more limited than that for detection.\nMost corpora just mentioned only concerns EN: FigurativeQA Rakshit and Flanigan (2023  ###reference_b58###  ###reference_b58###), Fig-QA Liu et al. (2022  ###reference_b43###  ###reference_b43###), IMPLI Stowe, Utama, and Gurevych (2022  ###reference_b74###  ###reference_b74###) or FLUTE Chakrabarty et al. (2022  ###reference_b16###  ###reference_b16###). The exception is the work of Kabra et al. (2023  ###reference_b31###  ###reference_b31###), who delve into figurative language understanding from a multi-lingual and multi-cultural perspective. They present MABL, a dataset for seven languages with a high number of speakers, such as Hindi, Swahili, or Sudanese. They demonstrate that socio-cultural features pose an essential impact on the conceptual mappings that are later materialized in linguistic metaphors. They exploit it to evaluate language models and provide an insight of how English- or Western-centered the training process of these models can be.\nThis overview gives an account of the prevalence of EN in the research of metaphor processing. Monolingual datasets for detection in other languages are of a very reduced size in comparison to those in EN. Other limitations of metaphor detection corpora is that datasets with a larger amount of samples do not include annotations at token level or are limited to metaphorical expressions of specific POS. Namely LCC dataset, which is the most utilised multilingual dataset for the task, however, its original annotations are developed at sentence level and it does not contain parallel text.\nMetaphor interpretation datasets are more scarce and the lack of variety of languages is more striking than in metaphor detection corpora. The only multilingual dataset is MABL, however, data and annotations are not parallel between languages. We can find different datasets according to the tasks used to approach interpretation. Nevertheless, these corpora tend to bring along biases and artifacts as a result of synthetic annotation processes Boisson, Espinosa-Anke, and\nCamacho-Collados (2023  ###reference_b13###  ###reference_b13###). In datasets developed for paraphrase, tuples are commonly composed of a sentence with a metaphorical expression that is replaced for its literal meaning. In the case of NLI datasets, premise-hypothesis pairs are typically constructed by lexical substitution: the entailment is based on the literal paraphrase of the metaphor and contradiction on the replacement of the metaphorical terms by their antonym. Thus, we reckon these instances are not representative of spontaneous occurrences of metaphor in natural language utterances.\nThese shortcomings when developing datasets for figurative language can induce misleading results and conclusions. To bridge this gap, we bring forward the first parallel corpus that contains metaphorical annotations for metaphor detection at token level that cover all POS, and for metaphor interpretation via NLI in ES and EN. In addition, this resource includes a sufficient number of instances and contains data from natural language utterances that were not specifically generated for metaphor processing tasks. We summarised the distinct features of most popular available datasets in Table 1  ###reference_###  ###reference_###.\nWe believe our proposed dataset is a valuable resource to continue with this line of research of multi- and cross-lingual approaches, which can allow to explore whether metaphors are transferable among languages.\nLang\nDE, EN\nEN, ES, FA, RU\nHI, ID, JV, KN, SU, SW, YO\nES, EN\nTo address these shortcomings, we introduce Meta4XNLI, a parallel dataset in ES and EN with metaphorical annotations for detection and interpretation via NLI in texts of natural language utterances and multiple domains. In the following subsections, we describe the collection of the dataset (Subsection 3.1  ###reference_###  ###reference_###), the methodology we employed to annotate metaphors for each task and language (Subsection 3.2  ###reference_###  ###reference_###) and, as a result, we will provide details of the outcoming dataset of this process (Subsection 3.3  ###reference_###  ###reference_###).\nThe annotation process in this language comprehends two phases, as depicted in Figure 2  ###reference_###  ###reference_###  ###reference_###  ###reference_###. First, we automatically label Meta4XNLIES by leveraging mDeBERTa He, Gao, and Chen (2021  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###) fine-tuned for metaphor detection in ES with CoMeta. We choose mDeBERTa since it is the multilingual model that achieved highest F1 score in the experimental setup of Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). This choice aims at reducing the heavy work load and time investment that manual annotation from scratch requires. Afterwards, we manually inspect and correct the predictions in the whole dataset. From the first phase of automatic labeling, 748 tokens were predicted as metaphor in the premises and 724 in the hypotheses. After a complete and manual revision of all sentences, we removed 74 tokens and added 481 undetected metaphors in the premises; whereas in the hypotheses, we deleted 118 false positives and labeled 533 false negatives.\n###figure_7### The main sources of ambiguity in ES emerge from multi-word expressions (MWE) and polysemy.\nThe main issue when labeling MWE is to decide whether to treat the MWE as a single lexical unit, a fixed expression where the meaning of each word is not transparent anymore, or if it should be regarded as a collocation. Collocations are expressions where the constituent words tend to co-occur with high frequency but are not fixed, since each of its elements can be replaced by others with similar meaning.\nDijo que era hora de entrar en p\u00e1nico (lit. \u201cThey said it was time to enter into panic\u201d).\nFor instance, in Example 2  ###reference_###  ###reference_###  ###reference_###  ###reference_###, the expression entrar en p\u00e1nico could be initially considered as a MWE with a single lexical unit, therefore, the three tokens would be labeled as metaphorical. However, this expression specifically means \u201cto panic\u201d, in which the verb entrar (lit. \u201cto enter\u201d) holds metaphorical meaning, since \u201cpanic\u201d is not a physical place you can get into. In this case, we do not think of the expression as a fixed MWE, since the verb is also used metaphorically with other terms that are not places, like entrar en c\u00f3lera (\u201cto get angry\u201d, lit. \u201cto enter into wrath\u201d) or entrar en calor (\u201cto feel hot\u201d, lit. \u201cto enter into heat\u201d). In all of these expressions the verb is conveying the sense of starting to feel the noun it complements, as if by entering to a place it transformed our sensitivity. The association of concepts arises from understanding emotions or sensations as physical locations.\nRegarding polysemy, the existence of multiple and very nuanced senses associated to the same token can lead to confusion. It can be challenging to determine whether the basic meaning of the lexical unit is currently and generally known and used by native speakers or if they directly associate the lexical unit to the figurative meaning, not identifying the basic meaning at all.\n[\u2026] ha mostrado su apoyo a la candidatura para ser sede [\u2026] (lit. \u201cThey showed their support to the candidacy to be head office\u201d).\nFor instance, in Example 2  ###reference_###  ###reference_###  ###reference_###  ###reference_### we label apoyo as metaphorical, since the most basic meaning of the verb apoyar in the dictionary defines it as \u201cto make something rest upon another thing\u201d. In this sentence, the contextual meaning refers to someone in favour of someone else\u2019s goal. Figuratively, the goal can be understood in terms of a physical object so heavy that requires more than one anchor point to distribute its weight. Doubts in this examples come from the fact that the figurative sense can be more used that its basic one, thus speakers might not identify the metaphorical meaning as such. We make use of the Diccionario de la Real Academia Espa\u00f1ola (DRAE) as a tool to help us clarify these ambiguous cases. Nonetheless, metaphor identification remains a highly subjective task.\nWe develop Meta4XNLIEN annotations semi-automatically and based on ES annotations, since our purpose is to publish a parallel resource to counterbalance monolingual English labeled data. These annotations serve as a starting point for further refinement and enable potential analysis of metaphors shared across these two languages. The whole process comprises mainly four phases, as depicted in Figure 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nThe first step involves the projection of ES labels onto EN sentences. For this end, we utilised Easy-Label-Projection Garc\u00eda-Ferrero, Agerri, and\nRigau (2022  ###reference_b28###  ###reference_b28###  ###reference_b28###  ###reference_b28###), a tool developed for cross-lingual sequence labeling that makes use of word alignments and data- and model-transfer to project the labels from a source language (ES) to an untagged target language (EN). This mechanism is suitable when the labeled entity is certainly appearing in both source and target sentences, however, metaphors present in a source sentence are not necessarily in its translation. It depends on a series of multiple factors, namely the type of translation, the knowledge of the translator, if it is human-translated, or socio-cultural knowledge, among others. The next step comprises the manual revision of the projections.\nIn order to alleviate this issue, we made use of XLM-RoBERTa Conneau et al. (2020  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###) trained on the VUAM dataset, as it is the multilingual model that showed best performance in the experiments in EN from the work of Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). Since projected sentences were manually reviewed (13% of the total), we only predicted automatically those sentences that did not receive a projection in the first place (87%). We manually reviewed the outcoming predictions from the MLM to correct errors and undetected metaphorical expressions following the MIPVU procedure. The number of metaphorical tokens extracted in each annotation phase is detailed in Table 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nThere are some issues that emerged during the whole annotation process. Firstly, the projections phase entails that for a metaphorical expression to be annotated in EN, it has to have been annotated in ES in advance. Hence, metaphors in EN sentences that were not expressed figuratively in their ES counterpart will not be spotted, as in Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###. In the ES sentence there is not a metaphorical expression annotated as such, however, in the EN version, the adjective heavy is holding metaphorical meaning, as a synonym of \u201cdemanding\u201d, which is expressed literally in ES with the adjective exigentes. In this kind of cases where the metaphor is \u201cgained in translation\u201d, the lack of annotation in the source language implies no label will be projected onto the target sentence, missing a metaphorical instance. These examples give an account of how translation and the singularities of metaphors according to languages may affect to the annotation process of this task.\n###figure_8### .\nA los usuarios m\u00e1s exigentes se les deber\u00eda cobrar m\u00e1s. \nThe heavy users should be charged the most.\nAnother issue that should be noted are false positives: all ES sentences with one or more labeled metaphors will transfer those tags to the EN sentence, regardless the translated tokens hold metaphoric meaning or not. To solve this question, we manually reviewed all sentences that received a projected tag. In this revision we had to remove metaphors that were \u201clost in translation\u201d and adjust some spans projected to the target language, e.g. some annotated verbs in ES were projected in EN to the subject pronoun and verb, since some ES verbs forms are synthetic and include the person information in a single morpheme. Therefore, we eliminated the label from the pronoun and maintained only that of the verb. In Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_### we can see how the verb peleaban (lit. \u201cthey fought\u201d) labeled as metaphorical in the ES sentence (Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###) was projected to the subject and verb in EN (Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###). Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_### represents the definitive version of the annotations after manual revision.\nPeleaban por lo ricos que eran los directores ejecutivos.\nThey fought about how rich CEOs were.\nThey fought about how rich CEOs were.\nRegarding the subset of sentences that were automatically labeled by XLM-RoBERTa, some concerns emerged with respect to annotations present in VUAM dataset during the phase of manual revision. Especially when it comes to phrasal verbs and abstract and polysemous terms that are lexicalised. For instance, due to Examples like 3  ###reference_###  ###reference_###  ###reference_###  ###reference_### or 3  ###reference_###  ###reference_###  ###reference_###  ###reference_### labeled as metaphorical in the training set VUAM, we observed a tendency to overannotate verbs that conform phrasal verbs as metaphorical, such as get, look, made or go. In most cases, these verbs appear in a context where they do not add any strong semantic information due to their lexicalization. Hence, we unmarked these instances that were predicted as metaphorical. Following this line of thought, we also discarded abstract and vague terms that are common-places of spontaneous discourse, like the word thing in Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###, since it can refer to any kind of entity, either concrete or abstract and might not be directly matched to a more broadly used basic meaning.\nHis lack of humbug about political balance has always made him more honest than all the employees [\u2026].\n\\enumsentenceTake what you want and leave the rest , your mother \u2019ll get rid of it . \n\\enumsentenceOne thing always linked to another thing.\nThe parallel data for this task comprises a total of 13320 sentences annotated at token level, since we approached metaphor detection as a sequence labeling task, following the criteria of the cited previous work in Section 2  ###reference_###  ###reference_###  ###reference_###.\nWith respect to Spanish annotations, there is a total of 1155 metaphorical tokens in premises and 1139 in hypotheses. Out of the 13320 sentences, 1873 contain at least one metaphorical expression, which constitutes the 14% of the whole dataset. This information is detailed in Table 4  ###reference_###  ###reference_###  ###reference_### according to the source datasets partitions. A higher proportion of metaphors in premises can be noticed. This might be due to the fact that premises are of larger length than hypotheses, as indicated in Conneau et al. (2018  ###reference_b23###  ###reference_b23###  ###reference_b23###). In addition, premises were collected from existing utterances, while hypotheses were generated from crowd-source workers in response to a given set of premises. Therefore, hypotheses sentences might tend to present shorter length and lower complexity.\n###table_7### Regarding English annotations, we can observe a similar trend to that of ES in Table 5  ###reference_###  ###reference_###  ###reference_###. A total of 3330 tokens were labeled as metaphor and 2736 sentences contain at least one metaphorical instance out of the total 13320 sentences. Premises show a higher metaphor ratio than hypotheses as in ES annotations. Additionally, we observe a larger amount of labeled metaphors in EN, caused by the different annotations processes specified in Subsection 3.2  ###reference_###  ###reference_###  ###reference_###. Since VUAM contains a significantly higher amount of labeled metaphorical expressions, the MLM fine-tuned with this dataset predicted many ambiguous metaphors, which we subsequently removed in manual revision. These discrepancies are not only noticeable in annotations but also in experiments results. Moreover, this gives an account of how guidelines for metaphor identification labeling are open to discussion and clarification, due to the subjective and nuanced nature of this cognitive-linguistic phenomenon.\n###table_8### ###table_9### Annotations for this task were developed at premise-hypothesis level. As shown in Table 6  ###reference_###  ###reference_###  ###reference_###, the average percentage of pairs with metaphors relevant to the inference relationship is 12%. This figure remains steady throughout each source dataset and inference labels. esXNLI shows a higher number of metaphor occurrence that might be caused by the difference of text domains and sentence characteristics with respect to XNLI data. Regarding non-relevant cases, we do not exploit them in the experiments in order to be able to analyse more clearly whether metaphor presence impacts models performance. We keep the same sample distribution for both languages to develop the experiments.\nIn this section we present the experimental setup designed with the objective of testing the capabilities of multilingual MLMs on metaphor detection in cross-domain, crosslingual and multilingual settings. Furthermore, we also experiment with their ability to perform NLI when the correct inference requires understanding of metaphorical language.\nTaking advantage of previous available resources and the corpus we present in this work, we conducted a series of experiments to evaluate and fine-tune MLMs.\nThe configuration of cross-domain experiments is specified in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). For the other three setups, monolingual, multilingual and zero-shot cross-lingual, we performed hyperparameter tuning for batch size (8, 16, 36), linear decay (0.1, 0.01), learning rate (in the [1e-5-5e-5] interval), sequence length of 128 and epochs from 4 to 10. A warm-up of 6% is specified. The results of the hyperparameter tuning showed that after 4 epochs development loss started to increase, so results reported here are obtained with 4 epochs, batch size of 8, weight decay of 0.1 and learning rate of 5e-5.\nCross-domain: the aim of this set of experiments is to evaluate the performance of MLMs fine-tuned with CoMeta and VUAM datasets on Meta4XNLIES and Meta4XNLIEN, respectively, since each dataset contains texts from different domains. The motivation is to explore the impact of text features and genres on the performance, as well as annotation criteria Aghazadeh, Fayyaz, and\nYaghoobzadeh (2022  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###); Lai, Toral, and Nissim (2023  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###). To do so, we chose the models with best performance from monolingual experiments developed in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). We conducted the evaluation on various data splits: within each source dataset, XNLIdev, XNLItest and esXNLI, we evaluated premises and hypotheses separated and combined. This is due to the dissimilarities and the unequal distribution of metaphorical expressions between premises and hypotheses sentences mentioned in Subsection 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nMonolingual: this scenario comprises the fine-tuning and evaluation of the MLM on Meta4XNLIES and Meta4XNLIEN separately. To accomplish this task, we split Meta4XNLI into train, development and test sets (0.6-0.2-0.2). We equally distributed the data to ensure each partition is balanced in terms of source datasets sentences and metaphor occurrence. Data statistics are detailed in Appendix A  ###reference_###  ###reference_###  ###reference_###  ###reference_###. These partitions will be used as well in subsequent multilingual and zero-shot cross-lingual scenarios. In addition to fine-tuning and evaluation on Meta4XNLIES and Meta4XNLIEN, we evaluated each trained monolingual model with the test sets of CoMeta and VUAM, following the same reasoning as in cross-domain experiments.\nMultilingual: the purpose of these experiments is to explore whether MLMs benefit from being trained on data in multiple languages. In this case, we combined Meta4XNLIES and Meta4XNLIEN train splits to fine-tune the models. Subsequently, we evaluated the trained models on each language test set, in order to analyse the impact on the performance for each language. The data splits used correspond to those from monolingual experiments.\nZero-shot cross-lingual: in this scenario we explore to what extent MLMs are able to generalize knowledge and metaphor transfer between these two languages in question. Therefore, we fine-tune the models with Meta4XNLI data in one language and evaluate it on the test set of the other. Data partitions used for these experiments are the same as in monolingual an multilingual scenarios.\nWe carried out two sets of experiments to evaluate metaphor interpretation within the NLI task. We performed hyperparameter tuning, with the same range of parameters specified on the task of metaphor detection. We report best results obtained on the development set, after 4 epochs, batch size of 8, learning rate of 1e-5, weight decay of 0.1 and 512 sequence length.\nThe purpose of the first experiments is to examine if the presence of metaphorical expressions in premise-hypothesis pairs impacts the performance of models in the NLI task. To that end, we fine-tuned the MLMs for the task with the MultiNLI dataset. Then, we evaluated them with Meta4XNLI. Among each source dataset, we discriminated pairs with metaphors relevant to the inference relationship from those without metaphorical expressions. We developed the evaluation on each of these subsets and for each language separately, e.g. one evaluation on XNLIdev metaphor set and another on XNLIdev without metaphors set, in EN and then in ES.\nThe goal of the second set of experiments is to analyse the effect on models performance of not being \u201cexposed\u201d to instances with metaphorical expressions during training. With this aim in mind, we extracted pairs with and without metaphors from Meta4XNLI train, dev and set splits. On the first scenario, we fine-tuned the models only with pairs from the train set that did not contain any metaphorical expression. On the second scenario, we mixed pairs with and without metaphors and fine-tuned the models as well. In both cases, we evaluated on the test sets with and without metaphors for each language.\nIn addition to F1 score, we performed in-vocabluary (Inv) and out-of-vocabulary (Oov) evaluations to test the impact of the labels seen from training data during the learning process of the MLMs. Precision and Recall metrics are reported in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###. For in-vocabulary evaluation, we computed F1 score taking into account predicted words seen in training labeled as metaphorical. While for out-of-vocabulary evaluation we considered only words not seen during training to compute the F1 score. We included this evaluation in all experiments but for the zero-shot cross-lingual setup, since the data of train and test sets are in different languages, thus the match of the exact same metaphorical token in both partitions is highly unlikely.\nAlthough the purpose of our experiments is not to beat state-of-the-art results but to evaluate the performance of MLMs on the task from a cross-lingual approach, we added two indicative baselines: on one hand, the system BasicBERT Li et al. (2023a  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###), which obtained 73.3 F1 score; on the other, the result of DeBERTa reported in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###), with 73.79 F1. We selected these results for comparison purposes, since they both were evaluated on the VUAM-2020 version of the dataset in EN used in the Shared Task 2020 Leong et al. (2020  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###) and in the same experimental setup we propose.\nCross-domain: We evaluated models trained on CoMeta and VUAM datasets with XNLI and esXNLI in ES and EN, respectively. From results reported in Table 7  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we can observe that in all cases mDeBERTa outperforms XLM-RoBERTa for ES. In EN the best result is obtained by DeBERTa, which also outperforms XLM-RoBERTa in all scenarios. In all datasets, except for XNLIdev in ES, premises sentences achieve better results than hypotheses and the combination of both. This goes in line with annotation statistics in Tables 4  ###reference_###  ###reference_###  ###reference_###  ###reference_### and 5  ###reference_###  ###reference_###  ###reference_###  ###reference_###, which show that premises contain a greater ratio of metaphors per sentence than hypotheses in both languages. In ES, in-vocabulary evaluation outperforms the general F1 score while out-of-vocabulary evaluation results decrease. The small distance in points of this cross-domain evaluation in ES exhibit stability of the models when it comes to predicting metaphors and coherence in annotations between both datasets despite the difference of text domains. Nevertheless, EN results do not demonstrate such consistency, as out-of-vocabulary obtains higher results than the overall F1 score. This might be due to the different labeling criteria used in VUAM and Meta4XNLI, as we mentioned in 3.2  ###reference_###  ###reference_###  ###reference_###  ###reference_###. These discrepancies are also reflected in a significant drop in performance with respect to ES experiments and in-domain evaluation with VUAM. The high recall scores from Table 16  ###reference_###  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_### show that the model tends to predict many metaphors, however, low precision scores indicate that a small amount of these predictions are correct.\n.\nMonolingual: Results from this set of experiments are specified in Table 8  ###reference_###  ###reference_###  ###reference_###  ###reference_###. After fine-tuning and evaluating the models with Meta4XNLIES and Meta4XNLIEN, the highest overall F1 score is obtained by mDeBERTa in ES. On the other hand, XLM-RoBERTa achieves better performance than mDeBERTa in EN but still lower than in ES. In both languages, in-vocabulary evaluation results are higher than the overall ones and out-of-vocabulary results, lower. This is not the case when we use VUAM corpus for testing the model fine-tuned with Meta4XNLIEN. In this setup, similarly to results from cross-domain experiments, performance drastically falls to 32.42 F1 score and out-of-vocabulary results are the highest. The reason behind might be the mismatches in the annotation process. In ES, when evaluating CoMeta, we also encounter a decrease in the performance, however, it is more subtle: from 67.17 to 56.24, obtained by XLM-RoBERTa. This might be caused by the variety of text genres and dissimilarities between sentences from each dataset.\nMultilingual: In this set of experiments we assembled Meta4XNLIEN and Meta4XNLIES to train the MLMs. The evaluation is conducted for each language separately and results detailed in Table 9  ###reference_###  ###reference_###  ###reference_###  ###reference_###. Best results in ES are obtained by mDeBERTa, which are higher than the top result from monolingual experiments. In EN, mDeBERTa is the model that achieves better performance but very close to that of XLM-RoBERTa. The highest F1 score is 8 points lower than that of ES but outweighs EN monolingual results. This suggests that the combination of parallel multilingual data for training is beneficial for the performance of the models.\nZero-shot cross-lingual: In these experiments we perform evaluation of Meta4XNLI in the opposite language to that utilised for training. Results are reported in Table 10  ###reference_###  ###reference_###  ###reference_###  ###reference_###. XLM-RoBERTa performance exceeds that of mDeBERTa in both languages. Nonetheless, F1 score for EN is almost 20 points lower than ES evaluation results. In addition to the differences in annotation criteria between languages and datasets, another aspect to bear in mind in this scenario could be the number of positive examples present in training sets. As we explained in Section 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###, Meta4XNLIEN contains a higher number of metaphorical instances, thus models are exposed to a greater variety of examples that can be transferred to ES. While a more reduced amount of instances in Meta4XNLIES seen during training might hinder model\u2019s generalization ability, as the low recall and high precision scores show in Table 19  ###reference_###  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nIn the first setup, we fine-tuned the models for the NLI task with MultiNLI dataset Williams, Nangia, and Bowman (2018  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###). Then we conducted the evaluation with two different splits for each source dataset conforming Meta4XNLI: pairs with at least one metaphorical expression and pairs lacking metaphors. From accuracy scores reported in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_### we can observe certain variability in the results. In the majority of cases, XLM-RoBERTa achieves better performance than mDeBERTa. We do observe a tendency of higher results on the sets of pairs without metaphors than on the set with metaphors. The exceptions to this current are the results from XLM-RoBERTa for XNLIdev and XNLItest in ES. In these partitions, the subset of pairs with metaphorical expressions obtained better results, although the difference does not even reach one point. We hypothesize the original language of the dataset might be involved in this performance, since XNLI includes natural utterances of EN that were afterwards manually translated to ES, thus some artifacts might have been introduced during this process Artetxe, Labaka, and Agirre (2020  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###).\nThe second scenario consisted in fine-tuning the models on two setups: a) with pairs without metaphors and b) pairs with and without metaphors. We performed the evaluation on subsets split by the criterion of metaphor occurrence as well. Results reported in Table 12  ###reference_###  ###reference_###  ###reference_###  ###reference_### show XLM-RoBERTa outperforming mDeBERTa in all contexts. Both models show best results for the NLI task on the examples without metaphors and the lowest performance with pairs that contain metaphorical expressions. This outcome replicates in both languages and experimental setups a) and b).\nIn this section, we manually inspected a subset of erroneous cases in order to provide a qualitative insight of results and with the intention of finding potential explanations of errors and models performance for both detection and interpretation tasks.\nWe selected the predictions from the model that obtained highest F1 score from the monolingual experiments in ES for both Meta4XNLI and CoMeta evaluations. We extracted false negatives and false positives and grouped the tokens by their number of occurrences. Within the false positives of CoMeta test set, we find tokens like\napoyo (lit. \u201csupport\u201d) or tensi\u00f3n (lit. \u201ctension\u201d) that appear more frequently used figuratively than with their literal sense or even only appear in the training set labeled as metaphor. Other wrong predictions are words that appeared in specific domains, such as texts that allude to the pandemic, e.g. ola (lit. \u201cwave\u201d), not detected as metaphorical due to its absence with metaphorical meaning in Meta4XNLI sentences. Something similar occurs with the misclassified tokens from Meta4XNLI test set. Most errors stem from conventional metaphors, namely gran, abrir, paso, claro (lit. \u201cgreat\u201d, \u201cto open\u201d, \u201cstep\u201d, \u201cclear\u201d) that occur regularly used with their metaphorical meaning. The lack of balanced examples might contribute to these predictions. However, we maintained the distribution as is, since our aim is to study the presence and prevalence of metaphor in natural language utterances.\nWe analysed a subset of 30 errors from each experimental setup, both EN and ES, and evaluation sets. We chose the predictions from XLM-RoBERTa since it is the model that performs better in this task in most scenarios. Although results show that models struggle more to identify the inference relation if there is a metaphorical expression involved in the pair sentences, we do not observe any particular feature within the errors. A remarkable aspect to highlight is the lower results of esXNLI with respect to XNLIdev and XNLItest in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_###. It could be motivated by the difference of the text domains, since XNLI is an extension of MultiNLI, which maintains the same set of textual domains. While esXNLI is a collection of texts from another set of genres and sources.\nRegarding EN, some of the errors might derive from the misclassification of some pairs, since annotations were developed on ES text. A metaphorical expression involved in the inference relationship in ES might not be present in its EN version and vice versa. Thus, samples from these two classes, pairs with and without metaphors, should be reexamined in EN and correctly classified for future experimentation.\nIn this work, we provided Meta4XNLI, a cross-lingual parallel dataset in ES and EN labeled for metaphor detection and interpretation framed within the task of NLI. With this new resource that contains parallel text and annotations, we developed a series of experiments to assess the capabilities of MLMs when dealing with this kind of figurative expressions present in natural language utterances.\nRegarding the task of metaphor detection, after the annotation process and experiments results, we can conclude the importance of establishing a unified criterion for annotation that is valid for different languages if the aim is to continue researching cross-lingual approaches. In addition, the semi-automatic process of annotation followed for EN shows that automatic labeling of cross-lingual metaphor is far from trivial. Metaphorical expressions are language and culture dependant. Moreover, the translation of the data introduces a new layer in which metaphorical expressions can either be lost from the translation of the source to the target language, or can be introduced in the target language by means of the translator, either human or automatic. Further work to explore automatic annotation methodologies would be of considerable value in order to reduce the demanding workload and effort of manual labelling in more than one language.\nWith respect to results, the purpose of our experiments is not to overcome the state-of-the-art results, but to evaluate models performance from a cross-lingual approach. Best results are obtained when Meta4XNLI in both languages is used for training. The augmentation of the training set size and the parallel annotations might boost this performance. Cross-domain and monolingual experiments show how the lack of consistency in the annotation criteria affects the performance of models. This can be observed as well in the zero-shot cross-lingual setup, although the scenario of training in EN and evaluating in ES shows competitive performance. It should be noted that the EN set contains a larger number of instances annotated as metaphorical in the training set. In addition, the in-vocabulary and out-of-vocabulary evaluation points to some kind of bias in the learning process. This could stem from the fact that the majority of the metaphor instances are conventional or due to lexical memorization Levy et al. (2015  ###reference_b39###  ###reference_b39###); Boisson, Espinosa-Anke, and\nCamacho-Collados (2023  ###reference_b13###  ###reference_b13###). Future research on this line of work should be carried out to clarify this issue.\nFor metaphor interpretation, we evaluated the ability of MLMs to understand metaphorical expressions framed within the task of NLI. We provide parallel annotations at premise-hypothesis pair level that mark if the presence of metaphorical expressions is relevant for the inference relationship. We exploited this information to conduct our experiments. From reported results, we can recognize a tendency of the models to show a lower performance with pairs that contain at least one metaphorical expression. However, this trend breaks with XLM-RoBERTa results when fine-tuned in ES and evaluated on the XNLIdev and XNLItest in the same language. Since these sets are sentences translated from EN, we presume the translation process might induce biases in metaphor occurrence and the \u201cnaturalness\u201d of the sentences. Similar to metaphor detection, future work to analyse the impact of translation in the development of metaphor parallel resources should be explored for the task of metaphor interpretation, as well as additional experimentation from a multilingual perspective.\nMetaphor annotation is an inherently subjective task. This variance in annotations is reflected on Meta4XNLIEN, due to the different criterion employed through the annotation process. Labels in this language should be updated and further revised to improve their quality. Disagreement and subjectivity could be counterbalanced by a larger number of annotators, in order to develop more consistent and reliable labeled data, however it requires a huge amount of time and workload. Data augmentation and semi-automatic methods could be exploited to create larger datasets with similar characteristics to the one we present and extend it to more languages, since most corpora available for metaphor processing is of reduced size and limited to a narrow set of languages. The existence of parallel resources in multiple languages others than EN that reflect cultural and real world knowledge nuances is of great importance to continue researching such a complex phenomenon as figurative language, specifically metaphors."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Description",
            "text": "Meta4XNLI is a compilation of XNLI Conneau et al. (2018  ###reference_b23###  ###reference_b23###) and esXNLI Artetxe, Labaka, and Agirre (2020  ###reference_b5###  ###reference_b5###). We decided to exploit these resources since we evaluate metaphor interpretation through NLI and these datasets were originally developed for this task. This enables both annotations at token level for detection and pair level for interpretation. In addition, it contains parallel text, from which we select data in ES and EN. Moreover, the combination of XNLI and esXNLI constitutes a dataset of large size compared to common available resources for metaphor processing and with natural language utterances and spontaneous usage of metaphors. The distribution of Meta4XNLI is detailed in Table 2  ###reference_###  ###reference_###.\nXNLI dataset is a cross-lingual evaluation set for MultiNLI Williams, Nangia, and Bowman (2018  ###reference_b82###  ###reference_b82###). It contains parallel data with original text in EN subsequently human-translated to other 14 languages, among which we exploited only EN and ES. It comprehends a total of 7500 premise-hypothesis pairs from 10 text genres, that is, 830 premises and 2490 hypotheses from XNLIdev, and 1670 premises and 5010 hypotheses from XNLItest. esXNLI comprises a total of 2490 pairs, as in XNLIdev, from 5 different genres. In this case, sentences were originally collected in ES and then human-translated to EN. The direction of translation (EN > ES in XNLI and ES > EN in esXNLI) is an interesting feature to explore how translation may affect metaphor cross-lingual transfer and whether it impacts models performance.\nThe collection methodology is shared between XNLI and esXNLI: a set of premise sentences were crawled from various sources in EN and ES respectively. Afterwards, crowd-source workers were asked to generate three hypotheses for each premise, one for each label. Both corpora are balanced in terms of inference tags and text domains."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Annotation Process",
            "text": "The methodology to label Meta4XNLI varies across tasks and languages. In the following subsections, we will delve into the annotation processes used for detection and interpretation in each language. First, we developed ES annotations, from which we transferred a subset of detection labels and interpretation annotations in EN. Manual labeling was developed by a linguist native speaker of Spanish with advanced knowledge of English.\nThe annotation process in this language comprehends two phases, as depicted in Figure 2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. First, we automatically label Meta4XNLIES by leveraging mDeBERTa He, Gao, and Chen (2021  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###) fine-tuned for metaphor detection in ES with CoMeta. We choose mDeBERTa since it is the multilingual model that achieved highest F1 score in the experimental setup of Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). This choice aims at reducing the heavy work load and time investment that manual annotation from scratch requires. Afterwards, we manually inspect and correct the predictions in the whole dataset. From the first phase of automatic labeling, 748 tokens were predicted as metaphor in the premises and 724 in the hypotheses. After a complete and manual revision of all sentences, we removed 74 tokens and added 481 undetected metaphors in the premises; whereas in the hypotheses, we deleted 118 false positives and labeled 533 false negatives.\n###figure_9### The main sources of ambiguity in ES emerge from multi-word expressions (MWE) and polysemy.\nThe main issue when labeling MWE is to decide whether to treat the MWE as a single lexical unit, a fixed expression where the meaning of each word is not transparent anymore, or if it should be regarded as a collocation. Collocations are expressions where the constituent words tend to co-occur with high frequency but are not fixed, since each of its elements can be replaced by others with similar meaning.\nDijo que era hora de entrar en p\u00e1nico (lit. \u201cThey said it was time to enter into panic\u201d).\nFor instance, in Example 2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, the expression entrar en p\u00e1nico could be initially considered as a MWE with a single lexical unit, therefore, the three tokens would be labeled as metaphorical. However, this expression specifically means \u201cto panic\u201d, in which the verb entrar (lit. \u201cto enter\u201d) holds metaphorical meaning, since \u201cpanic\u201d is not a physical place you can get into. In this case, we do not think of the expression as a fixed MWE, since the verb is also used metaphorically with other terms that are not places, like entrar en c\u00f3lera (\u201cto get angry\u201d, lit. \u201cto enter into wrath\u201d) or entrar en calor (\u201cto feel hot\u201d, lit. \u201cto enter into heat\u201d). In all of these expressions the verb is conveying the sense of starting to feel the noun it complements, as if by entering to a place it transformed our sensitivity. The association of concepts arises from understanding emotions or sensations as physical locations.\nRegarding polysemy, the existence of multiple and very nuanced senses associated to the same token can lead to confusion. It can be challenging to determine whether the basic meaning of the lexical unit is currently and generally known and used by native speakers or if they directly associate the lexical unit to the figurative meaning, not identifying the basic meaning at all.\n[\u2026] ha mostrado su apoyo a la candidatura para ser sede [\u2026] (lit. \u201cThey showed their support to the candidacy to be head office\u201d).\nFor instance, in Example 2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### we label apoyo as metaphorical, since the most basic meaning of the verb apoyar in the dictionary defines it as \u201cto make something rest upon another thing\u201d. In this sentence, the contextual meaning refers to someone in favour of someone else\u2019s goal. Figuratively, the goal can be understood in terms of a physical object so heavy that requires more than one anchor point to distribute its weight. Doubts in this examples come from the fact that the figurative sense can be more used that its basic one, thus speakers might not identify the metaphorical meaning as such. We make use of the Diccionario de la Real Academia Espa\u00f1ola (DRAE) as a tool to help us clarify these ambiguous cases. Nonetheless, metaphor identification remains a highly subjective task.\nWe develop Meta4XNLIEN annotations semi-automatically and based on ES annotations, since our purpose is to publish a parallel resource to counterbalance monolingual English labeled data. These annotations serve as a starting point for further refinement and enable potential analysis of metaphors shared across these two languages. The whole process comprises mainly four phases, as depicted in Figure 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nThe first step involves the projection of ES labels onto EN sentences. For this end, we utilised Easy-Label-Projection Garc\u00eda-Ferrero, Agerri, and\nRigau (2022  ###reference_b28###  ###reference_b28###  ###reference_b28###  ###reference_b28###  ###reference_b28###), a tool developed for cross-lingual sequence labeling that makes use of word alignments and data- and model-transfer to project the labels from a source language (ES) to an untagged target language (EN). This mechanism is suitable when the labeled entity is certainly appearing in both source and target sentences, however, metaphors present in a source sentence are not necessarily in its translation. It depends on a series of multiple factors, namely the type of translation, the knowledge of the translator, if it is human-translated, or socio-cultural knowledge, among others. The next step comprises the manual revision of the projections.\nIn order to alleviate this issue, we made use of XLM-RoBERTa Conneau et al. (2020  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###) trained on the VUAM dataset, as it is the multilingual model that showed best performance in the experiments in EN from the work of Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). Since projected sentences were manually reviewed (13% of the total), we only predicted automatically those sentences that did not receive a projection in the first place (87%). We manually reviewed the outcoming predictions from the MLM to correct errors and undetected metaphorical expressions following the MIPVU procedure. The number of metaphorical tokens extracted in each annotation phase is detailed in Table 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nThere are some issues that emerged during the whole annotation process. Firstly, the projections phase entails that for a metaphorical expression to be annotated in EN, it has to have been annotated in ES in advance. Hence, metaphors in EN sentences that were not expressed figuratively in their ES counterpart will not be spotted, as in Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. In the ES sentence there is not a metaphorical expression annotated as such, however, in the EN version, the adjective heavy is holding metaphorical meaning, as a synonym of \u201cdemanding\u201d, which is expressed literally in ES with the adjective exigentes. In this kind of cases where the metaphor is \u201cgained in translation\u201d, the lack of annotation in the source language implies no label will be projected onto the target sentence, missing a metaphorical instance. These examples give an account of how translation and the singularities of metaphors according to languages may affect to the annotation process of this task.\n###figure_10### .\nA los usuarios m\u00e1s exigentes se les deber\u00eda cobrar m\u00e1s. \nThe heavy users should be charged the most.\nAnother issue that should be noted are false positives: all ES sentences with one or more labeled metaphors will transfer those tags to the EN sentence, regardless the translated tokens hold metaphoric meaning or not. To solve this question, we manually reviewed all sentences that received a projected tag. In this revision we had to remove metaphors that were \u201clost in translation\u201d and adjust some spans projected to the target language, e.g. some annotated verbs in ES were projected in EN to the subject pronoun and verb, since some ES verbs forms are synthetic and include the person information in a single morpheme. Therefore, we eliminated the label from the pronoun and maintained only that of the verb. In Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### we can see how the verb peleaban (lit. \u201cthey fought\u201d) labeled as metaphorical in the ES sentence (Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###) was projected to the subject and verb in EN (Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###). Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### represents the definitive version of the annotations after manual revision.\nPeleaban por lo ricos que eran los directores ejecutivos.\nThey fought about how rich CEOs were.\nThey fought about how rich CEOs were.\nRegarding the subset of sentences that were automatically labeled by XLM-RoBERTa, some concerns emerged with respect to annotations present in VUAM dataset during the phase of manual revision. Especially when it comes to phrasal verbs and abstract and polysemous terms that are lexicalised. For instance, due to Examples like 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### or 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### labeled as metaphorical in the training set VUAM, we observed a tendency to overannotate verbs that conform phrasal verbs as metaphorical, such as get, look, made or go. In most cases, these verbs appear in a context where they do not add any strong semantic information due to their lexicalization. Hence, we unmarked these instances that were predicted as metaphorical. Following this line of thought, we also discarded abstract and vague terms that are common-places of spontaneous discourse, like the word thing in Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, since it can refer to any kind of entity, either concrete or abstract and might not be directly matched to a more broadly used basic meaning.\nHis lack of humbug about political balance has always made him more honest than all the employees [\u2026].\n\\enumsentenceTake what you want and leave the rest , your mother \u2019ll get rid of it . \n\\enumsentenceOne thing always linked to another thing."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Detection",
            "text": "Annotations for this task are developed at token level, since we approach metaphor detection as a sequence labeling task. We extract premises and hypotheses sentences and annotate them separately. Therefore, we do not take into account the premise as context to annotate its corresponding hypotheses and vice versa. Due to this split, the total number of labeled sentences amounts to 13320. With respect to the type of metaphors, we consider as candidates the tokens belonging to a semantically significant part of speech (POS): nouns, verbs, adjectives and adverbs.\n\nWe adopt the MIPVU guidelines Steen et al. (2010) throughout the whole annotation process, either manual revision or in automatic predictions, since models used were trained on data labeled accordingly to this procedure. It can be summarised in four main steps:\n\nRead the entire text\u2013discourse to establish a general understanding of the meaning. Determine the lexical units in the text\u2013discourse. For each lexical unit in the text, establish its meaning in context, that is, how it applies to an entity, relation, or attribute in the situation evoked by the text (contextual meaning). Take into account what comes before and after the lexical unit. For each lexical unit, determine if it has a more basic contemporary meaning in other contexts than the one in the given context. For our purposes, basic meanings tend to be: More concrete; what they evoke is easier to imagine, see, hear, feel, smell, and taste. Related to bodily action. More precise (as opposed to vague). Historically older. Basic meanings are not necessarily the most frequent meanings of the lexical unit. If the lexical unit has a more basic current\u2013contemporary meaning in other contexts than the given context, decide whether the contextual meaning contrasts with the basic meaning but can be understood in comparison with it. If yes, mark the lexical unit as metaphorical.\n\nThe main sources of ambiguity in ES emerge from multi-word expressions (MWE) and polysemy.\n\nThe main issue when labeling MWE is to decide whether to treat the MWE as a single lexical unit, a fixed expression where the meaning of each word is not transparent anymore, or if it should be regarded as a collocation. Collocations are expressions where the constituent words tend to co-occur with high frequency but are not fixed, since each of its elements can be replaced by others with similar meaning. Dijo que era hora de entrar en p\u00e1nico (lit. \u201cThey said it was time to enter into panic\u201d).\n\nFor instance, in Example 2, the expression entrar en p\u00e1nico could be initially considered as a MWE with a single lexical unit, therefore, the three tokens would be labeled as metaphorical. However, this expression specifically means \u201cto panic\u201d, in which the verb entrar (lit. \u201cto enter\u201d) holds metaphorical meaning, since \u201cpanic\u201d is not a physical place you can get into. In this case, we do not think of the expression as a fixed MWE, since the verb is also used metaphorically with other terms that are not places, like entrar en c\u00f3lera (\u201cto get angry\u201d, lit. \u201cto enter into wrath\u201d) or entrar en calor (\u201cto feel hot\u201d, lit. \u201cto enter into heat\u201d). In all of these expressions the verb is conveying the sense of starting to feel the noun it complements, as if by entering to a place it transformed our sensitivity. The association of concepts arises from understanding emotions or sensations as physical locations.\n\nRegarding polysemy, the existence of multiple and very nuanced senses associated to the same token can lead to confusion. It can be challenging to determine whether the basic meaning of the lexical unit is currently and generally known and used by native speakers or if they directly associate the lexical unit to the figurative meaning, not identifying the basic meaning at all.\n\n[\u2026] ha mostrado su apoyo a la candidatura para ser sede [\u2026] (lit. \u201cThey showed their support to the candidacy to be head office\u201d).\n\nFor instance, in Example 2 we label apoyo as metaphorical, since the most basic meaning of the verb apoyar in the dictionary defines it as \u201cto make something rest upon another thing\u201d. In this sentence, the contextual meaning refers to someone in favour of someone else\u2019s goal. Figuratively, the goal can be understood in terms of a physical object so heavy that requires more than one anchor point to distribute its weight. Doubts in this examples come from the fact that the figurative sense can be more used that its basic one, thus speakers might not identify the metaphorical meaning as such. We make use of the Diccionario de la Real Academia Espa\u00f1ola (DRAE) as a tool to help us clarify these ambiguous cases. Nonetheless, metaphor identification remains a highly subjective task.\n\nWe develop Meta4XNLIEN annotations semi-automatically and based on ES annotations, since our purpose is to publish a parallel resource to counterbalance monolingual English labeled data. These annotations serve as a starting point for further refinement"
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Interpretation",
            "text": "Similar to other works cited in Section 2  ###reference_###  ###reference_###, we framed metaphor interpretation within the task of NLI Agerri (2008  ###reference_b2###  ###reference_b2###); Mohler, Tomlinson, and Bracewell (2013  ###reference_b50###  ###reference_b50###); Chakrabarty et al. (2021  ###reference_b15###  ###reference_b15###); Stowe, Utama, and Gurevych (2022  ###reference_b74###  ###reference_b74###); Kabra et al. (2023  ###reference_b31###  ###reference_b31###). Our approach aims at evaluating whether MLMs struggle to identify the relationship of inference when there is a metaphorical expression either in the premise, hypothesis or both sentences. To do so, we labeled premise-hypothesis pairs with metaphorical expressions where the understanding of the figurative expression is crucial to determine the inference label. We summarised this annotation process in the following steps:\nRead the premise sentence and determine its general meaning.\nIdentify potential metaphorical expressions according to metaphor detection guidelines.\nRepeat the previous two instructions with the hypothesis sentence.\nEstablish the inference relation betweeen premise and hypothesis if not previously labeled.\nIf there is any metaphorical expression either in premise or hypothesis:\nIs it required to understand the literal meaning of the metaphorical expression to label the inference relationship between premise and hypothesis?\nYes: mark the pair.\nNo: mark the pair as an non-relevant case.\nRepeat the process with non-relevant cases until clarification. Otherwise, if they are intrinsically ambiguous or lack context to either identify the metaphors or determine if they are relevant to the inference, discard the pair.\nIn Example 3.2.2  ###reference_.SSS2###  ###reference_.SSS2###, we encounter the metaphor saltar (lit. \u201cto skip\u201d) with the meaning of omitting some information and not the literal sense of physically jumping. In the hypothesis, the sentence refers to the intention of telling the other interlocutor all the information, comprehensively, without ignoring any part, which contradicts the overall meaning of the premise. The understanding of this metaphorical expression, thus, is required to infer that they contradict each other.\nPremise: Hay tanto que se puede decir sobre eso, que sencillamente me voy a saltar eso. (lit. \u201cThere is so much you can say about that, that I am simply going to skip that\u201d).\nHypothesis: \u00a1Quiero contarte todo lo que s\u00e9 sobre eso! (lit. \u201cI want to tell you everything I know about it!\u201d). \nInference label: contradiction\nPremise: No hay necesidad de hurgar en ese tema, a menos que quieras asegurarte de que nos hundimos. (lit. \u201cThere is no need to rummage in that topic, unless you want to make sure we will sink\u201d).\nHypothesis: Hay una forma de que se hundan. (lit. \u201cThere is one way to make them sink\u201d). \nInference label: entailment\nNon-relevant cases comprehend premise-hypothesis pairs where the understanding of the literal sense of the metaphor is not essential to establish a relationship of entailment, contradiction or neutral. As we can see in Example 3.2.2  ###reference_.SSS2###  ###reference_.SSS2###, the metaphorical expression hurgar (lit. \u201cto rummage\u201d) is used with a sense of exploring an unpleasant topic, while the literal meaning implies to physically dig into an inner space. The entailment in this example is inferred from the likelihood of a sinking, not focusing on the willingness to talk about the mentioned topic. Thus, the interpretation of the metaphorical expression is not relevant to extract the inference relation.\nWe set aside the latter group as non-relevant cases, since we are not certain as to which extent the role of metaphor is significant in these occurrences. As a result, we discriminate three classes: a) pairs with metaphors that are relevant to the inference relationship, b) pairs with metaphors that are not relevant to the inference and c) pairs without metaphors.\nAnnotations were manually developed on ES text and were transferred to EN. Hence, we provide Meta4XNLIEN annotations as a silver standard for further refinement. Data about the number of samples and labels will be resumed in the following Subsection 3.3  ###reference_###  ###reference_###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Resulting Dataset",
            "text": "The outcome of this annotation process is Meta4XNLI, the first parallel dataset with labels for the tasks of metaphor detection and interpretation via NLI in ES and EN.\nThe parallel data for this task comprises a total of 13320 sentences annotated at token level, since we approached metaphor detection as a sequence labeling task, following the criteria of the cited previous work in Section 2  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nWith respect to Spanish annotations, there is a total of 1155 metaphorical tokens in premises and 1139 in hypotheses. Out of the 13320 sentences, 1873 contain at least one metaphorical expression, which constitutes the 14% of the whole dataset. This information is detailed in Table 4  ###reference_###  ###reference_###  ###reference_###  ###reference_### according to the source datasets partitions. A higher proportion of metaphors in premises can be noticed. This might be due to the fact that premises are of larger length than hypotheses, as indicated in Conneau et al. (2018  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###). In addition, premises were collected from existing utterances, while hypotheses were generated from crowd-source workers in response to a given set of premises. Therefore, hypotheses sentences might tend to present shorter length and lower complexity.\n###table_10### Regarding English annotations, we can observe a similar trend to that of ES in Table 5  ###reference_###  ###reference_###  ###reference_###  ###reference_###. A total of 3330 tokens were labeled as metaphor and 2736 sentences contain at least one metaphorical instance out of the total 13320 sentences. Premises show a higher metaphor ratio than hypotheses as in ES annotations. Additionally, we observe a larger amount of labeled metaphors in EN, caused by the different annotations processes specified in Subsection 3.2  ###reference_###  ###reference_###  ###reference_###  ###reference_###. Since VUAM contains a significantly higher amount of labeled metaphorical expressions, the MLM fine-tuned with this dataset predicted many ambiguous metaphors, which we subsequently removed in manual revision. These discrepancies are not only noticeable in annotations but also in experiments results. Moreover, this gives an account of how guidelines for metaphor identification labeling are open to discussion and clarification, due to the subjective and nuanced nature of this cognitive-linguistic phenomenon.\n###table_11### ###table_12### Annotations for this task were developed at premise-hypothesis level. As shown in Table 6  ###reference_###  ###reference_###  ###reference_###  ###reference_###, the average percentage of pairs with metaphors relevant to the inference relationship is 12%. This figure remains steady throughout each source dataset and inference labels. esXNLI shows a higher number of metaphor occurrence that might be caused by the difference of text domains and sentence characteristics with respect to XNLI data. Regarding non-relevant cases, we do not exploit them in the experiments in order to be able to analyse more clearly whether metaphor presence impacts models performance. We keep the same sample distribution for both languages to develop the experiments."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Inter-annotator Agreement",
            "text": "We selected a subset of 1000 pairs to the assess annotation quality in ES. The subset was labeled by another Spanish native speaker with a linguistics background. This annotator labeled the dataset subset from scratch and according to MIPVU guidelines in the case of detection, and the annotation process specified in Subsection 3.2  ###reference_###  ###reference_### for interpretation. We computed Cohen\u2019s Kappa score Cohen (1960  ###reference_b20###  ###reference_b20###) and obtained 0.74 in premises and 0.77 in hypotheses sentences for detection; for interpretation via the NLI task the obtained score was 0.64. Thus, these scores show a substantial agreement and are comparable to those obtained in the annotation of other datasets for metaphor detection and/or interpretation Steen et al. (2010  ###reference_b72###  ###reference_b72###); Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###); Maudslay and Teufel (2022  ###reference_b47###  ###reference_b47###); Badathala et al. (2023  ###reference_b7###  ###reference_b7###) thereby demonstrating the consistency of our annotations. Still, annotating metaphorical language remains a rather subjective task that requires exhaustive work and precise guidelines.\nWith the aim of developing cross-lingual experiments, we chose the multilingual language models and checkpoints that obtained best results in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###): mDeBERTa (base) and XLM-RoBERTa (large) Conneau et al. (2020  ###reference_b22###  ###reference_b22###  ###reference_b22###), the multilingual versions of DeBERTa He, Gao, and Chen (2021  ###reference_b29###  ###reference_b29###  ###reference_b29###) and RoBERTa Liu et al. (2019  ###reference_b44###  ###reference_b44###  ###reference_b44###) respectively.\nTaking advantage of previous available resources and the corpus we present in this work, we conducted a series of experiments to evaluate and fine-tune MLMs.\nThe configuration of cross-domain experiments is specified in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). For the other three setups, monolingual, multilingual and zero-shot cross-lingual, we performed hyperparameter tuning for batch size (8, 16, 36), linear decay (0.1, 0.01), learning rate (in the [1e-5-5e-5] interval), sequence length of 128 and epochs from 4 to 10. A warm-up of 6% is specified. The results of the hyperparameter tuning showed that after 4 epochs development loss started to increase, so results reported here are obtained with 4 epochs, batch size of 8, weight decay of 0.1 and learning rate of 5e-5.\nCross-domain: the aim of this set of experiments is to evaluate the performance of MLMs fine-tuned with CoMeta and VUAM datasets on Meta4XNLIES and Meta4XNLIEN, respectively, since each dataset contains texts from different domains. The motivation is to explore the impact of text features and genres on the performance, as well as annotation criteria Aghazadeh, Fayyaz, and\nYaghoobzadeh (2022  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###); Lai, Toral, and Nissim (2023  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###). To do so, we chose the models with best performance from monolingual experiments developed in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). We conducted the evaluation on various data splits: within each source dataset, XNLIdev, XNLItest and esXNLI, we evaluated premises and hypotheses separated and combined. This is due to the dissimilarities and the unequal distribution of metaphorical expressions between premises and hypotheses sentences mentioned in Subsection 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nMonolingual: this scenario comprises the fine-tuning and evaluation of the MLM on Meta4XNLIES and Meta4XNLIEN separately. To accomplish this task, we split Meta4XNLI into train, development and test sets (0.6-0.2-0.2). We equally distributed the data to ensure each partition is balanced in terms of source datasets sentences and metaphor occurrence. Data statistics are detailed in Appendix A  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. These partitions will be used as well in subsequent multilingual and zero-shot cross-lingual scenarios. In addition to fine-tuning and evaluation on Meta4XNLIES and Meta4XNLIEN, we evaluated each trained monolingual model with the test sets of CoMeta and VUAM, following the same reasoning as in cross-domain experiments.\nMultilingual: the purpose of these experiments is to explore whether MLMs benefit from being trained on data in multiple languages. In this case, we combined Meta4XNLIES and Meta4XNLIEN train splits to fine-tune the models. Subsequently, we evaluated the trained models on each language test set, in order to analyse the impact on the performance for each language. The data splits used correspond to those from monolingual experiments.\nZero-shot cross-lingual: in this scenario we explore to what extent MLMs are able to generalize knowledge and metaphor transfer between these two languages in question. Therefore, we fine-tune the models with Meta4XNLI data in one language and evaluate it on the test set of the other. Data partitions used for these experiments are the same as in monolingual an multilingual scenarios.\nWe carried out two sets of experiments to evaluate metaphor interpretation within the NLI task. We performed hyperparameter tuning, with the same range of parameters specified on the task of metaphor detection. We report best results obtained on the development set, after 4 epochs, batch size of 8, learning rate of 1e-5, weight decay of 0.1 and 512 sequence length.\nThe purpose of the first experiments is to examine if the presence of metaphorical expressions in premise-hypothesis pairs impacts the performance of models in the NLI task. To that end, we fine-tuned the MLMs for the task with the MultiNLI dataset. Then, we evaluated them with Meta4XNLI. Among each source dataset, we discriminated pairs with metaphors relevant to the inference relationship from those without metaphorical expressions. We developed the evaluation on each of these subsets and for each language separately, e.g. one evaluation on XNLIdev metaphor set and another on XNLIdev without metaphors set, in EN and then in ES.\nThe goal of the second set of experiments is to analyse the effect on models performance of not being \u201cexposed\u201d to instances with metaphorical expressions during training. With this aim in mind, we extracted pairs with and without metaphors from Meta4XNLI train, dev and set splits. On the first scenario, we fine-tuned the models only with pairs from the train set that did not contain any metaphorical expression. On the second scenario, we mixed pairs with and without metaphors and fine-tuned the models as well. In both cases, we evaluated on the test sets with and without metaphors for each language.\nIn addition to F1 score, we performed in-vocabluary (Inv) and out-of-vocabulary (Oov) evaluations to test the impact of the labels seen from training data during the learning process of the MLMs. Precision and Recall metrics are reported in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. For in-vocabulary evaluation, we computed F1 score taking into account predicted words seen in training labeled as metaphorical. While for out-of-vocabulary evaluation we considered only words not seen during training to compute the F1 score. We included this evaluation in all experiments but for the zero-shot cross-lingual setup, since the data of train and test sets are in different languages, thus the match of the exact same metaphorical token in both partitions is highly unlikely.\nAlthough the purpose of our experiments is not to beat state-of-the-art results but to evaluate the performance of MLMs on the task from a cross-lingual approach, we added two indicative baselines: on one hand, the system BasicBERT Li et al. (2023a  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###), which obtained 73.3 F1 score; on the other, the result of DeBERTa reported in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###), with 73.79 F1. We selected these results for comparison purposes, since they both were evaluated on the VUAM-2020 version of the dataset in EN used in the Shared Task 2020 Leong et al. (2020  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###) and in the same experimental setup we propose.\nCross-domain: We evaluated models trained on CoMeta and VUAM datasets with XNLI and esXNLI in ES and EN, respectively. From results reported in Table 7  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we can observe that in all cases mDeBERTa outperforms XLM-RoBERTa for ES. In EN the best result is obtained by DeBERTa, which also outperforms XLM-RoBERTa in all scenarios. In all datasets, except for XNLIdev in ES, premises sentences achieve better results than hypotheses and the combination of both. This goes in line with annotation statistics in Tables 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### and 5  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, which show that premises contain a greater ratio of metaphors per sentence than hypotheses in both languages. In ES, in-vocabulary evaluation outperforms the general F1 score while out-of-vocabulary evaluation results decrease. The small distance in points of this cross-domain evaluation in ES exhibit stability of the models when it comes to predicting metaphors and coherence in annotations between both datasets despite the difference of text domains. Nevertheless, EN results do not demonstrate such consistency, as out-of-vocabulary obtains higher results than the overall F1 score. This might be due to the different labeling criteria used in VUAM and Meta4XNLI, as we mentioned in 3.2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. These discrepancies are also reflected in a significant drop in performance with respect to ES experiments and in-domain evaluation with VUAM. The high recall scores from Table 16  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### show that the model tends to predict many metaphors, however, low precision scores indicate that a small amount of these predictions are correct.\n.\nMonolingual: Results from this set of experiments are specified in Table 8  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. After fine-tuning and evaluating the models with Meta4XNLIES and Meta4XNLIEN, the highest overall F1 score is obtained by mDeBERTa in ES. On the other hand, XLM-RoBERTa achieves better performance than mDeBERTa in EN but still lower than in ES. In both languages, in-vocabulary evaluation results are higher than the overall ones and out-of-vocabulary results, lower. This is not the case when we use VUAM corpus for testing the model fine-tuned with Meta4XNLIEN. In this setup, similarly to results from cross-domain experiments, performance drastically falls to 32.42 F1 score and out-of-vocabulary results are the highest. The reason behind might be the mismatches in the annotation process. In ES, when evaluating CoMeta, we also encounter a decrease in the performance, however, it is more subtle: from 67.17 to 56.24, obtained by XLM-RoBERTa. This might be caused by the variety of text genres and dissimilarities between sentences from each dataset.\nMultilingual: In this set of experiments we assembled Meta4XNLIEN and Meta4XNLIES to train the MLMs. The evaluation is conducted for each language separately and results detailed in Table 9  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. Best results in ES are obtained by mDeBERTa, which are higher than the top result from monolingual experiments. In EN, mDeBERTa is the model that achieves better performance but very close to that of XLM-RoBERTa. The highest F1 score is 8 points lower than that of ES but outweighs EN monolingual results. This suggests that the combination of parallel multilingual data for training is beneficial for the performance of the models.\nZero-shot cross-lingual: In these experiments we perform evaluation of Meta4XNLI in the opposite language to that utilised for training. Results are reported in Table 10  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. XLM-RoBERTa performance exceeds that of mDeBERTa in both languages. Nonetheless, F1 score for EN is almost 20 points lower than ES evaluation results. In addition to the differences in annotation criteria between languages and datasets, another aspect to bear in mind in this scenario could be the number of positive examples present in training sets. As we explained in Section 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, Meta4XNLIEN contains a higher number of metaphorical instances, thus models are exposed to a greater variety of examples that can be transferred to ES. While a more reduced amount of instances in Meta4XNLIES seen during training might hinder model\u2019s generalization ability, as the low recall and high precision scores show in Table 19  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nIn the first setup, we fine-tuned the models for the NLI task with MultiNLI dataset Williams, Nangia, and Bowman (2018  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###). Then we conducted the evaluation with two different splits for each source dataset conforming Meta4XNLI: pairs with at least one metaphorical expression and pairs lacking metaphors. From accuracy scores reported in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### we can observe certain variability in the results. In the majority of cases, XLM-RoBERTa achieves better performance than mDeBERTa. We do observe a tendency of higher results on the sets of pairs without metaphors than on the set with metaphors. The exceptions to this current are the results from XLM-RoBERTa for XNLIdev and XNLItest in ES. In these partitions, the subset of pairs with metaphorical expressions obtained better results, although the difference does not even reach one point. We hypothesize the original language of the dataset might be involved in this performance, since XNLI includes natural utterances of EN that were afterwards manually translated to ES, thus some artifacts might have been introduced during this process Artetxe, Labaka, and Agirre (2020  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###).\nThe second scenario consisted in fine-tuning the models on two setups: a) with pairs without metaphors and b) pairs with and without metaphors. We performed the evaluation on subsets split by the criterion of metaphor occurrence as well. Results reported in Table 12  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### show XLM-RoBERTa outperforming mDeBERTa in all contexts. Both models show best results for the NLI task on the examples without metaphors and the lowest performance with pairs that contain metaphorical expressions. This outcome replicates in both languages and experimental setups a) and b).\nWe selected the predictions from the model that obtained highest F1 score from the monolingual experiments in ES for both Meta4XNLI and CoMeta evaluations. We extracted false negatives and false positives and grouped the tokens by their number of occurrences. Within the false positives of CoMeta test set, we find tokens like\napoyo (lit. \u201csupport\u201d) or tensi\u00f3n (lit. \u201ctension\u201d) that appear more frequently used figuratively than with their literal sense or even only appear in the training set labeled as metaphor. Other wrong predictions are words that appeared in specific domains, such as texts that allude to the pandemic, e.g. ola (lit. \u201cwave\u201d), not detected as metaphorical due to its absence with metaphorical meaning in Meta4XNLI sentences. Something similar occurs with the misclassified tokens from Meta4XNLI test set. Most errors stem from conventional metaphors, namely gran, abrir, paso, claro (lit. \u201cgreat\u201d, \u201cto open\u201d, \u201cstep\u201d, \u201cclear\u201d) that occur regularly used with their metaphorical meaning. The lack of balanced examples might contribute to these predictions. However, we maintained the distribution as is, since our aim is to study the presence and prevalence of metaphor in natural language utterances.\nWe analysed a subset of 30 errors from each experimental setup, both EN and ES, and evaluation sets. We chose the predictions from XLM-RoBERTa since it is the model that performs better in this task in most scenarios. Although results show that models struggle more to identify the inference relation if there is a metaphorical expression involved in the pair sentences, we do not observe any particular feature within the errors. A remarkable aspect to highlight is the lower results of esXNLI with respect to XNLIdev and XNLItest in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. It could be motivated by the difference of the text domains, since XNLI is an extension of MultiNLI, which maintains the same set of textual domains. While esXNLI is a collection of texts from another set of genres and sources.\nRegarding EN, some of the errors might derive from the misclassification of some pairs, since annotations were developed on ES text. A metaphorical expression involved in the inference relationship in ES might not be present in its EN version and vice versa. Thus, samples from these two classes, pairs with and without metaphors, should be reexamined in EN and correctly classified for future experimentation."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental setup",
            "text": "With the aim of developing cross-lingual experiments, we chose the multilingual language models and checkpoints that obtained best results in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###): mDeBERTa (base) and XLM-RoBERTa (large) Conneau et al. (2020  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###), the multilingual versions of DeBERTa He, Gao, and Chen (2021  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###) and RoBERTa Liu et al. (2019  ###reference_b44###  ###reference_b44###  ###reference_b44###  ###reference_b44###) respectively.\nTaking advantage of previous available resources and the corpus we present in this work, we conducted a series of experiments to evaluate and fine-tune MLMs.\nThe configuration of cross-domain experiments is specified in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). For the other three setups, monolingual, multilingual and zero-shot cross-lingual, we performed hyperparameter tuning for batch size (8, 16, 36), linear decay (0.1, 0.01), learning rate (in the [1e-5-5e-5] interval), sequence length of 128 and epochs from 4 to 10. A warm-up of 6% is specified. The results of the hyperparameter tuning showed that after 4 epochs development loss started to increase, so results reported here are obtained with 4 epochs, batch size of 8, weight decay of 0.1 and learning rate of 5e-5.\nCross-domain: the aim of this set of experiments is to evaluate the performance of MLMs fine-tuned with CoMeta and VUAM datasets on Meta4XNLIES and Meta4XNLIEN, respectively, since each dataset contains texts from different domains. The motivation is to explore the impact of text features and genres on the performance, as well as annotation criteria Aghazadeh, Fayyaz, and\nYaghoobzadeh (2022  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###); Lai, Toral, and Nissim (2023  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###). To do so, we chose the models with best performance from monolingual experiments developed in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). We conducted the evaluation on various data splits: within each source dataset, XNLIdev, XNLItest and esXNLI, we evaluated premises and hypotheses separated and combined. This is due to the dissimilarities and the unequal distribution of metaphorical expressions between premises and hypotheses sentences mentioned in Subsection 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nMonolingual: this scenario comprises the fine-tuning and evaluation of the MLM on Meta4XNLIES and Meta4XNLIEN separately. To accomplish this task, we split Meta4XNLI into train, development and test sets (0.6-0.2-0.2). We equally distributed the data to ensure each partition is balanced in terms of source datasets sentences and metaphor occurrence. Data statistics are detailed in Appendix A  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. These partitions will be used as well in subsequent multilingual and zero-shot cross-lingual scenarios. In addition to fine-tuning and evaluation on Meta4XNLIES and Meta4XNLIEN, we evaluated each trained monolingual model with the test sets of CoMeta and VUAM, following the same reasoning as in cross-domain experiments.\nMultilingual: the purpose of these experiments is to explore whether MLMs benefit from being trained on data in multiple languages. In this case, we combined Meta4XNLIES and Meta4XNLIEN train splits to fine-tune the models. Subsequently, we evaluated the trained models on each language test set, in order to analyse the impact on the performance for each language. The data splits used correspond to those from monolingual experiments.\nZero-shot cross-lingual: in this scenario we explore to what extent MLMs are able to generalize knowledge and metaphor transfer between these two languages in question. Therefore, we fine-tune the models with Meta4XNLI data in one language and evaluate it on the test set of the other. Data partitions used for these experiments are the same as in monolingual an multilingual scenarios.\nWe carried out two sets of experiments to evaluate metaphor interpretation within the NLI task. We performed hyperparameter tuning, with the same range of parameters specified on the task of metaphor detection. We report best results obtained on the development set, after 4 epochs, batch size of 8, learning rate of 1e-5, weight decay of 0.1 and 512 sequence length.\nThe purpose of the first experiments is to examine if the presence of metaphorical expressions in premise-hypothesis pairs impacts the performance of models in the NLI task. To that end, we fine-tuned the MLMs for the task with the MultiNLI dataset. Then, we evaluated them with Meta4XNLI. Among each source dataset, we discriminated pairs with metaphors relevant to the inference relationship from those without metaphorical expressions. We developed the evaluation on each of these subsets and for each language separately, e.g. one evaluation on XNLIdev metaphor set and another on XNLIdev without metaphors set, in EN and then in ES.\nThe goal of the second set of experiments is to analyse the effect on models performance of not being \u201cexposed\u201d to instances with metaphorical expressions during training. With this aim in mind, we extracted pairs with and without metaphors from Meta4XNLI train, dev and set splits. On the first scenario, we fine-tuned the models only with pairs from the train set that did not contain any metaphorical expression. On the second scenario, we mixed pairs with and without metaphors and fine-tuned the models as well. In both cases, we evaluated on the test sets with and without metaphors for each language.\nIn addition to F1 score, we performed in-vocabluary (Inv) and out-of-vocabulary (Oov) evaluations to test the impact of the labels seen from training data during the learning process of the MLMs. Precision and Recall metrics are reported in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. For in-vocabulary evaluation, we computed F1 score taking into account predicted words seen in training labeled as metaphorical. While for out-of-vocabulary evaluation we considered only words not seen during training to compute the F1 score. We included this evaluation in all experiments but for the zero-shot cross-lingual setup, since the data of train and test sets are in different languages, thus the match of the exact same metaphorical token in both partitions is highly unlikely.\nAlthough the purpose of our experiments is not to beat state-of-the-art results but to evaluate the performance of MLMs on the task from a cross-lingual approach, we added two indicative baselines: on one hand, the system BasicBERT Li et al. (2023a  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###), which obtained 73.3 F1 score; on the other, the result of DeBERTa reported in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###), with 73.79 F1. We selected these results for comparison purposes, since they both were evaluated on the VUAM-2020 version of the dataset in EN used in the Shared Task 2020 Leong et al. (2020  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###) and in the same experimental setup we propose.\nCross-domain: We evaluated models trained on CoMeta and VUAM datasets with XNLI and esXNLI in ES and EN, respectively. From results reported in Table 7  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we can observe that in all cases mDeBERTa outperforms XLM-RoBERTa for ES. In EN the best result is obtained by DeBERTa, which also outperforms XLM-RoBERTa in all scenarios. In all datasets, except for XNLIdev in ES, premises sentences achieve better results than hypotheses and the combination of both. This goes in line with annotation statistics in Tables 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### and 5  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, which show that premises contain a greater ratio of metaphors per sentence than hypotheses in both languages. In ES, in-vocabulary evaluation outperforms the general F1 score while out-of-vocabulary evaluation results decrease. The small distance in points of this cross-domain evaluation in ES exhibit stability of the models when it comes to predicting metaphors and coherence in annotations between both datasets despite the difference of text domains. Nevertheless, EN results do not demonstrate such consistency, as out-of-vocabulary obtains higher results than the overall F1 score. This might be due to the different labeling criteria used in VUAM and Meta4XNLI, as we mentioned in 3.2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. These discrepancies are also reflected in a significant drop in performance with respect to ES experiments and in-domain evaluation with VUAM. The high recall scores from Table 16  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### show that the model tends to predict many metaphors, however, low precision scores indicate that a small amount of these predictions are correct.\n.\nMonolingual: Results from this set of experiments are specified in Table 8  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. After fine-tuning and evaluating the models with Meta4XNLIES and Meta4XNLIEN, the highest overall F1 score is obtained by mDeBERTa in ES. On the other hand, XLM-RoBERTa achieves better performance than mDeBERTa in EN but still lower than in ES. In both languages, in-vocabulary evaluation results are higher than the overall ones and out-of-vocabulary results, lower. This is not the case when we use VUAM corpus for testing the model fine-tuned with Meta4XNLIEN. In this setup, similarly to results from cross-domain experiments, performance drastically falls to 32.42 F1 score and out-of-vocabulary results are the highest. The reason behind might be the mismatches in the annotation process. In ES, when evaluating CoMeta, we also encounter a decrease in the performance, however, it is more subtle: from 67.17 to 56.24, obtained by XLM-RoBERTa. This might be caused by the variety of text genres and dissimilarities between sentences from each dataset.\nMultilingual: In this set of experiments we assembled Meta4XNLIEN and Meta4XNLIES to train the MLMs. The evaluation is conducted for each language separately and results detailed in Table 9  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. Best results in ES are obtained by mDeBERTa, which are higher than the top result from monolingual experiments. In EN, mDeBERTa is the model that achieves better performance but very close to that of XLM-RoBERTa. The highest F1 score is 8 points lower than that of ES but outweighs EN monolingual results. This suggests that the combination of parallel multilingual data for training is beneficial for the performance of the models.\nZero-shot cross-lingual: In these experiments we perform evaluation of Meta4XNLI in the opposite language to that utilised for training. Results are reported in Table 10  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. XLM-RoBERTa performance exceeds that of mDeBERTa in both languages. Nonetheless, F1 score for EN is almost 20 points lower than ES evaluation results. In addition to the differences in annotation criteria between languages and datasets, another aspect to bear in mind in this scenario could be the number of positive examples present in training sets. As we explained in Section 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, Meta4XNLIEN contains a higher number of metaphorical instances, thus models are exposed to a greater variety of examples that can be transferred to ES. While a more reduced amount of instances in Meta4XNLIES seen during training might hinder model\u2019s generalization ability, as the low recall and high precision scores show in Table 19  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nIn the first setup, we fine-tuned the models for the NLI task with MultiNLI dataset Williams, Nangia, and Bowman (2018  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###). Then we conducted the evaluation with two different splits for each source dataset conforming Meta4XNLI: pairs with at least one metaphorical expression and pairs lacking metaphors. From accuracy scores reported in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### we can observe certain variability in the results. In the majority of cases, XLM-RoBERTa achieves better performance than mDeBERTa. We do observe a tendency of higher results on the sets of pairs without metaphors than on the set with metaphors. The exceptions to this current are the results from XLM-RoBERTa for XNLIdev and XNLItest in ES. In these partitions, the subset of pairs with metaphorical expressions obtained better results, although the difference does not even reach one point. We hypothesize the original language of the dataset might be involved in this performance, since XNLI includes natural utterances of EN that were afterwards manually translated to ES, thus some artifacts might have been introduced during this process Artetxe, Labaka, and Agirre (2020  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###).\nThe second scenario consisted in fine-tuning the models on two setups: a) with pairs without metaphors and b) pairs with and without metaphors. We performed the evaluation on subsets split by the criterion of metaphor occurrence as well. Results reported in Table 12  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### show XLM-RoBERTa outperforming mDeBERTa in all contexts. Both models show best results for the NLI task on the examples without metaphors and the lowest performance with pairs that contain metaphorical expressions. This outcome replicates in both languages and experimental setups a) and b).\nWe selected the predictions from the model that obtained highest F1 score from the monolingual experiments in ES for both Meta4XNLI and CoMeta evaluations. We extracted false negatives and false positives and grouped the tokens by their number of occurrences. Within the false positives of CoMeta test set, we find tokens like\napoyo (lit. \u201csupport\u201d) or tensi\u00f3n (lit. \u201ctension\u201d) that appear more frequently used figuratively than with their literal sense or even only appear in the training set labeled as metaphor. Other wrong predictions are words that appeared in specific domains, such as texts that allude to the pandemic, e.g. ola (lit. \u201cwave\u201d), not detected as metaphorical due to its absence with metaphorical meaning in Meta4XNLI sentences. Something similar occurs with the misclassified tokens from Meta4XNLI test set. Most errors stem from conventional metaphors, namely gran, abrir, paso, claro (lit. \u201cgreat\u201d, \u201cto open\u201d, \u201cstep\u201d, \u201cclear\u201d) that occur regularly used with their metaphorical meaning. The lack of balanced examples might contribute to these predictions. However, we maintained the distribution as is, since our aim is to study the presence and prevalence of metaphor in natural language utterances.\nWe analysed a subset of 30 errors from each experimental setup, both EN and ES, and evaluation sets. We chose the predictions from XLM-RoBERTa since it is the model that performs better in this task in most scenarios. Although results show that models struggle more to identify the inference relation if there is a metaphorical expression involved in the pair sentences, we do not observe any particular feature within the errors. A remarkable aspect to highlight is the lower results of esXNLI with respect to XNLIdev and XNLItest in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. It could be motivated by the difference of the text domains, since XNLI is an extension of MultiNLI, which maintains the same set of textual domains. While esXNLI is a collection of texts from another set of genres and sources.\nRegarding EN, some of the errors might derive from the misclassification of some pairs, since annotations were developed on ES text. A metaphorical expression involved in the inference relationship in ES might not be present in its EN version and vice versa. Thus, samples from these two classes, pairs with and without metaphors, should be reexamined in EN and correctly classified for future experimentation."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Meta4XNLI Corpus",
            "text": "To address these shortcomings, we introduce Meta4XNLI, a parallel dataset in ES and EN with metaphorical annotations for detection and interpretation via NLI in texts of natural language utterances and multiple domains. In the following subsections, we describe the collection of the dataset (Subsection 3.1  ###reference_###  ###reference_###  ###reference_###), the methodology we employed to annotate metaphors for each task and language (Subsection 3.2  ###reference_###  ###reference_###  ###reference_###) and, as a result, we will provide details of the outcoming dataset of this process (Subsection 3.3  ###reference_###  ###reference_###  ###reference_###).\nThe annotation process in this language comprehends two phases, as depicted in Figure 2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. First, we automatically label Meta4XNLIES by leveraging mDeBERTa He, Gao, and Chen (2021  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###) fine-tuned for metaphor detection in ES with CoMeta. We choose mDeBERTa since it is the multilingual model that achieved highest F1 score in the experimental setup of Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). This choice aims at reducing the heavy work load and time investment that manual annotation from scratch requires. Afterwards, we manually inspect and correct the predictions in the whole dataset. From the first phase of automatic labeling, 748 tokens were predicted as metaphor in the premises and 724 in the hypotheses. After a complete and manual revision of all sentences, we removed 74 tokens and added 481 undetected metaphors in the premises; whereas in the hypotheses, we deleted 118 false positives and labeled 533 false negatives.\n###figure_13### The main sources of ambiguity in ES emerge from multi-word expressions (MWE) and polysemy.\nThe main issue when labeling MWE is to decide whether to treat the MWE as a single lexical unit, a fixed expression where the meaning of each word is not transparent anymore, or if it should be regarded as a collocation. Collocations are expressions where the constituent words tend to co-occur with high frequency but are not fixed, since each of its elements can be replaced by others with similar meaning.\nDijo que era hora de entrar en p\u00e1nico (lit. \u201cThey said it was time to enter into panic\u201d).\nFor instance, in Example 2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, the expression entrar en p\u00e1nico could be initially considered as a MWE with a single lexical unit, therefore, the three tokens would be labeled as metaphorical. However, this expression specifically means \u201cto panic\u201d, in which the verb entrar (lit. \u201cto enter\u201d) holds metaphorical meaning, since \u201cpanic\u201d is not a physical place you can get into. In this case, we do not think of the expression as a fixed MWE, since the verb is also used metaphorically with other terms that are not places, like entrar en c\u00f3lera (\u201cto get angry\u201d, lit. \u201cto enter into wrath\u201d) or entrar en calor (\u201cto feel hot\u201d, lit. \u201cto enter into heat\u201d). In all of these expressions the verb is conveying the sense of starting to feel the noun it complements, as if by entering to a place it transformed our sensitivity. The association of concepts arises from understanding emotions or sensations as physical locations.\nRegarding polysemy, the existence of multiple and very nuanced senses associated to the same token can lead to confusion. It can be challenging to determine whether the basic meaning of the lexical unit is currently and generally known and used by native speakers or if they directly associate the lexical unit to the figurative meaning, not identifying the basic meaning at all.\n[\u2026] ha mostrado su apoyo a la candidatura para ser sede [\u2026] (lit. \u201cThey showed their support to the candidacy to be head office\u201d).\nFor instance, in Example 2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### we label apoyo as metaphorical, since the most basic meaning of the verb apoyar in the dictionary defines it as \u201cto make something rest upon another thing\u201d. In this sentence, the contextual meaning refers to someone in favour of someone else\u2019s goal. Figuratively, the goal can be understood in terms of a physical object so heavy that requires more than one anchor point to distribute its weight. Doubts in this examples come from the fact that the figurative sense can be more used that its basic one, thus speakers might not identify the metaphorical meaning as such. We make use of the Diccionario de la Real Academia Espa\u00f1ola (DRAE) as a tool to help us clarify these ambiguous cases. Nonetheless, metaphor identification remains a highly subjective task.\nWe develop Meta4XNLIEN annotations semi-automatically and based on ES annotations, since our purpose is to publish a parallel resource to counterbalance monolingual English labeled data. These annotations serve as a starting point for further refinement and enable potential analysis of metaphors shared across these two languages. The whole process comprises mainly four phases, as depicted in Figure 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nThe first step involves the projection of ES labels onto EN sentences. For this end, we utilised Easy-Label-Projection Garc\u00eda-Ferrero, Agerri, and\nRigau (2022  ###reference_b28###  ###reference_b28###  ###reference_b28###  ###reference_b28###  ###reference_b28###  ###reference_b28###  ###reference_b28###), a tool developed for cross-lingual sequence labeling that makes use of word alignments and data- and model-transfer to project the labels from a source language (ES) to an untagged target language (EN). This mechanism is suitable when the labeled entity is certainly appearing in both source and target sentences, however, metaphors present in a source sentence are not necessarily in its translation. It depends on a series of multiple factors, namely the type of translation, the knowledge of the translator, if it is human-translated, or socio-cultural knowledge, among others. The next step comprises the manual revision of the projections.\nIn order to alleviate this issue, we made use of XLM-RoBERTa Conneau et al. (2020  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###) trained on the VUAM dataset, as it is the multilingual model that showed best performance in the experiments in EN from the work of Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). Since projected sentences were manually reviewed (13% of the total), we only predicted automatically those sentences that did not receive a projection in the first place (87%). We manually reviewed the outcoming predictions from the MLM to correct errors and undetected metaphorical expressions following the MIPVU procedure. The number of metaphorical tokens extracted in each annotation phase is detailed in Table 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nThere are some issues that emerged during the whole annotation process. Firstly, the projections phase entails that for a metaphorical expression to be annotated in EN, it has to have been annotated in ES in advance. Hence, metaphors in EN sentences that were not expressed figuratively in their ES counterpart will not be spotted, as in Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. In the ES sentence there is not a metaphorical expression annotated as such, however, in the EN version, the adjective heavy is holding metaphorical meaning, as a synonym of \u201cdemanding\u201d, which is expressed literally in ES with the adjective exigentes. In this kind of cases where the metaphor is \u201cgained in translation\u201d, the lack of annotation in the source language implies no label will be projected onto the target sentence, missing a metaphorical instance. These examples give an account of how translation and the singularities of metaphors according to languages may affect to the annotation process of this task.\n###figure_14### .\nA los usuarios m\u00e1s exigentes se les deber\u00eda cobrar m\u00e1s. \nThe heavy users should be charged the most.\nAnother issue that should be noted are false positives: all ES sentences with one or more labeled metaphors will transfer those tags to the EN sentence, regardless the translated tokens hold metaphoric meaning or not. To solve this question, we manually reviewed all sentences that received a projected tag. In this revision we had to remove metaphors that were \u201clost in translation\u201d and adjust some spans projected to the target language, e.g. some annotated verbs in ES were projected in EN to the subject pronoun and verb, since some ES verbs forms are synthetic and include the person information in a single morpheme. Therefore, we eliminated the label from the pronoun and maintained only that of the verb. In Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### we can see how the verb peleaban (lit. \u201cthey fought\u201d) labeled as metaphorical in the ES sentence (Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###) was projected to the subject and verb in EN (Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###). Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### represents the definitive version of the annotations after manual revision.\nPeleaban por lo ricos que eran los directores ejecutivos.\nThey fought about how rich CEOs were.\nThey fought about how rich CEOs were.\nRegarding the subset of sentences that were automatically labeled by XLM-RoBERTa, some concerns emerged with respect to annotations present in VUAM dataset during the phase of manual revision. Especially when it comes to phrasal verbs and abstract and polysemous terms that are lexicalised. For instance, due to Examples like 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### or 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### labeled as metaphorical in the training set VUAM, we observed a tendency to overannotate verbs that conform phrasal verbs as metaphorical, such as get, look, made or go. In most cases, these verbs appear in a context where they do not add any strong semantic information due to their lexicalization. Hence, we unmarked these instances that were predicted as metaphorical. Following this line of thought, we also discarded abstract and vague terms that are common-places of spontaneous discourse, like the word thing in Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, since it can refer to any kind of entity, either concrete or abstract and might not be directly matched to a more broadly used basic meaning.\nHis lack of humbug about political balance has always made him more honest than all the employees [\u2026].\n\\enumsentenceTake what you want and leave the rest , your mother \u2019ll get rid of it . \n\\enumsentenceOne thing always linked to another thing.\nThe parallel data for this task comprises a total of 13320 sentences annotated at token level, since we approached metaphor detection as a sequence labeling task, following the criteria of the cited previous work in Section 2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nWith respect to Spanish annotations, there is a total of 1155 metaphorical tokens in premises and 1139 in hypotheses. Out of the 13320 sentences, 1873 contain at least one metaphorical expression, which constitutes the 14% of the whole dataset. This information is detailed in Table 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### according to the source datasets partitions. A higher proportion of metaphors in premises can be noticed. This might be due to the fact that premises are of larger length than hypotheses, as indicated in Conneau et al. (2018  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###). In addition, premises were collected from existing utterances, while hypotheses were generated from crowd-source workers in response to a given set of premises. Therefore, hypotheses sentences might tend to present shorter length and lower complexity.\n###table_13### Regarding English annotations, we can observe a similar trend to that of ES in Table 5  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. A total of 3330 tokens were labeled as metaphor and 2736 sentences contain at least one metaphorical instance out of the total 13320 sentences. Premises show a higher metaphor ratio than hypotheses as in ES annotations. Additionally, we observe a larger amount of labeled metaphors in EN, caused by the different annotations processes specified in Subsection 3.2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. Since VUAM contains a significantly higher amount of labeled metaphorical expressions, the MLM fine-tuned with this dataset predicted many ambiguous metaphors, which we subsequently removed in manual revision. These discrepancies are not only noticeable in annotations but also in experiments results. Moreover, this gives an account of how guidelines for metaphor identification labeling are open to discussion and clarification, due to the subjective and nuanced nature of this cognitive-linguistic phenomenon.\n###table_14### ###table_15### Annotations for this task were developed at premise-hypothesis level. As shown in Table 6  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, the average percentage of pairs with metaphors relevant to the inference relationship is 12%. This figure remains steady throughout each source dataset and inference labels. esXNLI shows a higher number of metaphor occurrence that might be caused by the difference of text domains and sentence characteristics with respect to XNLI data. Regarding non-relevant cases, we do not exploit them in the experiments in order to be able to analyse more clearly whether metaphor presence impacts models performance. We keep the same sample distribution for both languages to develop the experiments.\nIn this section we present the experimental setup designed with the objective of testing the capabilities of multilingual MLMs on metaphor detection in cross-domain, crosslingual and multilingual settings. Furthermore, we also experiment with their ability to perform NLI when the correct inference requires understanding of metaphorical language.\nTaking advantage of previous available resources and the corpus we present in this work, we conducted a series of experiments to evaluate and fine-tune MLMs.\nThe configuration of cross-domain experiments is specified in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). For the other three setups, monolingual, multilingual and zero-shot cross-lingual, we performed hyperparameter tuning for batch size (8, 16, 36), linear decay (0.1, 0.01), learning rate (in the [1e-5-5e-5] interval), sequence length of 128 and epochs from 4 to 10. A warm-up of 6% is specified. The results of the hyperparameter tuning showed that after 4 epochs development loss started to increase, so results reported here are obtained with 4 epochs, batch size of 8, weight decay of 0.1 and learning rate of 5e-5.\nCross-domain: the aim of this set of experiments is to evaluate the performance of MLMs fine-tuned with CoMeta and VUAM datasets on Meta4XNLIES and Meta4XNLIEN, respectively, since each dataset contains texts from different domains. The motivation is to explore the impact of text features and genres on the performance, as well as annotation criteria Aghazadeh, Fayyaz, and\nYaghoobzadeh (2022  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###); Lai, Toral, and Nissim (2023  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###). To do so, we chose the models with best performance from monolingual experiments developed in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). We conducted the evaluation on various data splits: within each source dataset, XNLIdev, XNLItest and esXNLI, we evaluated premises and hypotheses separated and combined. This is due to the dissimilarities and the unequal distribution of metaphorical expressions between premises and hypotheses sentences mentioned in Subsection 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nMonolingual: this scenario comprises the fine-tuning and evaluation of the MLM on Meta4XNLIES and Meta4XNLIEN separately. To accomplish this task, we split Meta4XNLI into train, development and test sets (0.6-0.2-0.2). We equally distributed the data to ensure each partition is balanced in terms of source datasets sentences and metaphor occurrence. Data statistics are detailed in Appendix A  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. These partitions will be used as well in subsequent multilingual and zero-shot cross-lingual scenarios. In addition to fine-tuning and evaluation on Meta4XNLIES and Meta4XNLIEN, we evaluated each trained monolingual model with the test sets of CoMeta and VUAM, following the same reasoning as in cross-domain experiments.\nMultilingual: the purpose of these experiments is to explore whether MLMs benefit from being trained on data in multiple languages. In this case, we combined Meta4XNLIES and Meta4XNLIEN train splits to fine-tune the models. Subsequently, we evaluated the trained models on each language test set, in order to analyse the impact on the performance for each language. The data splits used correspond to those from monolingual experiments.\nZero-shot cross-lingual: in this scenario we explore to what extent MLMs are able to generalize knowledge and metaphor transfer between these two languages in question. Therefore, we fine-tune the models with Meta4XNLI data in one language and evaluate it on the test set of the other. Data partitions used for these experiments are the same as in monolingual an multilingual scenarios.\nWe carried out two sets of experiments to evaluate metaphor interpretation within the NLI task. We performed hyperparameter tuning, with the same range of parameters specified on the task of metaphor detection. We report best results obtained on the development set, after 4 epochs, batch size of 8, learning rate of 1e-5, weight decay of 0.1 and 512 sequence length.\nThe purpose of the first experiments is to examine if the presence of metaphorical expressions in premise-hypothesis pairs impacts the performance of models in the NLI task. To that end, we fine-tuned the MLMs for the task with the MultiNLI dataset. Then, we evaluated them with Meta4XNLI. Among each source dataset, we discriminated pairs with metaphors relevant to the inference relationship from those without metaphorical expressions. We developed the evaluation on each of these subsets and for each language separately, e.g. one evaluation on XNLIdev metaphor set and another on XNLIdev without metaphors set, in EN and then in ES.\nThe goal of the second set of experiments is to analyse the effect on models performance of not being \u201cexposed\u201d to instances with metaphorical expressions during training. With this aim in mind, we extracted pairs with and without metaphors from Meta4XNLI train, dev and set splits. On the first scenario, we fine-tuned the models only with pairs from the train set that did not contain any metaphorical expression. On the second scenario, we mixed pairs with and without metaphors and fine-tuned the models as well. In both cases, we evaluated on the test sets with and without metaphors for each language.\nIn addition to F1 score, we performed in-vocabluary (Inv) and out-of-vocabulary (Oov) evaluations to test the impact of the labels seen from training data during the learning process of the MLMs. Precision and Recall metrics are reported in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. For in-vocabulary evaluation, we computed F1 score taking into account predicted words seen in training labeled as metaphorical. While for out-of-vocabulary evaluation we considered only words not seen during training to compute the F1 score. We included this evaluation in all experiments but for the zero-shot cross-lingual setup, since the data of train and test sets are in different languages, thus the match of the exact same metaphorical token in both partitions is highly unlikely.\nAlthough the purpose of our experiments is not to beat state-of-the-art results but to evaluate the performance of MLMs on the task from a cross-lingual approach, we added two indicative baselines: on one hand, the system BasicBERT Li et al. (2023a  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###), which obtained 73.3 F1 score; on the other, the result of DeBERTa reported in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###), with 73.79 F1. We selected these results for comparison purposes, since they both were evaluated on the VUAM-2020 version of the dataset in EN used in the Shared Task 2020 Leong et al. (2020  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###) and in the same experimental setup we propose.\nCross-domain: We evaluated models trained on CoMeta and VUAM datasets with XNLI and esXNLI in ES and EN, respectively. From results reported in Table 7  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we can observe that in all cases mDeBERTa outperforms XLM-RoBERTa for ES. In EN the best result is obtained by DeBERTa, which also outperforms XLM-RoBERTa in all scenarios. In all datasets, except for XNLIdev in ES, premises sentences achieve better results than hypotheses and the combination of both. This goes in line with annotation statistics in Tables 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### and 5  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, which show that premises contain a greater ratio of metaphors per sentence than hypotheses in both languages. In ES, in-vocabulary evaluation outperforms the general F1 score while out-of-vocabulary evaluation results decrease. The small distance in points of this cross-domain evaluation in ES exhibit stability of the models when it comes to predicting metaphors and coherence in annotations between both datasets despite the difference of text domains. Nevertheless, EN results do not demonstrate such consistency, as out-of-vocabulary obtains higher results than the overall F1 score. This might be due to the different labeling criteria used in VUAM and Meta4XNLI, as we mentioned in 3.2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. These discrepancies are also reflected in a significant drop in performance with respect to ES experiments and in-domain evaluation with VUAM. The high recall scores from Table 16  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### show that the model tends to predict many metaphors, however, low precision scores indicate that a small amount of these predictions are correct.\n.\nMonolingual: Results from this set of experiments are specified in Table 8  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. After fine-tuning and evaluating the models with Meta4XNLIES and Meta4XNLIEN, the highest overall F1 score is obtained by mDeBERTa in ES. On the other hand, XLM-RoBERTa achieves better performance than mDeBERTa in EN but still lower than in ES. In both languages, in-vocabulary evaluation results are higher than the overall ones and out-of-vocabulary results, lower. This is not the case when we use VUAM corpus for testing the model fine-tuned with Meta4XNLIEN. In this setup, similarly to results from cross-domain experiments, performance drastically falls to 32.42 F1 score and out-of-vocabulary results are the highest. The reason behind might be the mismatches in the annotation process. In ES, when evaluating CoMeta, we also encounter a decrease in the performance, however, it is more subtle: from 67.17 to 56.24, obtained by XLM-RoBERTa. This might be caused by the variety of text genres and dissimilarities between sentences from each dataset.\nMultilingual: In this set of experiments we assembled Meta4XNLIEN and Meta4XNLIES to train the MLMs. The evaluation is conducted for each language separately and results detailed in Table 9  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. Best results in ES are obtained by mDeBERTa, which are higher than the top result from monolingual experiments. In EN, mDeBERTa is the model that achieves better performance but very close to that of XLM-RoBERTa. The highest F1 score is 8 points lower than that of ES but outweighs EN monolingual results. This suggests that the combination of parallel multilingual data for training is beneficial for the performance of the models.\nZero-shot cross-lingual: In these experiments we perform evaluation of Meta4XNLI in the opposite language to that utilised for training. Results are reported in Table 10  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. XLM-RoBERTa performance exceeds that of mDeBERTa in both languages. Nonetheless, F1 score for EN is almost 20 points lower than ES evaluation results. In addition to the differences in annotation criteria between languages and datasets, another aspect to bear in mind in this scenario could be the number of positive examples present in training sets. As we explained in Section 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, Meta4XNLIEN contains a higher number of metaphorical instances, thus models are exposed to a greater variety of examples that can be transferred to ES. While a more reduced amount of instances in Meta4XNLIES seen during training might hinder model\u2019s generalization ability, as the low recall and high precision scores show in Table 19  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nIn the first setup, we fine-tuned the models for the NLI task with MultiNLI dataset Williams, Nangia, and Bowman (2018  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###). Then we conducted the evaluation with two different splits for each source dataset conforming Meta4XNLI: pairs with at least one metaphorical expression and pairs lacking metaphors. From accuracy scores reported in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### we can observe certain variability in the results. In the majority of cases, XLM-RoBERTa achieves better performance than mDeBERTa. We do observe a tendency of higher results on the sets of pairs without metaphors than on the set with metaphors. The exceptions to this current are the results from XLM-RoBERTa for XNLIdev and XNLItest in ES. In these partitions, the subset of pairs with metaphorical expressions obtained better results, although the difference does not even reach one point. We hypothesize the original language of the dataset might be involved in this performance, since XNLI includes natural utterances of EN that were afterwards manually translated to ES, thus some artifacts might have been introduced during this process Artetxe, Labaka, and Agirre (2020  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###).\nThe second scenario consisted in fine-tuning the models on two setups: a) with pairs without metaphors and b) pairs with and without metaphors. We performed the evaluation on subsets split by the criterion of metaphor occurrence as well. Results reported in Table 12  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### show XLM-RoBERTa outperforming mDeBERTa in all contexts. Both models show best results for the NLI task on the examples without metaphors and the lowest performance with pairs that contain metaphorical expressions. This outcome replicates in both languages and experimental setups a) and b).\nIn this section, we manually inspected a subset of erroneous cases in order to provide a qualitative insight of results and with the intention of finding potential explanations of errors and models performance for both detection and interpretation tasks.\nWe selected the predictions from the model that obtained highest F1 score from the monolingual experiments in ES for both Meta4XNLI and CoMeta evaluations. We extracted false negatives and false positives and grouped the tokens by their number of occurrences. Within the false positives of CoMeta test set, we find tokens like\napoyo (lit. \u201csupport\u201d) or tensi\u00f3n (lit. \u201ctension\u201d) that appear more frequently used figuratively than with their literal sense or even only appear in the training set labeled as metaphor. Other wrong predictions are words that appeared in specific domains, such as texts that allude to the pandemic, e.g. ola (lit. \u201cwave\u201d), not detected as metaphorical due to its absence with metaphorical meaning in Meta4XNLI sentences. Something similar occurs with the misclassified tokens from Meta4XNLI test set. Most errors stem from conventional metaphors, namely gran, abrir, paso, claro (lit. \u201cgreat\u201d, \u201cto open\u201d, \u201cstep\u201d, \u201cclear\u201d) that occur regularly used with their metaphorical meaning. The lack of balanced examples might contribute to these predictions. However, we maintained the distribution as is, since our aim is to study the presence and prevalence of metaphor in natural language utterances.\nWe analysed a subset of 30 errors from each experimental setup, both EN and ES, and evaluation sets. We chose the predictions from XLM-RoBERTa since it is the model that performs better in this task in most scenarios. Although results show that models struggle more to identify the inference relation if there is a metaphorical expression involved in the pair sentences, we do not observe any particular feature within the errors. A remarkable aspect to highlight is the lower results of esXNLI with respect to XNLIdev and XNLItest in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. It could be motivated by the difference of the text domains, since XNLI is an extension of MultiNLI, which maintains the same set of textual domains. While esXNLI is a collection of texts from another set of genres and sources.\nRegarding EN, some of the errors might derive from the misclassification of some pairs, since annotations were developed on ES text. A metaphorical expression involved in the inference relationship in ES might not be present in its EN version and vice versa. Thus, samples from these two classes, pairs with and without metaphors, should be reexamined in EN and correctly classified for future experimentation.\nIn this work, we provided Meta4XNLI, a cross-lingual parallel dataset in ES and EN labeled for metaphor detection and interpretation framed within the task of NLI. With this new resource that contains parallel text and annotations, we developed a series of experiments to assess the capabilities of MLMs when dealing with this kind of figurative expressions present in natural language utterances.\nRegarding the task of metaphor detection, after the annotation process and experiments results, we can conclude the importance of establishing a unified criterion for annotation that is valid for different languages if the aim is to continue researching cross-lingual approaches. In addition, the semi-automatic process of annotation followed for EN shows that automatic labeling of cross-lingual metaphor is far from trivial. Metaphorical expressions are language and culture dependant. Moreover, the translation of the data introduces a new layer in which metaphorical expressions can either be lost from the translation of the source to the target language, or can be introduced in the target language by means of the translator, either human or automatic. Further work to explore automatic annotation methodologies would be of considerable value in order to reduce the demanding workload and effort of manual labelling in more than one language.\nWith respect to results, the purpose of our experiments is not to overcome the state-of-the-art results, but to evaluate models performance from a cross-lingual approach. Best results are obtained when Meta4XNLI in both languages is used for training. The augmentation of the training set size and the parallel annotations might boost this performance. Cross-domain and monolingual experiments show how the lack of consistency in the annotation criteria affects the performance of models. This can be observed as well in the zero-shot cross-lingual setup, although the scenario of training in EN and evaluating in ES shows competitive performance. It should be noted that the EN set contains a larger number of instances annotated as metaphorical in the training set. In addition, the in-vocabulary and out-of-vocabulary evaluation points to some kind of bias in the learning process. This could stem from the fact that the majority of the metaphor instances are conventional or due to lexical memorization Levy et al. (2015  ###reference_b39###  ###reference_b39###  ###reference_b39###); Boisson, Espinosa-Anke, and\nCamacho-Collados (2023  ###reference_b13###  ###reference_b13###  ###reference_b13###). Future research on this line of work should be carried out to clarify this issue.\nFor metaphor interpretation, we evaluated the ability of MLMs to understand metaphorical expressions framed within the task of NLI. We provide parallel annotations at premise-hypothesis pair level that mark if the presence of metaphorical expressions is relevant for the inference relationship. We exploited this information to conduct our experiments. From reported results, we can recognize a tendency of the models to show a lower performance with pairs that contain at least one metaphorical expression. However, this trend breaks with XLM-RoBERTa results when fine-tuned in ES and evaluated on the XNLIdev and XNLItest in the same language. Since these sets are sentences translated from EN, we presume the translation process might induce biases in metaphor occurrence and the \u201cnaturalness\u201d of the sentences. Similar to metaphor detection, future work to analyse the impact of translation in the development of metaphor parallel resources should be explored for the task of metaphor interpretation, as well as additional experimentation from a multilingual perspective.\nMetaphor annotation is an inherently subjective task. This variance in annotations is reflected on Meta4XNLIEN, due to the different criterion employed through the annotation process. Labels in this language should be updated and further revised to improve their quality. Disagreement and subjectivity could be counterbalanced by a larger number of annotators, in order to develop more consistent and reliable labeled data, however it requires a huge amount of time and workload. Data augmentation and semi-automatic methods could be exploited to create larger datasets with similar characteristics to the one we present and extend it to more languages, since most corpora available for metaphor processing is of reduced size and limited to a narrow set of languages. The existence of parallel resources in multiple languages others than EN that reflect cultural and real world knowledge nuances is of great importance to continue researching such a complex phenomenon as figurative language, specifically metaphors."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Description",
            "text": "Meta4XNLI is a compilation of XNLI Conneau et al. (2018  ###reference_b23###  ###reference_b23###  ###reference_b23###) and esXNLI Artetxe, Labaka, and Agirre (2020  ###reference_b5###  ###reference_b5###  ###reference_b5###). We decided to exploit these resources since we evaluate metaphor interpretation through NLI and these datasets were originally developed for this task. This enables both annotations at token level for detection and pair level for interpretation. In addition, it contains parallel text, from which we select data in ES and EN. Moreover, the combination of XNLI and esXNLI constitutes a dataset of large size compared to common available resources for metaphor processing and with natural language utterances and spontaneous usage of metaphors. The distribution of Meta4XNLI is detailed in Table 2  ###reference_###  ###reference_###  ###reference_###.\nXNLI dataset is a cross-lingual evaluation set for MultiNLI Williams, Nangia, and Bowman (2018  ###reference_b82###  ###reference_b82###  ###reference_b82###). It contains parallel data with original text in EN subsequently human-translated to other 14 languages, among which we exploited only EN and ES. It comprehends a total of 7500 premise-hypothesis pairs from 10 text genres, that is, 830 premises and 2490 hypotheses from XNLIdev, and 1670 premises and 5010 hypotheses from XNLItest. esXNLI comprises a total of 2490 pairs, as in XNLIdev, from 5 different genres. In this case, sentences were originally collected in ES and then human-translated to EN. The direction of translation (EN > ES in XNLI and ES > EN in esXNLI) is an interesting feature to explore how translation may affect metaphor cross-lingual transfer and whether it impacts models performance.\nThe collection methodology is shared between XNLI and esXNLI: a set of premise sentences were crawled from various sources in EN and ES respectively. Afterwards, crowd-source workers were asked to generate three hypotheses for each premise, one for each label. Both corpora are balanced in terms of inference tags and text domains."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Annotation Process",
            "text": "The methodology to label Meta4XNLI varies across tasks and languages. In the following subsections, we will delve into the annotation processes used for detection and interpretation in each language. First, we developed ES annotations, from which we transferred a subset of detection labels and interpretation annotations in EN. Manual labeling was developed by a linguist native speaker of Spanish with advanced knowledge of English.\nThe annotation process in this language comprehends two phases, as depicted in Figure 2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. First, we automatically label Meta4XNLIES by leveraging mDeBERTa He, Gao, and Chen (2021  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###) fine-tuned for metaphor detection in ES with CoMeta. We choose mDeBERTa since it is the multilingual model that achieved highest F1 score in the experimental setup of Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). This choice aims at reducing the heavy work load and time investment that manual annotation from scratch requires. Afterwards, we manually inspect and correct the predictions in the whole dataset. From the first phase of automatic labeling, 748 tokens were predicted as metaphor in the premises and 724 in the hypotheses. After a complete and manual revision of all sentences, we removed 74 tokens and added 481 undetected metaphors in the premises; whereas in the hypotheses, we deleted 118 false positives and labeled 533 false negatives.\n###figure_15### The main sources of ambiguity in ES emerge from multi-word expressions (MWE) and polysemy.\nThe main issue when labeling MWE is to decide whether to treat the MWE as a single lexical unit, a fixed expression where the meaning of each word is not transparent anymore, or if it should be regarded as a collocation. Collocations are expressions where the constituent words tend to co-occur with high frequency but are not fixed, since each of its elements can be replaced by others with similar meaning.\nDijo que era hora de entrar en p\u00e1nico (lit. \u201cThey said it was time to enter into panic\u201d).\nFor instance, in Example 2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, the expression entrar en p\u00e1nico could be initially considered as a MWE with a single lexical unit, therefore, the three tokens would be labeled as metaphorical. However, this expression specifically means \u201cto panic\u201d, in which the verb entrar (lit. \u201cto enter\u201d) holds metaphorical meaning, since \u201cpanic\u201d is not a physical place you can get into. In this case, we do not think of the expression as a fixed MWE, since the verb is also used metaphorically with other terms that are not places, like entrar en c\u00f3lera (\u201cto get angry\u201d, lit. \u201cto enter into wrath\u201d) or entrar en calor (\u201cto feel hot\u201d, lit. \u201cto enter into heat\u201d). In all of these expressions the verb is conveying the sense of starting to feel the noun it complements, as if by entering to a place it transformed our sensitivity. The association of concepts arises from understanding emotions or sensations as physical locations.\nRegarding polysemy, the existence of multiple and very nuanced senses associated to the same token can lead to confusion. It can be challenging to determine whether the basic meaning of the lexical unit is currently and generally known and used by native speakers or if they directly associate the lexical unit to the figurative meaning, not identifying the basic meaning at all.\n[\u2026] ha mostrado su apoyo a la candidatura para ser sede [\u2026] (lit. \u201cThey showed their support to the candidacy to be head office\u201d).\nFor instance, in Example 2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### we label apoyo as metaphorical, since the most basic meaning of the verb apoyar in the dictionary defines it as \u201cto make something rest upon another thing\u201d. In this sentence, the contextual meaning refers to someone in favour of someone else\u2019s goal. Figuratively, the goal can be understood in terms of a physical object so heavy that requires more than one anchor point to distribute its weight. Doubts in this examples come from the fact that the figurative sense can be more used that its basic one, thus speakers might not identify the metaphorical meaning as such. We make use of the Diccionario de la Real Academia Espa\u00f1ola (DRAE) as a tool to help us clarify these ambiguous cases. Nonetheless, metaphor identification remains a highly subjective task.\nWe develop Meta4XNLIEN annotations semi-automatically and based on ES annotations, since our purpose is to publish a parallel resource to counterbalance monolingual English labeled data. These annotations serve as a starting point for further refinement and enable potential analysis of metaphors shared across these two languages. The whole process comprises mainly four phases, as depicted in Figure 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nThe first step involves the projection of ES labels onto EN sentences. For this end, we utilised Easy-Label-Projection Garc\u00eda-Ferrero, Agerri, and\nRigau (2022  ###reference_b28###  ###reference_b28###  ###reference_b28###  ###reference_b28###  ###reference_b28###  ###reference_b28###  ###reference_b28###  ###reference_b28###), a tool developed for cross-lingual sequence labeling that makes use of word alignments and data- and model-transfer to project the labels from a source language (ES) to an untagged target language (EN). This mechanism is suitable when the labeled entity is certainly appearing in both source and target sentences, however, metaphors present in a source sentence are not necessarily in its translation. It depends on a series of multiple factors, namely the type of translation, the knowledge of the translator, if it is human-translated, or socio-cultural knowledge, among others. The next step comprises the manual revision of the projections.\nIn order to alleviate this issue, we made use of XLM-RoBERTa Conneau et al. (2020  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###) trained on the VUAM dataset, as it is the multilingual model that showed best performance in the experiments in EN from the work of Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). Since projected sentences were manually reviewed (13% of the total), we only predicted automatically those sentences that did not receive a projection in the first place (87%). We manually reviewed the outcoming predictions from the MLM to correct errors and undetected metaphorical expressions following the MIPVU procedure. The number of metaphorical tokens extracted in each annotation phase is detailed in Table 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nThere are some issues that emerged during the whole annotation process. Firstly, the projections phase entails that for a metaphorical expression to be annotated in EN, it has to have been annotated in ES in advance. Hence, metaphors in EN sentences that were not expressed figuratively in their ES counterpart will not be spotted, as in Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. In the ES sentence there is not a metaphorical expression annotated as such, however, in the EN version, the adjective heavy is holding metaphorical meaning, as a synonym of \u201cdemanding\u201d, which is expressed literally in ES with the adjective exigentes. In this kind of cases where the metaphor is \u201cgained in translation\u201d, the lack of annotation in the source language implies no label will be projected onto the target sentence, missing a metaphorical instance. These examples give an account of how translation and the singularities of metaphors according to languages may affect to the annotation process of this task.\n###figure_16### .\nA los usuarios m\u00e1s exigentes se les deber\u00eda cobrar m\u00e1s. \nThe heavy users should be charged the most.\nAnother issue that should be noted are false positives: all ES sentences with one or more labeled metaphors will transfer those tags to the EN sentence, regardless the translated tokens hold metaphoric meaning or not. To solve this question, we manually reviewed all sentences that received a projected tag. In this revision we had to remove metaphors that were \u201clost in translation\u201d and adjust some spans projected to the target language, e.g. some annotated verbs in ES were projected in EN to the subject pronoun and verb, since some ES verbs forms are synthetic and include the person information in a single morpheme. Therefore, we eliminated the label from the pronoun and maintained only that of the verb. In Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### we can see how the verb peleaban (lit. \u201cthey fought\u201d) labeled as metaphorical in the ES sentence (Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###) was projected to the subject and verb in EN (Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###). Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### represents the definitive version of the annotations after manual revision.\nPeleaban por lo ricos que eran los directores ejecutivos.\nThey fought about how rich CEOs were.\nThey fought about how rich CEOs were.\nRegarding the subset of sentences that were automatically labeled by XLM-RoBERTa, some concerns emerged with respect to annotations present in VUAM dataset during the phase of manual revision. Especially when it comes to phrasal verbs and abstract and polysemous terms that are lexicalised. For instance, due to Examples like 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### or 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### labeled as metaphorical in the training set VUAM, we observed a tendency to overannotate verbs that conform phrasal verbs as metaphorical, such as get, look, made or go. In most cases, these verbs appear in a context where they do not add any strong semantic information due to their lexicalization. Hence, we unmarked these instances that were predicted as metaphorical. Following this line of thought, we also discarded abstract and vague terms that are common-places of spontaneous discourse, like the word thing in Example 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, since it can refer to any kind of entity, either concrete or abstract and might not be directly matched to a more broadly used basic meaning.\nHis lack of humbug about political balance has always made him more honest than all the employees [\u2026].\n\\enumsentenceTake what you want and leave the rest , your mother \u2019ll get rid of it . \n\\enumsentenceOne thing always linked to another thing."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Detection",
            "text": "Annotations for this task are developed at token level, since we approach metaphor detection as a sequence labeling task. We extract premises and hypotheses sentences and annotate them separately. Therefore, we do not take into account the premise as context to annotate its corresponding hypotheses and vice versa. Due to this split, the total number of labeled sentences amounts to 13320. With respect to the type of metaphors, we consider as candidates the tokens belonging to a semantically significant part of speech (POS): nouns, verbs, adjectives and adverbs.\nWe adopt the MIPVU guidelines Steen et al. (2010) throughout the whole annotation process, either manual revision or in automatic predictions, since models used were trained on data labeled accordingly to this procedure. It can be summarised in four main steps:\nRead the entire text\u2013discourse to establish a general understanding of the meaning.\nDetermine the lexical units in the text\u2013discourse.\nFor each lexical unit in the text, establish its meaning in context, that is, how it applies to an entity, relation, or attribute in the situation evoked by the text (contextual meaning). Take into account what comes before and after the lexical unit.\nFor each lexical unit, determine if it has a more basic contemporary meaning in other contexts than the one in the given context. For our purposes, basic meanings tend to be:\nMore concrete; what they evoke is easier to imagine, see, hear, feel, smell, and taste.\nRelated to bodily action.\nMore precise (as opposed to vague).\nHistorically older.\nBasic meanings are not necessarily the most frequent meanings of the lexical unit.\nIf the lexical unit has a more basic current\u2013contemporary meaning in other contexts than the given context, decide whether the contextual meaning contrasts with the basic meaning but can be understood in comparison with it.\nIf yes, mark the lexical unit as metaphorical.\nThe main sources of ambiguity in ES emerge from multi-word expressions (MWE) and polysemy.\nThe main issue when labeling MWE is to decide whether to treat the MWE as a single lexical unit, a fixed expression where the meaning of each word is not transparent anymore, or if it should be regarded as a collocation. Collocations are expressions where the constituent words tend to co-occur with high frequency but are not fixed, since each of its elements can be replaced by others with similar meaning.\nDijo que era hora de entrar en p\u00e1nico (lit. \u201cThey said it was time to enter into panic\u201d).\nFor instance, in Example 2, the expression entrar en p\u00e1nico could be initially considered as a MWE with a single lexical unit, therefore, the three tokens would be labeled as metaphorical. However, this expression specifically means \u201cto panic\u201d, in which the verb entrar (lit. \u201cto enter\u201d) holds metaphorical meaning, since \u201cpanic\u201d is not a physical place you can get into. In this case, we do not think of the expression as a fixed MWE, since the verb is also used metaphorically with other terms that are not places, like entrar en c\u00f3lera (\u201cto get angry\u201d, lit. \u201cto enter into wrath\u201d) or entrar en calor (\u201cto feel hot\u201d, lit. \u201cto enter into heat\u201d). In all of these expressions the verb is conveying the sense of starting to feel the noun it complements, as if by entering to a place it transformed our sensitivity. The association of concepts arises from understanding emotions or sensations as physical locations.\nRegarding polysemy, the existence of multiple and very nuanced senses associated to the same token can lead to confusion. It can be challenging to determine whether the basic meaning of the lexical unit is currently and generally known and used by native speakers or if they directly associate the lexical unit to the figurative meaning, not identifying the basic meaning at all.\n[\u2026] ha mostrado su apoyo a la candidatura para ser sede [\u2026] (lit. \u201cThey showed their support to the candidacy to be head office\u201d).\nFor instance, in Example 2, we label apoyo as metaphorical, since the most basic meaning of the verb apoyar in the dictionary defines it as \u201cto make something rest upon another thing\u201d. In this sentence, the contextual meaning refers to someone in favour of someone else\u2019s goal. Figuratively, the goal can be understood in terms of a physical object so heavy that requires more than one anchor point to distribute its weight. Doubts in this examples come from the fact that the figurative sense can be more used that its basic one, thus speakers might not identify the metaphorical meaning as such. We make use of the Diccionario de la Real Academia Espa\u00f1ola (DRAE) as a tool to help us clarify these ambiguous cases. Nonetheless, metaphor identification remains a highly subjective task.\nWe develop Meta4XNLIEN annotations semi-automatically and based on ES annotations, since our purpose is to publish a parallel resource to counterbalance monolingual English labeled data. These annotations serve as a starting"
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Interpretation",
            "text": "Similar to other works cited in Section 2  ###reference_###  ###reference_###  ###reference_###, we framed metaphor interpretation within the task of NLI Agerri (2008  ###reference_b2###  ###reference_b2###  ###reference_b2###); Mohler, Tomlinson, and Bracewell (2013  ###reference_b50###  ###reference_b50###  ###reference_b50###); Chakrabarty et al. (2021  ###reference_b15###  ###reference_b15###  ###reference_b15###); Stowe, Utama, and Gurevych (2022  ###reference_b74###  ###reference_b74###  ###reference_b74###); Kabra et al. (2023  ###reference_b31###  ###reference_b31###  ###reference_b31###). Our approach aims at evaluating whether MLMs struggle to identify the relationship of inference when there is a metaphorical expression either in the premise, hypothesis or both sentences. To do so, we labeled premise-hypothesis pairs with metaphorical expressions where the understanding of the figurative expression is crucial to determine the inference label. We summarised this annotation process in the following steps:\nRead the premise sentence and determine its general meaning.\nIdentify potential metaphorical expressions according to metaphor detection guidelines.\nRepeat the previous two instructions with the hypothesis sentence.\nEstablish the inference relation betweeen premise and hypothesis if not previously labeled.\nIf there is any metaphorical expression either in premise or hypothesis:\nIs it required to understand the literal meaning of the metaphorical expression to label the inference relationship between premise and hypothesis?\nYes: mark the pair.\nNo: mark the pair as an non-relevant case.\nRepeat the process with non-relevant cases until clarification. Otherwise, if they are intrinsically ambiguous or lack context to either identify the metaphors or determine if they are relevant to the inference, discard the pair.\nIn Example 3.2.2  ###reference_.SSS2###  ###reference_.SSS2###  ###reference_.SSS2###, we encounter the metaphor saltar (lit. \u201cto skip\u201d) with the meaning of omitting some information and not the literal sense of physically jumping. In the hypothesis, the sentence refers to the intention of telling the other interlocutor all the information, comprehensively, without ignoring any part, which contradicts the overall meaning of the premise. The understanding of this metaphorical expression, thus, is required to infer that they contradict each other.\nPremise: Hay tanto que se puede decir sobre eso, que sencillamente me voy a saltar eso. (lit. \u201cThere is so much you can say about that, that I am simply going to skip that\u201d).\nHypothesis: \u00a1Quiero contarte todo lo que s\u00e9 sobre eso! (lit. \u201cI want to tell you everything I know about it!\u201d). \nInference label: contradiction\nPremise: No hay necesidad de hurgar en ese tema, a menos que quieras asegurarte de que nos hundimos. (lit. \u201cThere is no need to rummage in that topic, unless you want to make sure we will sink\u201d).\nHypothesis: Hay una forma de que se hundan. (lit. \u201cThere is one way to make them sink\u201d). \nInference label: entailment\nNon-relevant cases comprehend premise-hypothesis pairs where the understanding of the literal sense of the metaphor is not essential to establish a relationship of entailment, contradiction or neutral. As we can see in Example 3.2.2  ###reference_.SSS2###  ###reference_.SSS2###  ###reference_.SSS2###, the metaphorical expression hurgar (lit. \u201cto rummage\u201d) is used with a sense of exploring an unpleasant topic, while the literal meaning implies to physically dig into an inner space. The entailment in this example is inferred from the likelihood of a sinking, not focusing on the willingness to talk about the mentioned topic. Thus, the interpretation of the metaphorical expression is not relevant to extract the inference relation.\nWe set aside the latter group as non-relevant cases, since we are not certain as to which extent the role of metaphor is significant in these occurrences. As a result, we discriminate three classes: a) pairs with metaphors that are relevant to the inference relationship, b) pairs with metaphors that are not relevant to the inference and c) pairs without metaphors.\nAnnotations were manually developed on ES text and were transferred to EN. Hence, we provide Meta4XNLIEN annotations as a silver standard for further refinement. Data about the number of samples and labels will be resumed in the following Subsection 3.3  ###reference_###  ###reference_###  ###reference_###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Resulting Dataset",
            "text": "The outcome of this annotation process is Meta4XNLI, the first parallel dataset with labels for the tasks of metaphor detection and interpretation via NLI in ES and EN.\nThe parallel data for this task comprises a total of 13320 sentences annotated at token level, since we approached metaphor detection as a sequence labeling task, following the criteria of the cited previous work in Section 2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nWith respect to Spanish annotations, there is a total of 1155 metaphorical tokens in premises and 1139 in hypotheses. Out of the 13320 sentences, 1873 contain at least one metaphorical expression, which constitutes the 14% of the whole dataset. This information is detailed in Table 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### according to the source datasets partitions. A higher proportion of metaphors in premises can be noticed. This might be due to the fact that premises are of larger length than hypotheses, as indicated in Conneau et al. (2018  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###). In addition, premises were collected from existing utterances, while hypotheses were generated from crowd-source workers in response to a given set of premises. Therefore, hypotheses sentences might tend to present shorter length and lower complexity.\n###table_16### Regarding English annotations, we can observe a similar trend to that of ES in Table 5  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. A total of 3330 tokens were labeled as metaphor and 2736 sentences contain at least one metaphorical instance out of the total 13320 sentences. Premises show a higher metaphor ratio than hypotheses as in ES annotations. Additionally, we observe a larger amount of labeled metaphors in EN, caused by the different annotations processes specified in Subsection 3.2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. Since VUAM contains a significantly higher amount of labeled metaphorical expressions, the MLM fine-tuned with this dataset predicted many ambiguous metaphors, which we subsequently removed in manual revision. These discrepancies are not only noticeable in annotations but also in experiments results. Moreover, this gives an account of how guidelines for metaphor identification labeling are open to discussion and clarification, due to the subjective and nuanced nature of this cognitive-linguistic phenomenon.\n###table_17### ###table_18### Annotations for this task were developed at premise-hypothesis level. As shown in Table 6  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, the average percentage of pairs with metaphors relevant to the inference relationship is 12%. This figure remains steady throughout each source dataset and inference labels. esXNLI shows a higher number of metaphor occurrence that might be caused by the difference of text domains and sentence characteristics with respect to XNLI data. Regarding non-relevant cases, we do not exploit them in the experiments in order to be able to analyse more clearly whether metaphor presence impacts models performance. We keep the same sample distribution for both languages to develop the experiments."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Inter-annotator Agreement",
            "text": "We selected a subset of 1000 pairs to the assess annotation quality in ES. The subset was labeled by another Spanish native speaker with a linguistics background. This annotator labeled the dataset subset from scratch and according to MIPVU guidelines in the case of detection, and the annotation process specified in Subsection 3.2  ###reference_###  ###reference_###  ###reference_### for interpretation. We computed Cohen\u2019s Kappa score Cohen (1960  ###reference_b20###  ###reference_b20###  ###reference_b20###) and obtained 0.74 in premises and 0.77 in hypotheses sentences for detection; for interpretation via the NLI task the obtained score was 0.64. Thus, these scores show a substantial agreement and are comparable to those obtained in the annotation of other datasets for metaphor detection and/or interpretation Steen et al. (2010  ###reference_b72###  ###reference_b72###  ###reference_b72###); Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###); Maudslay and Teufel (2022  ###reference_b47###  ###reference_b47###  ###reference_b47###); Badathala et al. (2023  ###reference_b7###  ###reference_b7###  ###reference_b7###) thereby demonstrating the consistency of our annotations. Still, annotating metaphorical language remains a rather subjective task that requires exhaustive work and precise guidelines.\nWith the aim of developing cross-lingual experiments, we chose the multilingual language models and checkpoints that obtained best results in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###): mDeBERTa (base) and XLM-RoBERTa (large) Conneau et al. (2020  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###), the multilingual versions of DeBERTa He, Gao, and Chen (2021  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###) and RoBERTa Liu et al. (2019  ###reference_b44###  ###reference_b44###  ###reference_b44###  ###reference_b44###  ###reference_b44###) respectively.\nTaking advantage of previous available resources and the corpus we present in this work, we conducted a series of experiments to evaluate and fine-tune MLMs.\nThe configuration of cross-domain experiments is specified in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). For the other three setups, monolingual, multilingual and zero-shot cross-lingual, we performed hyperparameter tuning for batch size (8, 16, 36), linear decay (0.1, 0.01), learning rate (in the [1e-5-5e-5] interval), sequence length of 128 and epochs from 4 to 10. A warm-up of 6% is specified. The results of the hyperparameter tuning showed that after 4 epochs development loss started to increase, so results reported here are obtained with 4 epochs, batch size of 8, weight decay of 0.1 and learning rate of 5e-5.\nCross-domain: the aim of this set of experiments is to evaluate the performance of MLMs fine-tuned with CoMeta and VUAM datasets on Meta4XNLIES and Meta4XNLIEN, respectively, since each dataset contains texts from different domains. The motivation is to explore the impact of text features and genres on the performance, as well as annotation criteria Aghazadeh, Fayyaz, and\nYaghoobzadeh (2022  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###); Lai, Toral, and Nissim (2023  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###). To do so, we chose the models with best performance from monolingual experiments developed in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). We conducted the evaluation on various data splits: within each source dataset, XNLIdev, XNLItest and esXNLI, we evaluated premises and hypotheses separated and combined. This is due to the dissimilarities and the unequal distribution of metaphorical expressions between premises and hypotheses sentences mentioned in Subsection 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nMonolingual: this scenario comprises the fine-tuning and evaluation of the MLM on Meta4XNLIES and Meta4XNLIEN separately. To accomplish this task, we split Meta4XNLI into train, development and test sets (0.6-0.2-0.2). We equally distributed the data to ensure each partition is balanced in terms of source datasets sentences and metaphor occurrence. Data statistics are detailed in Appendix A  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. These partitions will be used as well in subsequent multilingual and zero-shot cross-lingual scenarios. In addition to fine-tuning and evaluation on Meta4XNLIES and Meta4XNLIEN, we evaluated each trained monolingual model with the test sets of CoMeta and VUAM, following the same reasoning as in cross-domain experiments.\nMultilingual: the purpose of these experiments is to explore whether MLMs benefit from being trained on data in multiple languages. In this case, we combined Meta4XNLIES and Meta4XNLIEN train splits to fine-tune the models. Subsequently, we evaluated the trained models on each language test set, in order to analyse the impact on the performance for each language. The data splits used correspond to those from monolingual experiments.\nZero-shot cross-lingual: in this scenario we explore to what extent MLMs are able to generalize knowledge and metaphor transfer between these two languages in question. Therefore, we fine-tune the models with Meta4XNLI data in one language and evaluate it on the test set of the other. Data partitions used for these experiments are the same as in monolingual an multilingual scenarios.\nWe carried out two sets of experiments to evaluate metaphor interpretation within the NLI task. We performed hyperparameter tuning, with the same range of parameters specified on the task of metaphor detection. We report best results obtained on the development set, after 4 epochs, batch size of 8, learning rate of 1e-5, weight decay of 0.1 and 512 sequence length.\nThe purpose of the first experiments is to examine if the presence of metaphorical expressions in premise-hypothesis pairs impacts the performance of models in the NLI task. To that end, we fine-tuned the MLMs for the task with the MultiNLI dataset. Then, we evaluated them with Meta4XNLI. Among each source dataset, we discriminated pairs with metaphors relevant to the inference relationship from those without metaphorical expressions. We developed the evaluation on each of these subsets and for each language separately, e.g. one evaluation on XNLIdev metaphor set and another on XNLIdev without metaphors set, in EN and then in ES.\nThe goal of the second set of experiments is to analyse the effect on models performance of not being \u201cexposed\u201d to instances with metaphorical expressions during training. With this aim in mind, we extracted pairs with and without metaphors from Meta4XNLI train, dev and set splits. On the first scenario, we fine-tuned the models only with pairs from the train set that did not contain any metaphorical expression. On the second scenario, we mixed pairs with and without metaphors and fine-tuned the models as well. In both cases, we evaluated on the test sets with and without metaphors for each language.\nIn addition to F1 score, we performed in-vocabluary (Inv) and out-of-vocabulary (Oov) evaluations to test the impact of the labels seen from training data during the learning process of the MLMs. Precision and Recall metrics are reported in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. For in-vocabulary evaluation, we computed F1 score taking into account predicted words seen in training labeled as metaphorical. While for out-of-vocabulary evaluation we considered only words not seen during training to compute the F1 score. We included this evaluation in all experiments but for the zero-shot cross-lingual setup, since the data of train and test sets are in different languages, thus the match of the exact same metaphorical token in both partitions is highly unlikely.\nAlthough the purpose of our experiments is not to beat state-of-the-art results but to evaluate the performance of MLMs on the task from a cross-lingual approach, we added two indicative baselines: on one hand, the system BasicBERT Li et al. (2023a  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###), which obtained 73.3 F1 score; on the other, the result of DeBERTa reported in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###), with 73.79 F1. We selected these results for comparison purposes, since they both were evaluated on the VUAM-2020 version of the dataset in EN used in the Shared Task 2020 Leong et al. (2020  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###) and in the same experimental setup we propose.\nCross-domain: We evaluated models trained on CoMeta and VUAM datasets with XNLI and esXNLI in ES and EN, respectively. From results reported in Table 7  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we can observe that in all cases mDeBERTa outperforms XLM-RoBERTa for ES. In EN the best result is obtained by DeBERTa, which also outperforms XLM-RoBERTa in all scenarios. In all datasets, except for XNLIdev in ES, premises sentences achieve better results than hypotheses and the combination of both. This goes in line with annotation statistics in Tables 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### and 5  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, which show that premises contain a greater ratio of metaphors per sentence than hypotheses in both languages. In ES, in-vocabulary evaluation outperforms the general F1 score while out-of-vocabulary evaluation results decrease. The small distance in points of this cross-domain evaluation in ES exhibit stability of the models when it comes to predicting metaphors and coherence in annotations between both datasets despite the difference of text domains. Nevertheless, EN results do not demonstrate such consistency, as out-of-vocabulary obtains higher results than the overall F1 score. This might be due to the different labeling criteria used in VUAM and Meta4XNLI, as we mentioned in 3.2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. These discrepancies are also reflected in a significant drop in performance with respect to ES experiments and in-domain evaluation with VUAM. The high recall scores from Table 16  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### show that the model tends to predict many metaphors, however, low precision scores indicate that a small amount of these predictions are correct.\n.\nMonolingual: Results from this set of experiments are specified in Table 8  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. After fine-tuning and evaluating the models with Meta4XNLIES and Meta4XNLIEN, the highest overall F1 score is obtained by mDeBERTa in ES. On the other hand, XLM-RoBERTa achieves better performance than mDeBERTa in EN but still lower than in ES. In both languages, in-vocabulary evaluation results are higher than the overall ones and out-of-vocabulary results, lower. This is not the case when we use VUAM corpus for testing the model fine-tuned with Meta4XNLIEN. In this setup, similarly to results from cross-domain experiments, performance drastically falls to 32.42 F1 score and out-of-vocabulary results are the highest. The reason behind might be the mismatches in the annotation process. In ES, when evaluating CoMeta, we also encounter a decrease in the performance, however, it is more subtle: from 67.17 to 56.24, obtained by XLM-RoBERTa. This might be caused by the variety of text genres and dissimilarities between sentences from each dataset.\nMultilingual: In this set of experiments we assembled Meta4XNLIEN and Meta4XNLIES to train the MLMs. The evaluation is conducted for each language separately and results detailed in Table 9  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. Best results in ES are obtained by mDeBERTa, which are higher than the top result from monolingual experiments. In EN, mDeBERTa is the model that achieves better performance but very close to that of XLM-RoBERTa. The highest F1 score is 8 points lower than that of ES but outweighs EN monolingual results. This suggests that the combination of parallel multilingual data for training is beneficial for the performance of the models.\nZero-shot cross-lingual: In these experiments we perform evaluation of Meta4XNLI in the opposite language to that utilised for training. Results are reported in Table 10  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. XLM-RoBERTa performance exceeds that of mDeBERTa in both languages. Nonetheless, F1 score for EN is almost 20 points lower than ES evaluation results. In addition to the differences in annotation criteria between languages and datasets, another aspect to bear in mind in this scenario could be the number of positive examples present in training sets. As we explained in Section 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, Meta4XNLIEN contains a higher number of metaphorical instances, thus models are exposed to a greater variety of examples that can be transferred to ES. While a more reduced amount of instances in Meta4XNLIES seen during training might hinder model\u2019s generalization ability, as the low recall and high precision scores show in Table 19  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nIn the first setup, we fine-tuned the models for the NLI task with MultiNLI dataset Williams, Nangia, and Bowman (2018  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###). Then we conducted the evaluation with two different splits for each source dataset conforming Meta4XNLI: pairs with at least one metaphorical expression and pairs lacking metaphors. From accuracy scores reported in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### we can observe certain variability in the results. In the majority of cases, XLM-RoBERTa achieves better performance than mDeBERTa. We do observe a tendency of higher results on the sets of pairs without metaphors than on the set with metaphors. The exceptions to this current are the results from XLM-RoBERTa for XNLIdev and XNLItest in ES. In these partitions, the subset of pairs with metaphorical expressions obtained better results, although the difference does not even reach one point. We hypothesize the original language of the dataset might be involved in this performance, since XNLI includes natural utterances of EN that were afterwards manually translated to ES, thus some artifacts might have been introduced during this process Artetxe, Labaka, and Agirre (2020  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###).\nThe second scenario consisted in fine-tuning the models on two setups: a) with pairs without metaphors and b) pairs with and without metaphors. We performed the evaluation on subsets split by the criterion of metaphor occurrence as well. Results reported in Table 12  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### show XLM-RoBERTa outperforming mDeBERTa in all contexts. Both models show best results for the NLI task on the examples without metaphors and the lowest performance with pairs that contain metaphorical expressions. This outcome replicates in both languages and experimental setups a) and b).\nWe selected the predictions from the model that obtained highest F1 score from the monolingual experiments in ES for both Meta4XNLI and CoMeta evaluations. We extracted false negatives and false positives and grouped the tokens by their number of occurrences. Within the false positives of CoMeta test set, we find tokens like\napoyo (lit. \u201csupport\u201d) or tensi\u00f3n (lit. \u201ctension\u201d) that appear more frequently used figuratively than with their literal sense or even only appear in the training set labeled as metaphor. Other wrong predictions are words that appeared in specific domains, such as texts that allude to the pandemic, e.g. ola (lit. \u201cwave\u201d), not detected as metaphorical due to its absence with metaphorical meaning in Meta4XNLI sentences. Something similar occurs with the misclassified tokens from Meta4XNLI test set. Most errors stem from conventional metaphors, namely gran, abrir, paso, claro (lit. \u201cgreat\u201d, \u201cto open\u201d, \u201cstep\u201d, \u201cclear\u201d) that occur regularly used with their metaphorical meaning. The lack of balanced examples might contribute to these predictions. However, we maintained the distribution as is, since our aim is to study the presence and prevalence of metaphor in natural language utterances.\nWe analysed a subset of 30 errors from each experimental setup, both EN and ES, and evaluation sets. We chose the predictions from XLM-RoBERTa since it is the model that performs better in this task in most scenarios. Although results show that models struggle more to identify the inference relation if there is a metaphorical expression involved in the pair sentences, we do not observe any particular feature within the errors. A remarkable aspect to highlight is the lower results of esXNLI with respect to XNLIdev and XNLItest in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. It could be motivated by the difference of the text domains, since XNLI is an extension of MultiNLI, which maintains the same set of textual domains. While esXNLI is a collection of texts from another set of genres and sources.\nRegarding EN, some of the errors might derive from the misclassification of some pairs, since annotations were developed on ES text. A metaphorical expression involved in the inference relationship in ES might not be present in its EN version and vice versa. Thus, samples from these two classes, pairs with and without metaphors, should be reexamined in EN and correctly classified for future experimentation."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental setup",
            "text": "With the aim of developing cross-lingual experiments, we chose the multilingual language models and checkpoints that obtained best results in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###): mDeBERTa (base) and XLM-RoBERTa (large) Conneau et al. (2020  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###), the multilingual versions of DeBERTa He, Gao, and Chen (2021  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###) and RoBERTa Liu et al. (2019  ###reference_b44###  ###reference_b44###  ###reference_b44###  ###reference_b44###  ###reference_b44###  ###reference_b44###) respectively.\nTaking advantage of previous available resources and the corpus we present in this work, we conducted a series of experiments to evaluate and fine-tune MLMs.\nThe configuration of cross-domain experiments is specified in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). For the other three setups, monolingual, multilingual and zero-shot cross-lingual, we performed hyperparameter tuning for batch size (8, 16, 36), linear decay (0.1, 0.01), learning rate (in the [1e-5-5e-5] interval), sequence length of 128 and epochs from 4 to 10. A warm-up of 6% is specified. The results of the hyperparameter tuning showed that after 4 epochs development loss started to increase, so results reported here are obtained with 4 epochs, batch size of 8, weight decay of 0.1 and learning rate of 5e-5.\nCross-domain: the aim of this set of experiments is to evaluate the performance of MLMs fine-tuned with CoMeta and VUAM datasets on Meta4XNLIES and Meta4XNLIEN, respectively, since each dataset contains texts from different domains. The motivation is to explore the impact of text features and genres on the performance, as well as annotation criteria Aghazadeh, Fayyaz, and\nYaghoobzadeh (2022  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###); Lai, Toral, and Nissim (2023  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###). To do so, we chose the models with best performance from monolingual experiments developed in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). We conducted the evaluation on various data splits: within each source dataset, XNLIdev, XNLItest and esXNLI, we evaluated premises and hypotheses separated and combined. This is due to the dissimilarities and the unequal distribution of metaphorical expressions between premises and hypotheses sentences mentioned in Subsection 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nMonolingual: this scenario comprises the fine-tuning and evaluation of the MLM on Meta4XNLIES and Meta4XNLIEN separately. To accomplish this task, we split Meta4XNLI into train, development and test sets (0.6-0.2-0.2). We equally distributed the data to ensure each partition is balanced in terms of source datasets sentences and metaphor occurrence. Data statistics are detailed in Appendix A  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. These partitions will be used as well in subsequent multilingual and zero-shot cross-lingual scenarios. In addition to fine-tuning and evaluation on Meta4XNLIES and Meta4XNLIEN, we evaluated each trained monolingual model with the test sets of CoMeta and VUAM, following the same reasoning as in cross-domain experiments.\nMultilingual: the purpose of these experiments is to explore whether MLMs benefit from being trained on data in multiple languages. In this case, we combined Meta4XNLIES and Meta4XNLIEN train splits to fine-tune the models. Subsequently, we evaluated the trained models on each language test set, in order to analyse the impact on the performance for each language. The data splits used correspond to those from monolingual experiments.\nZero-shot cross-lingual: in this scenario we explore to what extent MLMs are able to generalize knowledge and metaphor transfer between these two languages in question. Therefore, we fine-tune the models with Meta4XNLI data in one language and evaluate it on the test set of the other. Data partitions used for these experiments are the same as in monolingual an multilingual scenarios.\nWe carried out two sets of experiments to evaluate metaphor interpretation within the NLI task. We performed hyperparameter tuning, with the same range of parameters specified on the task of metaphor detection. We report best results obtained on the development set, after 4 epochs, batch size of 8, learning rate of 1e-5, weight decay of 0.1 and 512 sequence length.\nThe purpose of the first experiments is to examine if the presence of metaphorical expressions in premise-hypothesis pairs impacts the performance of models in the NLI task. To that end, we fine-tuned the MLMs for the task with the MultiNLI dataset. Then, we evaluated them with Meta4XNLI. Among each source dataset, we discriminated pairs with metaphors relevant to the inference relationship from those without metaphorical expressions. We developed the evaluation on each of these subsets and for each language separately, e.g. one evaluation on XNLIdev metaphor set and another on XNLIdev without metaphors set, in EN and then in ES.\nThe goal of the second set of experiments is to analyse the effect on models performance of not being \u201cexposed\u201d to instances with metaphorical expressions during training. With this aim in mind, we extracted pairs with and without metaphors from Meta4XNLI train, dev and set splits. On the first scenario, we fine-tuned the models only with pairs from the train set that did not contain any metaphorical expression. On the second scenario, we mixed pairs with and without metaphors and fine-tuned the models as well. In both cases, we evaluated on the test sets with and without metaphors for each language.\nIn addition to F1 score, we performed in-vocabluary (Inv) and out-of-vocabulary (Oov) evaluations to test the impact of the labels seen from training data during the learning process of the MLMs. Precision and Recall metrics are reported in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. For in-vocabulary evaluation, we computed F1 score taking into account predicted words seen in training labeled as metaphorical. While for out-of-vocabulary evaluation we considered only words not seen during training to compute the F1 score. We included this evaluation in all experiments but for the zero-shot cross-lingual setup, since the data of train and test sets are in different languages, thus the match of the exact same metaphorical token in both partitions is highly unlikely.\nAlthough the purpose of our experiments is not to beat state-of-the-art results but to evaluate the performance of MLMs on the task from a cross-lingual approach, we added two indicative baselines: on one hand, the system BasicBERT Li et al. (2023a  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###), which obtained 73.3 F1 score; on the other, the result of DeBERTa reported in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###), with 73.79 F1. We selected these results for comparison purposes, since they both were evaluated on the VUAM-2020 version of the dataset in EN used in the Shared Task 2020 Leong et al. (2020  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###) and in the same experimental setup we propose.\nCross-domain: We evaluated models trained on CoMeta and VUAM datasets with XNLI and esXNLI in ES and EN, respectively. From results reported in Table 7  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we can observe that in all cases mDeBERTa outperforms XLM-RoBERTa for ES. In EN the best result is obtained by DeBERTa, which also outperforms XLM-RoBERTa in all scenarios. In all datasets, except for XNLIdev in ES, premises sentences achieve better results than hypotheses and the combination of both. This goes in line with annotation statistics in Tables 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### and 5  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, which show that premises contain a greater ratio of metaphors per sentence than hypotheses in both languages. In ES, in-vocabulary evaluation outperforms the general F1 score while out-of-vocabulary evaluation results decrease. The small distance in points of this cross-domain evaluation in ES exhibit stability of the models when it comes to predicting metaphors and coherence in annotations between both datasets despite the difference of text domains. Nevertheless, EN results do not demonstrate such consistency, as out-of-vocabulary obtains higher results than the overall F1 score. This might be due to the different labeling criteria used in VUAM and Meta4XNLI, as we mentioned in 3.2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. These discrepancies are also reflected in a significant drop in performance with respect to ES experiments and in-domain evaluation with VUAM. The high recall scores from Table 16  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### show that the model tends to predict many metaphors, however, low precision scores indicate that a small amount of these predictions are correct.\n.\nMonolingual: Results from this set of experiments are specified in Table 8  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. After fine-tuning and evaluating the models with Meta4XNLIES and Meta4XNLIEN, the highest overall F1 score is obtained by mDeBERTa in ES. On the other hand, XLM-RoBERTa achieves better performance than mDeBERTa in EN but still lower than in ES. In both languages, in-vocabulary evaluation results are higher than the overall ones and out-of-vocabulary results, lower. This is not the case when we use VUAM corpus for testing the model fine-tuned with Meta4XNLIEN. In this setup, similarly to results from cross-domain experiments, performance drastically falls to 32.42 F1 score and out-of-vocabulary results are the highest. The reason behind might be the mismatches in the annotation process. In ES, when evaluating CoMeta, we also encounter a decrease in the performance, however, it is more subtle: from 67.17 to 56.24, obtained by XLM-RoBERTa. This might be caused by the variety of text genres and dissimilarities between sentences from each dataset.\nMultilingual: In this set of experiments we assembled Meta4XNLIEN and Meta4XNLIES to train the MLMs. The evaluation is conducted for each language separately and results detailed in Table 9  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. Best results in ES are obtained by mDeBERTa, which are higher than the top result from monolingual experiments. In EN, mDeBERTa is the model that achieves better performance but very close to that of XLM-RoBERTa. The highest F1 score is 8 points lower than that of ES but outweighs EN monolingual results. This suggests that the combination of parallel multilingual data for training is beneficial for the performance of the models.\nZero-shot cross-lingual: In these experiments we perform evaluation of Meta4XNLI in the opposite language to that utilised for training. Results are reported in Table 10  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. XLM-RoBERTa performance exceeds that of mDeBERTa in both languages. Nonetheless, F1 score for EN is almost 20 points lower than ES evaluation results. In addition to the differences in annotation criteria between languages and datasets, another aspect to bear in mind in this scenario could be the number of positive examples present in training sets. As we explained in Section 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, Meta4XNLIEN contains a higher number of metaphorical instances, thus models are exposed to a greater variety of examples that can be transferred to ES. While a more reduced amount of instances in Meta4XNLIES seen during training might hinder model\u2019s generalization ability, as the low recall and high precision scores show in Table 19  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nIn the first setup, we fine-tuned the models for the NLI task with MultiNLI dataset Williams, Nangia, and Bowman (2018  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###). Then we conducted the evaluation with two different splits for each source dataset conforming Meta4XNLI: pairs with at least one metaphorical expression and pairs lacking metaphors. From accuracy scores reported in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### we can observe certain variability in the results. In the majority of cases, XLM-RoBERTa achieves better performance than mDeBERTa. We do observe a tendency of higher results on the sets of pairs without metaphors than on the set with metaphors. The exceptions to this current are the results from XLM-RoBERTa for XNLIdev and XNLItest in ES. In these partitions, the subset of pairs with metaphorical expressions obtained better results, although the difference does not even reach one point. We hypothesize the original language of the dataset might be involved in this performance, since XNLI includes natural utterances of EN that were afterwards manually translated to ES, thus some artifacts might have been introduced during this process Artetxe, Labaka, and Agirre (2020  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###).\nThe second scenario consisted in fine-tuning the models on two setups: a) with pairs without metaphors and b) pairs with and without metaphors. We performed the evaluation on subsets split by the criterion of metaphor occurrence as well. Results reported in Table 12  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### show XLM-RoBERTa outperforming mDeBERTa in all contexts. Both models show best results for the NLI task on the examples without metaphors and the lowest performance with pairs that contain metaphorical expressions. This outcome replicates in both languages and experimental setups a) and b).\nWe selected the predictions from the model that obtained highest F1 score from the monolingual experiments in ES for both Meta4XNLI and CoMeta evaluations. We extracted false negatives and false positives and grouped the tokens by their number of occurrences. Within the false positives of CoMeta test set, we find tokens like\napoyo (lit. \u201csupport\u201d) or tensi\u00f3n (lit. \u201ctension\u201d) that appear more frequently used figuratively than with their literal sense or even only appear in the training set labeled as metaphor. Other wrong predictions are words that appeared in specific domains, such as texts that allude to the pandemic, e.g. ola (lit. \u201cwave\u201d), not detected as metaphorical due to its absence with metaphorical meaning in Meta4XNLI sentences. Something similar occurs with the misclassified tokens from Meta4XNLI test set. Most errors stem from conventional metaphors, namely gran, abrir, paso, claro (lit. \u201cgreat\u201d, \u201cto open\u201d, \u201cstep\u201d, \u201cclear\u201d) that occur regularly used with their metaphorical meaning. The lack of balanced examples might contribute to these predictions. However, we maintained the distribution as is, since our aim is to study the presence and prevalence of metaphor in natural language utterances.\nWe analysed a subset of 30 errors from each experimental setup, both EN and ES, and evaluation sets. We chose the predictions from XLM-RoBERTa since it is the model that performs better in this task in most scenarios. Although results show that models struggle more to identify the inference relation if there is a metaphorical expression involved in the pair sentences, we do not observe any particular feature within the errors. A remarkable aspect to highlight is the lower results of esXNLI with respect to XNLIdev and XNLItest in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. It could be motivated by the difference of the text domains, since XNLI is an extension of MultiNLI, which maintains the same set of textual domains. While esXNLI is a collection of texts from another set of genres and sources.\nRegarding EN, some of the errors might derive from the misclassification of some pairs, since annotations were developed on ES text. A metaphorical expression involved in the inference relationship in ES might not be present in its EN version and vice versa. Thus, samples from these two classes, pairs with and without metaphors, should be reexamined in EN and correctly classified for future experimentation."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section we present the experimental setup designed with the objective of testing the capabilities of multilingual MLMs on metaphor detection in cross-domain, crosslingual and multilingual settings. Furthermore, we also experiment with their ability to perform NLI when the correct inference requires understanding of metaphorical language.\nTaking advantage of previous available resources and the corpus we present in this work, we conducted a series of experiments to evaluate and fine-tune MLMs.\nThe configuration of cross-domain experiments is specified in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). For the other three setups, monolingual, multilingual and zero-shot cross-lingual, we performed hyperparameter tuning for batch size (8, 16, 36), linear decay (0.1, 0.01), learning rate (in the [1e-5-5e-5] interval), sequence length of 128 and epochs from 4 to 10. A warm-up of 6% is specified. The results of the hyperparameter tuning showed that after 4 epochs development loss started to increase, so results reported here are obtained with 4 epochs, batch size of 8, weight decay of 0.1 and learning rate of 5e-5.\nCross-domain: the aim of this set of experiments is to evaluate the performance of MLMs fine-tuned with CoMeta and VUAM datasets on Meta4XNLIES and Meta4XNLIEN, respectively, since each dataset contains texts from different domains. The motivation is to explore the impact of text features and genres on the performance, as well as annotation criteria Aghazadeh, Fayyaz, and\nYaghoobzadeh (2022  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###); Lai, Toral, and Nissim (2023  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###). To do so, we chose the models with best performance from monolingual experiments developed in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). We conducted the evaluation on various data splits: within each source dataset, XNLIdev, XNLItest and esXNLI, we evaluated premises and hypotheses separated and combined. This is due to the dissimilarities and the unequal distribution of metaphorical expressions between premises and hypotheses sentences mentioned in Subsection 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nMonolingual: this scenario comprises the fine-tuning and evaluation of the MLM on Meta4XNLIES and Meta4XNLIEN separately. To accomplish this task, we split Meta4XNLI into train, development and test sets (0.6-0.2-0.2). We equally distributed the data to ensure each partition is balanced in terms of source datasets sentences and metaphor occurrence. Data statistics are detailed in Appendix A  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. These partitions will be used as well in subsequent multilingual and zero-shot cross-lingual scenarios. In addition to fine-tuning and evaluation on Meta4XNLIES and Meta4XNLIEN, we evaluated each trained monolingual model with the test sets of CoMeta and VUAM, following the same reasoning as in cross-domain experiments.\nMultilingual: the purpose of these experiments is to explore whether MLMs benefit from being trained on data in multiple languages. In this case, we combined Meta4XNLIES and Meta4XNLIEN train splits to fine-tune the models. Subsequently, we evaluated the trained models on each language test set, in order to analyse the impact on the performance for each language. The data splits used correspond to those from monolingual experiments.\nZero-shot cross-lingual: in this scenario we explore to what extent MLMs are able to generalize knowledge and metaphor transfer between these two languages in question. Therefore, we fine-tune the models with Meta4XNLI data in one language and evaluate it on the test set of the other. Data partitions used for these experiments are the same as in monolingual an multilingual scenarios.\nWe carried out two sets of experiments to evaluate metaphor interpretation within the NLI task. We performed hyperparameter tuning, with the same range of parameters specified on the task of metaphor detection. We report best results obtained on the development set, after 4 epochs, batch size of 8, learning rate of 1e-5, weight decay of 0.1 and 512 sequence length.\nThe purpose of the first experiments is to examine if the presence of metaphorical expressions in premise-hypothesis pairs impacts the performance of models in the NLI task. To that end, we fine-tuned the MLMs for the task with the MultiNLI dataset. Then, we evaluated them with Meta4XNLI. Among each source dataset, we discriminated pairs with metaphors relevant to the inference relationship from those without metaphorical expressions. We developed the evaluation on each of these subsets and for each language separately, e.g. one evaluation on XNLIdev metaphor set and another on XNLIdev without metaphors set, in EN and then in ES.\nThe goal of the second set of experiments is to analyse the effect on models performance of not being \u201cexposed\u201d to instances with metaphorical expressions during training. With this aim in mind, we extracted pairs with and without metaphors from Meta4XNLI train, dev and set splits. On the first scenario, we fine-tuned the models only with pairs from the train set that did not contain any metaphorical expression. On the second scenario, we mixed pairs with and without metaphors and fine-tuned the models as well. In both cases, we evaluated on the test sets with and without metaphors for each language.\nIn addition to F1 score, we performed in-vocabluary (Inv) and out-of-vocabulary (Oov) evaluations to test the impact of the labels seen from training data during the learning process of the MLMs. Precision and Recall metrics are reported in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. For in-vocabulary evaluation, we computed F1 score taking into account predicted words seen in training labeled as metaphorical. While for out-of-vocabulary evaluation we considered only words not seen during training to compute the F1 score. We included this evaluation in all experiments but for the zero-shot cross-lingual setup, since the data of train and test sets are in different languages, thus the match of the exact same metaphorical token in both partitions is highly unlikely.\nAlthough the purpose of our experiments is not to beat state-of-the-art results but to evaluate the performance of MLMs on the task from a cross-lingual approach, we added two indicative baselines: on one hand, the system BasicBERT Li et al. (2023a  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###), which obtained 73.3 F1 score; on the other, the result of DeBERTa reported in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###), with 73.79 F1. We selected these results for comparison purposes, since they both were evaluated on the VUAM-2020 version of the dataset in EN used in the Shared Task 2020 Leong et al. (2020  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###) and in the same experimental setup we propose.\nCross-domain: We evaluated models trained on CoMeta and VUAM datasets with XNLI and esXNLI in ES and EN, respectively. From results reported in Table 7  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we can observe that in all cases mDeBERTa outperforms XLM-RoBERTa for ES. In EN the best result is obtained by DeBERTa, which also outperforms XLM-RoBERTa in all scenarios. In all datasets, except for XNLIdev in ES, premises sentences achieve better results than hypotheses and the combination of both. This goes in line with annotation statistics in Tables 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### and 5  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, which show that premises contain a greater ratio of metaphors per sentence than hypotheses in both languages. In ES, in-vocabulary evaluation outperforms the general F1 score while out-of-vocabulary evaluation results decrease. The small distance in points of this cross-domain evaluation in ES exhibit stability of the models when it comes to predicting metaphors and coherence in annotations between both datasets despite the difference of text domains. Nevertheless, EN results do not demonstrate such consistency, as out-of-vocabulary obtains higher results than the overall F1 score. This might be due to the different labeling criteria used in VUAM and Meta4XNLI, as we mentioned in 3.2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. These discrepancies are also reflected in a significant drop in performance with respect to ES experiments and in-domain evaluation with VUAM. The high recall scores from Table 16  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### show that the model tends to predict many metaphors, however, low precision scores indicate that a small amount of these predictions are correct.\n.\nMonolingual: Results from this set of experiments are specified in Table 8  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. After fine-tuning and evaluating the models with Meta4XNLIES and Meta4XNLIEN, the highest overall F1 score is obtained by mDeBERTa in ES. On the other hand, XLM-RoBERTa achieves better performance than mDeBERTa in EN but still lower than in ES. In both languages, in-vocabulary evaluation results are higher than the overall ones and out-of-vocabulary results, lower. This is not the case when we use VUAM corpus for testing the model fine-tuned with Meta4XNLIEN. In this setup, similarly to results from cross-domain experiments, performance drastically falls to 32.42 F1 score and out-of-vocabulary results are the highest. The reason behind might be the mismatches in the annotation process. In ES, when evaluating CoMeta, we also encounter a decrease in the performance, however, it is more subtle: from 67.17 to 56.24, obtained by XLM-RoBERTa. This might be caused by the variety of text genres and dissimilarities between sentences from each dataset.\nMultilingual: In this set of experiments we assembled Meta4XNLIEN and Meta4XNLIES to train the MLMs. The evaluation is conducted for each language separately and results detailed in Table 9  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. Best results in ES are obtained by mDeBERTa, which are higher than the top result from monolingual experiments. In EN, mDeBERTa is the model that achieves better performance but very close to that of XLM-RoBERTa. The highest F1 score is 8 points lower than that of ES but outweighs EN monolingual results. This suggests that the combination of parallel multilingual data for training is beneficial for the performance of the models.\nZero-shot cross-lingual: In these experiments we perform evaluation of Meta4XNLI in the opposite language to that utilised for training. Results are reported in Table 10  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. XLM-RoBERTa performance exceeds that of mDeBERTa in both languages. Nonetheless, F1 score for EN is almost 20 points lower than ES evaluation results. In addition to the differences in annotation criteria between languages and datasets, another aspect to bear in mind in this scenario could be the number of positive examples present in training sets. As we explained in Section 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, Meta4XNLIEN contains a higher number of metaphorical instances, thus models are exposed to a greater variety of examples that can be transferred to ES. While a more reduced amount of instances in Meta4XNLIES seen during training might hinder model\u2019s generalization ability, as the low recall and high precision scores show in Table 19  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nIn the first setup, we fine-tuned the models for the NLI task with MultiNLI dataset Williams, Nangia, and Bowman (2018  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###). Then we conducted the evaluation with two different splits for each source dataset conforming Meta4XNLI: pairs with at least one metaphorical expression and pairs lacking metaphors. From accuracy scores reported in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### we can observe certain variability in the results. In the majority of cases, XLM-RoBERTa achieves better performance than mDeBERTa. We do observe a tendency of higher results on the sets of pairs without metaphors than on the set with metaphors. The exceptions to this current are the results from XLM-RoBERTa for XNLIdev and XNLItest in ES. In these partitions, the subset of pairs with metaphorical expressions obtained better results, although the difference does not even reach one point. We hypothesize the original language of the dataset might be involved in this performance, since XNLI includes natural utterances of EN that were afterwards manually translated to ES, thus some artifacts might have been introduced during this process Artetxe, Labaka, and Agirre (2020  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###).\nThe second scenario consisted in fine-tuning the models on two setups: a) with pairs without metaphors and b) pairs with and without metaphors. We performed the evaluation on subsets split by the criterion of metaphor occurrence as well. Results reported in Table 12  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### show XLM-RoBERTa outperforming mDeBERTa in all contexts. Both models show best results for the NLI task on the examples without metaphors and the lowest performance with pairs that contain metaphorical expressions. This outcome replicates in both languages and experimental setups a) and b).\nIn this section, we manually inspected a subset of erroneous cases in order to provide a qualitative insight of results and with the intention of finding potential explanations of errors and models performance for both detection and interpretation tasks.\nWe selected the predictions from the model that obtained highest F1 score from the monolingual experiments in ES for both Meta4XNLI and CoMeta evaluations. We extracted false negatives and false positives and grouped the tokens by their number of occurrences. Within the false positives of CoMeta test set, we find tokens like\napoyo (lit. \u201csupport\u201d) or tensi\u00f3n (lit. \u201ctension\u201d) that appear more frequently used figuratively than with their literal sense or even only appear in the training set labeled as metaphor. Other wrong predictions are words that appeared in specific domains, such as texts that allude to the pandemic, e.g. ola (lit. \u201cwave\u201d), not detected as metaphorical due to its absence with metaphorical meaning in Meta4XNLI sentences. Something similar occurs with the misclassified tokens from Meta4XNLI test set. Most errors stem from conventional metaphors, namely gran, abrir, paso, claro (lit. \u201cgreat\u201d, \u201cto open\u201d, \u201cstep\u201d, \u201cclear\u201d) that occur regularly used with their metaphorical meaning. The lack of balanced examples might contribute to these predictions. However, we maintained the distribution as is, since our aim is to study the presence and prevalence of metaphor in natural language utterances.\nWe analysed a subset of 30 errors from each experimental setup, both EN and ES, and evaluation sets. We chose the predictions from XLM-RoBERTa since it is the model that performs better in this task in most scenarios. Although results show that models struggle more to identify the inference relation if there is a metaphorical expression involved in the pair sentences, we do not observe any particular feature within the errors. A remarkable aspect to highlight is the lower results of esXNLI with respect to XNLIdev and XNLItest in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. It could be motivated by the difference of the text domains, since XNLI is an extension of MultiNLI, which maintains the same set of textual domains. While esXNLI is a collection of texts from another set of genres and sources.\nRegarding EN, some of the errors might derive from the misclassification of some pairs, since annotations were developed on ES text. A metaphorical expression involved in the inference relationship in ES might not be present in its EN version and vice versa. Thus, samples from these two classes, pairs with and without metaphors, should be reexamined in EN and correctly classified for future experimentation.\nIn this work, we provided Meta4XNLI, a cross-lingual parallel dataset in ES and EN labeled for metaphor detection and interpretation framed within the task of NLI. With this new resource that contains parallel text and annotations, we developed a series of experiments to assess the capabilities of MLMs when dealing with this kind of figurative expressions present in natural language utterances.\nRegarding the task of metaphor detection, after the annotation process and experiments results, we can conclude the importance of establishing a unified criterion for annotation that is valid for different languages if the aim is to continue researching cross-lingual approaches. In addition, the semi-automatic process of annotation followed for EN shows that automatic labeling of cross-lingual metaphor is far from trivial. Metaphorical expressions are language and culture dependant. Moreover, the translation of the data introduces a new layer in which metaphorical expressions can either be lost from the translation of the source to the target language, or can be introduced in the target language by means of the translator, either human or automatic. Further work to explore automatic annotation methodologies would be of considerable value in order to reduce the demanding workload and effort of manual labelling in more than one language.\nWith respect to results, the purpose of our experiments is not to overcome the state-of-the-art results, but to evaluate models performance from a cross-lingual approach. Best results are obtained when Meta4XNLI in both languages is used for training. The augmentation of the training set size and the parallel annotations might boost this performance. Cross-domain and monolingual experiments show how the lack of consistency in the annotation criteria affects the performance of models. This can be observed as well in the zero-shot cross-lingual setup, although the scenario of training in EN and evaluating in ES shows competitive performance. It should be noted that the EN set contains a larger number of instances annotated as metaphorical in the training set. In addition, the in-vocabulary and out-of-vocabulary evaluation points to some kind of bias in the learning process. This could stem from the fact that the majority of the metaphor instances are conventional or due to lexical memorization Levy et al. (2015  ###reference_b39###  ###reference_b39###  ###reference_b39###  ###reference_b39###); Boisson, Espinosa-Anke, and\nCamacho-Collados (2023  ###reference_b13###  ###reference_b13###  ###reference_b13###  ###reference_b13###). Future research on this line of work should be carried out to clarify this issue.\nFor metaphor interpretation, we evaluated the ability of MLMs to understand metaphorical expressions framed within the task of NLI. We provide parallel annotations at premise-hypothesis pair level that mark if the presence of metaphorical expressions is relevant for the inference relationship. We exploited this information to conduct our experiments. From reported results, we can recognize a tendency of the models to show a lower performance with pairs that contain at least one metaphorical expression. However, this trend breaks with XLM-RoBERTa results when fine-tuned in ES and evaluated on the XNLIdev and XNLItest in the same language. Since these sets are sentences translated from EN, we presume the translation process might induce biases in metaphor occurrence and the \u201cnaturalness\u201d of the sentences. Similar to metaphor detection, future work to analyse the impact of translation in the development of metaphor parallel resources should be explored for the task of metaphor interpretation, as well as additional experimentation from a multilingual perspective.\nMetaphor annotation is an inherently subjective task. This variance in annotations is reflected on Meta4XNLIEN, due to the different criterion employed through the annotation process. Labels in this language should be updated and further revised to improve their quality. Disagreement and subjectivity could be counterbalanced by a larger number of annotators, in order to develop more consistent and reliable labeled data, however it requires a huge amount of time and workload. Data augmentation and semi-automatic methods could be exploited to create larger datasets with similar characteristics to the one we present and extend it to more languages, since most corpora available for metaphor processing is of reduced size and limited to a narrow set of languages. The existence of parallel resources in multiple languages others than EN that reflect cultural and real world knowledge nuances is of great importance to continue researching such a complex phenomenon as figurative language, specifically metaphors."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental setup",
            "text": "With the aim of developing cross-lingual experiments, we chose the multilingual language models and checkpoints that obtained best results in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###): mDeBERTa (base) and XLM-RoBERTa (large) Conneau et al. (2020  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###), the multilingual versions of DeBERTa He, Gao, and Chen (2021  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###  ###reference_b29###) and RoBERTa Liu et al. (2019  ###reference_b44###  ###reference_b44###  ###reference_b44###  ###reference_b44###  ###reference_b44###  ###reference_b44###  ###reference_b44###) respectively.\nTaking advantage of previous available resources and the corpus we present in this work, we conducted a series of experiments to evaluate and fine-tune MLMs.\nThe configuration of cross-domain experiments is specified in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). For the other three setups, monolingual, multilingual and zero-shot cross-lingual, we performed hyperparameter tuning for batch size (8, 16, 36), linear decay (0.1, 0.01), learning rate (in the [1e-5-5e-5] interval), sequence length of 128 and epochs from 4 to 10. A warm-up of 6% is specified. The results of the hyperparameter tuning showed that after 4 epochs development loss started to increase, so results reported here are obtained with 4 epochs, batch size of 8, weight decay of 0.1 and learning rate of 5e-5.\nCross-domain: the aim of this set of experiments is to evaluate the performance of MLMs fine-tuned with CoMeta and VUAM datasets on Meta4XNLIES and Meta4XNLIEN, respectively, since each dataset contains texts from different domains. The motivation is to explore the impact of text features and genres on the performance, as well as annotation criteria Aghazadeh, Fayyaz, and\nYaghoobzadeh (2022  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###  ###reference_b3###); Lai, Toral, and Nissim (2023  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###  ###reference_b33###). To do so, we chose the models with best performance from monolingual experiments developed in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###). We conducted the evaluation on various data splits: within each source dataset, XNLIdev, XNLItest and esXNLI, we evaluated premises and hypotheses separated and combined. This is due to the dissimilarities and the unequal distribution of metaphorical expressions between premises and hypotheses sentences mentioned in Subsection 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nMonolingual: this scenario comprises the fine-tuning and evaluation of the MLM on Meta4XNLIES and Meta4XNLIEN separately. To accomplish this task, we split Meta4XNLI into train, development and test sets (0.6-0.2-0.2). We equally distributed the data to ensure each partition is balanced in terms of source datasets sentences and metaphor occurrence. Data statistics are detailed in Appendix A  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. These partitions will be used as well in subsequent multilingual and zero-shot cross-lingual scenarios. In addition to fine-tuning and evaluation on Meta4XNLIES and Meta4XNLIEN, we evaluated each trained monolingual model with the test sets of CoMeta and VUAM, following the same reasoning as in cross-domain experiments.\nMultilingual: the purpose of these experiments is to explore whether MLMs benefit from being trained on data in multiple languages. In this case, we combined Meta4XNLIES and Meta4XNLIEN train splits to fine-tune the models. Subsequently, we evaluated the trained models on each language test set, in order to analyse the impact on the performance for each language. The data splits used correspond to those from monolingual experiments.\nZero-shot cross-lingual: in this scenario we explore to what extent MLMs are able to generalize knowledge and metaphor transfer between these two languages in question. Therefore, we fine-tune the models with Meta4XNLI data in one language and evaluate it on the test set of the other. Data partitions used for these experiments are the same as in monolingual an multilingual scenarios.\nWe carried out two sets of experiments to evaluate metaphor interpretation within the NLI task. We performed hyperparameter tuning, with the same range of parameters specified on the task of metaphor detection. We report best results obtained on the development set, after 4 epochs, batch size of 8, learning rate of 1e-5, weight decay of 0.1 and 512 sequence length.\nThe purpose of the first experiments is to examine if the presence of metaphorical expressions in premise-hypothesis pairs impacts the performance of models in the NLI task. To that end, we fine-tuned the MLMs for the task with the MultiNLI dataset. Then, we evaluated them with Meta4XNLI. Among each source dataset, we discriminated pairs with metaphors relevant to the inference relationship from those without metaphorical expressions. We developed the evaluation on each of these subsets and for each language separately, e.g. one evaluation on XNLIdev metaphor set and another on XNLIdev without metaphors set, in EN and then in ES.\nThe goal of the second set of experiments is to analyse the effect on models performance of not being \u201cexposed\u201d to instances with metaphorical expressions during training. With this aim in mind, we extracted pairs with and without metaphors from Meta4XNLI train, dev and set splits. On the first scenario, we fine-tuned the models only with pairs from the train set that did not contain any metaphorical expression. On the second scenario, we mixed pairs with and without metaphors and fine-tuned the models as well. In both cases, we evaluated on the test sets with and without metaphors for each language.\nIn addition to F1 score, we performed in-vocabluary (Inv) and out-of-vocabulary (Oov) evaluations to test the impact of the labels seen from training data during the learning process of the MLMs. Precision and Recall metrics are reported in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. For in-vocabulary evaluation, we computed F1 score taking into account predicted words seen in training labeled as metaphorical. While for out-of-vocabulary evaluation we considered only words not seen during training to compute the F1 score. We included this evaluation in all experiments but for the zero-shot cross-lingual setup, since the data of train and test sets are in different languages, thus the match of the exact same metaphorical token in both partitions is highly unlikely.\nAlthough the purpose of our experiments is not to beat state-of-the-art results but to evaluate the performance of MLMs on the task from a cross-lingual approach, we added two indicative baselines: on one hand, the system BasicBERT Li et al. (2023a  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###), which obtained 73.3 F1 score; on the other, the result of DeBERTa reported in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###), with 73.79 F1. We selected these results for comparison purposes, since they both were evaluated on the VUAM-2020 version of the dataset in EN used in the Shared Task 2020 Leong et al. (2020  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###) and in the same experimental setup we propose.\nCross-domain: We evaluated models trained on CoMeta and VUAM datasets with XNLI and esXNLI in ES and EN, respectively. From results reported in Table 7  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we can observe that in all cases mDeBERTa outperforms XLM-RoBERTa for ES. In EN the best result is obtained by DeBERTa, which also outperforms XLM-RoBERTa in all scenarios. In all datasets, except for XNLIdev in ES, premises sentences achieve better results than hypotheses and the combination of both. This goes in line with annotation statistics in Tables 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### and 5  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, which show that premises contain a greater ratio of metaphors per sentence than hypotheses in both languages. In ES, in-vocabulary evaluation outperforms the general F1 score while out-of-vocabulary evaluation results decrease. The small distance in points of this cross-domain evaluation in ES exhibit stability of the models when it comes to predicting metaphors and coherence in annotations between both datasets despite the difference of text domains. Nevertheless, EN results do not demonstrate such consistency, as out-of-vocabulary obtains higher results than the overall F1 score. This might be due to the different labeling criteria used in VUAM and Meta4XNLI, as we mentioned in 3.2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. These discrepancies are also reflected in a significant drop in performance with respect to ES experiments and in-domain evaluation with VUAM. The high recall scores from Table 16  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### show that the model tends to predict many metaphors, however, low precision scores indicate that a small amount of these predictions are correct.\n.\nMonolingual: Results from this set of experiments are specified in Table 8  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. After fine-tuning and evaluating the models with Meta4XNLIES and Meta4XNLIEN, the highest overall F1 score is obtained by mDeBERTa in ES. On the other hand, XLM-RoBERTa achieves better performance than mDeBERTa in EN but still lower than in ES. In both languages, in-vocabulary evaluation results are higher than the overall ones and out-of-vocabulary results, lower. This is not the case when we use VUAM corpus for testing the model fine-tuned with Meta4XNLIEN. In this setup, similarly to results from cross-domain experiments, performance drastically falls to 32.42 F1 score and out-of-vocabulary results are the highest. The reason behind might be the mismatches in the annotation process. In ES, when evaluating CoMeta, we also encounter a decrease in the performance, however, it is more subtle: from 67.17 to 56.24, obtained by XLM-RoBERTa. This might be caused by the variety of text genres and dissimilarities between sentences from each dataset.\nMultilingual: In this set of experiments we assembled Meta4XNLIEN and Meta4XNLIES to train the MLMs. The evaluation is conducted for each language separately and results detailed in Table 9  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. Best results in ES are obtained by mDeBERTa, which are higher than the top result from monolingual experiments. In EN, mDeBERTa is the model that achieves better performance but very close to that of XLM-RoBERTa. The highest F1 score is 8 points lower than that of ES but outweighs EN monolingual results. This suggests that the combination of parallel multilingual data for training is beneficial for the performance of the models.\nZero-shot cross-lingual: In these experiments we perform evaluation of Meta4XNLI in the opposite language to that utilised for training. Results are reported in Table 10  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. XLM-RoBERTa performance exceeds that of mDeBERTa in both languages. Nonetheless, F1 score for EN is almost 20 points lower than ES evaluation results. In addition to the differences in annotation criteria between languages and datasets, another aspect to bear in mind in this scenario could be the number of positive examples present in training sets. As we explained in Section 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, Meta4XNLIEN contains a higher number of metaphorical instances, thus models are exposed to a greater variety of examples that can be transferred to ES. While a more reduced amount of instances in Meta4XNLIES seen during training might hinder model\u2019s generalization ability, as the low recall and high precision scores show in Table 19  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nIn the first setup, we fine-tuned the models for the NLI task with MultiNLI dataset Williams, Nangia, and Bowman (2018  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###). Then we conducted the evaluation with two different splits for each source dataset conforming Meta4XNLI: pairs with at least one metaphorical expression and pairs lacking metaphors. From accuracy scores reported in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### we can observe certain variability in the results. In the majority of cases, XLM-RoBERTa achieves better performance than mDeBERTa. We do observe a tendency of higher results on the sets of pairs without metaphors than on the set with metaphors. The exceptions to this current are the results from XLM-RoBERTa for XNLIdev and XNLItest in ES. In these partitions, the subset of pairs with metaphorical expressions obtained better results, although the difference does not even reach one point. We hypothesize the original language of the dataset might be involved in this performance, since XNLI includes natural utterances of EN that were afterwards manually translated to ES, thus some artifacts might have been introduced during this process Artetxe, Labaka, and Agirre (2020  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###).\nThe second scenario consisted in fine-tuning the models on two setups: a) with pairs without metaphors and b) pairs with and without metaphors. We performed the evaluation on subsets split by the criterion of metaphor occurrence as well. Results reported in Table 12  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### show XLM-RoBERTa outperforming mDeBERTa in all contexts. Both models show best results for the NLI task on the examples without metaphors and the lowest performance with pairs that contain metaphorical expressions. This outcome replicates in both languages and experimental setups a) and b).\nWe selected the predictions from the model that obtained highest F1 score from the monolingual experiments in ES for both Meta4XNLI and CoMeta evaluations. We extracted false negatives and false positives and grouped the tokens by their number of occurrences. Within the false positives of CoMeta test set, we find tokens like\napoyo (lit. \u201csupport\u201d) or tensi\u00f3n (lit. \u201ctension\u201d) that appear more frequently used figuratively than with their literal sense or even only appear in the training set labeled as metaphor. Other wrong predictions are words that appeared in specific domains, such as texts that allude to the pandemic, e.g. ola (lit. \u201cwave\u201d), not detected as metaphorical due to its absence with metaphorical meaning in Meta4XNLI sentences. Something similar occurs with the misclassified tokens from Meta4XNLI test set. Most errors stem from conventional metaphors, namely gran, abrir, paso, claro (lit. \u201cgreat\u201d, \u201cto open\u201d, \u201cstep\u201d, \u201cclear\u201d) that occur regularly used with their metaphorical meaning. The lack of balanced examples might contribute to these predictions. However, we maintained the distribution as is, since our aim is to study the presence and prevalence of metaphor in natural language utterances.\nWe analysed a subset of 30 errors from each experimental setup, both EN and ES, and evaluation sets. We chose the predictions from XLM-RoBERTa since it is the model that performs better in this task in most scenarios. Although results show that models struggle more to identify the inference relation if there is a metaphorical expression involved in the pair sentences, we do not observe any particular feature within the errors. A remarkable aspect to highlight is the lower results of esXNLI with respect to XNLIdev and XNLItest in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. It could be motivated by the difference of the text domains, since XNLI is an extension of MultiNLI, which maintains the same set of textual domains. While esXNLI is a collection of texts from another set of genres and sources.\nRegarding EN, some of the errors might derive from the misclassification of some pairs, since annotations were developed on ES text. A metaphorical expression involved in the inference relationship in ES might not be present in its EN version and vice versa. Thus, samples from these two classes, pairs with and without metaphors, should be reexamined in EN and correctly classified for future experimentation."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "In addition to F1 score, we performed in-vocabluary (Inv) and out-of-vocabulary (Oov) evaluations to test the impact of the labels seen from training data during the learning process of the MLMs. Precision and Recall metrics are reported in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. For in-vocabulary evaluation, we computed F1 score taking into account predicted words seen in training labeled as metaphorical. While for out-of-vocabulary evaluation we considered only words not seen during training to compute the F1 score. We included this evaluation in all experiments but for the zero-shot cross-lingual setup, since the data of train and test sets are in different languages, thus the match of the exact same metaphorical token in both partitions is highly unlikely.\nAlthough the purpose of our experiments is not to beat state-of-the-art results but to evaluate the performance of MLMs on the task from a cross-lingual approach, we added two indicative baselines: on one hand, the system BasicBERT Li et al. (2023a  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###  ###reference_b40###), which obtained 73.3 F1 score; on the other, the result of DeBERTa reported in Sanchez-Bayona and\nAgerri (2022  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###  ###reference_b61###), with 73.79 F1. We selected these results for comparison purposes, since they both were evaluated on the VUAM-2020 version of the dataset in EN used in the Shared Task 2020 Leong et al. (2020  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###) and in the same experimental setup we propose.\nCross-domain: We evaluated models trained on CoMeta and VUAM datasets with XNLI and esXNLI in ES and EN, respectively. From results reported in Table 7  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we can observe that in all cases mDeBERTa outperforms XLM-RoBERTa for ES. In EN the best result is obtained by DeBERTa, which also outperforms XLM-RoBERTa in all scenarios. In all datasets, except for XNLIdev in ES, premises sentences achieve better results than hypotheses and the combination of both. This goes in line with annotation statistics in Tables 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### and 5  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, which show that premises contain a greater ratio of metaphors per sentence than hypotheses in both languages. In ES, in-vocabulary evaluation outperforms the general F1 score while out-of-vocabulary evaluation results decrease. The small distance in points of this cross-domain evaluation in ES exhibit stability of the models when it comes to predicting metaphors and coherence in annotations between both datasets despite the difference of text domains. Nevertheless, EN results do not demonstrate such consistency, as out-of-vocabulary obtains higher results than the overall F1 score. This might be due to the different labeling criteria used in VUAM and Meta4XNLI, as we mentioned in 3.2  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. These discrepancies are also reflected in a significant drop in performance with respect to ES experiments and in-domain evaluation with VUAM. The high recall scores from Table 16  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### show that the model tends to predict many metaphors, however, low precision scores indicate that a small amount of these predictions are correct.\n.\nMonolingual: Results from this set of experiments are specified in Table 8  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. After fine-tuning and evaluating the models with Meta4XNLIES and Meta4XNLIEN, the highest overall F1 score is obtained by mDeBERTa in ES. On the other hand, XLM-RoBERTa achieves better performance than mDeBERTa in EN but still lower than in ES. In both languages, in-vocabulary evaluation results are higher than the overall ones and out-of-vocabulary results, lower. This is not the case when we use VUAM corpus for testing the model fine-tuned with Meta4XNLIEN. In this setup, similarly to results from cross-domain experiments, performance drastically falls to 32.42 F1 score and out-of-vocabulary results are the highest. The reason behind might be the mismatches in the annotation process. In ES, when evaluating CoMeta, we also encounter a decrease in the performance, however, it is more subtle: from 67.17 to 56.24, obtained by XLM-RoBERTa. This might be caused by the variety of text genres and dissimilarities between sentences from each dataset.\nMultilingual: In this set of experiments we assembled Meta4XNLIEN and Meta4XNLIES to train the MLMs. The evaluation is conducted for each language separately and results detailed in Table 9  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. Best results in ES are obtained by mDeBERTa, which are higher than the top result from monolingual experiments. In EN, mDeBERTa is the model that achieves better performance but very close to that of XLM-RoBERTa. The highest F1 score is 8 points lower than that of ES but outweighs EN monolingual results. This suggests that the combination of parallel multilingual data for training is beneficial for the performance of the models.\nZero-shot cross-lingual: In these experiments we perform evaluation of Meta4XNLI in the opposite language to that utilised for training. Results are reported in Table 10  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. XLM-RoBERTa performance exceeds that of mDeBERTa in both languages. Nonetheless, F1 score for EN is almost 20 points lower than ES evaluation results. In addition to the differences in annotation criteria between languages and datasets, another aspect to bear in mind in this scenario could be the number of positive examples present in training sets. As we explained in Section 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, Meta4XNLIEN contains a higher number of metaphorical instances, thus models are exposed to a greater variety of examples that can be transferred to ES. While a more reduced amount of instances in Meta4XNLIES seen during training might hinder model\u2019s generalization ability, as the low recall and high precision scores show in Table 19  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### in Appendix B  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###.\nIn the first setup, we fine-tuned the models for the NLI task with MultiNLI dataset Williams, Nangia, and Bowman (2018  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###  ###reference_b82###). Then we conducted the evaluation with two different splits for each source dataset conforming Meta4XNLI: pairs with at least one metaphorical expression and pairs lacking metaphors. From accuracy scores reported in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### we can observe certain variability in the results. In the majority of cases, XLM-RoBERTa achieves better performance than mDeBERTa. We do observe a tendency of higher results on the sets of pairs without metaphors than on the set with metaphors. The exceptions to this current are the results from XLM-RoBERTa for XNLIdev and XNLItest in ES. In these partitions, the subset of pairs with metaphorical expressions obtained better results, although the difference does not even reach one point. We hypothesize the original language of the dataset might be involved in this performance, since XNLI includes natural utterances of EN that were afterwards manually translated to ES, thus some artifacts might have been introduced during this process Artetxe, Labaka, and Agirre (2020  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###).\nThe second scenario consisted in fine-tuning the models on two setups: a) with pairs without metaphors and b) pairs with and without metaphors. We performed the evaluation on subsets split by the criterion of metaphor occurrence as well. Results reported in Table 12  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### show XLM-RoBERTa outperforming mDeBERTa in all contexts. Both models show best results for the NLI task on the examples without metaphors and the lowest performance with pairs that contain metaphorical expressions. This outcome replicates in both languages and experimental setups a) and b).\nIn this section, we manually inspected a subset of erroneous cases in order to provide a qualitative insight of results and with the intention of finding potential explanations of errors and models performance for both detection and interpretation tasks.\nWe selected the predictions from the model that obtained highest F1 score from the monolingual experiments in ES for both Meta4XNLI and CoMeta evaluations. We extracted false negatives and false positives and grouped the tokens by their number of occurrences. Within the false positives of CoMeta test set, we find tokens like\napoyo (lit. \u201csupport\u201d) or tensi\u00f3n (lit. \u201ctension\u201d) that appear more frequently used figuratively than with their literal sense or even only appear in the training set labeled as metaphor. Other wrong predictions are words that appeared in specific domains, such as texts that allude to the pandemic, e.g. ola (lit. \u201cwave\u201d), not detected as metaphorical due to its absence with metaphorical meaning in Meta4XNLI sentences. Something similar occurs with the misclassified tokens from Meta4XNLI test set. Most errors stem from conventional metaphors, namely gran, abrir, paso, claro (lit. \u201cgreat\u201d, \u201cto open\u201d, \u201cstep\u201d, \u201cclear\u201d) that occur regularly used with their metaphorical meaning. The lack of balanced examples might contribute to these predictions. However, we maintained the distribution as is, since our aim is to study the presence and prevalence of metaphor in natural language utterances.\nWe analysed a subset of 30 errors from each experimental setup, both EN and ES, and evaluation sets. We chose the predictions from XLM-RoBERTa since it is the model that performs better in this task in most scenarios. Although results show that models struggle more to identify the inference relation if there is a metaphorical expression involved in the pair sentences, we do not observe any particular feature within the errors. A remarkable aspect to highlight is the lower results of esXNLI with respect to XNLIdev and XNLItest in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. It could be motivated by the difference of the text domains, since XNLI is an extension of MultiNLI, which maintains the same set of textual domains. While esXNLI is a collection of texts from another set of genres and sources.\nRegarding EN, some of the errors might derive from the misclassification of some pairs, since annotations were developed on ES text. A metaphorical expression involved in the inference relationship in ES might not be present in its EN version and vice versa. Thus, samples from these two classes, pairs with and without metaphors, should be reexamined in EN and correctly classified for future experimentation.\nIn this work, we provided Meta4XNLI, a cross-lingual parallel dataset in ES and EN labeled for metaphor detection and interpretation framed within the task of NLI. With this new resource that contains parallel text and annotations, we developed a series of experiments to assess the capabilities of MLMs when dealing with this kind of figurative expressions present in natural language utterances.\nRegarding the task of metaphor detection, after the annotation process and experiments results, we can conclude the importance of establishing a unified criterion for annotation that is valid for different languages if the aim is to continue researching cross-lingual approaches. In addition, the semi-automatic process of annotation followed for EN shows that automatic labeling of cross-lingual metaphor is far from trivial. Metaphorical expressions are language and culture dependant. Moreover, the translation of the data introduces a new layer in which metaphorical expressions can either be lost from the translation of the source to the target language, or can be introduced in the target language by means of the translator, either human or automatic. Further work to explore automatic annotation methodologies would be of considerable value in order to reduce the demanding workload and effort of manual labelling in more than one language.\nWith respect to results, the purpose of our experiments is not to overcome the state-of-the-art results, but to evaluate models performance from a cross-lingual approach. Best results are obtained when Meta4XNLI in both languages is used for training. The augmentation of the training set size and the parallel annotations might boost this performance. Cross-domain and monolingual experiments show how the lack of consistency in the annotation criteria affects the performance of models. This can be observed as well in the zero-shot cross-lingual setup, although the scenario of training in EN and evaluating in ES shows competitive performance. It should be noted that the EN set contains a larger number of instances annotated as metaphorical in the training set. In addition, the in-vocabulary and out-of-vocabulary evaluation points to some kind of bias in the learning process. This could stem from the fact that the majority of the metaphor instances are conventional or due to lexical memorization Levy et al. (2015  ###reference_b39###  ###reference_b39###  ###reference_b39###  ###reference_b39###  ###reference_b39###); Boisson, Espinosa-Anke, and\nCamacho-Collados (2023  ###reference_b13###  ###reference_b13###  ###reference_b13###  ###reference_b13###  ###reference_b13###). Future research on this line of work should be carried out to clarify this issue.\nFor metaphor interpretation, we evaluated the ability of MLMs to understand metaphorical expressions framed within the task of NLI. We provide parallel annotations at premise-hypothesis pair level that mark if the presence of metaphorical expressions is relevant for the inference relationship. We exploited this information to conduct our experiments. From reported results, we can recognize a tendency of the models to show a lower performance with pairs that contain at least one metaphorical expression. However, this trend breaks with XLM-RoBERTa results when fine-tuned in ES and evaluated on the XNLIdev and XNLItest in the same language. Since these sets are sentences translated from EN, we presume the translation process might induce biases in metaphor occurrence and the \u201cnaturalness\u201d of the sentences. Similar to metaphor detection, future work to analyse the impact of translation in the development of metaphor parallel resources should be explored for the task of metaphor interpretation, as well as additional experimentation from a multilingual perspective.\nMetaphor annotation is an inherently subjective task. This variance in annotations is reflected on Meta4XNLIEN, due to the different criterion employed through the annotation process. Labels in this language should be updated and further revised to improve their quality. Disagreement and subjectivity could be counterbalanced by a larger number of annotators, in order to develop more consistent and reliable labeled data, however it requires a huge amount of time and workload. Data augmentation and semi-automatic methods could be exploited to create larger datasets with similar characteristics to the one we present and extend it to more languages, since most corpora available for metaphor processing is of reduced size and limited to a narrow set of languages. The existence of parallel resources in multiple languages others than EN that reflect cultural and real world knowledge nuances is of great importance to continue researching such a complex phenomenon as figurative language, specifically metaphors."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Error Analysis",
            "text": "In this section, we manually inspected a subset of erroneous cases in order to provide a qualitative insight of results and with the intention of finding potential explanations of errors and models performance for both detection and interpretation tasks.\nWe selected the predictions from the model that obtained highest F1 score from the monolingual experiments in ES for both Meta4XNLI and CoMeta evaluations. We extracted false negatives and false positives and grouped the tokens by their number of occurrences. Within the false positives of CoMeta test set, we find tokens like\napoyo (lit. \u201csupport\u201d) or tensi\u00f3n (lit. \u201ctension\u201d) that appear more frequently used figuratively than with their literal sense or even only appear in the training set labeled as metaphor. Other wrong predictions are words that appeared in specific domains, such as texts that allude to the pandemic, e.g. ola (lit. \u201cwave\u201d), not detected as metaphorical due to its absence with metaphorical meaning in Meta4XNLI sentences. Something similar occurs with the misclassified tokens from Meta4XNLI test set. Most errors stem from conventional metaphors, namely gran, abrir, paso, claro (lit. \u201cgreat\u201d, \u201cto open\u201d, \u201cstep\u201d, \u201cclear\u201d) that occur regularly used with their metaphorical meaning. The lack of balanced examples might contribute to these predictions. However, we maintained the distribution as is, since our aim is to study the presence and prevalence of metaphor in natural language utterances.\nWe analysed a subset of 30 errors from each experimental setup, both EN and ES, and evaluation sets. We chose the predictions from XLM-RoBERTa since it is the model that performs better in this task in most scenarios. Although results show that models struggle more to identify the inference relation if there is a metaphorical expression involved in the pair sentences, we do not observe any particular feature within the errors. A remarkable aspect to highlight is the lower results of esXNLI with respect to XNLIdev and XNLItest in Table 11  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. It could be motivated by the difference of the text domains, since XNLI is an extension of MultiNLI, which maintains the same set of textual domains. While esXNLI is a collection of texts from another set of genres and sources.\nRegarding EN, some of the errors might derive from the misclassification of some pairs, since annotations were developed on ES text. A metaphorical expression involved in the inference relationship in ES might not be present in its EN version and vice versa. Thus, samples from these two classes, pairs with and without metaphors, should be reexamined in EN and correctly classified for future experimentation.\nIn this work, we provided Meta4XNLI, a cross-lingual parallel dataset in ES and EN labeled for metaphor detection and interpretation framed within the task of NLI. With this new resource that contains parallel text and annotations, we developed a series of experiments to assess the capabilities of MLMs when dealing with this kind of figurative expressions present in natural language utterances.\nRegarding the task of metaphor detection, after the annotation process and experiments results, we can conclude the importance of establishing a unified criterion for annotation that is valid for different languages if the aim is to continue researching cross-lingual approaches. In addition, the semi-automatic process of annotation followed for EN shows that automatic labeling of cross-lingual metaphor is far from trivial. Metaphorical expressions are language and culture dependant. Moreover, the translation of the data introduces a new layer in which metaphorical expressions can either be lost from the translation of the source to the target language, or can be introduced in the target language by means of the translator, either human or automatic. Further work to explore automatic annotation methodologies would be of considerable value in order to reduce the demanding workload and effort of manual labelling in more than one language.\nWith respect to results, the purpose of our experiments is not to overcome the state-of-the-art results, but to evaluate models performance from a cross-lingual approach. Best results are obtained when Meta4XNLI in both languages is used for training. The augmentation of the training set size and the parallel annotations might boost this performance. Cross-domain and monolingual experiments show how the lack of consistency in the annotation criteria affects the performance of models. This can be observed as well in the zero-shot cross-lingual setup, although the scenario of training in EN and evaluating in ES shows competitive performance. It should be noted that the EN set contains a larger number of instances annotated as metaphorical in the training set. In addition, the in-vocabulary and out-of-vocabulary evaluation points to some kind of bias in the learning process. This could stem from the fact that the majority of the metaphor instances are conventional or due to lexical memorization Levy et al. (2015  ###reference_b39###  ###reference_b39###  ###reference_b39###  ###reference_b39###  ###reference_b39###  ###reference_b39###); Boisson, Espinosa-Anke, and\nCamacho-Collados (2023  ###reference_b13###  ###reference_b13###  ###reference_b13###  ###reference_b13###  ###reference_b13###  ###reference_b13###). Future research on this line of work should be carried out to clarify this issue.\nFor metaphor interpretation, we evaluated the ability of MLMs to understand metaphorical expressions framed within the task of NLI. We provide parallel annotations at premise-hypothesis pair level that mark if the presence of metaphorical expressions is relevant for the inference relationship. We exploited this information to conduct our experiments. From reported results, we can recognize a tendency of the models to show a lower performance with pairs that contain at least one metaphorical expression. However, this trend breaks with XLM-RoBERTa results when fine-tuned in ES and evaluated on the XNLIdev and XNLItest in the same language. Since these sets are sentences translated from EN, we presume the translation process might induce biases in metaphor occurrence and the \u201cnaturalness\u201d of the sentences. Similar to metaphor detection, future work to analyse the impact of translation in the development of metaphor parallel resources should be explored for the task of metaphor interpretation, as well as additional experimentation from a multilingual perspective.\nMetaphor annotation is an inherently subjective task. This variance in annotations is reflected on Meta4XNLIEN, due to the different criterion employed through the annotation process. Labels in this language should be updated and further revised to improve their quality. Disagreement and subjectivity could be counterbalanced by a larger number of annotators, in order to develop more consistent and reliable labeled data, however it requires a huge amount of time and workload. Data augmentation and semi-automatic methods could be exploited to create larger datasets with similar characteristics to the one we present and extend it to more languages, since most corpora available for metaphor processing is of reduced size and limited to a narrow set of languages. The existence of parallel resources in multiple languages others than EN that reflect cultural and real world knowledge nuances is of great importance to continue researching such a complex phenomenon as figurative language, specifically metaphors."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusions and Future Work",
            "text": "In this work, we provided Meta4XNLI, a cross-lingual parallel dataset in ES and EN labeled for metaphor detection and interpretation framed within the task of NLI. With this new resource that contains parallel text and annotations, we developed a series of experiments to assess the capabilities of MLMs when dealing with this kind of figurative expressions present in natural language utterances.\nRegarding the task of metaphor detection, after the annotation process and experiments results, we can conclude the importance of establishing a unified criterion for annotation that is valid for different languages if the aim is to continue researching cross-lingual approaches. In addition, the semi-automatic process of annotation followed for EN shows that automatic labeling of cross-lingual metaphor is far from trivial. Metaphorical expressions are language and culture dependant. Moreover, the translation of the data introduces a new layer in which metaphorical expressions can either be lost from the translation of the source to the target language, or can be introduced in the target language by means of the translator, either human or automatic. Further work to explore automatic annotation methodologies would be of considerable value in order to reduce the demanding workload and effort of manual labelling in more than one language.\nWith respect to results, the purpose of our experiments is not to overcome the state-of-the-art results, but to evaluate models performance from a cross-lingual approach. Best results are obtained when Meta4XNLI in both languages is used for training. The augmentation of the training set size and the parallel annotations might boost this performance. Cross-domain and monolingual experiments show how the lack of consistency in the annotation criteria affects the performance of models. This can be observed as well in the zero-shot cross-lingual setup, although the scenario of training in EN and evaluating in ES shows competitive performance. It should be noted that the EN set contains a larger number of instances annotated as metaphorical in the training set. In addition, the in-vocabulary and out-of-vocabulary evaluation points to some kind of bias in the learning process. This could stem from the fact that the majority of the metaphor instances are conventional or due to lexical memorization Levy et al. (2015  ###reference_b39###  ###reference_b39###  ###reference_b39###  ###reference_b39###  ###reference_b39###  ###reference_b39###  ###reference_b39###); Boisson, Espinosa-Anke, and\nCamacho-Collados (2023  ###reference_b13###  ###reference_b13###  ###reference_b13###  ###reference_b13###  ###reference_b13###  ###reference_b13###  ###reference_b13###). Future research on this line of work should be carried out to clarify this issue.\nFor metaphor interpretation, we evaluated the ability of MLMs to understand metaphorical expressions framed within the task of NLI. We provide parallel annotations at premise-hypothesis pair level that mark if the presence of metaphorical expressions is relevant for the inference relationship. We exploited this information to conduct our experiments. From reported results, we can recognize a tendency of the models to show a lower performance with pairs that contain at least one metaphorical expression. However, this trend breaks with XLM-RoBERTa results when fine-tuned in ES and evaluated on the XNLIdev and XNLItest in the same language. Since these sets are sentences translated from EN, we presume the translation process might induce biases in metaphor occurrence and the \u201cnaturalness\u201d of the sentences. Similar to metaphor detection, future work to analyse the impact of translation in the development of metaphor parallel resources should be explored for the task of metaphor interpretation, as well as additional experimentation from a multilingual perspective.\nMetaphor annotation is an inherently subjective task. This variance in annotations is reflected on Meta4XNLIEN, due to the different criterion employed through the annotation process. Labels in this language should be updated and further revised to improve their quality. Disagreement and subjectivity could be counterbalanced by a larger number of annotators, in order to develop more consistent and reliable labeled data, however it requires a huge amount of time and workload. Data augmentation and semi-automatic methods could be exploited to create larger datasets with similar characteristics to the one we present and extend it to more languages, since most corpora available for metaphor processing is of reduced size and limited to a narrow set of languages. The existence of parallel resources in multiple languages others than EN that reflect cultural and real world knowledge nuances is of great importance to continue researching such a complex phenomenon as figurative language, specifically metaphors."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Metaphor annotation is an inherently subjective task. This variance in annotations is reflected on Meta4XNLIEN, due to the different criterion employed through the annotation process. Labels in this language should be updated and further revised to improve their quality. Disagreement and subjectivity could be counterbalanced by a larger number of annotators, in order to develop more consistent and reliable labeled data, however it requires a huge amount of time and workload. Data augmentation and semi-automatic methods could be exploited to create larger datasets with similar characteristics to the one we present and extend it to more languages, since most corpora available for metaphor processing is of reduced size and limited to a narrow set of languages. The existence of parallel resources in multiple languages others than EN that reflect cultural and real world knowledge nuances is of great importance to continue researching such a complex phenomenon as figurative language, specifically metaphors."
        }
    ],
    "url": "http://arxiv.org/html/2404.07053v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.07053v1",
        "paper_title": "Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and Interpretation",
        "research_background": "### Motivation, Research Problem, and Relevant Prior Work\n\n**Motivation:**\nMetaphors are a pervasive feature of natural language, crucial for effective interaction between users and language models. Their significance extends beyond day-to-day communication, impacting various Natural Language Processing (NLP) applications like machine translation, political discourse analysis, and hate speech detection. Despite the pressing need to process metaphors efficiently, existing resources and models are predominantly English-centric, and the availability of metaphor-annotated datasets in other languages is limited. There is also a growing interest in metaphor interpretation, but existing datasets and approaches often lack natural language usage and might introduce biases due to synthetic annotation processes. The paper aims to bridge these gaps by proposing a multilingual parallel corpus for metaphor detection and interpretation, focusing on Spanish (ES) and English (EN).\n\n**Research Problem:**\nThe research addresses two primary problems:\n1. **Metaphor Detection:** How to effectively detect metaphors in natural language sentences using sequence labeling tasks across multiple languages.\n2. **Metaphor Interpretation:** How to frame metaphor interpretation within the task of Natural Language Inference (NLI) and evaluate whether language models struggle with metaphorical expressions.\n\nThe paper introduces Meta4XNLI (M4X), a cross-lingual parallel dataset annotated for metaphor detection and interpretation in EN and ES, to facilitate research in these areas.\n\n**Relevant Prior Work:**\n1. **Theoretical Foundations:**\n   - Lakoff and Johnson's (1980) distinction between conceptual and linguistic metaphors.\n   - Widely adopted metaphor identification methodologies like MIPVU (Steen et al., 2010), which relies on the basic-contextual meaning mismatch.\n\n2. **Metaphor Detection:**\n   - **Historical Approaches:** Initial works were corpus-based (Charteris-Black, 2004; Semino, 2017), transitioning to deep learning techniques.\n   - **Datasets and Models:** VUAM dataset developed using MIPVU, and models like Mr-BERT, RoPPT, MelBERT, BasicBERT, and FrameBERT focused on metaphor span extraction.\n   - **Cross-lingual and Multilingual Efforts:** Resources like LCC (Mohammad, Shutova, and Turney, 2016), CCM corpora (Levin et al., 2014), and newer works exploiting neural machine translation and multilingual models (Berger, 2022; Schuster and Markert, 2023).\n\n3. **Metaphor Interpretation:**\n   - **Paraphrasing Task:** Initial works framed interpretation as paraphrasing (Shutova, 2010, 2013), with datasets containing literal paraphrases of metaphorical verbs.\n   - **Inference Task:** More recent works frame interpretation within the NLI task, with datasets like IMPLI (Stowe, Utama, and Gurevych, 2022), FLUTE (Chakrabarty et al., 2022), and others based on lexical substitution mechanisms for entailment and contradiction pairs.\n\n4. **Multilingual Efforts:**\n   - Various works have started exploring multilingual models for metaphor detection and interpretation, although datasets remain limited in size and linguistic diversity.\n\n**Contribution:**\nMeta4XNLI fills a critical gap by providing parallel metaphor annotations at token and premise-hypothesis pair levels in EN and ES across multiple domains. This resource enables cross-lingual metaphor analysis and facilitates the evaluation of metaphor processing in language models from a cross-lingual perspective. The dataset includes translations in both directions (EN->ES and ES->EN) and aims to overcome limitations like synthetic annotations and biases from previous works.",
        "methodology": "**Methodology:** The proposed method or model revolves around the creation and utilization of a crosslingual parallel corpus named **Meta4XNLI** for the tasks of metaphor detection and interpretation. This corpus is constructed through the combination of two existing datasets: **XNLI** and **esXNLI**. **Key Components:** 1. **Combination of Datasets:** - **XNLI:** This dataset, initially developed for cross-lingual evaluation with MultiNLI, contains parallel data where original English text (EN) has been human-translated into 14 other languages. For this work, only English (EN) and Spanish (ES) texts are utilized. XNLI includes 7500 premise-hypothesis pairs from 10 different text genres, split into 830 premises and 2490 hypotheses for XNLIdev, and 1670 premises and 5010 hypotheses for XNLItest. - **esXNLI:** Specifically, this dataset includes 2490 pairs that are collected natively in Spanish (ES) and then human-translated to English (EN). It spans five different text genres. 2. **Data Selection and Size:** - The Meta4XNLI leverages the volume and quality of these datasets to compile a crosslingual parallel corpus larger than most existing resources for metaphor processing. - Both XNLI and esXNLI provide naturally occurring language with spontaneous metaphor usage, which is key for real-world applicability. 3. **Translation Directionality:** - The distinct translation directions (EN > ES for XNLI and ES > EN for esXNLI) allow exploration into how translation affects metaphor cross-lingual transfer and influences model performance. 4. **Annotation Levels:** - The corpus permits annotations at both the token level for metaphor detection and the pair level for metaphor interpretation, thus addressing multiple granularity levels in metaphor processing. 5. **Method of Collection:** - Both XNLI and esXNLI share a collection methodology where premise sentences were sourced from various places in EN and ES. - Crowd-source workers were employed to create three hypotheses for each premise, one corresponding to each label: entailment, contradiction, and neutral. - The corpora are balanced regarding inference tags and text domains, contributing to robust and comprehensive model training and evaluation. The described methodology aims to harness these annotations and the parallel nature of the datasets for crosslingual metaphor detection and interpretation, providing a rich and expansive resource for this purpose.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Main Experiment Setup:\n\n1. **Models Used:**\n   - mDeBERTa (base)\n   - XLM-RoBERTa (large)\n\n2. **Datasets:**\n   - CoMeta\n   - VUAM\n   - Meta4XNLI (split into Meta4XNLIES for Spanish and Meta4XNLIEN for English)\n\n3. **Experimental Configurations:**\n   - Hyperparameter tuning for batch size (8, 16, 36), linear decay (0.1, 0.01), learning rate (1e-5 to 5e-5), sequence length (128), and epochs (4 to 10) with a 6% warm-up.\n\n4. **Evaluation Metrics:**\n   - F1 score\n   - In-vocabulary (Inv) and out-of-vocabulary (Oov) evaluations\n   - Precision and Recall metrics\n\n5. **Baselines for Comparison:**\n   - BasicBERT (73.3 F1 score)\n   - DeBERTa (73.79 F1 score)\n\n#### Experiment Scenarios:\n\n1. **Cross-domain:**\n   - Fine-tuning on CoMeta and VUAM datasets\n   - Evaluating on Meta4XNLIES and Meta4XNLIEN\n   - Different data splits: within each source dataset, XNLIdev, XNLItest, and esXNLI\n\n2. **Monolingual:**\n   - Fine-tuning and evaluating on Meta4XNLIES and Meta4XNLIEN separately\n   - Splits: train, development, and test sets (0.6-0.2-0.2)\n   - Evaluation on CoMeta and VUAM\n\n3. **Multilingual:**\n   - Combined training on Meta4XNLIES and Meta4XNLIEN train splits\n   - Evaluation on each language test set\n\n4. **Zero-shot Cross-lingual:**\n   - Fine-tuning on Meta4XNLI data in one language and evaluating on the test set of the other language\n\n#### Main Experimental Results:\n\n1. **Cross-domain Experiments:**\n   - mDeBERTa outperformed XLM-RoBERTa in Spanish (ES).\n   - DeBERTa outperformed in English (EN).\n   - In ES, in-vocabulary evaluation outperformed general F1 scores, while out-of-vocabulary evaluations dropped.\n   - EN results were less consistent, with out-of-vocabulary scores higher than overall F1, suggesting discrepancies in labeling criteria.\n\n2. **Monolingual Experiments:**\n   - mDeBERTa achieved the highest F1 score in ES.\n   - XLM-RoBERTa performed better in EN but was still lower compared to ES.\n   - In-vocabulary results were higher than overall F1 scores, while out-of-vocabulary scores were lower.\n   - Testing with VUAM led to significantly lower performance (32.42 F1 score).\n\n3. **Multilingual Experiments:**\n   - mDeBERTa achieved the best results in ES, outperforming monolingual experiments.\n   - In EN, mDeBERTa similarly outperformed but only marginally better than XLM-RoBERTa.\n   - Overall, multilingual training positively impacted model performance.\n\n4. **Zero-shot Cross-lingual Experiments:**\n   - XLM-RoBERTa exceeded mDeBERTa performance in both languages.\n   - EN results were almost 20 points lower than ES, potentially due to fewer positive examples in training sets and different annotation criteria.\n\n#### Interpretation Experiments:\n\n- Models fine-tuned on MultiNLI and evaluated on Meta4XNLI.\n- **Accuracy findings:**\n  - XLM-RoBERTa generally outperformed mDeBERTa.\n  - Higher results on pairs without metaphors.\n  - Some variability observed, particularly in sets with metaphors.\n  - Mixed training of pairs with and without metaphors led to better overall performance.\n\nThe main experiments conclusively indicate that:\n- mDeBERTa and XLM-RoBERTa are robust models for metaphor detection and interpretation across multiple languages.\n- Multilingual and cross-domain training consistently enhance model performance, though annotation differences and domain variance impact results.\n- Zero-shot cross-lingual capabilities show potential but require balanced datasets for effective generalization."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the performance of multilingual language models (MLMs) in metaphor detection and interpretation within a cross-lingual and multilingual context.",
            "experiment_process": "We performed a series of experiments involving different setups: cross-domain, monolingual, multilingual, and zero-shot cross-lingual. These experiments were conducted using the Meta4XNLI dataset, which includes data in both Spanish (ES) and English (EN). Each experimental setup involved fine-tuning and evaluating MLMs using various data splits and parameters, such as batch size, linear decay, learning rate, sequence length, and number of epochs. For metaphor detection, models were fine-tuned using datasets like CoMeta and VUAM, and their performance was evaluated on sections of Meta4XNLI. For metaphor interpretation, models were evaluated on the accuracy of inferring relationships between premise and hypothesis sentences that contain metaphors.",
            "result_discussion": "Cross-domain experiments showed that the models performed better with ES data compared to EN data, indicating issues related to annotation consistency and domain differences. Monolingual experiments revealed that the highest performance in ES was achieved by mDeBERTa, whereas in EN, XLM-RoBERTa performed better. Multilingual experiments demonstrated that training on multilingual data improved performance in both languages. Zero-shot cross-lingual experiments highlighted the challenge of generalizing knowledge across languages, with better results observed when models were trained on metaphorically richer datasets. Overall, the experiments underscored the need for consistent annotation criteria and suggested benefits from training on parallel multilingual data.",
            "ablation_id": "2404.07053v1.No1"
        },
        {
            "research_objective": "To investigate the impact of the presence of metaphorical expressions on the performance of MLMs in natural language inference (NLI).",
            "experiment_process": "Two sets of experiments were conducted. In the first set, models were fine-tuned on the MultiNLI dataset and evaluated on pairs from Meta4XNLI, categorized into those with metaphors and those without. In the second set, models were fine-tuned separately on pairs without metaphors and a mix of pairs with and without metaphors, and then evaluated on corresponding test sets. Hyperparameter tuning was performed, including adjustments for batch size, learning rate, and sequence length. The performance was assessed using accuracy scores to compare how well models understood inference relationships in the presence or absence of metaphorical expressions.",
            "result_discussion": "The accuracy scores showed variability with a general trend of better performance on pairs without metaphors. However, XLM-RoBERTa performed relatively well on ES versions of XNLIdev and XNLItest even with metaphors, suggesting potential artifacts introduced during translation. The second set of experiments revealed that models trained on data with mixed pairs performed better, but still showed lower accuracy when metaphors were involved. These findings suggest that metaphors pose a significant challenge for MLMs in NLI tasks, indicating the need for further work to improve metaphor understanding in models.",
            "ablation_id": "2404.07053v1.No2"
        }
    ]
}