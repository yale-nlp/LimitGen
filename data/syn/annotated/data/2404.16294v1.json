{
    "title": "LLM-Based Section Identifiers Excel on Open Source but Stumble in Real World Applications",
    "abstract": "Electronic health records (EHR) even though a boon for healthcare practitioners, are growing convoluted and longer every day. Sifting around these lengthy EHRs is taxing and becomes a cumbersome part of physician-patient interaction. Several approaches have been proposed to help alleviate this prevalent issue either via summarization or sectioning, however, only a few approaches have truly been helpful in the past. With the rise of automated methods, machine learning (ML) has shown promise in solving the task of identifying relevant sections in EHR. However, most ML methods rely on labeled data which is difficult to get in healthcare. Large language models (LLMs) on the other hand, have performed impressive feats in natural language processing (NLP), that too in a zero-shot manner, i.e. without any labeled data. To that end, we propose using LLMs to identify relevant section headers. We find that GPT-4 can effectively solve the task on both zero and few-shot settings as well as segment dramatically better than state-of-the-art methods. Additionally, we also annotate a much harder real world dataset and find that GPT-4 struggles to perform well, alluding to further research and harder benchmarks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### Modern day healthcare systems are increasingly moving towards large scale adoption of maintaining electronic health records (EHR) of patients Congress (2009  ###reference_b5###). EHRs help healthcare practitioners with relevant information about a patient such as history, medications, etc. However, in recent times this practice has led to very long and convoluted EHRs Rule et al. (2021  ###reference_b23###). Naturally, the need for better information retrieval tools emerged due to the progressively lengthy and unstructured doctor notes. One such need is the accurate identification of sections in an EHR, pertinent to a physician\u2019s inquiry. For instance, a question like \u201cWhat treatments has the patient undergone in the past?\u201d concerning prior treatments administered to a patient necessitates the swift extraction of information from the \u201ctreatments\u201d and \u201cpast medical history\u201d sections, while excluding sections related to \u201cancestral medical history\u201d. This swift extraction is vital for timely decision-making in patient care. Additionally, during critical procedures such as the evaluation of medical necessity for prior authorization requests, it is customary for experienced clinicians to locate vital data within specific sections. An illustrative case entails examining the \u201cphysical exam\u201d section to identify particular findings, such as signs of neurological disorders or movement-associated pain, indicating the need for additional diagnostic tests. The timely identification of such information is of utmost importance in ensuring the provision of appropriate care and reducing the risk of potential complications.\nIn general, regions found in EHR would often have a section heading preceding the body of the section, as can be seen in example Table 1  ###reference_###. Even though these section types have limited cardinality, however, more often than not, physicians would fail to adhere to standards and use lexical variations generated on the fly. Moreover, practitioners not only will generate lexical variations of sections on the fly but also completely new sections altogether for valid reasons like imaging reports, etc. Apart from these variations, oftentimes there would be no headers at all, even though the information present could ideally be part of a pre-existing section in a document or a new section altogether. While studies like Gao et al. (2022  ###reference_b8###) utilize the Subjective, Objective, Assessment and Plan heading (SOAP) framework, real-world clinical notes often contain sections beyond these categories. This limitation is further emphasized in Landes et al. (2022  ###reference_b13###), warranting further investigation and analysis.\nThe aforementioned factors have consequently contributed to the establishment of Section Identification (SI) as a distinct and enduring problem within the academic discourse (McKnight and Srinivasan, 2003  ###reference_b16###), making it an indispensable component of any clinical natural language processing (NLP) pipeline. A SI task entails finding regions of text that are semantically related to an aspect of a patient\u2019s medical profile. More importantly, it helps to improve pre-existing information retrieval systems by enabling them to be more targeted and specific. Lastly, in light of recent findings of the negative impact of note bloat within EHRs on even the most sophisticated systems Liu et al. (2022  ###reference_b14###), using SI to shorten or create from EHR, a sub-EHR specific to a given task would prove to be a worthwhile effort for humans and machines both.\nBecause finding sections and hence their corresponding headers involves inherent variability, machine learning (ML) methods have played an important role in this natural language processing Pomares-Quimbaya et al. (2019  ###reference_b21###). ML has increasingly been shown to be efficient in finding relevant sections within a document, however, a key drawback of traditional ML methods has been the dependence on labeled data Tepper et al. (2012  ###reference_b24###). Reliance on annotated data for training ML models to be able to predict the beginning and end of section headers has stalled the field from fully solving the task. The emergence of large language models (LLMs) in contemporary research presents a promising avenue to overcome the limitations inherent in traditional machine learning approaches, thereby expanding the scope of their applications.\nLLMs have emerged as the de-facto system for NLP in scenarios where data is scarce OpenAI (2023  ###reference_b19###). The key distinction between traditional Machine Learning (ML) models and Large Language Models (LLMs) lies in their ability to understand tasks in natural language. While traditional ML models require labeled data for training, LLMs can leverage pre-training on vast amounts of unstructured text data, enabling them to perform tasks with minimal task-specific fine-tuning. This makes ML possible in an unsupervised manner (no need for labeled data) and therefore opens room for applications in domains where annotated data is hard to acquire like healthcare.\nWhile LLMs have been evaluated on a wide array of NLP tasks in healthcare Nori et al. (2023  ###reference_b18###), they are yet to be evaluated on their effectiveness in segmenting a document into semantically relevant sections.\nIn this work, we address this gap and evaluate the efficacy of our approach on a widely-known datasets in the clinical medical domain. Findings show that GPT-4 OpenAI (2023  ###reference_b19###) almost solved the section identification problem on the benchmark open-sourced dataset, however, on a private dataset the performance lags. Our contributions are three-fold, listed as follows:\nWe show that GPT-4 can generate zero-shot headings of records with very high accuracy.\nContrary to the above, we find that its performance drops on internal real-world datasets.\nAn ontology of numerous section headers seen in real world EHR systems is shared which has much higher coverage."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Traditionally, SI task has been done using a pre-defined dictionary of plausible candidates. Pomares-Quimbaya et al. (2019  ###reference_b21###) performed a comprehensive survey and found that rule-based methods still dominated the array of methods proposed while ML systems increasingly achieved better coverage when combined in a hybrid manner with rule-based methods. McKnight and Srinivasan (2003  ###reference_b16###) later on extracted bag-of-words from MedLINE abstracts and used a support vector machine to train a classifier to categorize sentences into either Introduction, Method, Result, or Conclusion, demonstrating promising results. Similarly, Hirohata et al. (2008  ###reference_b10###) achieved very high accuracy by using conditional random fields to label scientific abstracts into Objectives, Methods, Results, and Conclusions.\nOver time and with the inclusion of ML, the field re-framed this problem as one of span-level entity identification i.e. the system would be tasked with predicting whether each token in a sequence belongs to one of the predefined section types using the Inside-Outside-Beginning (IOB) tagging system Ramshaw and Marcus (1999  ###reference_b22###). Tepper et al. (2012  ###reference_b24###) addresses the task of segmenting clinical records into distinct sections using a two-step approach. First, the section boundaries are identified. Then, the sections are passed to the second step, where a classifier is used to label each token as Begin, In or Out of the span of a section.\nNair et al. (2021  ###reference_b17###) proposes several transfer learning models based on clinical contextual embeddings for classifying clinical notes into the major SOAP sections Podder et al. (2023  ###reference_b20###).\nZhou et al. (2023  ###reference_b29###) investigates the effectiveness of continued pre-training in enhancing the transferability of clinical note section classification models. Both of the above papers resemble our work, however, they restrict them to SOAP sections and train specific models to do so. While the techniques devised so far have shown promise, to the best of our knowledge none of the previous works have tried in an unsupervised manner.\nWith the advent of LLMs (Devlin et al., 2018  ###reference_b6###; OpenAI, 2023  ###reference_b19###), several works have shown the efficacy of LLMs in doing unsupervised zero-shot information extraction. The primary method for interacting with generative LLMs is by the use of natural language prompts. Wei et al. (2022  ###reference_b28###) found a significant performance boost by asking the model to explain its chain of thought before answering the query. Further, Brown et al. (2020  ###reference_b3###) showed that additional performance can be gained by passing some examples as part of the prompt, they named it Few-Shot prompting. Wang et al. (2023  ###reference_b27###); Bian et al. (2023  ###reference_b2###); Ashok and Lipton (2023  ###reference_b1###) have shown the efficacy of prompting the LLM to extract biomedical named entities from scientific articles. More recently, Liu et al. (2023  ###reference_b15###) used GPT-4 to de-identify documents in a zero-shot manner. This hints at the immense document understanding capabilities of LLMs and opens doors to its application to a wide array of previously unresolved tasks such as SI.\nApart from the advancements in the field of ML and SI, to evaluate how well SI systems perform, a standardization of tasks as well as datasets is required. To that end, Uzuner et al. (2011  ###reference_b26###) first proposed a SI task as part of Informatics for Integrating Biology and the Bedside (i2b2) benchmarks. Recently, Landes et al. (2022  ###reference_b13###) argued that the previous dataset did not fully cover the nuances in SI task and proposed a dataset an order of magnitude larger as well as more comprehensive than one by Uzuner et al. (2011  ###reference_b26###). However, the dataset proposed by Landes et al. (2022  ###reference_b13###) is based on a clean source Johnson et al. (2016  ###reference_b12###), which oftentimes is not the case in real-world scenarios. To that end, we also annotated a real-world dataset to evaluate LLMs on it as well."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Datasets",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "i2b2 2010",
            "text": "In their study, Tepper et al. (2012  ###reference_b24###) meticulously curated a corpus comprising 183 annotated clinical notes extracted from a selection of discharge summaries within the i2b2 2010 Uzuner et al. (2011  ###reference_b26###) dataset. This dataset was annotated by an expert and served as a valuable resource for their research. However, owing to constraints imposed by Institutional Review Boards (IRBs), our current access to the i2b2 2010 dataset is limited. As a result, we were only able to procure clinical notes for 96 out of the originally annotated 183 documents."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "MedSecID",
            "text": "MedSecID (Landes et al., 2022  ###reference_b13###) is a publicly available corpus of 2,002 fully annotated medical notes from the MIMIC-III (Johnson et al., 2016  ###reference_b12###) clinical record database. Each note has been manually annotated with section boundaries and section labels (See Table 1  ###reference_### for an example of a typical clinical note consisting of well-defined sections). The section labels correspond to different types of information that are typically found in clinical notes, such as history of present illness, physical exam findings, and progress notes."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Real-world",
            "text": "In an increasingly digital world, one would be inclined to assume healthcare data also lives digitally. Surprisingly, that is not the case almost 75% of the healthcare dataset still lives in faxes CCSI (2022  ###reference_b4###) (see figure 1  ###reference_### for a sample handwritten and faxed clinical notes). Whereas all preexisting SI datasets are digitally derived from clean EHR systems, which even though offer us some insight into the performance of state of art, however, fail to paint the full picture. Therefore, we use an internal dataset of prior authorization requests derived from faxed-in images being transcribed to text via an optical character recognition system (OCR). These requests contain EHR of patients in the form of doctors\u2019 notes, submitted in both PDF and image formats. These documents lack a standardized structure, with segments and titles that can vary significantly in length. Although it\u2019s possible to group these titles into clusters of similar meaning, the language and number of titles differ across documents. Additionally, OCR inaccuracies arise from unclear text, spelling errors, complex table structures, and handwritten content, resulting in highly noisy input for any SI system to process."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Annotation Methods",
            "text": "###figure_2### In this section, we describe the dataset and the annotation design in our study. As we described before we decided to choose section identification (SI), a method to identify sections and sub-sections in EHR documents to split them into smaller text chunks and create some structure in these unstructured data. We designed a manual annotation task to identify these sections and create categorical section types. Below we explain the annotation task design, the result, and the challenges."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Annotation Design",
            "text": "We randomly selected 100 records from a pool of one million records we have in our corpus. These records are in two forms, PDF or fax images which doctors submit to insurance companies, and hence, can arrive from any arbitrary format. We refer to these records as documents in the span of this manuscript. These documents have no standard structures and sometimes they contain multiple patients information at the same time. Six annotators with higher education and non-native speakers of English carry the annotation task. Each annotates an equal amount and random selection of these documents."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Annotation Result",
            "text": "We aggregate the sections per document to form the final section and sub-section list. A total of 912 sections and subsections are identified which makes 14 sections and sub-sections on average per document. Then one annotator, different from the ones who have annotated the documents, categorized these sections and sub-sections into more general categories based on the Consolidated Clinical Document Architecture (C-CDA) implementation guide222C-CDA contains a library of CDA templates, incorporating and harmonizing previous efforts from Health Level Seven (HL7), Integrating the Healthcare Enterprise (IHE), and Health Information Technology Standards Panel (HITSP). https://www.hl7.org/ccdasearch/. In other words, the diverse categories are mapped to a category to unify them. This allows us to calculate IAA and be able to use the text semantic similarity method to find these sections in the unannotated documents. A total of 464 categories are coded of which 394 of these categories have a frequency of 1 and 70 categories have a frequency of 2 or more. We provide a small sample of the most frequent categories in Table 4  ###reference_### and Figure 2  ###reference_###.\n24 documents have been randomly selected and on each of these documents, a second annotator annotated the document. Further, we calculated the Jaccard similarity to report Inter-Annotator Agreement (IAA), The Jaccard similarity is a measure of the similarity between two sets of data. We obtained a Jaccard distance of 0.40, which is a fair agreement and an indication that the annotation task is challenging. The most diverse section and sub-section lists that each normalized into one section name are shown in table 4  ###reference_###. Notably, the diversity of these two general categories indicates the challenge involved in structuring and identifying these sections in these documents. In some cases, categories such as Order Report or Medication Reconciliation can be both a section and sub-section according to the annotation results. This characteristic does not enforce the decision to select the general category for these types.\nYou are a clinician and you read the given clinical document and identify section headers from them.\n Find section headers only from the clinical text.\n For each section header, return the answer as a JSON object by filling in the following dictionary.\n {section_title:  string representing the section header}\n Here are some clinical notes of a patient from a doctor. ### {context_text} ###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "Our task here is to take as input a document and output all the section headers found in it. For our underlying use case, we carried out testing with various LLMs like GPT-4 8k OpenAI (2023  ###reference_b19###), LLaMa-2 7B Touvron et al. (2023  ###reference_b25###), and more recent Mistral 7B Jiang et al. (2023  ###reference_b11###) prompting strategies333CoT A5  ###reference_###, One Shot A4  ###reference_### and Close Ended A6  ###reference_### prompting strategies are elaborated in appendix A  ###reference_###. (as shown in figure 3  ###reference_###) and contrasted them with a baseline experiment that used keyword search, regex, MedSpacy library Eyre et al. (2021  ###reference_b7###) and the best model reported by Landes et al. (2022  ###reference_b13###). MedSpacy is a clinical NLP toolkit built on the foundation of SpaCy, specifically designed to address the unique challenges of processing and extracting information from clinical text. This enables healthcare professionals to efficiently process and derive valuable insights from unstructured medical narratives. We did not restrict the tokens and used the entire clinical note for MedSecId. We extracted the actual section header using the header span mentioned in the MedSecId annotation and used it as the ground truth for our task.\nBecause of the longer length of real-world data, we used the 32k version of GPT-4 while keeping all the hyper-parameters to default such as the temperature, frequency penalty, and presence penalty to 0 and max tokens to 1000. Lastly, in this study, we utilized a privately hosted instance of GPT-4 to ensure the prevention of any potential data leakage. Prior to initiating the experiment, we implemented a thorough anonymization procedure to protect the dataset Protected health information (PHI). This involved substituting all personal identifiers, such as names, identification numbers, and ages, with fictitious entities.\nApart from the basic prompts, we also experiment with combining them with Few-Shot Brown et al. (2020  ###reference_b3###) and CoT Prompting Wei et al. (2022  ###reference_b28###) where we ask the LLM to think step-by-step along with providing an example of the clinical note and a list of headings. We keep the prompts same across all the datasets. Lastly, the evaluation metric used here is the exact match (EM) accuracy as well as precision (P), recall (R), and F1-score calculated by comparing GPT-4\u2019s output to that of ground truth in the Inside-Outside-Beginning (IOB) scheme Ramshaw and Marcus (1999  ###reference_b22###) as used in work by Landes et al. (2022  ###reference_b13###).\nSimilar GPT-4 experiments were conducted on i2b2 2010 dataset but as the context length of i2b2 was smaller, in all the experiments we use GPT-4 8K. Lastly, because of cost constraints, we chose the best-performing model on above mentioned benchmarks to be evaluated against our internal real-world dataset."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "###table_1### Even though GPT-4 was able to perform very well on open source benchmark datasets, it was unable to reach the same level of performance on our internal corpus due to its complexity as shown in table 7  ###reference_###. Experiments showed that GPT-4 was able to achieve an accuracy of only 37% in contrast to that of 96% on MedSecId corpus. LLaMa-2 and MedSpacy performed equally well, in that, former achieved higher recall than latter. This can be attributed to the global knowledge encoded in the LLMs, which is not the case with MedSpacy, while on the other hand MedSpacy would be much faster to run with less overhead.\nResults in table 5  ###reference_### and 6  ###reference_### show that one-shot GPT-4 OpenAI (2023  ###reference_b19###) performed the best and achieved a new state of the art on MedSecId outperforming previous models by a significant margin. This unsupervised methodology beats all the supervised models on the MedSecId corpus Landes et al. (2022  ###reference_b13###). Similarly, one-shot also had a state-of-the-art performance on i2b2 2010 dataset. On the other hand, LLaMa-2 did not perform as well as GPT-4, but nevertheless had on par performance with regex. Additionally, LLaMa-2 Touvron et al. (2023  ###reference_b25###) performance on i2b2 dataset came very close to that of GPT-4 itself. This disparity in performance of LLaMa-2 as well as its variation in results across the experiments leads to inconclusive results. Lastly, Mistral Jiang et al. (2023  ###reference_b11###) performance was sub-optimal, exhibiting only a marginal improvement than a naive keyword based approach."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "We performed an in-depth error analysis on the subset of records that GPT-4 was unable to predict correction. Our analysis found errors in the MedSecId dataset itself, which is one of the reasons GPT-4 did not get a 100% performance. Error analysis reveals on the rest of 2.8% missed sections of the GPT-4 finds that 18% of the above stated 2.8% belong to the \u201cFindings\u201d section label and 13% belong to the \u201cImage-Type\u201d category. Most of the documents did not have those section headers explicitly mentioned and were hidden as part of the text. Even though the precision was 100% in i2b2 2010 dataset, the granularity of the subsections, the presence of ambiguous language, or the lack of clear markers for section boundaries could be the contributors to the slight dip in recall of the section headers. We leave fixing the issues in the dataset and advanced prompting for future work.\nSurprisingly, we found that GPT-4 was even able to extract sub-sections that were missed in the human annotations in MedSecId. This raises the question of whether GPT-4\u2019s superior performance on these datasets can be attributed to its prior exposure to them? We found out that MedSecId is derived from MIMIC dataset which forbids being used for LLM training, therefore, it is highly unlikely it was used during model training.\nFurther analysis of our internal dataset revealed that high variation in the structure of the document is the root cause of such a wide gap between benchmark and our internal datasets. The original version of our data is in the form of images and PDF files. While GPT was resilient to most OCR errors it did contribute to some misspelled sections.\nWe acknowledge the difference in GPT\u2019s and the gold standard\u2019s approach to section title extraction. While the gold standard highlights literal text, GPT summarizes the content, potentially providing a more concise and informative overview.\nExample GPT output Patient Information and Visit Details encompasses multiple headers like Chief Complaint, History of Present Illness, and Patient Information.\nGPT also extracted irrelevant titles as section headers Provider Information and Signature, Page Footer, etc. We aim to work on addressing these issues by incorporating context awareness into the title-generation process.\nThe major challenge in performance drop on internal dataset is due to the nature of our data itself. More specifically, there is neither standard structure nor format. The situation exacerbates with the document being an out of an OCR system which introduces numerous morphological errors.\nConsequently, GPT-4\u2019s responses on our dataset are more creative and semantically similar which is something an exact match evaluation is unable to measure.\nAs zero-shot was performing extremely well on public corpus and the improvement with other prompting techniques gave only minor improvements, we conducted only zero shot on our internal datasets.\nApart from conducting experiments on the state of art LLMs like GPT-4 (OpenAI, 2023  ###reference_b19###), we also wanted to experiment with smaller open-source models that offer flexibility. We experimented with two of the best-performing models LLaMa-2 (Touvron et al., 2023  ###reference_b25###) and Mistral (Jiang et al., 2023  ###reference_b11###). However, in reality, both the open source models found it hard to follow the prompts and the outputs are not consistent. The challenges were further exacerbated when the models were required to generate results in a uniform format. Sometimes, both LLaMa-2 and Mistral would just output the summarization of the text. LLaMa-2 demonstrated a significantly superior performance than Mistral on both i2b2 and MedSecID.\nFurther, each section name is categorised to either its top-header section or a category is selected by human to represent the topic of the section. This annotation is done manually by two annotators where one selected a course-grained category list and other selected a fine-grained one. The one we show in table 8  ###reference_### is the coarse-grained category list, along with the number of sections in each category, frequency, and frequency percentage. 25 categories are created by the annotator to represent the coarse-grained categories. There are some section names that both annotators are unable to assess or select a category. These sections are categorized as UNKNOWN. If we consider that the top nodes in an ontology network, on average each node will have 26 child nodes in this ontology."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we evaluated LLMs capabilities in segmenting a clinical document into individual sections. More specifically, we show that an unsupervised GPT-4 can nearly solve the Section Identification task. Even though GPT-4 has a very high accuracy on the benchmark datasets, however, its performance on a real-world dataset has a significant lag. We further analyze the reasons for such a wide gap and find that the source dataset has cleanly defined section headers which is not the case with its real-world counterpart. To show how diverse the real-world dataset is, we further derived an ontology using another set of annotators that we share with the community at large.\nTo that end, we create a harder benchmark, one that is derived from real-world data generating process. Moreover, we conducted an annotation study with five annotators to create the final dataset and found high ambiguity in the identification of headers on the newly introduced benchmark. As a takeaway, we suggest that if the source dataset or EHR is clean, then there is no need anymore to train specific supervised models to detect sections as an unsupervised LLM can perform that task."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Future Work",
            "text": "After realizing the close-to-perfect performance and poor performance on the internal real world dataset of an unsupervised LLM in this study, we believe currently released datasets do not paint a clear picture of how the techniques proposed so far would perform in real world scenarios. Using our own internal dataset, we would like to fine-tune the LLM to see whether it can improve performance in a way that is comparable to open-source. Lastly, because sharing sensitive patient data is not possible, we plan to work on de-identifying and training an LLM to generate synthetic but realistic datasets which could lead to better real world benchmarks."
        }
    ],
    "url": "http://arxiv.org/html/2404.16294v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "5"
        ],
        "main_experiment_and_results_sections": [
            "6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "6",
            "7"
        ]
    },
    "research_context": {
        "paper_id": "2404.16294v1",
        "paper_title": "LLM-Based Section Identifiers Excel on Open Source but Stumble in Real World Applications",
        "research_background": "### Paper's Motivation:\nModern healthcare systems are increasingly adopting electronic health records (EHR), which provide comprehensive information about a patient's medical history. However, EHRs have become lengthy and unstructured, making information retrieval challenging. Accurate identification of relevant sections within these records, pertinent to specific inquiries, is essential for timely decision-making and efficient patient care. Traditional manual handling of these tasks by clinicians is time-consuming and error-prone, hence necessitating automated solutions. \n\n### Research Problem:\nThe core research problem addressed in this paper is the Section Identification (SI) task, which involves finding and categorizing regions of text within EHRs that are semantically related to an aspect of a patient\u2019s medical profile. Traditional machine learning models have limitations due to their reliance on labeled data, which is often scarce, particularly in healthcare settings. Large Language Models (LLMs), such as GPT-4, promise to overcome these limitations, yet their effectiveness in SI tasks, especially with real-world data, needs further investigation.\n\n### Relevant Prior Work:\n1. **Challenges in EHRs:**\n   - The unstructured and variable nature of EHRs is well-documented, with physicians frequently creating new sections or using lexical variations, sometimes omitting headers altogether (Gao et al., 2022; Landes et al., 2022).\n\n2. **Section Identification:**\n   - SI has been recognized as a critical issue in clinical NLP, aiming to enhance information retrieval systems and manage the negative impacts of note bloat (McKnight & Srinivasan, 2003; Liu et al., 2022).\n\n3. **Traditional Machine Learning:**\n   - Traditional ML methods for section identification depend heavily on labeled data, constraining their scalability and adaptability (Tepper et al., 2012; Pomares-Quimbaya et al., 2019).\n\n4. **Emergence of LLMs:**\n   - LLMs, such as GPT-4, offer potential solutions by leveraging pre-training on vast unstructured datasets, mitigating the need for extensive labeled data (OpenAI, 2023). Their application has broadened across various NLP tasks in healthcare (Nori et al., 2023), yet their utility in practical SI tasks required evaluation.\n\n### Contribution:\nThis paper addresses the gap by evaluating GPT-4\u2019s ability to perform SI in EHR datasets. The findings suggest that while GPT-4 excels in zero-shot heading generation on open-source datasets, its performance disproportionately declines on private, real-world datasets. The paper also introduces an ontology for section headers observed in real-world EHRs, offering better coverage and utility for future research and applications.",
        "methodology": "The methodology section outlines a detailed approach for identifying section headers in documents using various advanced language models and techniques, with a focus on both open-source and real-world applications.\n\n### Key Components and Innovations:\n\n1. **Task Definition:**\n   - **Input:** A document.\n   - **Output:** All section headers within the document.\n\n2. **Models and Techniques:**\n   - **Language Models:** Various Large Language Models (LLMs) were tested:\n     - **GPT-4 8k/OpenAI (2023).**\n     - **LLaMa-2 7B (Touvron et al., 2023).**\n     - **Mistral 7B (Jiang et al., 2023).**\n   - **Prompting Strategies:**\n     - Utilized different prompting strategies including CoT (Chain of Thought), One Shot, and Close Ended, detailed in the appendix.\n\n3. **Baseline Comparisons:**\n   - Used standard techniques such as keyword search, regular expressions (regex), and the MedSpacy library, a clinical NLP toolkit.\n   - Compared results with the best model reported by Landes et al. (2022).\n\n4. **Data Handling:**\n   - Applied models to clinical notes without restricting tokens, leveraging the MedSecId annotation to extract actual section headers.\n   - Handled real-world data length with the 32k version of GPT-4, maintaining default hyper-parameters (temperature, frequency penalty, presence penalty set to 0, and max tokens to 1000).\n\n5. **Privacy Measures:**\n   - Anonymized the dataset to protect personal health information (PHI), replacing identifiers with fictitious entities.\n   - Used a privately hosted instance of GPT-4 to eliminate potential data leakage.\n\n6. **Advanced Prompting Techniques:**\n   - Experimented with combining basic prompts with Few-Shot learning and CoT Prompting, encouraging the LLM to think step-by-step and providing examples of clinical notes along with a list of headings.\n\n7. **Evaluation Metrics:**\n   - Metrics used include Exact Match (EM) accuracy, Precision (P), Recall (R), and F1-score.\n   - Comparisons made using the Inside-Outside-Beginning (IOB) scheme, as employed by Landes et al. (2022).\n\n8. **Specific Dataset Handling:**\n   - Conducted experiments on the i2b2 2010 dataset using GPT-4 8K due to its smaller context length.\n   - Evaluated the best-performing model on internal real-world datasets due to cost constraints.\n\n### Innovations:\n- **Combination of LLMs and Prompting Strategies:** The study explores the efficacy of combining basic prompting strategies with Few-Shot and CoT techniques.\n- **Application to Clinical Texts:** Utilizing MedSpacy, the methodology is tailored for extracting information from unstructured medical narratives.\n- **Custom Evaluation Process:** A specific evaluation approach using EM, P, R, and F1 scores within the IOB scheme ensures robust performance measurement. \n\nThis method aims to provide a comprehensive framework for identifying section headers using state-of-the-art LLMs while ensuring practical applicability in real-world scenarios, particularly in the medical domain.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets:**\n1. **MedSecId Corpus:** An open-source benchmark dataset for identifying sections in medical documents.\n2. **i2b2 2010 Dataset:** Another widely-used medical dataset comprising de-identified clinical records for natural language processing tasks.\n3. **Internal Corpus:** A proprietary dataset featuring more complex and diverse medical texts used for internal assessment.\n\n**Baselines:**\n1. **GPT-4:** A large language model developed by OpenAI.\n2. **LLaMa-2:** Another large language model.\n3. **MedSpacy:** A specialized tool designed for medical document processing.\n4. **Regex:** A baseline using regular expressions for section identification.\n5. **Mistral:** Referenced as another model designed for similar tasks.\n\n**Evaluation Metrics:**\n- **Accuracy:** The primary measure to compare performance across different models.\n- **Recall:** Specifically used to assess the performance of LLaMa-2 in comparison to MedSpacy.\n\n### Main Experimental Results\n\n- **GPT-4 Performance:**\n  - Achieved **96% accuracy** on the MedSecId corpus.\n  - Reduced to **37% accuracy** on the internal corpus due to its complexity.\n\n- **LLaMa-2 vs. MedSpacy:**\n  - LLaMa-2 outperformed MedSpacy in recall, leveraging its global knowledge from large-scale training.\n  - MedSpacy, although less performant in recall, was noted for being faster with less overhead.\n\n- **Benchmark Results:**\n  - **One-shot GPT-4** achieved state-of-the-art performance on the MedSecId corpus, significantly outperforming previous models.\n  - Demonstrated similar state-of-the-art results on the **i2b2 2010 dataset**.\n  \n- **LLaMa-2 Results:**\n  - Did not match GPT-4 in performance but had results on par with regex.\n  - Came close to GPT-4 on the i2b2 dataset, indicating some potential but with inconsistent performance across different evaluations.\n\n- **Mistral's Performance:**\n  - Was sub-optimal, showing only marginal improvements over a naive keyword-based approach.\n\nThese results highlight a stark difference in the capabilities of GPT-4 when applied to well-defined benchmark datasets versus more complex real-world data, as well as the varied effectiveness of different models in different settings."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To determine if GPT-4 can achieve superior performance over other models in identifying relevant sections in electronic health records (EHR) on both open-source benchmark datasets and a more challenging internal corpus.",
            "experiment_process": "The study involves comparing the performance of GPT-4, LLaMa-2, MedSpacy, and Mistral on multiple datasets. The datasets include the MedSecId corpus and the i2b2 2010 dataset. The experimental setup evaluates both zero-shot and few-shot settings. The accuracy, recall, and other relevant metrics are captured to gauge performance. Specific attention is given to the disparity between performance on public and internal datasets. The internal dataset consists of complex EHR documents converted from images and PDFs, often with OCR errors.",
            "result_discussion": "GPT-4 demonstrated high performance on the MedSecId corpus with an accuracy of 96%, outperforming all other models including achieving state-of-the-art on one-shot performance for i2b2 2010 dataset. However, GPT-4's performance dropped to 37% on the internal corpus due to its complexity. LLaMa-2 showed higher recall than MedSpacy on the internal dataset but had variable results across different datasets. Mistral performed marginally better than a naive keyword-based approach. The main issues identified include the lack of explicit section headers in some documents and the variability in document structure. Moreover, OCR errors introduced in the internal dataset contributed to performance drops, revealing the need for fixing dataset issues and advanced prompting.",
            "ablation_id": "2404.16294v1.No1"
        },
        {
            "research_objective": "To assess the robustness of smaller, open-source models like LLaMa-2 and Mistral in comparison to state-of-the-art models like GPT-4 when tasked with identifying sections in electronic health records (EHR).",
            "experiment_process": "Experiments were conducted using zero-shot settings due to the initial superior performance on the public corpus. Models tested include LLaMa-2 and Mistral, and outcomes were compared against those of GPT-4 and MedSpacy. The experiments involved comparing the models under consistent formats to generate results. The results also focus on the model\u2019s ability to follow prompts and generate uniform outputs across datasets.",
            "result_discussion": "Both LLaMa-2 and Mistral struggled to consistently follow prompts and maintain a uniform format in their outputs. LLaMa-2 showed better performance than Mistral but still underperformed compared to GPT-4 and MedSpacy. Issues included inconsistent output quality and the tendency of LLaMa-2 and Mistral to generate summaries instead of section headers. This variability in results highlights the challenges smaller open-source models face in emulating the performance of more advanced models like GPT-4 when dealing with complex EHR datasets.",
            "ablation_id": "2404.16294v1.No2"
        }
    ]
}