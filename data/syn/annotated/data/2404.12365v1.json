{
    "title": "When LLMs are Unfit Use \\fastfit: Fast and Effective Text Classification with Many Classes",
    "abstract": "We present \\fastfit, a method, and a Python package design to provide fast and accurate few-shot classification, especially for scenarios with many semantically similar classes. \\fastfit utilizes a novel approach integrating batch contrastive learning and token-level similarity score. Compared to existing few-shot learning packages, such as SetFit, Transformers, or few-shot prompting of large language models via API calls, \\fastfit significantly improves multi-class classification performance in speed and accuracy across FewMany, our newly curated English benchmark, and Multilingual datasets. \\fastfit demonstrates a 3-20x improvement in training speed, completing training in just a few seconds. The \\fastfit package is now available on GitHub and PyPi, presenting a user-friendly solution for NLP practitioners.\nCode: https://github.com/IBM/fastfit\nData: https://huggingface.co/FastFit",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Few-shot classification presents a unique challenge, especially when dealing with a multitude of classes that share similar semantic meanings. Expanding the training data can be both time-consuming and costly. To address this challenge, two primary categories of tools have been developed: few-shot prompting of large language models (LLMs) via API calls, or packages designed for fine-tuning smaller language models using the limited available data. However, we recognize the drawbacks of applying both approaches in practice.\nFew-shot prompting of LLMs leverages their multitasking abilities to tackle data scarcity. However, in the presence of many classes, LLMs encounter three major challenges: (1) LLMs struggle to incorporate demonstrations of all classes within their context window. (2) Utilization of the long context for the classification task can be challenging Liu et al. (2023  ###reference_b15###). (3) Inference time is slow due to model size, and prompt length.\nIn contrast, the approach of fine-tuning smaller language models capitalizes on their adaptability to specific tasks, as demonstrated to be effective in recent works. However, these methods can be challenging to deploy as they require architectural adjustments Yehudai et al. (2023  ###reference_b29###) or, like SetFit, may prove less suitable for classification with many classes Tunstall et al. (2022  ###reference_b26###).\n###figure_1### In this work, we present \\fastfit, a fast and accurate method, and a pip-installable Python package designed for fine-tuning small language models in few-shot classification tasks involving many classes. Through various experiments, on our newly curated FewMany benchmark, we demonstrate that \\fastfit training is significantly faster, providing a 3-20x speedup. This enables training within seconds, as illustrated in Fig. 1  ###reference_###. \\fastfit outperforms earlier packages, including SetFit, Transformer, and multi-task models like FLAN, or larger LLMs like LLama-70B, in both English and Multilingual settings.\nThe core contribution facilitating this speedup and improvement lies in \\fastfit\u2019s use of batch contrastive training, recognized for its efficiency and effectiveness Khosla et al. (2021  ###reference_b10###). This technique brings same-class texts closer while pushing apart all other texts. \\fastfit also incorporates token-level text similarity measures that leverage fine-grained information Zhang et al. (2020  ###reference_b30###); Khattab and Zaharia (2020  ###reference_b9###). Additionally, we integrate text augmentation techniques to enhance the robustness of the training process Gao et al. (2021  ###reference_b5###).\nThe \\fastfit package is easy to install and use, interfacing with standard training APIs (See \u00a72  ###reference_###). We hope that \\fastfit will help make text classification easier and faster for the benefit of the whole community."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "The \\fastfit Library",
            "text": "The \\fastfit Python package is available on PyPI and can be installed with:\nTo utilize \\fastfit, import the \\fastfit trainer, which inherits from the Hugging Face (HF) trainer. This enables \\fastfit to be customizable, inheriting all parameters from the HF trainer.\n\\fastfit supports loading datasets either by directly passing the dataset or providing file paths.\nHere is a simple code example of loading and training \\fastfit. In App. \u00a7A  ###reference_###, we provide a complete code example.\nAs \\fastfit utilizes example texts and class names, it expects the data to have text and label fields or to map the existing fields to them using the label_column_name and text_column_name parameters of the FastFitTrainer. Our trainer also supports training with either CLS or token-level similarity metrics, set by the sim_rep parameter. The trainer allows to modify the number of augmentation repetitions with the num_repeats parameter.\nThen after training, we can easily save the model:\nAnd later load it for inference, See App. \u00a7A  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "Given a few-shot text classification dataset containing texts and their corresponding classes denoted as , let  represent all possible classes. Our task is to classify each  into a class . To achieve this goal we aim to encode both texts and class names into a shared embedding space, where they are represented closely, according to a similarity metric , when they belong to the same class and are represented further apart when they do not. To accomplish this, we optimize the following batch contrastive loss:\nHere,  represents a batch of  texts, and  refers to the set of texts in the same class as  in the batch, given by . The function  is the similarity metric, and  is a scalar temperature parameter regulating the penalty for negative texts.\nFor each text in the batch, we augment the batch by including its class name as an additional example. Additionally, we repeat the texts in the batch  times as a data augmentation technique, following Gao et al. (2021  ###reference_b5###) by treating the dropout as a minimal augmentation at the representation level. This method has demonstrated significant success in generating sentence embeddings, and we leverage it here to enhance representation for text classification.\nIn our data-scarce setting, we employ fine-grained token-level similarity metrics, leveraging textual details. This approach, successful in works like BERT-Score and ColBERT, defines the similarity metric between texts  and  as the sum of cosine similarities between  and the most similar tokens in . Specifically, with tokens denoted as  and  respectively, the similarity score is computed as follows:\nwhere  is a dense representation of token  produced by a parametric encoder model with parameters .\nDuring inference, when provided with a new text,  we classify it to the most similar class  with respect to a similarity metric . This method draws inspiration from the way inference is conducted in retrieval systems, eliminating the need for a classification head and aligning the training and inference objectives."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "FewMany Benchmark",
            "text": "To rigorously evaluate the capabilities of models in few-shot text classification with many classes, we introduce the FewMany benchmark, a collection of eight diverse classification datasets, each featuring at least 50 classes.\nThe benchmark spans several domains, including intent detection, topic classification, question classification, and product classification. Each domain in FewMany presents a unique input type, such as short informal user queries, arguments, claims, long-form Wikipedia articles, questions, and product descriptions. By covering a wide spectrum of cases, FewMany enables a comprehensive evaluation of model performance in distinguishing between many semantically similar classes, often with subtle distinctions. In this work, we conduct experiments on FewMany under 5-shot and 10-shot scenarios, where the -shot scenario refers to a training set with  examples per class. Further details and data statistics can be found in Appendix B  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "###table_1###"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Baselines",
            "text": "We compare \\fastfit with a few classification methods, including fine-tuning methods, like Standard and SetFit classifiers, and few-shot promoting of LLMs including Flan-XXL Wei et al. (2022  ###reference_b27###), Flan-ul2 Tay et al. (2023  ###reference_b24###), llama-2-70b-chat Touvron et al. (2023  ###reference_b25###), and Mistral-7b Jiang et al. (2023  ###reference_b8###). For all fine-tuning methods, we use small and large versions, where small is MPNet (110M parameters) Song et al. (2020  ###reference_b23###), and large is Roberta-large (355M parameters) Liu et al. (2019b  ###reference_b17###) or equivalent.\nStandard Classifier. A simple yet strong baseline is a standard fine-tuning of an encoder-only model. Since we assume no validation sets, we use best practices as described in previous works, and train for 40 epochs, with a learning rate of , and batch size of 16 Lin et al. (2023  ###reference_b13###). We recovered runs that didn\u2019t converge.\nSetFit. Sentence Transformer Fine-tuning (SetFit) Tunstall et al. (2022  ###reference_b26###) is a two-stage method for training a Sentence Transformer model Reimers and Gurevych (2019  ###reference_b21###), specifically designed for few-shot classification tasks. In the first stage, the encoder undergoes fine-tuning using triplet loss, and in the second stage, the classification head is trained. For the small model we use paraphrase-mpnet-base-v2111ST-MPNet  ###reference_rs/paraphrase-mpnet-base-v2###, and for the large model, we used all-Roberta-Large-v1222ST-Roberta-Large  ###reference_rs/all-roberta-large-v1###, both trained with sentence transformer objective before. We trained the model with a learning rate of , a batch size of 16, for one epoch, based on the parameters defined in SetFit\u2019s paper.\nFlan. Flan language models are fine-tuned on a diverse range of NLP tasks and datasets, making them adaptable for various NLP tasks in a few-shot manner. Here, we experimented with Flan-XXL (11B) and Flan-ul2 (20B) models. These models have a 4K tokens context window.\nLlama. Llama-2-chat is a set of large language models developed for conversational applications and has strong multi-task few-shot capabilities. Here, we experimented with a Llama model that supports a 4K tokens context window.\nMistral. Mistral is a strong 7B open-source large language model. Here, we used the instruct-tuned version. Mistral supports an 8K tokens context window."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Experimental Setup",
            "text": "Training Setup. We fine-tune the \\fastfit model with a learning rate of , a batch size of 32, and a maximum sequence length of 128 tokens, for 40 epochs. We used AdamW optimizer, 16-bit floating-point (FP16) precision, and applied 4 batch repetitions that act as augmentations. For all LLMs, we fit the maximum possible number of examples into their context window. For AP106 and DB70 test sets even a 1-shot example do not fit into the context. Hence we compare LLM results on the remaining six test sets.\nEvaluation Setup. Few-shot evaluations can be noisy due to variations in the small datasets Dodge et al. (2020  ###reference_b3###); Zhang et al. (2021  ###reference_b31###)."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Results",
            "text": "In Table 1  ###reference_###, we present the results of \\fastfit, SetFit, and the standard classifier for FewMany eight datasets under 5/10-shot settings. \\fastfit large outperforms SetFit by  and the standard classifier by , in the 5-shot case. In the 10-shot case, \\fastfit outperforms SetFit, and a standard classifier by  and , respectively. Moreover, \\fastfit small is comparable to SetFit large in 5-shot and outperforms it in 10-shot.\nNotably, \\fastfit shows greater improvement in the 5-shot case compared to the 10-shot case and for the small model compared to the large one.\nTable 2  ###reference_### displays the results of few-shot prompting for several LLMs. The Flan models exhibit higher performance than other LLMs, likely due to the presence of many classification datasets in the Flan dataset333To the best of our knowledge, the Flan dataset includes only T50 from our test sets. This observation aligns with findings in zero-shot classification Gretz et al. (2023  ###reference_b7###). Llama-70B outperforms Llama-13B, and is comparable to Mistral-7B\u2019s performance, possibly due to Mistral\u2019s larger context length, allowing it to incorporate more examples per class.\nThe results suggest that in our setting, where numerous classes are present, even the best-performing LLMs we tested (Flan\u2019s) underperform compared to large standard classifiers and face challenges compared to \\fastfit. It\u2019s important to note that, due to the model\u2019s size and the length of the few-shot prompt, inference time can be slow, with throughput exceeding 1 second per input, in contrast to about 1 millisecond with \\fastfit."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Multilingual Experiments",
            "text": ""
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Datasets",
            "text": "To evaluate \\fastfit\u2019s multilingual classification abilities we adopt Amazon Multilingual MASSIVE dataset FitzGerald et al. (2022  ###reference_b4###). From the 51 available languages, we selected six typologically diverse languages: English, Japanese, German, French, Spanish, and Chinese. MASSIVE is a parallel dataset, with 60 classes (See App. \u00a7B  ###reference_###)."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Baselines",
            "text": "For multilingual training, we utilized paraphrase-multilingual-mpnet-base-v2 as a small model and XLM-Roberta-Large as a large model. Both models underwent pretraining in a large number of languages. Notably, to the best of our knowledge, there is no multilingual sentence transformer model equivalent to Roberta-Large for SetFit training. Monolingual and XLM-Roberta-Large models were tested, but they yielded lower performance than the small model; hence, their results are detailed in Appendix \u00a7D  ###reference_###. In English experiments, we maintained the use of monolingual models (see \u00a75.1  ###reference_###), conducting training and evaluation with the same setup outlined in \u00a75.2  ###reference_###."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Results",
            "text": "In Table 3  ###reference_###, we present the results on MASSIVE in 5/10-shot scenarios using \\fastfit, SetFit, and the standard classifier. \\fastfit consistently outperforms both SetFit and the standard classifier in both 5-shot and 10-shot settings, across small and large models. In the 5-shot scenario, \\fastfit large achieves an  improvement over SetFit small and a  improvement over the standard classifier. Meanwhile, \\fastfit small shows a  improvement over SetFit small and a  improvement over the standard classifier. In the 10-shot case, \\fastfit large outperforms SetFit small by  and the standard large classifier by . Similarly, \\fastfit small exhibits improvements of  and  over SetFit small and the standard classifier, respectively.\nIt is noteworthy that \\fastfit demonstrates improvement when scaling from a small to a large model, with gains of 5.3% and 2.6% in the 5-shot and 10-shot settings, respectively. This enhancement highlights the fact that \\fastfit is not model-specific and thus is highly flexible for different sizes and types of models, unlike SetFit. Such flexibility is particularly crucial in few-shot settings where limited examples are available, highlighting the potential to train enhanced classifiers using domain- or language-specific models. Moreover, if unlabeled or pairwise data is available, using it for pretraining can lead to even further improvement."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Fast Training",
            "text": "###figure_2### Training Times for \\fastfit, SetFit, and the standard classifier are illustrated in Figure 2  ###reference_###. Results are average across all languages in MASSIVE, and all 5 seeds. \\fastfit exhibits faster training times compared to both SetFit and the standard classifier, with a 3-20x decrease, and a training time of about 30 seconds (See more results at App. \u00a7E  ###reference_###). This can be attributed to a combination of technical and methodological factors. The improved implementation includes pre-training tokenization and FP16 training. Furthermore, the methodological advantage stems from using batch contrastive training, which leverages in-batch examples as negatives, in contrast to the triplet loss utilized by SetFit.\nConvergence.\nFigure 3  ###reference_### presents the average FewMany accuracy results per training second of \\fastfit for 5-shot with both small and large, and regular and ST backbone models. Results demonstrate \\fastfit rapid convergence, achieving top performances within a few seconds before reaching a plateau. Notably, both small and large Sentence Transformer (ST) models exhibit higher initial performance and faster convergence than their non-ST base model counterparts. We can also see that \\fastfit achieves state-of-the-art results on FewMany, above 81.2, within 30 seconds as illustrated in Fig. 1  ###reference_###.\n###figure_3###"
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Ablation & Full Training",
            "text": "To further examine the contribution of some of our method modifications, we compare training with CLS and token-level similarity metrics, as well as training with a different number of batch repetitions. We conduct these experiments on three datasets: Hwu64, Banking77, and Clinc150, with 5 random splits, and average their results. We assess the effect of these modifications for both small and large models, with 5 and 10 shots.\nIn Table 4  ###reference_###, we present the differences in performance caused by our changes; full results are available in App. \u00a7F  ###reference_###. The Token-level similarity metric proves beneficial across all settings, with a more pronounced effect for smaller models and when less data is available (5-shot compared to 10-shot). Concerning the number of repetitions, we observe that, in most cases, adding repetitions helps. Additionally, it appears that overall, four repetitions are more effective than two.\nRegarding the relationship between the number of shots and the effectiveness of repetition, no clear connection is apparent. While an increase in the number of shots enhances effectiveness in small models, the opposite is observed for large models, where the effect decreases. Nevertheless, it seems that, in general, larger models benefit more from batch repetition.\nAlthough our primary focus is few-shot classification, we also wanted to examine the effectiveness of \\fastfit when training on the full dataset. We conducted two sets of experiments. In the first, we compared \\fastfit-small, \\fastfit-large, and a large standard classifier on Hwu64, Banking77, and Clinc150. In the second, we compared \\fastfit-small and \\fastfit-large with a few base-sized multilingual baseline models on MASSIVE, using the set of six languages mentioned in \u00a76.1  ###reference_###. These baselines are based on the MASSIVE paper, where Classifier-B and mT5-B Encoder are standard classifiers based on XLM-R-BASE and mT5-Base with 270M and 258M parameters, respectively. mT5-B T2T is a text-2-text classifier with 580M parameters.\nResults in Table 5  ###reference_### demonstrate that when training on all the data, \\fastfit-Small outperforms the large Classifier, and \\fastfit-Large performs even better. From Table 6  ###reference_###, we can see that \\fastfit-Small outperforms all other baselines even with fewer than half the number of parameters. Moreover, \\fastfit-Large further improves performances by  on average. These results indicate that \\fastfit is not only a fast few-shot classifier but can also outperform even larger classifiers when training on the full dataset."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "For fine-tuning baselines, we focus on readily available methods. , including SetFit with its package, a standard classifier accessible through HF Transformers Wolf et al. (2019  ###reference_b28###), or LLMs through API calls. However, there are various few-shot classifiers, and we will briefly discuss a couple of them. QAID Yehudai et al. (2023  ###reference_b29###) proposed pre- and fine-tuning training stages with unsupervised and supervised loss, using ColBERT architecture, achieving SOTA results. T-Few Liu et al. (2022  ###reference_b14###), a parameter-efficient fine-tuning method based on T0 Sanh et al. (2021  ###reference_b22###), claims to be better and cheaper than In-Context Learning.\nRegarding few-shot prompting of LLMs approaches, a question arises about whether our results will withstand stronger LLMs or improved prompting techniques. According to Loukas et al. (2023  ###reference_b18###) we can deduce that \\fastfit outperforms GPT4 OpenAI et al. (2023  ###reference_b20###) with a fraction of the cost. Additionally, Milios et al. (2023  ###reference_b19###) demonstrate that retrieval-based few-shot prompts can lead to improved results. However, it\u2019s worth noting that currently, these models remain slow and costly."
        }
    ],
    "url": "http://arxiv.org/html/2404.12365v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "9"
        ],
        "methodology_sections": [
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "8"
        ]
    },
    "research_context": {
        "paper_id": "2404.12365v1",
        "paper_title": "When LLMs are Unfit Use \\fastfit: Fast and Effective Text Classification with Many Classes",
        "research_background": "**Motivation:**\nThe paper is motivated by the need for efficient and effective methods for few-shot text classification, particularly in scenarios involving a large number of classes with similar semantic meanings. Traditional methods, such as expanding training data, are often impractical due to their time-consuming and costly nature. The authors identify that existing approaches\u2014few-shot prompting of large language models (LLMs) and fine-tuning smaller language models\u2014each have significant drawbacks when applied in practice, especially in the context of many classes.\n\n**Research Problem:**\nThe primary research problem addressed in this paper is the challenge of performing efficient and accurate few-shot text classification with many classes. Specifically, the paper seeks to develop a method that overcomes the limitations of few-shot prompting of LLMs (such as limited context window, difficulty utilizing long contexts, and slow inference time) and the challenges associated with fine-tuning smaller models (such as the need for architectural adjustments and unsuitability for classification with many classes).\n\n**Relevant Prior Work:**\n1. **Few-Shot Prompting of LLMs:**\n   - Few-shot prompting leverages the multitasking abilities of LLMs but struggles with incorporating all class demonstrations within their context window, effectively utilizing long contexts, and slow inference times due to model size and prompt length (cited work, Liu et al., 2023).\n\n2. **Fine-Tuning Smaller Language Models:**\n   - Smaller language models are adaptable to specific tasks but can be difficult to deploy due to architectural adjustments required (cited work, Yehudai et al., 2023). Methods like SetFit are mentioned as less suitable for classification involving many classes (cited work, Tunstall et al., 2022).\n   \n3. **Core Techniques for Improvement:**\n   - The use of batch contrastive training is highlighted for its efficiency and effectiveness in bringing same-class texts closer while pushing apart other texts (cited work, Khosla et al., 2021).\n   - Token-level text similarity measures that leverage fine-grained information are integrated (cited works, Zhang et al., 2020; Khattab and Zaharia, 2020).\n   - The incorporation of text augmentation to enhance robustness, making training more effective (cited work, Gao et al., 2021). \n\nThe introduction of the \\fastfit method aims to provide a fast, accurate, and easy-to-use tool for few-shot text classification in scenarios with many classes, addressing the limitations observed in previous approaches.",
        "methodology": "**Methodology:**\n\nThe proposed method, \\fastfit, addresses the task of few-shot text classification by encoding both texts and class names into a shared embedding space. The goal is to ensure that texts and their corresponding class names are closely represented within this space when they belong to the same class, and more distantly when they do not. This is achieved using a specific similarity metric, .\n\n**Key Components:**\n\n1. **Shared Embedding Space**: Both texts and class names are transformed into a common embedding space where their spatial proximity indicates class membership.\n  \n2. **Batch Contrastive Loss**: The method optimizes a batch contrastive loss function. \n\n  - For a batch of texts \\( T_i \\), defines a similarity metric \\( s(\\cdot, \\cdot) \\) and a penalty regulation parameter \\( \\tau \\).\n  - The loss encourages texts and their class names to be closer in the embedding space when they belong to the same class.\n\n3. **Data Augmentation**: \n   \n   - **Class Name Augmentation**: Each text's class name is added as an additional example in the batch, enhancing the model's ability to discern and learn class-specific features.\n   - **Repeat and Dropout**: Texts are repeated \\( K \\) times within the batch, and dropout is employed as minimal augmentation at the representation level, following the approach of Gao et al. (2021).\n   \n4. **Fine-Grained Token-Level Similarity Metrics**:\n\n   - The similarity between two texts \\( x_i \\) and \\( x_j \\) is calculated by summing the cosine similarities between each token in \\( x_i \\) and its most similar token in \\( x_j \\).\n   - This metric leverages detailed token-level information, inspired by methods like BERT-Score and ColBERT.\n\n5. **Inference Strategy**: \n\n   - During inference, a new text \\( x \\) is classified by finding the class \\( y \\) with the highest similarity score \\( s(x, y) \\).\n   - This approach is akin to retrieval systems, aligning the training and inference phases and bypassing the need for a traditional classification head.\n\n**Innovations:**\n\n1. **Embedding of Class Names**: Integrating class names directly into the batch for representation learning, which enriches the model's contextual understanding of classes in a few-shot setting.\n\n2. **Repeat and Dropout Data Augmentation**: Capitalizing on minimal augmentations through dropout, which reinforces the robustness of sentence embeddings.\n\n3. **Fine-Grained Token-Level Similarity**: Implementing a detailed token-level similarity measure that encapsulates nuanced textual details, enhancing the accuracy of the embedding space.\n\n4. **Classification Without a Head**: Using a similarity-based retrieval method for classification, aligning the objectives of training and inference and simplifying the model architecture.\n\nBy leveraging these components and innovations, \\fastfit provides a fast, effective means for text classification, particularly in scenarios with many classes and limited data.",
        "main_experiment_and_results": "To rigorously evaluate the capabilities of models in few-shot text classification with many classes, we introduce the FewMany benchmark, a collection of eight diverse classification datasets, each featuring at least 50 classes. The benchmark spans several domains, including:\n\n- Intent detection\n- Topic classification\n- Question classification\n- Product classification\n\nEach domain in FewMany presents a unique input type, such as:\n\n- Short informal user queries\n- Arguments\n- Claims\n- Long-form Wikipedia articles\n- Questions\n- Product descriptions\n\nBy covering a wide spectrum of cases, FewMany enables a comprehensive evaluation of model performance in distinguishing between many semantically similar classes, often with subtle distinctions.\n\nIn this work, we conduct experiments on FewMany under two scenarios:\n\n- 5-shot scenario\n- 10-shot scenario\n\nThe -shot scenario refers to a training set with a specific number of examples per class.\n\nFurther details and data statistics can be found in Appendix B."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the effects of various method modifications in \\fastfit, specifically the use of CLS vs token-level similarity metrics and the impact of different batch repetitions, on few-shot classification performance.",
            "experiment_process": "The experiments were conducted on three datasets: Hwu64, Banking77, and Clinc150, using 5 random splits and averaging the results. Both small and large models were tested with 5 and 10 shots. The modifications examined included training with CLS and token-level similarity metrics, and varying the number of batch repetitions. Full results were revealed in App. \u00a7F. The dataset configurations included both 5-shot and 10-shot setups, and the effectiveness of the changes was measured using performance metrics not specified in detail.",
            "result_discussion": "The token-level similarity metric was beneficial across all settings, with a more pronounced effect for smaller models and when less data was available (5-shot compared to 10-shot). Adding more repetitions generally helped, with four repetitions being more effective than two. Larger models benefited more from batch repetition, but there was no clear correlation between the number of shots and the effectiveness of repetition. In contrast to small models, larger models showed decreased effectiveness with more shots.",
            "ablation_id": "2404.12365v1.No1"
        },
        {
            "research_objective": "To evaluate \\fastfit's performance when trained on the full dataset, compared to standard classifiers.",
            "experiment_process": "Two sets of experiments were conducted. In the first, \\fastfit-small, \\fastfit-large, and a large standard classifier were compared on the Hwu64, Banking77, and Clinc150 datasets. In the second, \\fastfit-small and \\fastfit-large were compared with a few base-sized multilingual baseline models on the MASSIVE dataset using six languages. The baselines included Classifier-B and mT5-B Encoder based on XLM-R-BASE and mT5-Base, respectively, and mT5-B T2T, a text-2-text classifier.",
            "result_discussion": "When training on the full dataset, \\fastfit-Small outperformed the large standard classifier, and \\fastfit-Large performed even better. \\fastfit-Small outperformed all multilingual baseline models with fewer than half the number of parameters, and \\fastfit-Large further improved performance.",
            "ablation_id": "2404.12365v1.No2"
        }
    ]
}