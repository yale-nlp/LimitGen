{
    "title": "A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing1footnote 11footnote 1Our replication package can be accessed at https://doi.org/10.7910/DVN/QTT84C.",
    "abstract": "The pretrain-finetune paradigm represents a transformative approach in natural language processing (NLP). This paradigm distinguishes itself through the use of large pretrained language models, demonstrating remarkable efficiency in finetuning tasks, even with limited training data. This efficiency is especially beneficial for research in social sciences, where the number of annotated samples is often quite limited. Our tutorial offers a comprehensive introduction to the pretrain-finetune paradigm. We first delve into the fundamental concepts of pretraining and finetuning, followed by practical exercises using real-world applications. We demonstrate the application of the paradigm across various tasks, including multi-class classification and regression. Emphasizing its efficacy and user-friendliness, the tutorial aims to encourage broader adoption of this paradigm. To this end, we have provided open access to all our code and datasets. The tutorial is particularly valuable for quantitative researchers in psychology, offering them an insightful guide into this innovative approach.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The rise of pretrain-finetune paradigm has greatly reshaped the landscape of natural language processing (NLP) (E. Hu \\BOthers., \\APACyear2022  ###reference_b3###). This new paradigm, characterized by the application of large pretrained language models, most notably BERT (Devlin \\BOthers., \\APACyear2019  ###reference_b2###) and RoBERTa (Liu \\BOthers., \\APACyear2019  ###reference_b8###), and high efficacy on finetuned tasks even in the face of relatively few training samples, is now being widely applied in social sciences (Zhang \\BOthers., \\APACyear2021  ###reference_b18###; Wang, \\APACyear2023\\APACexlab\\BCnt1  ###reference_b15###, \\APACyear2023\\APACexlab\\BCnt2  ###reference_b16###). While earlier works have recommended a minimum of 3,000 samples for NLP tasks using the bag-of-words approach (Wang \\BOthers., \\APACyear2022  ###reference_b17###), recent research applying the pretrain-finetune paradigm has shown that finetuned large models with just a few hundred labeled samples could yield competitive performance (Wang, \\APACyear2023\\APACexlab\\BCnt2  ###reference_b16###).\n###figure_1### In the current tutorial, we aim to provide (1) an overview of the pretrain-finetune paradigm and (2) illustrative applications of the pretrain-finetune paradigm to research questions in social sciences (Figure 1  ###reference_###). We first provide an introduction to the key concepts in pretraining and finetuning, such as tokenization and encoding. We then use some of the most recent applications of the pretrain-finetune paradigm in social sciences to illustrate how to use this paradigm to advance quantitative research in our discipline."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Pretrain",
            "text": "Pretraining is the process of training a model with unlabeled raw data with no particular downstream tasks. Some of the most widely used pretrained models include BERT and RoBERTa. These large language models usually contain hundreds of millions of parameters. Pretraining these models from scratch requires access to large amounts of raw data and specialized hardware like graphics processing units (GPUs). The pretraining process consists of the following steps: (1) tokenization, (2) encoding, and (3) pretraining tasks, such as masked language modeling, next sentence prediction (Devlin \\BOthers., \\APACyear2019  ###reference_b2###) and sentence-order prediction (Lan \\BOthers., \\APACyear2020  ###reference_b7###)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Tokenization",
            "text": "Unlike earlier methods such as bag of words (Wang \\BOthers., \\APACyear2022  ###reference_b17###) or word embeddings (Mikolov \\BOthers., \\APACyear2013  ###reference_b10###), large langauge models mostly use subwords as a token, such as wordpieces in BERT (Devlin \\BOthers., \\APACyear2019  ###reference_b2###) and Byte-Pair Encoding (BPE) in RoBERTa (Liu \\BOthers., \\APACyear2019  ###reference_b8###) and GPT-3 (Brown \\BOthers., \\APACyear2020  ###reference_b1###). To illustrate how words are broken into subwords, let\u2019s use a text snippet from the Journal\u2019s description and tokenize it using the BPE tokenizer used in GPT-3.5 and GPT-4.222The snippet is taken from https://journals.sagepub.com/description/AMP. The tokenizer is available at https://platform.openai.com/tokenizer and was accessed on January 6th 2024..\nThe word \u201cAMPPS\u201d is split into \u201cAM\u201d, \u201cPP\u201d and \u201cS\u201d. The word \u201cmethodological\u201d is split into \u201cmethod\u201d, \u201cological\u201d. And lastly the word \u201cnon-methodology\u201d is split into \u201cnon\u201d, \u201c-method\u201d, and \u201cology\u201d. All other words, including punctuation marks, remain individual units without further splitting. These units are then turned into tokens represented with non-negative integers.\nThese units are then turned into tokens represented with integers."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Encoding",
            "text": "Once we have tokenized the input text into tokens, we then retrieve the corresponding embeddings for these tokens, where the token id, a non-negative integer, would serve the retrieval key. These token embeddings are vector representations. For the BERT-large model, each such vector contains 1024 float numbers. These sets of vectors are then fed into the encoder layers.333BERT models use encoders and are the focus of this tutorial. Note that GPT models use decoders.\n###figure_2### The key component of the encoders is self-attention (Vaswani \\BOthers., \\APACyear2017  ###reference_b13###). Intuitively, it computes how much attention each token pays to the other tokens and generates the new representation for a token by calculating the weighted average of all the tokens where the weight is based on self-attention.444For a similar illustration, readers can refer to Kjell \\BOthers. (\\APACyear2023  ###reference_b6###). For a more in-depth illustration, please refer to the original paper (Vaswani \\BOthers., \\APACyear2017  ###reference_b13###) and the tutorial on transformers at http://jalammar.github.io/illustrated-transformer/. Such encoders are then put on top of each other in a sequential manner. For example, in BERT-large models there are 24 encoder layers. In BERT-base models there are 12 encoder layers."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Pretraining tasks",
            "text": "Now that we have a good understanding of tokenization and encoding, let\u2019s proceed to discuss how these parameters in these layers are trained using pretraining tasks. The goal is to train these large language models from scratch and embue them with world knowledge that exists in the training corpus (Longpre \\BOthers., \\APACyear2020  ###reference_b9###). Commonly used pretraining tasks include masked language modeling, next sentence prediction (Devlin \\BOthers., \\APACyear2019  ###reference_b2###) and sentence order prediction (Lan \\BOthers., \\APACyear2020  ###reference_b7###).555Later research has shown that the next sentence prediction does not contribute much to the pretraining process (Liu \\BOthers., \\APACyear2019  ###reference_b8###).\n###figure_3### Let\u2019s see an example of how masked language modeling works. Suppose we have the following input text: \u201cI enjoy trying new activities\u201d. During training, some of the tokens will be randomly masked and the language model is tasked with predicting these masked tokens. In Figure 3  ###reference_###, the token \u201cactivities\u201d is masked. The language model then is tasked with predicting this masked token. All tokens in the vocabulary are eligible candidates. Suppose the model predicts \u201cactivities\u201d with a probability of 0.2. If we use cross entropy loss, then the loss term for this prediction is -log(0.2), which is its negative log-likelihood.666For details, please see https://github.com/google-research/bert/blob/master/run_pretraining.py#L303. Note that the higher probability the model assigns to the corrected token, the lower the loss. The language model is trained on this task over the entire corpus for a few times until the loss in prediction stops decreasing."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Finetuning",
            "text": "Finetuning is the process of training an already-pretrained model on a downstream task. It is characterized by (1) adding a small set new parameters to accommodate the new tasks and (2) a relatively small learning rate given the fact that the vast majority of the parameters are already reasonably trained during the pretraining stage.777In terms of training procedures, it is the same as training a supervised model from scratch. Readers could refer to a recent tutorial on how to train supervised models by Pargent \\BOthers. (\\APACyear2023  ###reference_b12###). Downstream tasks at this stage can be broadly grouped into classification and regression. Examples of classification tasks include sentiment analysis and topic classification; examples of regression include fatality prediction. This is the step where quantitative psychologists need to apply a pretrained model to a particular task, such as depression detection and personality classification. For each specific task, researchers need to specify the model\u2019s input, targeted output (including the number of labels), and whether this is a classification task or a regression task.\n###figure_4### In Figure 4  ###reference_###, we illustrate how a k-class classification works. From the encoder layers, we retrieve a 1 by N vector for each input sample, where N is 768 in the case of BERT-base and 1024 in the case of BERT-large. Then we project this vector to 1 by K using an N by K matrix. This layer is often referred to as the fully connected layer. What follows next depends on whether the task is classification or regression. In the case of classification, where K is equal to or greater than 2, we apply a softmax function such that each of the K elements represents the probability of that corresponding class being the predicted class:888For implementation details, refer to https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html.\nIn the case of regression, where K is 1, we can apply the loss function directly to the 1 by K vector (now 1 by 1). Mean squared error is a commonly used loss function for regression tasks.999For implementation details, refer to https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Practical Exercises",
            "text": "In this section, we provide two practical exercises to illustrate how researchers can finetune a large language model with relative ease to achieve state-of-the-art results in classification and regression, respectively.101010Some of the original results on classification and regression have been published at Political Analysis. All our exercises are written in Python in the format of Jupyter notebooks so that readers can follow along in an interactive manner.111111https://jupyter.org. For easy reproducibility and access to computation resources, all our computation is done on Google Colab.121212https://colab.research.google.com."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Multi-class classification",
            "text": "###figure_5### When researchers need to classify text inputs into more than two classes, this is what we call multi-class classification. In the following case study, we classify a given text into one of eight topics. The texts are the speech transcripts from the New Zealand Parliament from 1987 to 2002 (Osnabr\u00fcgge \\BOthers., \\APACyear2021  ###reference_b11###). In Figure 5  ###reference_###, we illustrate the data input and list all the eight classes. We use using 2,915 annotated speech transcripts for training, 625 for validation, and another 625 for testing.\nWe finetune a RoBERTa-base model (Liu \\BOthers., \\APACyear2019  ###reference_b8###) for topic classification. RoBERTa-base has 12 layers of transformers and 125 million parameters in total. On top of its 12 layers of transformers, we add a classification layer for 8-topic classification. We use cross entropy as the loss function. We finetune the RoBERTa-base model for 20 epochs with a learning rate of 2e-5, a batch size of 16, and an input sequence length of 512 on an A100 GPU. We use the validation set\u2019s cross entropy loss to select the best epoch and the optimal checkpoint. We then use the optimal checkpoint to make inferences on the test set with a batch size of 64 (see Listing 1  ###reference_###). As our baseline, we use results from Osnabr\u00fcgge \\BOthers. (\\APACyear2021  ###reference_b11###), where the authors use 115,410 related but out-of-domain policy statements from Australia, Canada, Ireland, New Zealand, the United Kingdom, and the United States to train a regularized multinomial logistic regression model. For easy comparison, we use the same evaluation metrics as in Osnabr\u00fcgge \\BOthers. (\\APACyear2021  ###reference_b11###).\nAcross all metrics, finetuning the RoBERTa model with 2,915 New Zealand parliamentary speeches substantially outperforms the cross-domain topic classifier by Osnabr\u00fcgge \\BOthers. (\\APACyear2021  ###reference_b11###), which is trained using 115,420 annotated policy statements (Table 1  ###reference_###). For example, for top-1 accuracy the finetuned RoBERTa model outperforms the cross-domain baseline by 26%. For top-3 accuracy the finetuned RoBERTa model outperforms the cross-domain baseline by 10%."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "The pretrain-finetune paradigm has revolutionized the field of natural language processing. In this tutorial, we have provided an intuitive and thorough walk-through of the key concepts therein. We also introduced some of the most common use cases that the pretrain-finetune paradigm supports. To help facilitate the wider adoption of this new paradigm, we have included easy-to-follow examples and made the related datasets and scripts publicly available. Practitioners and scholars in quantitative psychology and psychology more broadly should find our tutorial useful."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Finetuning a RoBERTa-base model with 2,915 labeled in-domain samples can outperform the cross-domain regularized multinomial logistic regression model, trained with 115,420 out-of-domain samples, in the 8-topic classification task by a large margin. Cross-domain classifiers are from\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Osnabr\u00fcgge\u00a0<span class=\"ltx_ERROR undefined\">\\BOthers</span>. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02504v1#bib.bib11\" title=\"\"><span class=\"ltx_ERROR undefined\">\\APACyear</span>2021</a>)</cite>. Test set is the same for both models. Mean of three random runs is reported, with standard deviation in the brackets. Better results are in bold.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T1.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T1.1.1.1.1.1\">Metrics</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S4.T1.1.1.1.2\">8 topics</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.2.2.1\">Cross-domain</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.2.2.2\">Finetuning LM</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T1.1.3.1.1\">Top-1 accuracy</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.3.1.2\">0.512 (0.007)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.3.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.3.1.3.1\">0.643 (0.007)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.1.4.2.1\">Top-3 accuracy</th>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.4.2.2\">0.821 (0.001)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.4.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.4.2.3.1\">0.899 (0.001)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.1.5.3.1\">Top-5 accuracy</th>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.5.3.2\">0.917 (0.007)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.5.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.5.3.3.1\">0.968 (0.007)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.1.6.4.1\">Balanced accuracy</th>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.6.4.2\">0.465 (0.004)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.6.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.6.4.3.1\">0.592 (0.004)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S4.T1.1.7.5.1\">F1 macro</th>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T1.1.7.5.2\">0.456 (0.011)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T1.1.7.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.7.5.3.1\">0.584 (0.011)</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 1: Finetuning a RoBERTa-base model with 2,915 labeled in-domain samples can outperform the cross-domain regularized multinomial logistic regression model, trained with 115,420 out-of-domain samples, in the 8-topic classification task by a large margin. Cross-domain classifiers are from\u00a0Osnabr\u00fcgge\u00a0\\BOthers. (\\APACyear2021). Test set is the same for both models. Mean of three random runs is reported, with standard deviation in the brackets. Better results are in bold."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Finetuned ConfliBERT models outperform dictionary-based models by a large margin. Results in Columns 1 and 2 are from Table 2 in\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">H\u00e4ffner\u00a0<span class=\"ltx_ERROR undefined\">\\BOthers</span>. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02504v1#bib.bib5\" title=\"\"><span class=\"ltx_ERROR undefined\">\\APACyear</span>2023</a>)</cite>. By increasing the maximum sequence length to 512, we are able to further improve the performance of those finetuned models. Best results in bold.</figcaption>\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S4.T2.1.2.1.1\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.2.1.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.1.1.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.2.1.1.1.1.1\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">Model</td>\n</tr>\n</table>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.2.1.2\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.2.1.2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.1.2.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.2.1.2.1.1.1\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">OCoDi</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.1.2.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.2.1.2.1.2.1\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">Random Forest</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.1.2.1.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.2.1.2.1.3.1\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">(1)</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.2.1.3\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.2.1.3.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.1.3.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.2.1.3.1.1.1\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">OCoDi</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.1.3.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.2.1.3.1.2.1\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">XGBoost</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.1.3.1.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.2.1.3.1.3.1\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">(2)</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.2.1.4\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.2.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.1.4.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.2.1.4.1.1.1\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">ConfliBERT</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.1.4.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.2.1.4.1.2.1\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">Length-256</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.1.4.1.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.2.1.4.1.3.1\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">(3)</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.2.1.5\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.2.1.5.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.1.5.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.2.1.5.1.1.1\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">ConfliBERT</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.1.5.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.2.1.5.1.2.1\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">Length-512</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.1.5.1.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.2.1.5.1.3.1\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">(4)</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T2.1.3.2.1\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">MSE</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.3.2.2\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">1.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.3.2.3\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">1.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.3.2.4\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">0.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.3.2.5\" style=\"padding-left:11.0pt;padding-right:11.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.3.2.5.1\">0.82</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.1\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">R\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.2\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">0.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.3\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">0.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.4\" style=\"padding-left:11.0pt;padding-right:11.0pt;\">0.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.5\" style=\"padding-left:11.0pt;padding-right:11.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.5.1\">0.81</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 2: Finetuned ConfliBERT models outperform dictionary-based models by a large margin. Results in Columns 1 and 2 are from Table 2 in\u00a0H\u00e4ffner\u00a0\\BOthers. (\\APACyear2023). By increasing the maximum sequence length to 512, we are able to further improve the performance of those finetuned models. Best results in bold."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.02504v1_figure_1.png",
            "caption": "Figure 1: High-level illustration of the pretraining (left) and finetuning (right) workflows."
        },
        "2": {
            "figure_path": "2403.02504v1_figure_2.png",
            "caption": "Figure 2: Illustration of the BERT model\u2019s self-attention mechanism."
        },
        "3": {
            "figure_path": "2403.02504v1_figure_3.png",
            "caption": "Figure 3: Illustration of the BERT model\u2019s masked language modeling."
        },
        "4": {
            "figure_path": "2403.02504v1_figure_4.png",
            "caption": "Figure 4: Illustration of the finetuning process. Same as with pretraining, the model takes in a text input and generates an encoding for each token. What differentiates finetuning from pretraining is that it takes a particular token representation to make task-specific predictions."
        },
        "5": {
            "figure_path": "2403.02504v1_figure_5.png",
            "caption": "Figure 5: \nGiven an input text, we categorize it into one of eight mutually exclusive classes. In the provided example, the text is classified under the second topic, External Relations."
        },
        "6": {
            "figure_path": "2403.02504v1_figure_6.png",
            "caption": "Figure 6: Given an input text, we predict a numerical value. In the example above, given an expert-written International Crisis Group (ICG) CrisisWatch report on Afghanistan for October, 2006, the model is expected to predict 6.34 as the natural logarithm of fatalities."
        }
    },
    "references": [
        {
            "1": {
                "title": "\\APACrefYearMonthDay2020.",
                "author": "\\APACinsertmetastargpt3{APACrefauthors}Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J\\BPBID., Dhariwal, P.\\BDBLAmodei, D.",
                "venue": "\\BBOQ\\APACrefatitleLanguage Models are Few-Shot Learners Language models are few-shot learners.\\BBCQ",
                "url": null
            }
        },
        {
            "2": {
                "title": "\\APACrefYearMonthDay2019.",
                "author": "\\APACinsertmetastarbert{APACrefauthors}Devlin, J., Chang, M\\BHBIW., Lee, K.\\BCBL \\BBA Toutanova, K.",
                "venue": "\\BBOQ\\APACrefatitleBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\\BBCQ",
                "url": null
            }
        },
        {
            "3": {
                "title": "\\APACrefYearMonthDay2022.",
                "author": "\\APACinsertmetastarlora{APACrefauthors}Hu, E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S.\\BDBLChen, W.",
                "venue": "\\BBOQ\\APACrefatitleLoRA: Low-Rank Adaptation of Large Language Models Lora: Low-rank adaptation of large language models.\\BBCQ",
                "url": null
            }
        },
        {
            "4": {
                "title": "\\APACrefYearMonthDay2022.",
                "author": "\\APACinsertmetastarconflibert{APACrefauthors}Hu, Y., Hosseini, M., Parolin, E\\BPBIS., Osorio, J., Khan, L., Brandt, P\\BPBIT.\\BCBL \\BBA D\u2019Orazio, V\\BPBIJ.",
                "venue": "\\BBOQ\\APACrefatitleConfliBERT: A Pre-trained Language Model for Political Conflict and Violence Conflibert: A pre-trained language model for political conflict and violence.\\BBCQ",
                "url": null
            }
        },
        {
            "5": {
                "title": "\\APACrefYearMonthDay2023.",
                "author": "\\APACinsertmetastarinterpretable{APACrefauthors}H\u00e4ffner, S., Hofer, M., Nagl, M.\\BCBL \\BBA Walterskirchen, J.",
                "venue": "\\BBOQ\\APACrefatitleIntroducing an Interpretable Deep Learning Approach to Domain-Specific Dictionary Creation: A Use Case for Conflict Prediction Introducing an interpretable deep learning approach to domain-specific dictionary creation: A use case for conflict prediction.\\BBCQ",
                "url": null
            }
        },
        {
            "6": {
                "title": "\\APACrefYearMonthDay2023.",
                "author": "\\APACinsertmetastartext_package{APACrefauthors}Kjell, O., Giorgi, S.\\BCBL \\BBA Schwartz, H\\BPBIA.",
                "venue": "\\BBOQ\\APACrefatitleThe Text-Package: An R-Package for Analyzing and Visualizing Human Language Using Normal Language Processing and Transformers The text-package: An r-package for analyzing and visualizing human language using normal language processing and transformers.\\BBCQ",
                "url": null
            }
        },
        {
            "7": {
                "title": "\\APACrefYearMonthDay2020.",
                "author": "\\APACinsertmetastaralbert{APACrefauthors}Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P.\\BCBL \\BBA Soricut, R.",
                "venue": "\\BBOQ\\APACrefatitleALBERT: A Lite BERT for Self-supervised Learning of Language Representations ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.\\BBCQ",
                "url": null
            }
        },
        {
            "8": {
                "title": "\\APACrefYearMonthDay2019.",
                "author": "\\APACinsertmetastarroberta{APACrefauthors}Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.\\BDBLStoyanov, V.",
                "venue": "\\BBOQ\\APACrefatitleRoBERTa: A Robustly Optimized BERT Pretraining Approach RoBERTa: A Robustly Optimized BERT Pretraining Approach.\\BBCQ",
                "url": null
            }
        },
        {
            "9": {
                "title": "\\APACrefYearMonthDay2020.",
                "author": "\\APACinsertmetastaraug{APACrefauthors}Longpre, S., Wang, Y.\\BCBL \\BBA DuBois, C.",
                "venue": "\\BBOQ\\APACrefatitleHow Effective is Task-Agnostic Data Augmentation for Pretrained Transformers? How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?\\BBCQ",
                "url": null
            }
        },
        {
            "10": {
                "title": "\\APACrefYearMonthDay2013.",
                "author": "\\APACinsertmetastarword2vec{APACrefauthors}Mikolov, T., Sutskever, I., Chen, K., Corrado, G.\\BCBL \\BBA Dean, J.",
                "venue": "\\BBOQ\\APACrefatitleDistributed Representations of Words and Phrases and their Compositionality Distributed representations of words and phrases and their compositionality.\\BBCQ",
                "url": null
            }
        },
        {
            "11": {
                "title": "\\APACrefYearMonthDay2021.",
                "author": "\\APACinsertmetastarcross_domain{APACrefauthors}Osnabr\u00fcgge, M., Ash, E.\\BCBL \\BBA Morelli, M.",
                "venue": "\\BBOQ\\APACrefatitleCross-Domain Topic Classification for Political Texts Cross-Domain Topic Classification for Political Texts.\\BBCQ",
                "url": null
            }
        },
        {
            "12": {
                "title": "\\APACrefYearMonthDay2023.",
                "author": "\\APACinsertmetastarbest_practices_supervised_ml{APACrefauthors}Pargent, F., Schoedel, R.\\BCBL \\BBA Stachl, C.",
                "venue": "\\BBOQ\\APACrefatitleBest Practices in Supervised Machine Learning: A Tutorial for Psychologists Best practices in supervised machine learning: A tutorial for psychologists.\\BBCQ",
                "url": null
            }
        },
        {
            "13": {
                "title": "\\APACrefYearMonthDay2017.",
                "author": "\\APACinsertmetastarattention{APACrefauthors}Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A\\BPBIN.\\BDBLPolosukhin, I.",
                "venue": "\\BBOQ\\APACrefatitleAttention Is All You Need Attention Is All You Need.\\BBCQ",
                "url": null
            }
        },
        {
            "14": {
                "title": "\\APACrefYearMonthDay2019.",
                "author": "\\APACinsertmetastarpredicted_acc2{APACrefauthors}Wang, Y.",
                "venue": "\\BBOQ\\APACrefatitleComparing Random Forest with Logistic Regression for Predicting Class-Imbalanced Civil War Onset Data: A Comment Comparing Random Forest with Logistic Regression for Predicting Class-Imbalanced Civil War Onset Data: A Comment.\\BBCQ",
                "url": null
            }
        },
        {
            "15": {
                "title": "\\APACrefYearMonthDay2023\\BCnt1.",
                "author": "\\APACinsertmetastarfinetune_pa{APACrefauthors}Wang, Y.",
                "venue": "\\BBOQ\\APACrefatitleOn Finetuning Large Language Models On finetuning large language models.\\BBCQ",
                "url": null
            }
        },
        {
            "16": {
                "title": "\\APACrefYearMonthDay2023\\BCnt2.",
                "author": "\\APACinsertmetastarpretrained_topic_classification{APACrefauthors}Wang, Y.",
                "venue": "\\BBOQ\\APACrefatitleTopic Classification for Political Texts with Pretrained Language Models Topic Classification for Political Texts with Pretrained Language Models.\\BBCQ",
                "url": null
            }
        },
        {
            "17": {
                "title": "\\APACrefYearMonthDay2022.",
                "author": "\\APACinsertmetastarbag_of_words{APACrefauthors}Wang, Y., Tian, J., Yazar, Y., Ones, D\\BPBIS.\\BCBL \\BBA Landers, R\\BPBIN.",
                "venue": "\\BBOQ\\APACrefatitleUsing Natural Language Processing and Machine Learning to Replace Human Content Coders Using natural language processing and machine learning to replace human content coders.\\BBCQ",
                "url": null
            }
        },
        {
            "18": {
                "title": "\\APACrefYearMonthDay2021.",
                "author": "\\APACinsertmetastarmonitor_depression{APACrefauthors}Zhang, Y., Lyu, H., Liu, Y., Zhang, X., Wang, Y.\\BCBL \\BBA Luo, J.",
                "venue": "\\BBOQ\\APACrefatitleMonitoring Depression Trends on Twitter During the COVID-19 Pandemic: Observational Study Monitoring depression trends on twitter during the covid-19 pandemic: Observational study.\\BBCQ",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.02504v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3",
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.02504v1",
        "paper_title": "A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing1footnote 11footnote 1Our replication package can be accessed at https://doi.org/10.7910/DVN/QTT84C.",
        "research_background": "### Motivation\nThe paper is motivated by the significant impact that the pretrain-finetune paradigm has had on the field of natural language processing (NLP). This paradigm, powered by large pretrained language models like BERT and RoBERTa, has demonstrated high efficacy in various tasks, even with a limited number of training samples. The ability to achieve competitive performance with relatively few labeled samples has made this approach increasingly popular and valuable, particularly in the social sciences. The authors aim to demystify this paradigm and show its practical applications in social science research.\n\n### Research Problem\nThe research problem this paper tackles is the complexity and inaccessibility of the pretrain-finetune paradigm for researchers, particularly those interested in applying these methods to social sciences. Although the paradigm has been widely recognized for its effectiveness, there remains a gap in the literature that explicitly guides researchers in understanding and implementing these techniques in their work. The tutorial seeks to fill this gap by (1) providing a comprehensive overview of the pretrain-finetune paradigm and (2) showcasing its applications in social science research.\n\n### Relevant Prior Work\n1. **General Advances in NLP through Pretrain-Finetune Paradigm**: The influence of large pretrained models like BERT (Devlin \\BOthers., 2019) and RoBERTa (Liu \\BOthers., 2019) has been substantial in reshaping NLP practices.\n2. **Pretrain-Finetune in Social Sciences**: Recent works in social sciences (Zhang \\BOthers., 2021; Wang, 2023a, 2023b) have demonstrated the paradigm\u2019s effectiveness, even with just a few hundred labeled samples.\n3. **Minimum Sample Recommendations for NLP**: Earlier, a minimum of 3,000 samples was recommended for NLP tasks using traditional methods like bag-of-words (Wang \\BOthers., 2022). In contrast, the pretrain-finetune paradigm can achieve similar or superior results with significantly fewer samples (Wang, 2023b).\n\nBy building on these prior works, the tutorial aims to guide researchers through the foundational concepts of pretraining and finetuning and demonstrate their application in advancing social science research.",
        "methodology": "The proposed methodology in the paper focuses on the widely adopted pretrain-finetune paradigm in Natural Language Processing (NLP). The process begins with pretraining a model using unlabeled raw data, without targeting any specific downstream tasks. This approach is exemplified by widely used models like BERT and RoBERTa, both containing hundreds of millions of parameters. The pretraining of these substantial models typically demands not only extensive raw datasets but also specialized hardware, like Graphics Processing Units (GPUs).\n\nThe pretraining process itself comprises three main steps:\n\n1. **Tokenization**: This involves breaking down the text data into smaller units called tokens, which are the base units that the model will process.\n2. **Encoding**: In this step, the tokens are converted into numerical representations that the neural network can use.\n3. **Pretraining Tasks**: These tasks are designed to help the model learn the underlying structure and patterns of the language. Common pretraining tasks include:\n   - **Masked Language Modeling**: Predicting missing tokens in a sentence.\n   - **Next Sentence Prediction**: Determining if one sentence naturally follows another (as used in BERT).\n   - **Sentence-order Prediction**: Predicting the correct order of sentences (as used in models like ALBERT).\n\nBy following these steps, the model is trained to grasp the fundamental aspects of the language, which can then be fine-tuned for specific downstream tasks such as sentiment analysis or named entity recognition.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Dataset:**\n- The main dataset consists of speech transcripts from the New Zealand Parliament spanning from 1987 to 2002.\n- It includes 2,915 annotated speech transcripts for training, 625 for validation, and 625 for testing.\n- The task involves classifying each transcript into one of eight topics.\n\n**Baselines:**\n- The baseline is taken from Osnabr\u00fcgge et al. (2021), where a regularized multinomial logistic regression model is trained using 115,410 out-of-domain policy statements from six different countries (Australia, Canada, Ireland, New Zealand, the United Kingdom, and the United States).\n\n**Model:**\n- A RoBERTa-base model is used, which has 12 layers of transformers and a total of 125 million parameters.\n- An additional classification layer is added on top of the model for 8-topic classification.\n\n**Training Setup:**\n- Loss Function: Cross entropy\n- Finetuning Parameters:\n  - Number of Epochs: 20\n  - Learning Rate: 2e-5\n  - Batch Size: 16\n  - Input Sequence Length: 512\n  - Hardware: A100 GPU\n- Validation:\n  - The best epoch and optimal checkpoint are selected based on the validation set\u2019s cross entropy loss.\n- Inference:\n  - The optimal checkpoint is used to make inferences on the test set with a batch size of 64.\n\n**Evaluation Metrics:**\n- Same evaluation metrics as those used by Osnabr\u00fcgge et al. (2021), which include top-1 accuracy and top-3 accuracy.\n\n**Results:**\n- The finetuned RoBERTa model significantly outperforms the cross-domain topic classifier from Osnabr\u00fcgge et al. (2021).\n- Example Performance Gains:\n  - Top-1 Accuracy: The finetuned RoBERTa model outperforms the baseline by 26%.\n  - Top-3 Accuracy: The finetuned RoBERTa model outperforms the baseline by 10%."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the performance of finetuning the RoBERTa-base model for multi-class topic classification of speech transcripts from the New Zealand Parliament.",
            "experiment_process": "The researchers used 2,915 annotated speech transcripts for training, 625 for validation, and another 625 for testing. They finetuned a RoBERTa-base model, which has 12 layers and 125 million parameters, adding a classification layer for 8-topic classification. Fine-tuning was performed for 20 epochs with a learning rate of 2e-5, a batch size of 16, and an input sequence length of 512 on an A100 GPU. The validation set's cross-entropy loss was used to select the best epoch and the optimal checkpoint. The test set was then inferenced using a batch size of 64. As a baseline, the performance was compared to a multinomial logistic regression model trained on 115,410 out-of-domain policy statements from various countries.",
            "result_discussion": "The finetuned RoBERTa model with 2,915 New Zealand parliamentary speeches significantly outperformed the cross-domain topic classifier. For top-1 accuracy, the RoBERTa model outperformed the baseline by 26%, and for top-3 accuracy, it outperformed the baseline by 10%.",
            "ablation_id": "2403.02504v1.No1"
        },
        {
            "research_objective": "To predict the natural logarithm of fatalities on a country-month level using text from CrisisWatch and evaluate the performance improvements of finetuning the ConfliBERT model.",
            "experiment_process": "The training set, validation set, and testing set used were the same as in the reference study, covering the periods from 2003 to the first half of 2020, the second half of 2020, and the year 2021, respectively. The large language model, ConfliBERT, which shares the BERT-base architecture, was pretrained using a large corpus in the politics and conflicts domain. Fine-tuning was performed with a learning rate of 2e-5 for 10 epochs. Two variations were tested: ConfliBERT Length-256 with a maximum of 256 tokens and ConfliBERT Length-512 with a maximum of 512 tokens. The performance of these finetuned models was compared against two baseline dictionary-based models using random forest and XGBoost.",
            "result_discussion": "Finetuning ConfliBERT significantly improved performance compared to dictionary-based approaches. Increasing the maximum sequence length from 256 to 512 further reduced the mean squared error from 0.99 to 0.82 and increased R from 0.77 to 0.81. The finetuned ConfliBERT achieved an MSE 49% lower and an R 29% higher than the OCoDi-XGBoost model.",
            "ablation_id": "2403.02504v1.No2"
        }
    ]
}