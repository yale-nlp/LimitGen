{
    "title": "Membership Inference Attacks and Privacy in Topic Modeling",
    "abstract": "Recent research shows that large language models are susceptible to privacy attacks that infer aspects of the training data. However, it is unclear if simpler generative models, like topic models, share similar vulnerabilities. In this work, we propose an attack against topic models that can confidently identify members of the training data in Latent Dirichlet Allocation. Our results suggest that the privacy risks associated with generative modeling are not restricted to large neural models. Additionally, to mitigate these vulnerabilities, we explore differentially private (DP) topic modeling. We propose a framework for private topic modeling that incorporates DP vocabulary selection as a pre-processing step, and show that it improves privacy while having limited effects on practical utility.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Deep learning models\u2019 propensity to memorize training data presents many privacy concerns. Notably, large language models are particularly susceptible to privacy attacks that exploit memorization [6  ###reference_b6###, 5  ###reference_b5###]. In this paper, we aim to investigate whether simpler probabilistic models, such as topic models, raise similar concerns. Our specific focus lies on examining probabilistic topic models like Latent Dirichlet Allocation (LDA) [2  ###reference_b2###].\nTopic modeling is an unsupervised machine learning (ML) method that aims to identify underlying themes or topics within a corpus. Despite the recent successes of large language models (LLMs), Probabilistic topic models are still widely used due to their interpretability and straightforward implementation for text analysis. Furthermore, topic models continue to be developed for a variety of applications and downstream tasks like document classification, summarization, or generation [3  ###reference_b3###, 32  ###reference_b32###].\nResearchers apply topic models across a variety of domains where privacy concerns arise. For example, many studies in the medical domain use topic models to analyze data sets with sensitive attributes [21  ###reference_b21###]. Additionally, topic models are widely used for various government or national defense applications. For instance, researchers applied LDA to better understand Twitter activity aimed at discrediting NATO during the Trident Juncture Exercises in 2018 [29  ###reference_b29###]. To ensure ethical and responsible use of ML, it is crucial to consider the privacy implications associated with topic modeling.\nProbabilistic topic models like LDA serve as a basic Bayesian generative model for text. Neural topic models and language models can learn complex generative processes for text based on observed patterns in the training data. However, LDA requires the generative process for documents to be explicitly defined a priori with far fewer parameters and a Bag-of-Words (BoW) text representation. Investigating LDA\u2019s susceptibility to privacy attacks provides broader context for the privacy vulnerabilities researched in contemporary ML models. Additionally, our work informs practitioners who may opt to use simpler topic models under an unjustified impression that they do not share the same vulnerabilities of LLMs.\nTo explore the privacy in topic modeling, we conduct membership inference attacks (MIAs) which infer whether or not a specific document was used to train LDA. We propose an attack based on an LDA-specific query statistic designed to exploit memorization. This query statistic is integrated into the Likelihood Ratio Attack (LiRA) framework introduced by Carlini et al. [4  ###reference_b4###]. We show that our attack can confidently infer the membership of documents included the training data of LDA which indicates that the privacy risks in generative modeling are not restricted to large neural models.\nTo mitigate such vulnerabilities, we explore differential privacy (DP) for topic modeling. DP acts a direct defense against MIAs by providing a statistical guarantee that the output of some data analysis is indistinguishable regardless of any one user\u2019s inclusion in the analysis. While several DP topic modeling algorithms in the literature attempt to protect individual privacy, previous works largely disregard the privacy of the model\u2019s accompanying vocabulary set [33  ###reference_b33###, 19  ###reference_b19###, 18  ###reference_b18###, 9  ###reference_b9###, 22  ###reference_b22###, 31  ###reference_b31###]. Instead, they consider the vocabulary set as given or public information. However, in practice, the vocabulary set is derived from the training data and can leak sensitive information. We propose an algorithm for DP topic modeling provides privacy in both the vocabulary selection and learning method. By incorporating DP vocabulary selection into the private topic modeling workflow, our algorithm enhances privacy guarantees and bolsters defenses against the LiRA, while having minimal impact on practical utility."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background and Notation",
            "text": "To begin, let us establish notation and provide a formal definition of topic models.\nFor a vocabulary size of  and  topics, a topic model  is a matrix whose rows sum to 1 representing each topic as a distribution over words.\nWe let  denote the process of learning the model parameterized by latent variables in  on a corpus  with  documents drawn from the underlying data distribution . The function  incorporates the learning algorithm to estimate \u2019s latent variables and returns the learned topic model . Our definition assumes that the primary objective of topic modeling is to estimate . While  is also associated with other latent variables,  best achieves the goals associated with topic modeling by summarizing the relevant themes in the corpus."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Latent Dirichlet Allocation",
            "text": "Blei et al. [2  ###reference_b2###] define LDA where each document is assumed to contain a mixture of topics. LDA assumes that each document  is represented by a -dimensional document-topic distribution , and the entire corpus is represented by  topic-word distributions . Furthermore, it models each word in a document as generated by sampling a topic from the document\u2019s topic distribution and then sampling a word from the chosen topic\u2019s distribution over words.\nThe process of estimating  presents a Bayesian inference problem typically solved using collapsed Gibbs sampling or variational inference [14  ###reference_b14###, 15  ###reference_b15###]. Each entry  represents the probability of drawing word  from topic . The entire topic model  represents the likelihood of each word appearing in each topic, which can be used to estimate the document-topic distribution  for any given document."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Memorization and Privacy",
            "text": "Understanding memorization in machine learning is critical for trustworthy deployment. Neural model\u2019s large parameter space and learning method introduce training data memorization which increases with model size or observation duplication [5  ###reference_b5###, 12  ###reference_b12###]. Memorization introduces vulnerabilities to attacks on privacy, such as membership inference, attribute inference or data extraction attacks [25  ###reference_b25###, 27  ###reference_b27###, 6  ###reference_b6###].\nSchofield et al. [23  ###reference_b23###] note that document duplication in topic models concentrates the duplicated document\u2019s topic distribution  over less topics, and briefly refer to this phenomenon as memorization. Their findings are consistent with studies on memorization and text duplication in large language models [5  ###reference_b5###]. In this work, we investigate the memorization and privacy of topic models using membership inference attacks (MIA)."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Membership Inference Attacks",
            "text": "In an MIA the adversary learns if a specific observation appeared in the model\u2019s training data [16  ###reference_b16###, 25  ###reference_b25###]. These attacks serve as the foundation for other attacks like data extraction, and could violate privacy alone if the adversary learns than an individual contributed to a data set with a sensitive global attribute (i.e. a data set containing only patients with disease X).\nPrior to attacking ML models, researchers explored exploiting aggregate statistics released in genome-wide association studies (GWAS) [16  ###reference_b16###]. Shokri et al. [25  ###reference_b25###] introduced one of the first MIAs on ML models.\nWhile most MIAs exploit deep learning models, few studies investigate topic models. Huang et al. [18  ###reference_b18###] propose three simple MIAs to evaluate their privatized LDA learning algorithm. However, they evaluate their attacks using precision and recall and fail to show that their attack confidently identify members of the training data.\nTo evaluate an MIA\u2019s ability to confidently infer membership, we examine the attack\u2019s true positive rate (TPR) at low false positive rates (FPR) [4  ###reference_b4###]. Receiver operator characteristic (ROC) and area under the curve (AUC) analysis is useful, but we must be wary of the axes. To accurately capture the TPR at the low FPRs of interest, we visualize ROC curves using log-scaled axes."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "MIAs Against Topic Models",
            "text": "In this section, we detail our MIA against topic models based on the LiRA framework, and empirically demonstrate its effectiveness."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Threat Model",
            "text": "We consider a threat model where the adversary has black-box access to the learned topic-word distribution , but not other latent variables captured by . The adversary can not observe intermediate word counts, learning method, or hyper-parameters used while learning . We also assume that the adversary has query access to the underlying data distribution , which allows them to train shadow models on data sets drawn from .\nUnder the typical query model, we could consider queries as a request to predict a document-topic distribution  given a document. However,  would hold little meaning without access to . Furthermore, a clever adversary could query many specially selected documents to reconstruct . Therefore, we remove this level of abstraction and assume that the adversary is given direct access to  as the output of our topic model."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Attack Framework",
            "text": "In the LiRA framework of Carlini et al. [4  ###reference_b4###], the adversary performs hypothesis testing to determine whether or not a target document  was in the training data. Specifically, we consider  and  where  is the distribution of  learned with , and  is the distribution of  learned without . Given an observed , would like to construct the likelihood ratio test statistic  as follows:\nwhere  is the probability density function of  under  [4  ###reference_b4###].\nHowever, the ration in Equation 1  ###reference_### is infeasible to calculate directly because all  represent a set of  -dimensional distributions. Instead, the adversary applies a carefully chosen query statistic  on all  to reduce the problem to 1-dimension. The test proceeds by calculating the probability density of  under estimated normal distributions from  and :\nwhere  and  are the mean and variance of  and .\nIn the online LiRA, the adversary must train  shadow topic models with and without  to estimate  and . Appendix A  ###reference_### contains the full online LiRA algorithm. To ease the computational burden of training  shadow topic models for each target document, Carlini et al. propose an offline variant of the LiRA. In the offline LiRA, the adversary can evaluate any  after learning a collection of  shadow topic models once by comparing  to a normal estimated from  with a one-sided hypothesis test. Appendix B  ###reference_### contains the full offline LiRA algorithm.\nDesigning  is crucial to the attack\u2019s success. In supervised learning scenarios, the adversary can directly query the model and use the loss of an observation to inform the statistic [4  ###reference_b4###]. However, because topic models are unsupervised learning tasks, the adversary encounters the challenge of identifying an informative statistic that can be computed efficiently using . Hence,  must be careful tailored to topic models for the attack to be effective."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Designing an Effective Query Statistic",
            "text": "Effective model query statistics for the LiRA framework must satisfy the following criteria:\nThe statistic should increase for  when  is included in the training data () to enable the offline LiRA.\nThe estimated distributions  and  are approximately normal to allow for parametric modeling.\nIdeally, the statistic should also be related to how the model may memorize the training data to provide an intuitive interpretation for attack performance.\nFirst, we consider statistics from the simple MIAs on LDA presented by Huang et al. [18  ###reference_b18###]. They propose three statistics derived from the target document\u2019s estimated topic distribution : the entropy of , the standard deviation of , and the maximum value in . The intuition behind their chosen statistics comes from the observation that a document\u2019s topic distribution tends to concentrate in a few topics when included (or duplicated) in the training data [23  ###reference_b23###]. However, Huang et al. apply global thresholds to these statistics instead of using them as query statistics to derive the LiRA test statistic as in Equation 2  ###reference_###.\nWe propose an attack that leverages the LiRA framework with an improved query statistic to directly exploit LDAs generative process. Our query statistic is a heuristic for the target document\u2019s log-likelihood under LDA. To compute this statistic on  for a given document , the adversary maximizes the log-likelihood of the document over all possible document-topic distributions such that:\nwhere we optimzie over  s.t. . In practice, we use SciPy\u2019s out-of-the-box optimization methods to estimate  [30  ###reference_b30###].\nThe intuition behind our statistic is that a document is more likely generated by the target model when included the training data. Therefore, using  allows us to reason about memorization in topic models. We verify our criteria for the proposed statistic in Appendix D  ###reference_###. Additionally, Appendix D  ###reference_### contains an evaluation of other candidate statistics where we show that our statistic significantly outperforms other statistics based on [18  ###reference_b18###]\nThe LiRA with statistic  directly exploits memorization and per-example hardness. The attack accounts for the natural differences in  on various documents by estimating the likelihood ratio under distributions based on  and . This enables the LiRA to outperform simple attacks like in [18  ###reference_b18###] that use global thresholds on the other query statistics based on . Therefore, we propose the LiRA with statistic  as a stronger alternative to attack topic models."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Memorization and Per-Example Hardness",
            "text": "To verify aspects of memorization and per-example hardness for probabilistic topic models, we estimate and visualize  and  in an experiment similar to [4  ###reference_b4###] and [12  ###reference_b12###]. Consistent with their findings, we show that outlying and hard-to-fit observations tend to have a large effect on the learned model when included in the training set for LDA. The histograms in Figure 1  ###reference_### display the estimated distributions for various types of documents.\n###figure_1### When included in the training data for , longer and outlying documents increase . We note that the word probability  in certain topics dramatically changes for words that appear infrequently in  or words that occur many times in . Together, these factors affect the learned topic model  and shift .\nThe simple fact that model is more likely to generate specific documents after inclusion in the training data hints toward memorization. Because the generative process for documents in LDA is extremely simple compared to language models, they do not learn verbatim sequences of words. Instead, topic model\u2019s ability to \u201cmemorize\" the training data is due to learning based on word co-occurrences.\nLong document\u2019s are inherently harder to fit than shorter documents. Because the log-likelihood for a document  is the sum of independent log-probabilities for generating each word in , the magnitude of  is naturally higher for longer documents. Additionally, the model may struggle to capture coherent topic structures for longer documents because they tend to contain words from a wider range of topics.\nThe LiRA turns our observations on memorization and per-example hardness into a MIA. When the words in a document become more common in the training set,  will reflect the new word co-occurrences in a few specific topics which tends to increase . This effect is amplified when the document contains many rare words from the vocabulary set. Therefore, the attacker can easily differentiate between  and  for long and outlying documents."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Attack Evaluation Set-Up",
            "text": "We evaluate our attack against three data sets: TweetRumors, 20Newsgroup and NIPS111The source for each dataset is included in the Availability section. To initiate the attack, we randomly sample half of the data to learn and release. Next, we train shadow topic models by repeatedly sampling half of the data set to simulate sampling from. In this scenario, the shadow model training data and the target model\u2019s training data likely overlap. This is a strong assumption made to accommodate the smaller size of our data sets. As observed by [4 ###reference_b4###], we do not expect attack performance to drop significantly using disjoint data sets. We compare our LiRA against each of the attacks presented by [18 ###reference_b18###]. To replicate their attacks, we randomly sample half of the data set to learn. Using, we estimate each documents\u2019 topic-distribution and compute the maximum posterior, standard deviation, and entropy on. We directly threshold each of these statistics to evaluate membership for every document in the data set. For each experiment, we learn using scikit-learn\u2019s implementation of LDA with default learning parameters.333https://scikit-learn.org/ ###reference_scikit-learn.org/### For TweetSet we set the number of topics, 20Newsgroup we set, and for NIPS we vary. We learn shadow models, replicate each experiment 10 times and report our results across all iterations. We interpret as a predicted membership score where a higher value indicates that the document is more likely to be a member of the training data. We empirically estimate the attack\u2019s TPR at low FPRs and plot the attack\u2019s ROC curve on log-scaled axes."
        },
        {
            "section_id": "3.6",
            "parent_section_id": "3",
            "section_name": "Attack Evaluation Results",
            "text": "First, we compare our online attack performance against the attacks in [18  ###reference_b18###] at an FPR of 0.1% in Table 1  ###reference_###. Figure 2  ###reference_### displays the ROC curves for the online and offline variant of our attacks. Table 1  ###reference_### and Figure 2  ###reference_### demonstrate that our LiRA outperforms each of attacks in [18  ###reference_b18###].\nBecause the LiRA considers the likelihood-ratio of , it accounts for per-example hardness and dominates the attack in Huang et al [18  ###reference_b18###] at all FPR\u2019s. As noted by [4  ###reference_b4###], attacks that directly apply global thresholds do not consider document level differences on the learned model and fail to confidently identify members of the training data. Stronger attacks, like our LiRA, should be used to empirically evaluate the privacy associated with topic models.\n###figure_2### To understand how the number of topics in  influence attack performance, we vary the number of topics  on NIPS and attack the resulting model. Figure 3  ###reference_### shows the ROC of the offline attack while varying . As  increases, we see that TPR increases at all FPRs. Furthermore, based on the Table 2  ###reference_###, we note minor differences between online and offline attack performance.\n###figure_3### The number of parameters in  grows linearly by the length of the vocabulary set as  increases. With more topics and more parameters, document\u2019s word co-occurrence structure is typically better represented in the document\u2019s learned topic distribution. Consequently, increasing  enhances the impact of including the document in the training data on the likelihood of a document resulting in better attack performance.\nOverall, our findings demonstrate that despite their simple architecture, topic models exhibit behavior that resembles memorization and are vulnerable to strong MIAs."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Private Topic Modeling",
            "text": "Designing private ML solutions is an active field of research. In topic modeling, the literature is largely focused on differential privacy (DP). DP acts as a direct defense against privacy attacks like MIAs by limiting the effect one data point can have on the learned model. Furthermore, DP provides a clear, quantifiable method for reasoning about privacy loss."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Differential Privacy",
            "text": "DP provides strong theoretical guarantees of individual-level privacy by requiring the output of some data analysis to be indistinguishable (with respect to a small multiplicity factor) between any two adjacent data sets  and .\nLet  be a randomized algorithm. For any  and , we say that  is -differentially private if for all neighboring databases  and every\nThe notion of adjacency is critical. For any text data set, we say that two corpora  and  are author-level adjacency if they differ by one author\u2019s documents. This notion enforces DPs promise of individual level privacy by considering adjacency at the user level.\nIf two corpora  and  differ by one document, then adjacency is at the document-level. This relaxed notion of adjacency can satisfy author-level adjacency if we assume that each document has a unique author. For corpora the most relaxed notion of adjacency is word-level adjacency. Two corpora  and  are word-level adjacent if they differ by one word in one document.\nTo achieve differential privacy we must add noise whose magnitude is scaled to the worst-case sensitivity of the target statistics from all adjacent data sets. Researchers have explored word-level adjacency to control sensitivity, but this approach offers weak privacy guarantees. For instance, most DP collapsed Gibbs sampling algorithms for learning LDA rely on word-level adjacency [33  ###reference_b33###, 19  ###reference_b19###, 18  ###reference_b18###].\nThere are a variety of other algorithms for DP topic modeling: [22  ###reference_b22###] propose DP stochastic variational inference to learn LDA, [9  ###reference_b9###] use a privatized spectral algorithm to learn LDA, and [31  ###reference_b31###] satisfy DP by adding noise directly to . These algorithms satisfy various notions of DP and use differing notions of adjacency. Table 5  ###reference_### in Appendix E  ###reference_### summarizes each DP topic modeling implementation.\nWhile each DP topic modeling algorithm has their advantages and disadvantages, there are some common themes across implementations. First, the iterative nature of learning algorithms for LDA posses a difficult composition issue when managing the privacy loss. Next, many implementations use relaxed notions of adjacency to control sensitivity. Finally, none of the existing methods address the privacy concerns with releasing the vocabulary set corresponding to ."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "DP Vocabulary Selection",
            "text": "Releasing the topic-word distribution  without ensuring the privacy of the accompanying vocabulary set can lead to privacy violations. Topic models with comprehensive vocabulary sets tend to include many infrequently used words, and section 3.4  ###reference_### shows that documents with infrequently used words are more vulnerable to MIAs. Furthermore, even if the values in  are private, the overall release of the topic-word distribution can not satisfy DP because the vocabulary set accompanied by  is not private.\nTo illustrate this point, consider a scenario where we have a corpus  with a single document . If we apply DP topic modeling to release  without changing the vocabulary set, then we release all of the words in . This clearly compromises the privacy of the document in . If we chose not to release the vocabulary set,  loses practical interpretability. To address this issue, it is necessary to explore methods for differentially private vocabulary selection.\nDP vocabulary selection can be formalized as the DP Set-Union (DPSU) [13  ###reference_b13###]. In the DPSU each author  contributes a subset  of terms in their documents, and our goal is to design a -DP algorithm to release a vocabulary set  such that the size of  is as large as possible. Carvalho et al. propose a solution for DPSU which is particularly well fit for vocabulary selection because their method allows for one user to contribute the same term multiple times [7  ###reference_b7###]."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Fully Differentially Private Topic Modeling",
            "text": "We propose a high-level procedure for fully DP topic modeling (FDPTM), in Algorithm 1  ###reference_###. FDPTM composes the privately selected vocabulary set  and the private topic-word distribution . Theorem 4.1  ###reference_theorem1### states the privacy guarantees associated with FDPTM. The proof is deferred to Appendix F  ###reference_###.\nIf  for selecting the vocabulary set satisfies -DP and  for topic modeling satisfies -DP, then the overall release of  satisfies -DP.\nImplementing FDPTM requires a few key considerations. First, the process depends on carefully tuning the privacy parameters and other parameters within  and . Second, the data curator must ensure that  and  satisfy the same notion of differential privacy and adjacency.\nOverall, FDPTM is a modular framework for releasing meaningful topic-word distributions. Although our evaluations focus on LDA, other topic models for specific uses can easily fit into the proposed procedure."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "FDPTM Evaluation Set-Up",
            "text": "We empirically evaluate our FDPTM using the TweetRumors data set with . We ensure author-level privacy via document-level adjacency under the assumption that each document has a unique author. For vocabulary selection we use the DPSU solution proposed in [7  ###reference_b7###]. We use the DP LDA learning algorithm proposed by [34  ###reference_b34###]. For DPSU, we fix the privacy parameter  and choose the cut-off value  with the parameter , following [7  ###reference_b7###].\nTo evaluate utility, we first vary the privacy loss parameter for DPSU and analyze utility when LDA is non-private. We study the interaction between DP vocabulary selection and DP LDA by fix the privacy loss parameter for one mechanism and vary the privacy loss parameter for the other mechanism. Specifically, we first fix  for DP LDA and vary  for DPSU. Then, we fix  and vary .\nWe evaluate the utility of the FDPTM using topic coherence [20  ###reference_b20###], which can be a useful proxy for measuring the interpretability of topics. Topic coherence for a topic  is defined as\nwhere  is the document frequency of word  (i.e. the number of documents where word  occurs),  is the co-document frequency of words  and  (i.e. the number of documents where  and  both occur), and  is list of the  most probable words in topic . In our analysis, we select each topic\u2019s top  words and report the average across each topic.\nWe also test FDPTM against the online LiRA to empirically evaluate the privacy of FDPTM. For each attack we learn 64 shadow models. Like before, we conduct attacks while varying  and fixing . Then, we fix  and vary . We repeat each experiment 10 times and report the results across each iteration."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "FDPTM Evaluation Results",
            "text": "Figure 4  ###reference_### displays topic coherence as we increase . Figure 5  ###reference_### displays topic coherence as we vary either  and hold the other constant. We include the average vocabulary size as we increase  in Appendix G  ###reference_###.\n###figure_4### ###figure_5### Topic coherence remains stable after , suggesting that increasing the privacy loss for DPSU may not affect topic coherence after a certain point. When comparing coherence while increasing , we see that  decreases coherence regardless of . In Figure 5  ###reference_###, coherence plateaus around  while increasing  continues to boost coherence. Consequently, increasing  yields diminishing utility returns faster.\nFigure 6  ###reference_### presents the ROC curves that explore the how the interaction between DP vocabulary selection and DP LDA affect attack performance. We gain some empirical privacy under FDPTM by decreasing LiRA performance at all FPRs. Generally, we see that attack performance decreases similarly as we decrease either privacy loss parameter. Furthermore, we gain empirical privacy by decreasing LiRA performance at all FPRs.\n###figure_6### DPSU decreases attack performance because we shrink the size of the vocabulary set, limiting the number of parameters in , and forcing each documents\u2019 topic distribution to look more like other documents\u2019 topic distributions. The nature of learning on word co-occurrences forces  to reflect identifiable changes when infrequently used words suddenly become more common to the training data. By removing these words with DPSU, we limit the ability for outliers to have large effects on the topic model.\nOur results indicate that dedicating most of the privacy budget to DP LDA, rather than DPSU, increases utility at the same privacy level. Intuitively, we can inject less noise into DP LDA, and more into the vocabulary selection algorithm () to increase model interpretability at the same global privacy loss () and similar empirical privacy against the LiRA. This approach allows us balance model privacy and utility in topic modeling."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "In our work, we show that topic models exhibit aspects of memorization and successfully implement strong MIAs against them. Although probabilistic topic models do not memorize verbatim sequences of text like language models, they do memorize word frequency and occurrence. In some instances, knowledge of the frequency of certain terms in a document constitutes a privacy violation regardless of their word ordering.\nTo combat MIAs and memorization, we propose a modular framework for implementing better private topic models using differential privacy. Overall, the addition of DP vocabulary selection to the DP topic modeling work-flow is important for guaranteeing private, interpretable releases of . Not only does DP vocabulary selection allow for fully DP releases of , it also provides an effective defense against MIAs.\nWe highlight the greater need for continued development in the field of privacy-preserving ML. If simple probabilistic topics models for text memorize their training data, then it\u2019s only inevitable that large language models and neural topic models memorize their training data. As ML models continue to become more sophisticated and widely used, privacy concerns become increasingly relevant."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A The Online LiRA",
            "text": "The LiRA on topic models follows directly from the LiRA on supervised ML models proposed by [4  ###reference_b4###]. However, the attack on topic models differs because we use statistic  queried on . Algorithm 2  ###reference_### contains the online LiRA."
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B The Offline LiRA",
            "text": "The offline LiRA on topic models also follows directly from the offline LiRA on supervised ML models proposed by [4  ###reference_b4###]. The offline variant benefits over the online variant because lines 3-9 can be executed regardless of the target document , and lines 11-17 can be repeated for any  after estimating  once. Additionally, note that the test statistic  changes in line 15. Algorithm 3  ###reference_### contains the offline LiRA."
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C Data Profile",
            "text": "Table 3  ###reference_### contains the data profile for each data set after pre-processing."
        },
        {
            "section_id": "Appendix 4",
            "parent_section_id": null,
            "section_name": "Appendix D Evaluating Candidate Query Statistics",
            "text": "In this section, we provide analysis for the proposed statistic  w.r.t. the two query statistic requirements detailed in 3.3  ###reference_###:\nthe statistic should increase for  when  is included in the training data () to enable the offline LiRA.\nThe estimated distributions  and  are approximately normal to allow for parametric modeling.\nAdditionally, we compare the proposed statistic to alternative statistics based on [18  ###reference_b18###] and show that our proposed statistic outperforms the alternatives.\nHuang et al.[18  ###reference_b18###] propose three statistics: the entropy of , the standard deviation of , and the maximum value in  where  is a document\u2019s estimated topic distribution [18  ###reference_b18###]. Each statistic requires estimating the document\u2019s topic distribution . They estimate  using sampling techniques, but for these experiments, we choose to estimate  by performing the optimization from Eq. 3  ###reference_### for increased efficiency and consistency with the proposed statistic in Eq. 3  ###reference_###.\nTo tailor each statistic to fit within the LiRA framework, we make some minor modifications to the statistics proposed in [18  ###reference_b18###]. Because entropy naturally decreases when the document is included in the training data, we use the negative entropy to satisfy the first requirement. Additionally, because the maximum value in  is bound between [0,1] and tends to concentrate toward 1 for outliers, we apply the logit transformation to promote adherence to the normality requirement, as illustrated in [4  ###reference_b4###].\nTo evaluate the query statistic selection criteria, we fit an online attack using  shadow models using each of our candidate statistics. We sample 1000 documents from each dataset to extract  and  for each candidate statistic. We assess the first requirement by evaluating the KL-divergence between two normal distributions estimated from  and . A positive KL-divergence between (mean(), var()) and (mean(), var()) indicates that the statistic is greater when  is included in the training data. Figure 7  ###reference_### displays a boxplot for the KL divergences for each statistic. We note that all KL-divergences in Figure 7  ###reference_### are greater than 0 which indicates that  > .\n###figure_7### We evaluate the normality requirement by performing a Shipiro-Wilk test for each  and  with control for the False Discovery Rate (FDR) using the Benjamini-Hochberg Procedure [24  ###reference_b24###, 1  ###reference_b1###]. The null hypothesis is that the data is drawn from a normal distribution. Table 4  ###reference_### contains the the number of tests rejected with FDR control for each statistic. Our analysis indicates that the distributions of  for our statistic tend to exhibit normality. We can expect to encounter fewer rejections as we increase  (i.e. increase the sample size of ).\nFinally, in Figure 8  ###reference_###, we evaluate the attack performance of all statistics across each dataset. We show that the LiRA with our proposed statistic dominates the other candidate statistics at all FPRs. While each of the candidate statistics satisfies requirement 1, the long-whiskers and outliers associated with the logit maximum posterior statistic in Figure 7  ###reference_### suggest that it would outperform other statistics because the model can more easily differentiate between  and . However, the statistics\u2019 lack of normality seems to hinder performance.\n###figure_8###"
        },
        {
            "section_id": "Appendix 5",
            "parent_section_id": null,
            "section_name": "Appendix E Existing DP Topic Models",
            "text": "Table 5  ###reference_### summarizes centralized DP topic modeling algorithms available in the literature. The learning methods included are Collapesed Gibbs Sampling (CGS), variational inference (VI), spectral algorithm (SA), and Model Agnostic or Post-Hoc (PH)."
        },
        {
            "section_id": "Appendix 6",
            "parent_section_id": null,
            "section_name": "Appendix F Proof of Theorem 4.1",
            "text": "Proof (sketch). First, let  be a -DP vocabulary selection algorithm that returns a private vocabulary set , and  be a -DP topic modeling algorithm that returns a private topic-word distribution . Now, let PRE be a function that takes a corpus  and applies standard pre-processing procedures, and SAN be a function that takes a corpus  and a vocabulary set  and removes all words  if . Finally, let  represent the FDPTM algorithm such that for a corpus\nA function  is -stable on input  if adding or removing any  elements from  does not change the value of , that is,  for all  such that . We say  is stable on  if it is (at least) 1-stable on D, and unstable otherwise.\nLet  be a stable function, and let  be an -DP algorithm. Then, their composition  satisfies -DP.\nThe functions PRE and SAN are stable algorithms because each document is processed independently of the others based on a standard set of rules. Simply, adding or removing a document from the corpus does not affect the functions behavior on other documents,. Therefore, PRE and SAN are stable functions.\nVia our definition of , and using the fact that PRE is a stable function, then the first term in , , satisfies -DP. The second term of  applies  which directly depends on  and the stable functions PRE and SAN. Therefore, via adaptive composition,  is -DP [10  ###reference_b10###]."
        },
        {
            "section_id": "Appendix 7",
            "parent_section_id": null,
            "section_name": "Appendix G Vocabulary Size as  Increases",
            "text": "Figure 9  ###reference_### contains a plot for the size of the vocabulary set as the privacy loss parameter  for DPSU increases.\n###figure_9###"
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Attack TPR at 0.1% FPR with 128 Shadow Models</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S3.T1.1.2.1.1\">Data Set</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S3.T1.1.2.1.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.2.1.2.1\">Our Attack</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S3.T1.1.2.1.3\">Huang et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04451v1#bib.bib18\" title=\"\">18</a>]</cite></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.3.2\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S3.T1.1.3.2.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S3.T1.1.3.2.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.3.2.2.1\">Online LiRA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.1.3.2.3\">Maximum Posterior</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.1.3.2.4\">Standard Deviation</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.1.3.2.5\">Entropy</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.4.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T1.1.4.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.4.1.1.1\">TweetRumors</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T1.1.4.1.2\">12.8%</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.4.1.3\">0.19%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.4.1.4\">0.19%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.4.1.5\">0.18%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S3.T1.1.1.1\">\n<span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.1.1.1\">NIPS</span> ()</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S3.T1.1.1.2\">44.9%</th>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.3\">1.69%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.4\">1.69%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.5\">1.31%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.5.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S3.T1.1.5.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.5.2.1.1\">20Newsgroup</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S3.T1.1.5.2.2\">21.1%</th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T1.1.5.2.3\">0.57%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T1.1.5.2.4\">0.57%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T1.1.5.2.5\">0.53%</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 1: Attack TPR at 0.1% FPR with 128 Shadow Models"
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Attack TPR at FPR of 0.1% While Varying  on <span class=\"ltx_text ltx_font_italic\" id=\"S3.T2.5.1\">NIPS</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T2.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T2.3.1.1\">Number of Topics \n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.3.1.2\">Online LiRA</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.3.1.3\">Offline LiRA</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.3.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.3.2.1.1\">5</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.3.2.1.2\">31.8%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.3.2.1.3\">31.8%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.3.3.2.1\">10</th>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.3.3.2.2\">44.9%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.3.3.2.3\">43.6%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.3.4.3.1\">15</th>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.3.4.3.2\">60.5%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.3.4.3.3\">56.0%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.3.5.4.1\">20</th>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.3.5.4.2\">67.9%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.3.5.4.3\">63.2%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T2.3.6.5.1\">25</th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.3.6.5.2\">72.5%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.3.6.5.3\">70.2%</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 2: Attack TPR at FPR of 0.1% While Varying  on NIPS"
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"A3.T3\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Data Set Profile After Pre-Processing</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A3.T3.2\" style=\"width:505.9pt;height:97.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(66.2pt,-12.8pt) scale(1.35440520875194,1.35440520875194) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A3.T3.2.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A3.T3.2.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A3.T3.2.2.2.3\">Data Set</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A3.T3.1.1.1.1\">Number of Documents \n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A3.T3.2.2.2.4\">Average Document Length</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A3.T3.2.2.2.2\">Vocabulary Size \n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A3.T3.2.2.3.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T3.2.2.3.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A3.T3.2.2.3.1.1.1\">TweetRumors</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T3.2.2.3.1.2\">5,698</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T3.2.2.3.1.3\">9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T3.2.2.3.1.4\">5,942</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T3.2.2.4.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T3.2.2.4.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A3.T3.2.2.4.2.1.1\">NIPS</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T3.2.2.4.2.2\">1,494</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T3.2.2.4.2.3\">893</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T3.2.2.4.2.4\">10,346</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T3.2.2.5.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A3.T3.2.2.5.3.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A3.T3.2.2.5.3.1.1\">20Newsgroup</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A3.T3.2.2.5.3.2\">18,037</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A3.T3.2.2.5.3.3\">84</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A3.T3.2.2.5.3.4\">74,781</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 3: Data Set Profile After Pre-Processing"
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"A4.T4\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Shapiro-Wilk Tests with FDR Control for 6000 Tests on Each Statistic</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A4.T4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A4.T4.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T4.1.1.1.1\">Statistic</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T4.1.1.1.2\">Number of Rejections</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T4.1.1.1.3\">Rejection %</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A4.T4.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T4.1.2.1.1\">Our Statistic</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T4.1.2.1.2\">1335</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T4.1.2.1.3\">22.25%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T4.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T4.1.3.2.1\">Negative Entropy</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T4.1.3.2.2\">3584</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T4.1.3.2.3\">59.73%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T4.1.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T4.1.4.3.1\">Logit Max Posterior</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T4.1.4.3.2\">5191</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T4.1.4.3.3\">86.52%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T4.1.5.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T4.1.5.4.1\">Standard Deviation</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T4.1.5.4.2\">4621</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T4.1.5.4.3\">77.02%</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 4: Shapiro-Wilk Tests with FDR Control for 6000 Tests on Each Statistic"
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"A5.T5\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>A Brief Summary of the Existing Literature on DP Topic Modeling</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A5.T5.4\" style=\"width:505.9pt;height:105.7pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-91.8pt,19.2pt) scale(0.73373360912186,0.73373360912186) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A5.T5.4.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A5.T5.4.4.5.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"A5.T5.4.4.5.1.1\">Authors</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"A5.T5.4.4.5.1.2\">Notion of DP</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A5.T5.4.4.5.1.3\">Notion of Adjacency</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A5.T5.4.4.5.1.4\">Learning Method</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A5.T5.4.4.5.1.5\">Other Technical Details</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A5.T5.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A5.T5.1.1.1.2\">Zhu et al. (2016) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04451v1#bib.bib34\" title=\"\">34</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A5.T5.1.1.1.1\">\n-DP</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A5.T5.1.1.1.3\">document-level</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A5.T5.1.1.1.4\">CGS</td>\n<td class=\"ltx_td ltx_border_t\" id=\"A5.T5.1.1.1.5\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T5.2.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T5.2.2.2.2\">Zhao et al. (2021) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04451v1#bib.bib33\" title=\"\">33</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T5.2.2.2.1\">\n-DP</th>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T5.2.2.2.3\">word-level</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T5.2.2.2.4\">CGS</td>\n<td class=\"ltx_td\" id=\"A5.T5.2.2.2.5\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T5.3.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T5.3.3.3.2\">Huang and Chen (2021) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04451v1#bib.bib19\" title=\"\">19</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T5.3.3.3.1\">\n-DP</th>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T5.3.3.3.3\">word-level</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T5.3.3.3.4\">CGS</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T5.3.3.3.5\">sub-sampling</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T5.4.4.6.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T5.4.4.6.1.1\">Huang et al. (2022) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04451v1#bib.bib18\" title=\"\">18</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T5.4.4.6.1.2\">R\u00e9nyi-DP</th>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T5.4.4.6.1.3\">word-level</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T5.4.4.6.1.4\">CGS</td>\n<td class=\"ltx_td\" id=\"A5.T5.4.4.6.1.5\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T5.4.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T5.4.4.4.2\">Park et al. (2018) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04451v1#bib.bib22\" title=\"\">22</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T5.4.4.4.1\">\n-DP</th>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T5.4.4.4.3\">document-level</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T5.4.4.4.4\">VI</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T5.4.4.4.5\">moments accountant and sub-sampling</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T5.4.4.7.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T5.4.4.7.2.1\">Decarolis et al. (2020) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04451v1#bib.bib9\" title=\"\">9</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T5.4.4.7.2.2\">R\u00e9nyi-DP</th>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T5.4.4.7.2.3\">document-level</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T5.4.4.7.2.4\">SA</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T5.4.4.7.2.5\">local sensitivity from propose-test-release</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T5.4.4.8.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A5.T5.4.4.8.3.1\">Wang et al. (2022) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04451v1#bib.bib31\" title=\"\">31</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A5.T5.4.4.8.3.2\">R\u00e9nyi-DP</th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A5.T5.4.4.8.3.3\">author-level</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A5.T5.4.4.8.3.4\">PH</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A5.T5.4.4.8.3.5\">pain-free smooth sensitivity</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 5: A Brief Summary of the Existing Literature on DP Topic Modeling"
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.04451v1_figure_1.png",
            "caption": "Figure 1: Histograms of the statistic \u03b6\u2062(\u03a6,d)\ud835\udf01\u03a6\ud835\udc51\\zeta(\\Phi,d)italic_\u03b6 ( roman_\u03a6 , italic_d ) evaluated on different types of documents in TweetRumors when d\u2208Dt\u2062r\u2062a\u2062i\u2062n\ud835\udc51subscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bd\\in D_{train}italic_d \u2208 italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT (blue) and when d\u2209Dt\u2062r\u2062a\u2062i\u2062n\ud835\udc51subscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bd\\notin D_{train}italic_d \u2209 italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT (orange). Outliers are documents that contains many words that appear infrequently in D\ud835\udc37Ditalic_D and inliers contain many words that appear frequently in D\ud835\udc37Ditalic_D. Long documents contain more words than a standard deviation away from the mean document length and short documents contain less. The word count for the inlying document is within one standard deviation of the mean."
        },
        "2": {
            "figure_path": "2403.04451v1_figure_2.png",
            "caption": "Figure 2: Online and Offline ROC Attack Comparison on Each Dataset (128 Shadow Models, NIPS k\ud835\udc58kitalic_k=10)"
        },
        "3": {
            "figure_path": "2403.04451v1_figure_3.png",
            "caption": "Figure 3: Offline ROC Analysis While Varying k\ud835\udc58kitalic_k on NIPS."
        },
        "4": {
            "figure_path": "2403.04451v1_figure_4.png",
            "caption": "Figure 4: Topic coherence as \u03b51subscript\ud835\udf001\\varepsilon_{1}italic_\u03b5 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT increases and LDA is not private. The non-private baseline for topic coherence is very small compared to the figure (\u2248\u22121117absent1117\\approx-1117\u2248 - 1117). The error bars represent one standard deviation from the mean topic coherence."
        },
        "5": {
            "figure_path": "2403.04451v1_figure_5.png",
            "caption": "Figure 5: Topic Coherence as \u03b5\ud835\udf00\\varepsilonitalic_\u03b5 Increases. The blue line shows the the results while varying \u03b51subscript\ud835\udf001\\varepsilon_{1}italic_\u03b5 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT for DPSU and holding \u03b52=3subscript\ud835\udf0023\\varepsilon_{2}=3italic_\u03b5 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 3. Orange displays results for varying \u03b52subscript\ud835\udf002\\varepsilon_{2}italic_\u03b5 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT for DP LDA while holding \u03b51=3subscript\ud835\udf0013\\varepsilon_{1}=3italic_\u03b5 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 3."
        },
        "6": {
            "figure_path": "2403.04451v1_figure_6.png",
            "caption": "Figure 6: Attack ROCs while varying privacy loss parameters for DPSU and DP LDA. On the right-hand side, we fix \u03b52=5subscript\ud835\udf0025\\varepsilon_{2}=5italic_\u03b5 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 5 for DP LDA and vary \u03b51\u2208{1,5,10}subscript\ud835\udf0011510\\varepsilon_{1}\\in\\{1,5,10\\}italic_\u03b5 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2208 { 1 , 5 , 10 } for DPSU. On the left-hand side, we fix \u03b52subscript\ud835\udf002\\varepsilon_{2}italic_\u03b5 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and vary \u03b51subscript\ud835\udf001\\varepsilon_{1}italic_\u03b5 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT at the same intervals. We provide the non-private baseline in red for reference."
        },
        "7": {
            "figure_path": "2403.04451v1_figure_7.png",
            "caption": "Figure 7: Boxplot of KL Divergences between \ud835\udca9i\u2062n\u2062(d)subscript\ud835\udca9\ud835\udc56\ud835\udc5b\ud835\udc51\\mathcal{N}_{in}(d)caligraphic_N start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT ( italic_d ) and \ud835\udca9i\u2062n\u2062(d)subscript\ud835\udca9\ud835\udc56\ud835\udc5b\ud835\udc51\\mathcal{N}_{in}(d)caligraphic_N start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT ( italic_d ) evaluated on 3000 documents."
        },
        "8": {
            "figure_path": "2403.04451v1_figure_8.png",
            "caption": "Figure 8: Online Attack Comparison Across Statistics on Each Dataset (256 Shadow Models, NIPS k\ud835\udc58kitalic_k=10)"
        },
        "9": {
            "figure_path": "2403.04451v1_figure_9.png",
            "caption": "Figure 9: The length of the vocabulary set as \u03b51subscript\ud835\udf001\\varepsilon_{1}italic_\u03b5 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT for DPSU increases. The original vocabulary size is 5,942."
        }
    },
    "references": [
        {
            "1": {
                "title": "Controlling the false discovery rate: a practical and powerful\napproach to multiple testing.",
                "author": "Yoav Benjamini and Yosef Hochberg.",
                "venue": "Journal of the Royal Statistical Society: Series B\n(Methodological), 57(1):289\u2013300, 1995.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Latent dirichlet allocation.",
                "author": "David M Blei, Andrew Y Ng, and Michael I Jordan.",
                "venue": "Journal of machine Learning research, 3(Jan):993\u20131022, 2003.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Applications of Topic Models, volume 11.",
                "author": "Jordan L Boyd-Graber, Yuening Hu, David Mimno, and et al.",
                "venue": "Now Publishers Incorporated, 2017.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Membership inference attacks from first principles.",
                "author": "Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and\nFlorian Tram\u00e8r.",
                "venue": "CoRR, abs/2112.03570, 2021.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Quantifying memorization across neural language models, 2023.",
                "author": "Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian\nTramer, and Chiyuan Zhang.",
                "venue": null,
                "url": null
            }
        },
        {
            "6": {
                "title": "Extracting training data from large language models.",
                "author": "Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel\nHerbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, \u00dalfar\nErlingsson, Alina Oprea, and Colin Raffel.",
                "venue": "In 30th USENIX Security Symposium (USENIX Security 21), pages\n2633\u20132650. USENIX Association, August 2021.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Incorporating item frequency for differentially private set union.",
                "author": "Ricardo Silva Carvalho, Ke Wang, and Lovedeep Singh Gondara.",
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,\n36(9):9504\u20139511, Jun. 2022.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Reading tea leaves: How humans interpret topic models.",
                "author": "Jonathan Chang, Sean Gerrish, Chong Wang, Jordan Boyd-graber, and David Blei.",
                "venue": "In Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and\nA. Culotta, editors, Advances in Neural Information Processing Systems,\nvolume 22. Curran Associates, Inc., 2009.",
                "url": null
            }
        },
        {
            "9": {
                "title": "An end-to-end differentially private latent Dirichlet allocation\nusing a spectral algorithm.",
                "author": "Chris Decarolis, Mukul Ram, Seyed Esmaeili, Yu-Xiang Wang, and Furong Huang.",
                "venue": "In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the\n37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 2421\u20132431. PMLR, 13\u201318 Jul\n2020.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Our data, ourselves: Privacy via distributed noise generation.",
                "author": "Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni\nNaor.",
                "venue": "In Serge Vaudenay, editor, Advances in Cryptology - EUROCRYPT\n2006, pages 486\u2013503, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Calibrating noise to sensitivity in private data analysis.",
                "author": "Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith.",
                "venue": "In Lecture notes in computer science, Lecture Notes in Computer\nScience, pages 265\u2013284, Berlin, Heidelberg, 2006. Springer Berlin\nHeidelberg.",
                "url": null
            }
        },
        {
            "12": {
                "title": "What neural networks memorize and why: Discovering the long tail via\ninfluence estimation.",
                "author": "Vitaly Feldman and Chiyuan Zhang.",
                "venue": "In Proceedings of the 34th International Conference on Neural\nInformation Processing Systems, NIPS\u201920, Red Hook, NY, USA, 2020. Curran\nAssociates Inc.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Differentially private set union.",
                "author": "Sivakanth Gopi, Pankaj Gulhane, Janardhan Kulkarni, Judy Hanwen Shen, Milad\nShokouhi, and Sergey Yekhanin.",
                "venue": "In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the\n37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 3627\u20133636. PMLR, 13\u201318 Jul\n2020.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Finding scientific topics.",
                "author": "Thomas L. Griffiths and Mark Steyvers.",
                "venue": "Proceedings of the National Academy of Sciences,\n101(suppl_1):5228\u20135235, 2004.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Online learning for latent dirichlet allocation.",
                "author": "Matthew Hoffman, Francis Bach, and David Blei.",
                "venue": "In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and\nA. Culotta, editors, Advances in Neural Information Processing Systems,\nvolume 23. Curran Associates, Inc., 2010.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Resolving individuals contributing trace amounts of DNA to highly\ncomplex mixtures using high-density SNP genotyping microarrays.",
                "author": "Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe,\nJill Muehling, John V. Pearson, Dietrich A. Stephan, Stanley F. Nelson, and\nDavid W. Craig.",
                "venue": "PLoS Genet, 4(8):e1000167, 08 2008.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Is automated topic model evaluation broken? the incoherence of\ncoherence.",
                "author": "Alexander Hoyle, Pranav Goel, Andrew Hian-Cheong, Denis Peskov, Jordan\nBoyd-Graber, and Philip Resnik.",
                "venue": "In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman\nVaughan, editors, Advances in Neural Information Processing Systems,\nvolume 34, pages 2018\u20132033. Curran Associates, Inc., 2021.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Improving parameter estimation and defensive ability of latent\ndirichlet allocation model training under r\u00e9nyi differential privacy.",
                "author": "T. Huang, S.Y. Zhao, H. Chen, J. Liu, and Y. Xu.",
                "venue": "Journal of Computer Science and Technology, 37(6):1382\u20131397,\n2022.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Improving privacy guarantee and efficiency of Latent Dirichlet\nAllocation model training under differential privacy.",
                "author": "Tao Huang and Hong Chen.",
                "venue": "In Findings of the Association for Computational Linguistics:\nEMNLP 2021, pages 143\u2013152, Punta Cana, Dominican Republic, November 2021.\nAssociation for Computational Linguistics.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Optimizing semantic coherence in topic models.",
                "author": "David Mimno, Hanna M. Wallach, Edmund Talley, Miriam Leenders, and Andrew\nMcCallum.",
                "venue": "In Proceedings of the Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP \u201911, page 262\u2013272, USA, 2011. Association for\nComputational Linguistics.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Latent dirichlet allocation for medical records topic modeling:\nSystematic literature review.",
                "author": "M. Mustakim, Retantyo Wardoyo, Khabib Mustofa, Gandes Retno Rahayu, and Ida\nRosyidah.",
                "venue": "In 2021 Sixth International Conference on Informatics and\nComputing (ICIC), pages 1\u20137, 2021.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Private topic modeling, 2018.",
                "author": "Mijung Park, James Foulds, Kamalika Chaudhuri, and Max Welling.",
                "venue": null,
                "url": null
            }
        },
        {
            "23": {
                "title": "Quantifying the effects of text duplication on semantic models.",
                "author": "Alexandra Schofield, Laure Thompson, and David Mimno.",
                "venue": "In Proceedings of the 2017 conference on empirical methods in\nnatural language processing, pages 2737\u20132747, 2017.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Analysis of variance test for normality (complete samples).",
                "author": "Samuel S Shapiro and Maurice B Wilk.",
                "venue": "Biometrika, 52(3-4):591\u2013611, 1965.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Membership inference attacks against machine learning models.",
                "author": "Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.",
                "venue": "In 2017 IEEE Symposium on Security and Privacy (SP), pages\n3\u201318, 2017.",
                "url": null
            }
        },
        {
            "26": {
                "title": "LDAvis: A method for visualizing and interpreting topics.",
                "author": "Carson Sievert and Kenneth Shirley.",
                "venue": "In Proceedings of the Workshop on Interactive Language Learning,\nVisualization, and Interfaces, pages 63\u201370, Baltimore, Maryland, USA, June\n2014. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Information leakage in embedding models.",
                "author": "Congzheng Song and Ananth Raghunathan.",
                "venue": "In Proceedings of the 2020 ACM SIGSAC Conference on Computer and\nCommunications Security, CCS \u201920, page 377\u2013390, New York, NY, USA, 2020.\nAssociation for Computing Machinery.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Differentially private feature selection via stability arguments, and\nthe robustness of the lasso.",
                "author": "Abhradeep Guha Thakurta and Adam Smith.",
                "venue": "In Shai Shalev-Shwartz and Ingo Steinwart, editors, Proceedings\nof the 26th Annual Conference on Learning Theory, volume 30 of Proceedings of Machine Learning Research, pages 819\u2013850, Princeton, NJ,\nUSA, 12\u201314 Jun 2013. PMLR.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Interoperable pipelines for social cyber-security: Assessing twitter\ninformation operations during nato trident juncture 2018.",
                "author": "Joshua Uyheng, Thomas Magelinski, Ramon Villa-Cox, Christine Sowa, and\nKathleen M. Carley.",
                "venue": "Comput. Math. Organ. Theory, 26(4):465\u2013483, dec 2020.",
                "url": null
            }
        },
        {
            "30": {
                "title": "SciPy 1.0: Fundamental Algorithms for Scientific Computing in\nPython.",
                "author": "Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy,\nDavid Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan\nBright, St\u00e9fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod\nMillman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric\nLarson, C J Carey, \u0130lhan Polat, Yu Feng, Eric W. Moore, Jake\nVanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen,\nE. A. Quintero, Charles R. Harris, Anne M. Archibald, Ant\u00f4nio H. Ribeiro,\nFabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors.",
                "venue": "Nature Methods, 17:261\u2013272, 2020.",
                "url": null
            }
        },
        {
            "31": {
                "title": "A model-agnostic approach to differentially private topic mining.",
                "author": "Han Wang, Jayashree Sharma, Shuya Feng, Kai Shu, and Yuan Hong.",
                "venue": "In Proceedings of the 28th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining, pages 1835\u20131845, 2022.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Topic-guided variational auto-encoder for text generation.",
                "author": "Wenlin Wang, Zhe Gan, Hongteng Xu, Ruiyi Zhang, Guoyin Wang, Dinghan Shen,\nChangyou Chen, and Lawrence Carin.",
                "venue": "In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pages 166\u2013177, Minneapolis,\nMinnesota, June 2019. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Latent dirichlet allocation model training with differential privacy.",
                "author": "Fangyuan Zhao, Xuebin Ren, Shusen Yang, Qing Han, Peng Zhao, and Xinyu Yang.",
                "venue": "IEEE transactions on information forensics and security,\n16:1290\u20131305, 2021.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Privacy-preserving topic model for tagging recommender systems.",
                "author": "Tianqing Zhu, Gang Li, Wanlei Zhou, Ping Xiong, and Cao Yuan.",
                "venue": "Knowledge and information systems, 46(1):33\u201358, 2016.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.04451v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "3.2",
            "3.3",
            "3.4",
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "3.1",
            "3.5",
            "3.6",
            "4.4",
            "4.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.3",
            "4.4",
            "4.5"
        ]
    },
    "research_context": {
        "paper_id": "2403.04451v1",
        "paper_title": "Membership Inference Attacks and Privacy in Topic Modeling",
        "research_background": "**Motivation:**\nThe primary motivation of this research is to address the privacy concerns associated with probabilistic topic models, particularly Latent Dirichlet Allocation (LDA). While large language models (LLMs) have shown remarkable success, they are also known to memorize training data, which leads to significant privacy issues. The concern is that simpler probabilistic models like topic models may also exhibit similar vulnerabilities. Given that topic models are still widely used in various domains due to their interpretability and simplicity, there is a pressing need to understand and mitigate any privacy risks associated with their use.\n\n**Research Problem:**\nThe main research problem this paper addresses is whether probabilistic topic models, such as LDA, are susceptible to privacy attacks, specifically membership inference attacks (MIAs). MIAs attempt to determine if a given data sample was part of the model's training dataset. The investigation aims to determine if these simpler models, often considered less vulnerable due to their basic structure, are indeed as vulnerable to privacy breaches as more complex models like LLMs. Furthermore, the work explores methodologies to enhance the privacy of topic models, including differentially private algorithms.\n\n**Relevant Prior Work:**\n1. **Privacy in Deep Learning Models:** The propensity of deep learning models to memorize training data and be susceptible to privacy attacks has been discussed in prior works [6, 5].\n2. **Probabilistic Topic Models:** The foundational work on LDA and its applications in various fields, including document classification, summarization, and generation, is well-documented [2, 3, 32].\n3. **Application Domains with Privacy Concerns:** Studies applying topic models in sensitive areas such as the medical field [21] and governmental or defense contexts [29] highlight the necessity to ensure these models' privacy.\n4. **Privacy Attacks:** The likeliness of models to be subjected to MIAs, especially how Carlini et al.'s Likelihood Ratio Attack (LiRA) framework can be integrated into assessing LDA's vulnerabilities [4].\n5. **Differential Privacy in Topic Modeling:** Previous efforts in developing differentially private algorithms for topic models, though often neglecting the confidentiality of the vocabulary set, have been explored [33, 19, 18, 9, 22, 31]. This gap in consideration for vocabulary privacy is significant for improving overall data security.\n\nThe paper aims to augment the understanding of privacy vulnerabilities in simpler models like LDA and proposes new methods to safeguard against such vulnerabilities through differential privacy mechanisms, extending protection to vocabulary sets alongside model outputs.",
        "methodology": "The proposed method addresses membership inference attacks and privacy concerns in topic modeling. Here is a breakdown of the methodology:\n\n### Key Components and Notations:\n\n1. **Vocabulary Size (\\(|V|\\)) and Topics (\\(k\\))**:\n   - \\(|V|\\): Size of the vocabulary used in the topic model.\n   - \\(k\\): Number of topics the model aims to find.\n\n2. **Topic Model (\\(\\theta\\))**:\n   - \\(\\theta\\) is a matrix representing the topic model.\n   - Each row in \\(\\theta\\) sums to 1, indicating a distribution over words for each topic.\n\n3. **Learning Process (\\(\\mathcal{L}\\))**:\n   - \\(\\mathcal{L}\\) denotes the process for learning the model, which is parameterized by latent variables.\n   - Given a corpus \\(C\\) with \\(N\\) documents drawn from a data distribution \\(\\mathcal{D}\\), \\(\\mathcal{L}\\) estimates the latent variables to learn the topic model.\n\n4. **Model Learning Function (\\(\\mathcal{L}(C) \\rightarrow \\theta\\))**:\n   - Incorporates a learning algorithm that estimates the latent variables present in \\(\\theta\\).\n   - The function \\(\\mathcal{L}\\) returns the learned topic model \\(\\theta\\) after processing the corpus \\(C\\).\n\n### Formal Definition of Topic Models:\n\n- A topic model \\(\\theta\\) is essentially a matrix where each row represents a topic.\n- Each row is a probability distribution over the vocabulary, and all rows collectively summarize the relevant themes in the corpus.\n\n### Assumptions:\n\n- The primary objective of the topic modeling process is to estimate \\(\\theta\\), which succinctly captures the themes present in the corpus.\n- While \\(\\theta\\) is linked with other latent variables, it is the best representation for summarizing relevant themes in the dataset.\n\n### Innovations:\n\n- The methodology neatly encapsulates the process of topic modeling by introducing clear notations and breaking down the sequence of learning the model.\n- Defining \\(\\mathcal{L}\\) as a function to incorporate learning algorithms ensures a structured approach to estimate the latent variables in topic models.\n\nBy establishing this formal framework, the paper aims to construct a solid foundation for addressing membership inference attacks and enhancing privacy in topic modeling.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Threat Model\n- **Adversary Capabilities**:\n  - *Access*: Black-box access to the learned topic-word distribution \n  - *Limits*: No access to other latent variables, intermediate word counts, learning methods, or hyper-parameters.\n  - *Query Access*: Access to the underlying data distribution , enabling training of shadow models with datasets from .\n\n- Adversary's Output: The adversary is assumed to be given direct access to  (the topic-word distribution).\n\n#### Datasets\n- The specific datasets used for training the topic models and shadow models are not mentioned in the provided text.\n\n#### Baselines\n- The text does not specify the baseline models used to compare the performance of the adversary or the effectiveness of the membership inference attacks.\n\n#### Evaluation Metrics\n- The exact metrics used to evaluate the success of membership inference attacks or to measure privacy are not detailed in the provided text.\n\n#### Main Experimental Results\n- Results on how well the adversary performs membership inference attacks using the black-box access to the learned topic-word distribution are not provided in the text excerpt.\n\nOverall, the main experiment focuses on assessing the threat model where an adversary, with black-box access to the topic-word distribution of a topic modeling process, attempts to infer membership information about the data. The specific datasets, baselines, evaluation metrics, and detailed experimental results are not included in the provided description."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To identify and design an effective query statistic for the LiRA framework to improve attack performance in topic models.",
            "experiment_process": "The researchers first consider statistics from simple membership inference attacks (MIAs) on LDA presented by Huang et al. These statistics include the entropy of the target document's estimated topic distribution, the standard deviation, and the maximum value. Instead of using global thresholds, they propose a heuristic for the target document\u2019s log-likelihood under LDA. Specifically, they optimize the log-likelihood of the document over all possible document-topic distributions using SciPy's out-of-the-box optimization methods. They verify their criteria for the proposed statistic and evaluate other candidate statistics in Appendix D.",
            "result_discussion": "The LiRA with the proposed log-likelihood statistic directly exploits memorization and per-example hardness, outperforming simple attacks that use global thresholds on other query statistics. The proposed statistic provides a stronger alternative for attacking topic models as it accounts for natural differences in document topic distributions, thereby improving attack performance.",
            "ablation_id": "2403.04451v1.No1"
        },
        {
            "research_objective": "To evaluate the utility and privacy of the FDPTM using the TweetRumors dataset with author-level privacy.",
            "experiment_process": "The researchers use DPSU for vocabulary selection and the DP LDA learning algorithm. They fix the privacy parameter \\\\(\\epsilon\\\\) for DPSU and choose a cut-off value \\\\(\\delta\\\\) with the parameter \\\\(\\beta\\\\). They vary the privacy loss parameter for DPSU and analyze utility with non-private LDA, studying DP vocabulary selection and DP LDA. They fix \\\\(\\epsilon\\\\) for DP LDA, varying it for DPSU, and vice versa. Topic coherence is used to evaluate utility, defined based on document frequency and co-document frequency of words, reporting the average topic coherence across each topic. Additionally, they test FDPTM against the online LiRA, learning 64 shadow models, and vary \\\\(\\epsilon\\\\) and \\\\(\\delta\\\\) for privacy evaluation, repeating each experiment 10 times.",
            "result_discussion": "The findings show that topic coherence remains stable after a specific threshold of \\\\(\\epsilon\\\\), suggesting increased privacy loss for DPSU may not affect topic coherence. Decreasing \\\\(\\epsilon\\\\) or \\\\(\\delta\\\\) reduces coherence, with diminishing returns in utility gains. ROC curves indicate that both DPSU and DP LDA decrease attack performance, enhancing empirical privacy by lowering LiRA performance. Results demonstrate that allocating more of the privacy budget to DP LDA benefits utility at the same privacy level by injecting less noise into DP LDA and more into vocabulary selection, effectively balancing model privacy and utility.",
            "ablation_id": "2403.04451v1.No2"
        }
    ]
}