{
    "title": "CHisIEC: An Information Extraction Corpus for Ancient Chinese History",
    "abstract": "Natural Language Processing (NLP) plays a pivotal role in the realm of Digital Humanities (DH) and serves as the cornerstone for advancing the structural analysis of historical and cultural heritage texts. This is particularly true for the domains of named entity recognition (NER) and relation extraction (RE). In our commitment to expediting ancient history and culture, we present the \u201cChinese Historical Information Extraction Corpus\u201d(CHisIEC). CHisIEC is a meticulously curated dataset designed to develop and evaluate NER and RE tasks, offering a resource to facilitate research in the field. Spanning a remarkable historical timeline encompassing data from 13 dynasties spanning over 1830 years, CHisIEC epitomizes the extensive temporal range and text heterogeneity inherent in Chinese historical documents. The dataset encompasses four distinct entity types and twelve relation types, resulting in a meticulously labeled dataset comprising 14,194 entities and 8,609 relations. To establish the robustness and versatility of our dataset, we have undertaken comprehensive experimentation involving models of various sizes and paradigms. Additionally, we have evaluated the capabilities of Large Language Models (LLMs) in the context of tasks related to ancient Chinese history. The dataset and code are available at https://github.com/tangxuemei1995/CHisIEC. \n\n\nKeywords:\u2009Ancient Chinese History, Dataset Annotation, Named Entity Recognition, Relation Extraction",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Historical and cultural heritage preservation is an important branch of digital humanities, where the rich tapestry of the past meets the cutting-edge tools of the digital age. This field has been significantly enhanced by applying various technologies, including Natural Language Processing (NLP), Computer Vision (CV), and Knowledge Graphs (KG).\nIn recent works, many studies have endeavored to structure cultural heritage and historical documents. For example,  Kim et al. (2022  ###reference_b11###) annotated a mixed multilingual corpus of Korean cultural heritage related to entities.\nIn addition, some historical documents, such as newspapers and periodicals, have also received attention.  Neudecker (2016  ###reference_b20###) and  Ehrmann et al. (2020  ###reference_b5###) focused on the entity annotation and recognition of European historical newspapers.  Bekele et al. (2016  ###reference_b1###) extracted the spatial and temporal entities from the Brazilian historic expedition gazetteer. Moreover, Nundloll et al. (2022  ###reference_b22###) identified custom entities and domain entities from the annals of a historical Botany journal.\nThe cornerstone of advancing the automatic Information Extraction (IE) models in this field lies in the availability of labeled data. However, the domain of ancient Chinese historical documents presents a unique challenge due to the extensive time span they encompass and the linguistic heterogeneity they exhibit. Although  Li et al. (2021  ###reference_b12###) and  Ji et al. (2021  ###reference_b10###) attempted to build a corpus of information extraction based on ancient Chinese historical documents, there are only 1,600 and 4,000 pieces of data, respectively. Unfortunately, these corpora are still significantly undersized to serve as a solid foundation for developing deep learning models. This limitation greatly hinders the implementation of IE techniques in the ancient Chinese historical documents domain.\nTo tackle the challenges associated with information extraction from ancient Chinese historical documents, we introduce an ancient Chinese Historical Information Extraction Corpus (CHisIEC), which is a high-quality specialized dataset for ancient Chinese historical documents and can be used for NER and RE tasks. In constructing this dataset, considering the large time span of ancient Chinese history, we select 13 historical books from the representative Twenty-Four Histories as the raw data, spanning over 1830 years. Then, we further combine the content and linguistic characteristics of the historical documents, define specific entity types and relation types, and craft detailed annotation guidelines. Finally, we invite annotators to annotate according to these guidelines to create the annotated dataset.\nThe contributions of this paper are as follows.\nWe construct an information extraction dataset for ancient Chinese historical documents, which is the largest available and contains both NER and RE tasks. Our dataset includes more than 130K tokens, 14,194 entities, and 8,609 relations.\nThe data in the corpus come from 13 historical books spanning up to 1830 years, preserving the characteristics of ancient Chinese historical documents in terms of time span and text heterogeneity.\nWe validate the utility of our dataset by conducting comprehensive experiments using models of varying sizes and within different paradigms, including pre-trained language models (PLMs) and large language models (LLMs)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Historical Dataset",
            "text": "Texts in history and cultural heritage exhibit heterogeneity and noise due to their association with diverse time periods, domains, and the influence of OCR results. Several works have attempted to preserve these data features in annotated data.\nFor instance,  Kim et al. (2022  ###reference_b11###) introduced an entity-related Korean cultural heritage corpus KoCHET, which encompasses three sub-tasks: NER, RE, and entity typing (ET). The raw data for this corpus is sourced from e-museum digitized data in both Korean and Chinese; Nundloll et al. (2022  ###reference_b22###) annotated custom entities such as people, nationalities, buildings, organizations, countries, times, and events in the Journal of Historical Botany, as well as domain entities such as plant names, observers, locations, spatial relationships, topographic attributes, and abundance.\nNewspapers serve as typical historical sources and form the foundational material for creating historical datasets. For instance,\n Ehrmann et al. (2020  ###reference_b5###) released the entity dataset HIPE, designed for evaluating named entity processing in French, German, and English historical newspapers. The dataset includes tasks related to entity recognition, classification, and linking, with corpus texts originating from newspapers spanning from 1798 to 2018. Additionally,  Neudecker (2016  ###reference_b20###) produced a corpus of 400 Dutch/French/German newspaper pages that were manually filtered and annotated with named entities such as people, locations, and organizations. The corpus consists mainly of pre-1900 newspaper texts with historical spelling variations.\nIn the field of Chinese history and cultural heritage, there are publicly available datasets.  Zinin and Xu (2020  ###reference_b31###) created a historical lexicon and semantic corpus named CCDH, utilizing open-source Twenty-four Histories, which consists of Classical Chinese gender-specific terms. CCDH supports both synchronic and asynchronous studies of gender terms in ancient Chinese.\nIn the domains of NER and RE,  Li et al. (2021  ###reference_b12###) constructed a few-shot ancient Chinese relation dataset (TinyACD-RC) containing 1,600 instances and 32 relation types. Additionally,  Ji et al. (2021  ###reference_b10###) developed a RE corpus with 25 relation types and 4413 samples based on Twenty-Four Histories."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Information Extraction",
            "text": "Information extraction is the foundation of NLP systems and aims to extract structured information from unstructured or semi-structured data sources automatically. In recent years, deep learning methods have made achievements in information extraction tasks Wu and He (2019  ###reference_b27###); Nguyen et al. (2022  ###reference_b21###); Liang et al. (2022  ###reference_b14###), these methods are categorized into two types, one is to divide the information extraction into multiple sub-tasks, and model the multiple sub-tasks separately, such as named entity recognition, relation classification, event triggering detection, and event argument classification. Another category is modeling IE as a joint task, e.g. named entity recognition and relation classification are often modeled as a joint task. For the joint task of RE, different modeling paradigms have been proposed, such as machine reading comprehension-based approach Li et al. (2019  ###reference_b13###); Zhao et al. (2020  ###reference_b28###), sequence labeling-based approach Zheng et al. (2017  ###reference_b30###), span-based approach Eberts and Ulges (2021  ###reference_b4###); Ji et al. (2020  ###reference_b9###), and generation-based approach Huguet Cabot and Navigli (2021  ###reference_b8###); Nayak and Ng (2020  ###reference_b19###).\nRecently, the development of large language models, such as GPT-3 Brown et al. (2020a  ###reference_b2###), ChatGPT Ouyang et al. (2022  ###reference_b23###), and GPT-4 111https://openai.com/research/gpt-4, has significantly advanced the field of natural language understanding and generation. These LLMs have been trained on massive text corpora to generate coherent and contextually accurate text.\nInstruction tuning Lou et al. (2023  ###reference_b18###) is a novel paradigm for using natural language instructions to guide LLMs to complete downstream tasks, and it shows great promise for observing the generalization of task sets. Some works  Gui et al. (2023  ###reference_b6###); Wang et al. (2023  ###reference_b25###) tried to transfer the IE task samples to instruction-formatted instances, then fine-tune LLMs in a supervised learning way Zhao et al. (2023  ###reference_b29###).\nRecent studies on LLMs such as GPT-3 Brown et al. (2020a  ###reference_b2###) have shown that LLMs perform well in a variety of downstream tasks without any training or fine-tuning, only formulating the task description and demonstrations in the form of natural language text as instructions, which is known as in-context learning  Zhao et al. (2023  ###reference_b29###). Many studies have achieved information extraction by adapting the in-context learning strategies. For example,  Wei et al. (2023  ###reference_b26###) convert the IE task into a multi-turn question-answering task, and then get structured data by asking ChatGPT in two-stages;  Ling et al. (2023  ###reference_b15###) added an error correction mechanism to enhance the confidence of the generated relations.\n###figure_1### ###figure_2###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Corpus Annotation",
            "text": "CHisIEC is a specialized corpus designed for the study of ancient Chinese history. The raw data for this corpus is sourced from the Twenty-four Histories, a compilation of the official histories of the Chinese twenty-four dynasties. The Twenty-four Histories, also known as the \u201cOfficial History\u201d chronicle over 4,000 years of Chinese history. It consists of 3,213 volumes containing approximately 40 million tokens.\nWe select 22 volumes from 13 books within the Twenty-four Histories as the texts to be labeled, including The Records of the Grand Historian, The Book of Han, The Book of Tang, The History of Song, The History of Ming, and so on. These texts span over 1830 years of Chinese history. Subsequently, we randomly divided the text into segments, each approximately 100 tokens in length, resulting in a total of 150K characters."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Analysis on CHisIEC",
            "text": ""
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1.   Statistics",
            "text": "###table_1### As mentioned earlier, our raw data consists of 150K characters. After annotation, the statistics of the corpus data are presented in Table 1  ###reference_###. We filter out samples with no entities in the raw data, resulting in 2,185 samples with 135K characters, and we labeled a total of 14,194 entities, forming the named entity dataset. Using the NER dataset as a foundation, we further screen samples without relation annotations to create the RE dataset, comprising 1,948 samples with 88K characters and a total of 8,609 triplets. To facilitate model training, we divided both datasets into training, validation, and test sets in an 8:1:1 ratio.\nWe plot the category occupancy of the two datasets as shown in Figure 1  ###reference_###.\nAs we can see in Figure 1(a)  ###reference_sf1###, the entity types in the dataset exhibit an uneven distribution, with a significant number of person, place, and official entities, while book entities are relatively scarce, accounting for only 2% of the dataset. This suggests that persons are a prominent focus in ancient Chinese historical documents, whereas books play a less central role in the narratives found in the Twenty-four Histories.\nIn Figure 1(b)  ###reference_sf2###, we observe that the most frequent relation type is the Title/Office Holding relation, followed by the Attack relation, with the Superior-subordinate relation also demonstrating a relatively high frequency. All other relation types have frequencies below 800. The relation types that accounted for the lowest percentage were the Alias relation, the Birthplace relation, the Defend relation, and the Brother relation. These frequency patterns align with the predominant themes of political and military topics in ancient Chinese historical texts."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2.   Linguistic Analysis",
            "text": "###figure_3### In this section, we analyze the linguistic features of the annotated corpus.\nFirst, the corpus has a long time span. It is based on the official histories of 13 dynasties, with the earliest text from The Records of the Grand Historian dating back to about 91 B.C. and the latest from The History of Ming in 1739 A.D. This resulted in a remarkable period of 1830 years.\nSecond, the long time span leads to high heterogeneity within the corpus. The language used in the corpus is ancient Chinese, which differs significantly from modern Chinese in vocabulary and grammar. Ancient Chinese is categorized into three developmental stages: Early Ancient, Middle Ancient, and Near Ancient, each with distinct linguistic features. Our dataset primarily contains texts from the latter two periods, contributing to the corpus\u2019s high heterogeneity.\nThird, the corpus exhibits exceptionally high linguistic information density. While modern Chinese is known for its information-dense nature, ancient Chinese surpasses it in this aspect. This heightened information density is evident in the annotated data. Based on the statistics presented in Table 1  ###reference_###, on average, each sample contains six entities and four pairs of triplets."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Annotation Process",
            "text": "In this section, we will provide an overview of the annotation process and the annotation schema.\nAnnotation mode. In the practice of large-scale data annotation, we adopt the mode of \u201cmulti-person annotation\u201d and \u201cprofessional review\u201d, the annotation process is shown in Figure 2  ###reference_###. Initially, we recruit 18 undergraduate students as annotators, dividing them into six groups, with an equal distribution of data to each group. Within these groups, annotators independently annotate the same text based on the provided annotation guidelines.\nSubsequently, any inconsistencies in the annotations within each group are identified, and resolutions are determined through discussions involving both a task researcher and an expert with a historical background. After these discussions, the annotators perform a second round of proofreading on the initial annotations, incorporating the suggestions provided by the experts.\nAnnotation guidelines. We design the following annotation rules to ensure the annotation consistency among the annotators.\nEntity type annotation is context-dependent. Certain words can function as both personal names and official positions. For example, \u201c\u7e47\u738b\u4e0d\u80fd\u77eb\u5176\u4f17\u6301\u6b63\u3002 (Yao King is not able to correct his people.)\u201d, where \u201c\u7e47\u738b (Yao King)\u201d is an official position, but due to the context in the sentence, it is labeled as a person.\nWhen labeling entities, fine-grained spans take precedence over coarse-grained spans. For example, in the sentence \u201c\u95fd\u8d8a\u738b\u65e0\u8bf8\u53ca\u8d8a\u4e1c\u6d77\u738b\u6447\u8005\u3002 (King of Minyue, Wuzhu, and the King of Yuedonghai, Yao)\u201d, \u201c\u95fd\u8d8a\u738b (King of Minyue)\u201d and \u201c\u65e0\u8bf8 (Wuzhu)\u201d are co-referential and are annotated as separate entities.\nRelation annotation is determined by context. In certain cases, there is a semantic overlap between relations, such as Collegiality and Superior-subordinate. Consequently, if the context unambiguously suggests a Superior-subordinate relationship, it will be labeled as such; if only two people are mentioned as working together, the relationship is labeled as Collegiality."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Schema for Task Annotation",
            "text": "Before corpus annotation, we formulate annotation specifications for different types of named entities and relations in ancient Chinese historical texts. We constantly update and improve the specifications in the annotation practice to make them more suitable for annotating ancient Chinese historical texts of different periods."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1.   Named Entity Type",
            "text": "Establishing a named entity taxonomy is essential for annotating Chinese historical texts. Drawing from Chinese historiography, there are four pivotal facets for comprehending history: \u201cCatalog Studies\u201d, \u201cChronology Studies\u201d, \u201cHistorical Geography\u201d, and \u201cOfficial Systems\u201d. Catalog studies focus on book-related matters, chronology studies delve into the timing of events and materials, historical geography explores the locations of people\u2019s activities and events, and official systems research addresses changes within the administrative framework.\nIn line with this framework, we identify four primary entity categories: Person, Location, Official, and Book.\nPerson. As creators and witnesses of history, people are central in historical records across eras and dynasties, including emperors, politicians, cultural figures, generals, and other influential societal members.\nLocation. Spatial context situates people and events, providing the stage and background for historical activities, decisions, and changes. Locations range from regions, capitals, counties, geographical features, and palaces.\nOfficial position. Political figures commonly hold formal posts, with the evolution of official systems intertwined with the rise and fall of dynasties. Tracking offices illustrate sociopolitical characteristics of different periods.\nBook. As vessels of thought and culture, books offer insights into academic advancements and biographical details of historical figures."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2.   Relation Type",
            "text": "Twenty-Four Histories are mainly concerned with politics, military, and culture. By combining expert knowledge with textual content analysis, we focus on five prominent types of relations: war, political, geopolitical, family, and personal attributes.\nAs shown in Table 2  ###reference_###, within each of these domains, we identify common relation types to capture connections between entities. We describe in detail the definition of each relation type as follows.\n###figure_4### Political Support.\nPolitical support is a significant and recurring theme in ancient Chinese historical texts. It can manifest in various forms, including support from subjects to rulers and interactions among political allies.\nThis type of relation occurs between individuals, with the direction going from the supporter to the supported.\nTitle/office Holding. Officials, posthumous titles, seals, and temple titles record the organizational structure of the ancient Chinese political and social system, the transmission of power, and the performance of official duties. The Title/Office Holding relation signifies that a person holds a specific position and title. This relation is directional, pointing from the person to the official position.\nIn ancient Chinese historical documents, the appearance of a character is usually accompanied by their position or title, making this type of relation particularly prevalent.\nCollegiality. Collegiality describes the relationship between individuals who work in the same organization or institution, hold similar positions, or share similar status. In ancient Chinese historical documents, this relationship often signifies two or more people working for the same ruler or collaborating on a common task. Collegiality is a non-directional relation.\nSuperior-subordinate. This relation pertains to two individuals who hold a superior-subordinate relationship. It involves texts that explicitly indicate someone as a superior or subordinate or contain actions implying such a relationship. The direction of this relation type is that the superior points to the subordinate.\nAttack. This type of relationship is closely linked to politics and warfare. Ancient Chinese historical documents contain a wealth of records about wars, encompassing conflicts between nations, tribes, political factions, and individuals. In our corpus, this relation primarily involves two individuals or a person and a location, with the direction going from the aggressor to the target.\nDefend. This war-related relation is highly prevalent in ancient Chinese historical texts. It typically involves individuals leading armies stationed at specific locations for defensive purposes, a common military tactic in historical conflicts. This relation represents an action carried out by a person at a particular location. The direction of this relation goes from the person to the location.\nManage. This type of relation, categorized as a geopolitical relation, indicates that individuals are responsible for the management of specific locations. It often involves a person serving in a particular place and overseeing the state or county affairs. The direction of this relation goes from the person to the location.\nArrive. This is a geopolitical relation indicating that an individual arrives at a specific location. The direction of this relation goes from the person to the location.\nBirthplace. In ancient Chinese historical texts, it was common for individuals to be introduced along with their place of birth. The direction of this relation goes from the person to the location.\nParents. In ancient China, there was a strong emphasis on blood ties, with the simplest and most direct blood relation being that of parents. The direction of this relation type goes from parents to children.\nBrother. Brotherhood is one of the fundamental blood relations outside of parental relationships. In certain periods of China\u2019s social history, brother relations within the family could be intertwined with political relations.\nIn our corpus, the direction of the Brother relation points from the older brother to the younger brother\nAlias. In ancient China, people usually had aliases such as Zi (\u5b57) and Hao (\u53f7) in addition to their names. The direction of this relation goes from persons to their aliases.\nWe give an example of the annotation, as shown in Figure 3  ###reference_###, where the annotator first annotates all the entities in the sentence and then annotates the relationships between the entities.\n###table_2###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Experiments",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Experimental Setting",
            "text": "We model NER as a sequence labeling task and RE as a relation classification task.\nWe select baseline models from different paradigms to assess the challenges of information extraction in ancient Chinese historical documents. In recent studies, researchers Wang et al. (2023  ###reference_b25###); Gui et al. (2023  ###reference_b6###) have achieved success using LLMs for information extraction. Therefore, we choose both open-source and closed-source LLMs as our baseline models. Our experiments involve techniques such as In-Context Learning Brown et al. (2020b  ###reference_b3###), LoRA Hu et al. (2021  ###reference_b7###), P-tuning Liu et al. (2022  ###reference_b17###), and Fine-Tuning on language models of various sizes.\nOur baseline models are as follows.\nSikuBERT 222https://huggingface.co/SIKU-BERT/sikubert  ###reference_###: a BERT model that has been incrementally trained with an ancient Chinese corpus. In our approach, we utilize SikuBERT as an encoder for both the NER task, where we used CRF as the decoder, and the RE task, for which we employed MLP+softmax as the classifier.\nSikuRoBERTa 333https://huggingface.co/SIKU-BERT/sikuroberta  ###reference_ta###: a RoBERTa model that has been incrementally trained with an ancient Chinese corpus. All experiments are conducted in a manner similar to those using SikuBERT.\nChatGLM2-6B 444https://huggingface.co/THUDM/chatglm2-6b  ###reference_###: an open-source bilingual (Chinese-English) chat model. We fine-tune it using the P-Tuning v2 technique Liu et al. (2021  ###reference_b16###). Our training samples, as shown in Table 3  ###reference_###, include three components: task description, input, and output.\nAlpaca2-7B 555https://github.com/ymcui/Chinese-LLaMA-Alpaca-2  ###reference_aca-2###:\na model based on LLaMA2 Touvron et al. (2023  ###reference_b24###), and it has been further pre-trained on a Chinese corpus. We fine-tuned it using LoRA Hu et al. (2021  ###reference_b7###), following the same training samples as for ChatGLM2-6B.\nGPT3.5: a large language model with approximately 200B parameters. Due to its closed-source nature, fine-tuning was performed exclusively through the In-Context Learning method. For the NER task, we select 5 random examples from the training set as demonstrations. In the case of the RE task, we draw one sample from the training set for each relation type as demonstrations, i.e., 12-way 1-shot."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   NER Experimental Results and Analysis",
            "text": "The experimental results for named entity recognition are shown in Table 4  ###reference_###. We report micro F1, macro F1, and the model\u2019s performance on each entity type.\nFrom the NER experimental results, it\u2019s clear that PLMs outperform LLMs. This could be due to two possible factors. First, PLMs are incrementally trained in Ancient Chinese, giving them a superior understanding of this language. Whereas ChatGLM2 and Alpaca2 may have a small percentage of Ancient Chinese in the training corpus, therefore, the Ancient Chinese understanding ability of them is inferior to that of PLMs. Second, fine-tuning, which involves adjusting all model parameters, appears to be more effective than the partial modifications made by LoRA and P-tuning. Remarkably, GPT-3.5 shows promising results with just five examples, suggesting the potential of in-context learning in the NER task with this model.\nIn addition, we observe the performance of models on each entity class and find that the two classes of entities, Official and Book, are relatively ineffective, which is probably because, officials are era-specific, with different names for different dynasties, while books may be due to insufficient training data.\n###table_3### ###table_4###"
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   RE Experimental Results and Analysis",
            "text": "In summary, pre-trained language models remain highly potent base models when ample training data is available. It\u2019s important to note that large language models exhibit substantial performance variations across different tasks. For instance, ChatGLM2 and Alpaca2 demonstrate superior performance in the RE task as opposed to the NER task. This can be attributed to the NER task\u2019s greater demand for polyglot features from the model, including entity position identification and entity type recognition. In contrast, the RE task shares similarities with sentence classification, making it a more manageable challenge for these large language models."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Conclusion",
            "text": "In this paper, we propose CHisIEC, an ancient Chinese history corpus for NER and RE tasks. Our dataset contains texts from 13 dynasties, epitomizing the extensive temporal scope and text heterogeneity of Chinese historical literature. We conduct experiments on both the pre-trained language models and the large language models to validate the applicability of the dataset, and also evaluate the capability of the LLMs in the domain tasks of ancient Chinese history."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Acknowledgments",
            "text": "This research is supported by the NSFC project \u201cthe Construction of the Knowledge Graph for the History of Chinese Confucianism\u201d (Grant No. 72010107003)."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Bibliographical References",
            "text": ""
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T1.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.1.1.1\" style=\"padding-left:1.4pt;padding-right:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1\">Dataset</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.1.1.2\" style=\"padding-left:1.4pt;padding-right:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.2.1\">Samples</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.1.1.3\" style=\"padding-left:1.4pt;padding-right:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.3.1\">Characters</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S3.T1.1.1.4\" style=\"padding-left:1.4pt;padding-right:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.4.1\">Entities/Relations</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.1.2.1\" style=\"padding-left:1.4pt;padding-right:1.4pt;\">NER</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.1.2.2\" style=\"padding-left:1.4pt;padding-right:1.4pt;\">2,185</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.1.2.3\" style=\"padding-left:1.4pt;padding-right:1.4pt;\">135,713</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.1.2.4\" style=\"padding-left:1.4pt;padding-right:1.4pt;\">14,194</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.3\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T1.1.3.1\" style=\"padding-left:1.4pt;padding-right:1.4pt;\">RE</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T1.1.3.2\" style=\"padding-left:1.4pt;padding-right:1.4pt;\">1,948</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T1.1.3.3\" style=\"padding-left:1.4pt;padding-right:1.4pt;\">88,177</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T1.1.3.4\" style=\"padding-left:1.4pt;padding-right:1.4pt;\">8609</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Statistics of CHisIEC dataset.</figcaption>\n</figure>",
            "capture": "Table 1: Statistics of CHisIEC dataset."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1\">Domain</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.2.1\">Relation Type</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.1.2.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S3.T2.1.2.1.1\">Politics</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.2.2\">Political Support,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.3.1\">Title/office Holding,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.4.1\">Collegiality,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.5.1\">Superior-subordinate</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.1.6.1\">War</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.6.2\">Attack, Defend</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.1.7.1\">Geography</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.7.2\">Arrive, Manage</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.1.8.1\">Family</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.8.2\">Parents, Brother</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S3.T2.1.9.1\">Personal Information</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T2.1.9.2\">Alias, Birthplace</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>The history domains and the relation types.</figcaption>\n</figure>",
            "capture": "Table 2: The history domains and the relation types."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T3.1.1.1\" rowspan=\"11\">\n<span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span><span class=\"ltx_text\" id=\"S3.T3.1.1.1.1\" style=\"font-size:90%;\">\n</span><span class=\"ltx_text\" id=\"S3.T3.1.1.1.2\" style=\"font-size:90%;\">NER</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T3.1.1.2\" rowspan=\"6\"><span class=\"ltx_text\" id=\"S3.T3.1.1.2.1\" style=\"font-size:90%;\">Instruction</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T3.1.1.3\"><span class=\"ltx_text\" id=\"S3.T3.1.1.3.1\" style=\"font-size:90%;\">\u4f60\u662f\u4e00\u4e2a\u5b9e\u4f53\u8bc6\u522b\u5de5\u5177\uff0c\u4f60\u9700\u8981\u8bc6\u522b\u51fa\u8f93\u5165\u53e5\u5b50\u4e2d\u7684\u4eba\u7269\u3001\u5730\u70b9\u3001\u804c\u5b98\u548c\u4e66\u7c4d\uff0c</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.2.1\"><span class=\"ltx_text\" id=\"S3.T3.1.2.1.1\" style=\"font-size:90%;\">\u8f93\u51fa\u683c\u5f0f\u4e3a\uff1a\u4eba\u7269\uff1a\u4eba\u72691\uff0c\u4eba\u72692\uff1b\u5730\u70b9\uff1a\u5730\u70b91\uff0c\u5730\u70b92\uff1b\u804c\u5b98\uff1a\u804c\u5b981\uff0c\u804c\u5b982\uff1b</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.3.1\"><span class=\"ltx_text\" id=\"S3.T3.1.3.1.1\" style=\"font-size:90%;\">\u4e66\u7c4d\uff1a\u4e66\u7c4d1\uff0c\u4e66\u7c4d2\u3002 (You are an entity recognition tool and you need to recognize</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.4.1\"><span class=\"ltx_text\" id=\"S3.T3.1.4.1.1\" style=\"font-size:90%;\">persons, locations, officials, and books in an input sentence, and the output</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.5.1\"><span class=\"ltx_text\" id=\"S3.T3.1.5.1.1\" style=\"font-size:90%;\">format: persons: person 1, person 2; locations: location 1, location 2;</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.6.1\"><span class=\"ltx_text\" id=\"S3.T3.1.6.1.1\" style=\"font-size:90%;\">officials: official 1, official 2; books: book 1, book 2.)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T3.1.7.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S3.T3.1.7.1.1\" style=\"font-size:90%;\">Input</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T3.1.7.2\"><span class=\"ltx_text\" id=\"S3.T3.1.7.2.1\" style=\"font-size:90%;\">\u6c38\u6cf0\u5143\u5e74\uff0c\u5410\u8543\u8bf7\u548c\uff0c\u8bcf\u5bb0\u76f8\u5143\u8f7d\u3001\u675c\u9e3f\u6e10\u4e0e\u864f\u4f7f\u8005\u540c\u76df\u3002 (In the first year of the</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.8.1\"><span class=\"ltx_text\" id=\"S3.T3.1.8.1.1\" style=\"font-size:90%;\">Yongtai era, Tubo asked for peace, and the Emperor ordered Chancellor Yuanzai and</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.9.1\"><span class=\"ltx_text\" id=\"S3.T3.1.9.1.1\" style=\"font-size:90%;\">Du Hongjian to make an alliance with the captive\u2019s emissaries.)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T3.1.10.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T3.1.10.1.1\" style=\"font-size:90%;\">Output</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T3.1.10.2\"><span class=\"ltx_text\" id=\"S3.T3.1.10.2.1\" style=\"font-size:90%;\">\u4eba\u7269\uff1a\u5143\u8f7d\uff0c\u675c\u9e3f\u6e10\uff1b\u5730\u70b9\uff1a\u5410\u8543\uff1b\u804c\u5b98\uff1a\u5bb0\u76f8\uff0c\u4f7f\u8005\uff1b\u4e66\u7c4d\uff1a\u65e0\u3002(Person:</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.11.1\"><span class=\"ltx_text\" id=\"S3.T3.1.11.1.1\" style=\"font-size:90%;\">Yuan Zai, Du Hongjian; Locations: Tubo; Officials: chancellor, emissary; Books: none.)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T3.1.12.1\" rowspan=\"13\"><span class=\"ltx_text\" id=\"S3.T3.1.12.1.1\" style=\"font-size:90%;\">RE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T3.1.12.2\" rowspan=\"10\"><span class=\"ltx_text\" id=\"S3.T3.1.12.2.1\" style=\"font-size:90%;\">Instruction</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T3.1.12.3\"><span class=\"ltx_text\" id=\"S3.T3.1.12.3.1\" style=\"font-size:90%;\">\u4f60\u662f\u4e00\u4e2a\u8bed\u4e49\u62bd\u53d6\u5de5\u5177\uff0c\u73b0\u5728\u5df2\u5b9a\u4e49\u5173\u7cfb\u5305\u62ec\u4ee5\u4e0b\u8fd9\u4e9b\uff1a\u654c\u5bf9\u653b\u4f10\uff0c\u4efb\u804c\uff0c\u540c\u50da\uff0c</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.13\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.13.1\"><span class=\"ltx_text\" id=\"S3.T3.1.13.1.1\" style=\"font-size:90%;\">\u4e0a\u4e0b\u7ea7\uff0c\u7ba1\u7406\uff0c\u9a7b\u5b88\uff0c\u5230\u8fbe\uff0c\u51fa\u751f\u4e8e\u67d0\u5730\uff0c\u5144\u5f1f\uff0c\u7236\u6bcd\uff0c\u522b\u540d\u3002\u4ee5\u4e0b\u53e5\u5b50\u4e2d\uff0c</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.14\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.14.1\"><span class=\"ltx_text\" id=\"S3.T3.1.14.1.1\" style=\"font-size:90%;\">\u5b9e\u4f53\u7531\u201c\u3010\u201d\uff0c \u201c\u3011\u201d\u6807\u6ce8\u51fa\u6765\uff0c\u8bf7\u4f60\u627e\u51fa\u4e24\u4e2a\u5b9e\u4f53\u4e4b\u95f4\u7684\u5173\u7cfb\u3002</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.15\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.15.1\"><span class=\"ltx_text\" id=\"S3.T3.1.15.1.1\" style=\"font-size:90%;\">\u5173\u7cfb\u7684\u65b9\u5411\u7531\u9996\u5b9e\u4f53\u6307\u5411\u5c3e\u5b9e\u4f53\uff0c\u8f93\u51fa\u5f62\u5f0f\u4e3a\uff1a(\u9996\u5b9e\u4f53\uff0c\u5173\u7cfb\uff0c\u5c3e\u5b9e\u4f53)\u3002</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.16\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.16.1\"><span class=\"ltx_text\" id=\"S3.T3.1.16.1.1\" style=\"font-size:90%;\">(You are a semantic extraction tool that now defined relations including the</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.17\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.17.1\"><span class=\"ltx_text\" id=\"S3.T3.1.17.1.1\" style=\"font-size:90%;\">following Attack, title/office Holding, Collegiality, Superior-subordinate, Manage, Defend,</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.18\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.18.1\"><span class=\"ltx_text\" id=\"S3.T3.1.18.1.1\" style=\"font-size:90%;\">Arrive, Birthplace, Brother, Parents, Alias. In the following sentences, entities are marked</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.19\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.19.1\"><span class=\"ltx_text\" id=\"S3.T3.1.19.1.1\" style=\"font-size:90%;\">by \u201c\u3010\u201d and \u201c\u3011\u201d. You should identify the relation between the two entities.</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.20\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.20.1\"><span class=\"ltx_text\" id=\"S3.T3.1.20.1.1\" style=\"font-size:90%;\">The direction of the relation is from the head entity to the tail entity and the output form is:</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.21\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.21.1\"><span class=\"ltx_text\" id=\"S3.T3.1.21.1.1\" style=\"font-size:90%;\">(head entity, relation, tail entity).)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.22\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T3.1.22.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T3.1.22.1.1\" style=\"font-size:90%;\">Input</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T3.1.22.2\"><span class=\"ltx_text\" id=\"S3.T3.1.22.2.1\" style=\"font-size:90%;\">\u5176\u5e74\u95f0\u4e03\u6708\u6666\uff0c\u674e\u7b60\u3001\u3010\u4f55\u798f\u8fdb\u3011\u76f8\u7387\u6740\u5951\u4e39\u5e05\u3010\u6ee1\u8fbe\u52d2\u3011\u3002(On the 7th day of the leap</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.23\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.23.1\"><span class=\"ltx_text\" id=\"S3.T3.1.23.1.1\" style=\"font-size:90%;\">month of this year, Li Yun and He Fujin killed the Khitan commander Mandalay.)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.24\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T3.1.24.1\"><span class=\"ltx_text\" id=\"S3.T3.1.24.1.1\" style=\"font-size:90%;\">Output</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T3.1.24.2\"><span class=\"ltx_text\" id=\"S3.T3.1.24.2.1\" style=\"font-size:90%;\">(\u4f55\u798f\u8fdb\uff0c\u654c\u5bf9\u653b\u4f10\uff0c\u6ee1\u8fbe\u52d2) (He Fujin, Attack, Mandalay)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.25\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T3.1.25.1\"><span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S3.T3.1.25.2\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S3.T3.1.25.3\"></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Sample instructions for NER and RE fine-tuning ChatGLM2 and Alpaca2.</figcaption>\n</figure>",
            "capture": "Table 3: Sample instructions for NER and RE fine-tuning ChatGLM2 and Alpaca2."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T4.1\">\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"8\" id=\"S4.T4.1.1.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<span class=\"ltx_rule\" style=\"width:100%;height:1.2pt;background:black;display:inline-block;\">\u00a0</span><span class=\"ltx_text\" id=\"S4.T4.1.1.1.1\" style=\"font-size:90%;\">\n</span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.2\" style=\"font-size:90%;\">Techniques</span><span class=\"ltx_text\" id=\"S4.T4.1.1.1.3\" style=\"font-size:90%;\"></span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T4.1.1.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.2.1\" style=\"font-size:90%;\">Models</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.1.1.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.3.1\" style=\"font-size:90%;\">Entity Type</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.1.1.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.4.1\" style=\"font-size:90%;\">P</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.1.1.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.5.1\" style=\"font-size:90%;\">R</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.1.1.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.6.1\" style=\"font-size:90%;\">F1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T4.1.1.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.7.1\" style=\"font-size:90%;\">MacroF1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T4.1.1.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.8.1\" style=\"font-size:90%;\">MicroF1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.2.1\" rowspan=\"18\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.2.1.1\" style=\"font-size:90%;\">Finu-tuning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"7\" id=\"S4.T4.1.2.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.2.2.1\" style=\"font-size:90%;\">Ancient Chinese fine-tuned Models</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.3.1\" rowspan=\"4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.3.1.1\" style=\"font-size:90%;\">SikuBERT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.3.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.3.2.1\" style=\"font-size:90%;\">PER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.3.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.3.3.1\" style=\"font-size:90%;\">94.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.3.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.3.4.1\" style=\"font-size:90%;\">96.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.3.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.3.5.1\" style=\"font-size:90%;\">95.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.3.6\" rowspan=\"4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.3.6.1\" style=\"font-size:90%;\">88.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.3.7\" rowspan=\"4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.3.7.1\" style=\"font-size:90%;\">92.31</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.4.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.4.1.1\" style=\"font-size:90%;\">LOC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.4.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.4.2.1\" style=\"font-size:90%;\">92.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.4.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.4.3.1\" style=\"font-size:90%;\">93.41</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.4.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.4.4.1\" style=\"font-size:90%;\">93.08</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.5.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.5.1.1\" style=\"font-size:90%;\">OFI</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.5.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.5.2.1\" style=\"font-size:90%;\">83.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.5.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.5.3.1\" style=\"font-size:90%;\">84.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.5.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.5.4.1\" style=\"font-size:90%;\">84.45</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.6.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.6.1.1\" style=\"font-size:90%;\">BOOK</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.6.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.6.2.1\" style=\"font-size:90%;\">78.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.6.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.6.3.1\" style=\"font-size:90%;\">84.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.6.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.6.4.1\" style=\"font-size:90%;\">81.48</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.7.1\" rowspan=\"4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.7.1.1\" style=\"font-size:90%;\">SikuRoBERTa</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.7.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.7.2.1\" style=\"font-size:90%;\">PER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.7.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.7.3.1\" style=\"font-size:90%;\">94.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.7.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.7.4.1\" style=\"font-size:90%;\">96.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.7.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.7.5.1\" style=\"font-size:90%;\">95.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.7.6\" rowspan=\"4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.7.6.1\" style=\"font-size:90%;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.7.6.1.1\">90.47</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.7.7\" rowspan=\"4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.7.7.1\" style=\"font-size:90%;\">92.46</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.8.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.8.1.1\" style=\"font-size:90%;\">LOC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.8.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.8.2.1\" style=\"font-size:90%;\">92.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.8.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.8.3.1\" style=\"font-size:90%;\">92.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.8.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.8.4.1\" style=\"font-size:90%;\">92.47</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.9.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.9.1.1\" style=\"font-size:90%;\">OFI</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.9.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.9.2.1\" style=\"font-size:90%;\">83.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.9.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.9.3.1\" style=\"font-size:90%;\">87.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.9.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.9.4.1\" style=\"font-size:90%;\">85.76</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.10.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.10.1.1\" style=\"font-size:90%;\">BOOK</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.10.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.10.2.1\" style=\"font-size:90%;\">91.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.10.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.10.3.1\" style=\"font-size:90%;\">84.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.10.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.10.4.1\" style=\"font-size:90%;\">88.00</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"7\" id=\"S4.T4.1.11.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.11.1.1\" style=\"font-size:90%;\">Large Language Models</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.12.1\" rowspan=\"4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.12.1.1\" style=\"font-size:90%;\">ChatGLM2 (6B, P-tuning)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.12.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.12.2.1\" style=\"font-size:90%;\">PER</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.12.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.12.3.1\" style=\"font-size:90%;\">84.57</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.12.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.12.4.1\" style=\"font-size:90%;\">78.62</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.12.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.12.5.1\" style=\"font-size:90%;\">81.49</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.12.6\" rowspan=\"4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.12.6.1\" style=\"font-size:90%;\">64.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.12.7\" rowspan=\"4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.12.7.1\" style=\"font-size:90%;\">74.89</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.13\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.13.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.13.1.1\" style=\"font-size:90%;\">LOC</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.13.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.13.2.1\" style=\"font-size:90%;\">77.11</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.13.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.13.3.1\" style=\"font-size:90%;\">72.94</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.13.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.13.4.1\" style=\"font-size:90%;\">74.97</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.14.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.14.1.1\" style=\"font-size:90%;\">OFI</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.14.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.14.2.1\" style=\"font-size:90%;\">64.75</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.14.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.14.3.1\" style=\"font-size:90%;\">58.59</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.14.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.14.4.1\" style=\"font-size:90%;\">61.51</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.15\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.15.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.15.1.1\" style=\"font-size:90%;\">BOOK</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.15.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.15.2.1\" style=\"font-size:90%;\">45.45</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.15.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.15.3.1\" style=\"font-size:90%;\">38.46</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.15.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.15.4.1\" style=\"font-size:90%;\">41.67</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.16\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.16.1\" rowspan=\"4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.16.1.1\" style=\"font-size:90%;\">Alpaca2 (7B, LoRA)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.16.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.16.2.1\" style=\"font-size:90%;\">PER</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.16.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.16.3.1\" style=\"font-size:90%;\">90.29</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.16.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.16.4.1\" style=\"font-size:90%;\">87.62</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.16.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.16.5.1\" style=\"font-size:90%;\">88.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.16.6\" rowspan=\"4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.16.6.1\" style=\"font-size:90%;\">78.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.16.7\" rowspan=\"4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.16.7.1\" style=\"font-size:90%;\">85.83</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.17\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.17.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.17.1.1\" style=\"font-size:90%;\">LOC</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.17.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.17.2.1\" style=\"font-size:90%;\">89.10</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.17.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.17.3.1\" style=\"font-size:90%;\">86.59</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.17.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.17.4.1\" style=\"font-size:90%;\">87.83</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.18\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.18.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.18.1.1\" style=\"font-size:90%;\">OFI</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.18.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.18.2.1\" style=\"font-size:90%;\">78.37</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.18.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.18.3.1\" style=\"font-size:90%;\">76.69</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.18.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.18.4.1\" style=\"font-size:90%;\">77.52</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.19\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.19.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.19.1.1\" style=\"font-size:90%;\">BOOK</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.19.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.19.2.1\" style=\"font-size:90%;\">63.64</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.19.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.19.3.1\" style=\"font-size:90%;\">53.85</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.19.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.19.4.1\" style=\"font-size:90%;\">58.33</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.20\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.20.1\" rowspan=\"4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.20.1.1\" style=\"font-size:90%;\">In-Context Learning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.20.2\" rowspan=\"4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.20.2.1\" style=\"font-size:90%;\">GPT-3.5 (5 shot)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.20.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.20.3.1\" style=\"font-size:90%;\">PER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.20.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.20.4.1\" style=\"font-size:90%;\">55.26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.20.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.20.5.1\" style=\"font-size:90%;\">61.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.20.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.20.6.1\" style=\"font-size:90%;\">58.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.20.7\" rowspan=\"4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.20.7.1\" style=\"font-size:90%;\">45.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.20.8\" rowspan=\"4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.20.8.1\" style=\"font-size:90%;\">56.72</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.21\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.21.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.21.1.1\" style=\"font-size:90%;\">LOC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.21.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.21.2.1\" style=\"font-size:90%;\">52.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.21.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.21.3.1\" style=\"font-size:90%;\">60.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.21.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.21.4.1\" style=\"font-size:90%;\">56.49</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.22\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.22.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.22.1.1\" style=\"font-size:90%;\">OFI</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.22.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.22.2.1\" style=\"font-size:90%;\">54.29</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.22.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.22.3.1\" style=\"font-size:90%;\">59.79</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.22.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.22.4.1\" style=\"font-size:90%;\">56.91</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.23\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.23.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.23.1.1\" style=\"font-size:90%;\">BOOK</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.23.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.23.2.1\" style=\"font-size:90%;\">9.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.23.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.23.3.1\" style=\"font-size:90%;\">10.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.23.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" id=\"S4.T4.1.23.4.1\" style=\"font-size:90%;\">9.76</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.24\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.24.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_rule\" style=\"width:100%;height:1.2pt;background:black;display:inline-block;\">\u00a0</span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T4.1.24.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T4.1.24.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T4.1.24.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T4.1.24.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T4.1.24.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T4.1.24.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T4.1.24.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Experimental results for the NER task. Both the macro F1 (%) and the micro F1 (%) are evaluation metrics. We highlight the highest performance in bold.</figcaption>\n</figure>",
            "capture": "Table 4: Experimental results for the NER task. Both the macro F1 (%) and the micro F1 (%) are evaluation metrics. We highlight the highest performance in bold."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T5.1\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.1.1\" rowspan=\"3\" style=\"padding-left:11.4pt;padding-right:11.4pt;\">\n<span class=\"ltx_rule\" style=\"width:100%;height:1.2pt;background:black;display:inline-block;\">\u00a0</span><span class=\"ltx_text\" id=\"S4.T5.1.1.1.1\" style=\"font-size:90%;\">\n</span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.2\" style=\"font-size:90%;\">Techniques</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.1.2\" rowspan=\"3\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.2.1\" style=\"font-size:90%;\">Models</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\" id=\"S4.T5.1.1.3\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.3.1\" style=\"font-size:90%;\">RE</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\" id=\"S4.T5.1.2.1\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.2.1.1\" style=\"font-size:90%;\">Macro</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.2.2\" rowspan=\"2\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.2.2.1\" style=\"font-size:90%;\">Acc.</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.3.1\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.3.1.1\" style=\"font-size:90%;\">P</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.3.2\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.3.2.1\" style=\"font-size:90%;\">R</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.3.3\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.3.3.1\" style=\"font-size:90%;\">F1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.4.1\" rowspan=\"6\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.4.1.1\" style=\"font-size:90%;\">Fine-tuning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\" id=\"S4.T5.1.4.2\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.4.2.1\" style=\"font-size:90%;\">Ancient Chinese fine-tuned Models</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.5.1\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.5.1.1\" style=\"font-size:90%;\">SikuBERT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.5.2\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.5.2.1\" style=\"font-size:90%;\">83.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.5.3\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.5.3.1\" style=\"font-size:90%;\">82.48</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.5.4\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.5.4.1\" style=\"font-size:90%;\">83.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.5.5\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.5.5.1\" style=\"font-size:90%;\">88.47</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.6.1\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.6.1.1\" style=\"font-size:90%;\">SikuRoBERTa</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.6.2\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.6.2.1\" style=\"font-size:90%;\">79.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.6.3\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.6.3.1\" style=\"font-size:90%;\">79.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.6.4\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.6.4.1\" style=\"font-size:90%;\">79.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.6.5\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.6.5.1\" style=\"font-size:90%;\">87.45</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\" id=\"S4.T5.1.7.1\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.7.1.1\" style=\"font-size:90%;\">Large Language Models</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.8.1\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.8.1.1\" style=\"font-size:90%;\">ChatGLM2 (6B, P-tuning)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.8.2\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.8.2.1\" style=\"font-size:90%;\">76.14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.8.3\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.8.3.1\" style=\"font-size:90%;\">76.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.8.4\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.8.4.1\" style=\"font-size:90%;\">76.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.8.5\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.8.5.1\" style=\"font-size:90%;\">83.86</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.9.1\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.9.1.1\" style=\"font-size:90%;\">Alpaca2 (7B, LoRA)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.9.2\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.9.2.1\" style=\"font-size:90%;\">84.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.9.3\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.9.3.1\" style=\"font-size:90%;\">85.46</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.9.4\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.9.4.1\" style=\"font-size:90%;\">85.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.9.5\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.9.5.1\" style=\"font-size:90%;\">89.48</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.10.1\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.10.1.1\" style=\"font-size:90%;\">In-Context Learning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.10.2\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.10.2.1\" style=\"font-size:90%;\">GPT-3.5 (12-way 1-shot)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.10.3\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.10.3.1\" style=\"font-size:90%;\">40.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.10.4\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.10.4.1\" style=\"font-size:90%;\">14.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.10.5\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.10.5.1\" style=\"font-size:90%;\">21.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.10.6\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" id=\"S4.T5.1.10.6.1\" style=\"font-size:90%;\">53.74</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.11.1\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_rule\" style=\"width:100%;height:1.2pt;background:black;display:inline-block;\">\u00a0</span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T5.1.11.2\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T5.1.11.3\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T5.1.11.4\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T5.1.11.5\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T5.1.11.6\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Experimental results for the RE task. Both the macro F1 (%) and accuracy (Acc. %) are evaluation metrics. We highlight the best performance in bold.</figcaption>\n</figure>",
            "capture": "Table 5: Experimental results for the RE task. Both the macro F1 (%) and accuracy (Acc. %) are evaluation metrics. We highlight the best performance in bold."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T6\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T6.1\">\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\" id=\"S4.T6.1.1.1\">\n<span class=\"ltx_rule\" style=\"width:100%;height:1.2pt;background:black;display:inline-block;\">\u00a0</span><span class=\"ltx_text\" id=\"S4.T6.1.1.1.1\" style=\"font-size:90%;\">\n</span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.1.1.2\" style=\"font-size:90%;\">Models</span><span class=\"ltx_text\" id=\"S4.T6.1.1.1.3\" style=\"font-size:90%;\"></span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.1.1.2\">\n<span class=\"ltx_text\" id=\"S4.T6.1.1.2.1\" style=\"font-size:90%;\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0</span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.1.2.2\" style=\"font-size:90%;\">Political support</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.2\">\n<td class=\"ltx_td ltx_border_r\" id=\"S4.T6.1.2.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.2.2.1\" style=\"font-size:90%;\">P</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.2.3.1\" style=\"font-size:90%;\">R</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.1.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.2.4.1\" style=\"font-size:90%;\">F</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.3.1\"><span class=\"ltx_text\" id=\"S4.T6.1.3.1.1\" style=\"font-size:90%;\">SikuBERT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.3.2\"><span class=\"ltx_text\" id=\"S4.T6.1.3.2.1\" style=\"font-size:90%;\">61.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.3.3\"><span class=\"ltx_text\" id=\"S4.T6.1.3.3.1\" style=\"font-size:90%;\">61.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.1.3.4\"><span class=\"ltx_text\" id=\"S4.T6.1.3.4.1\" style=\"font-size:90%;\">61.82</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.4.1\"><span class=\"ltx_text\" id=\"S4.T6.1.4.1.1\" style=\"font-size:90%;\">SikuRoBERTa</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.4.2\"><span class=\"ltx_text\" id=\"S4.T6.1.4.2.1\" style=\"font-size:90%;\">52.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.4.3\"><span class=\"ltx_text\" id=\"S4.T6.1.4.3.1\" style=\"font-size:90%;\">50.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.1.4.4\"><span class=\"ltx_text\" id=\"S4.T6.1.4.4.1\" style=\"font-size:90%;\">51.85</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.5.1\"><span class=\"ltx_text\" id=\"S4.T6.1.5.1.1\" style=\"font-size:90%;\">ChatGLM2 (6B)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.5.2\"><span class=\"ltx_text\" id=\"S4.T6.1.5.2.1\" style=\"font-size:90%;\">56.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.5.3\"><span class=\"ltx_text\" id=\"S4.T6.1.5.3.1\" style=\"font-size:90%;\">49.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.1.5.4\"><span class=\"ltx_text\" id=\"S4.T6.1.5.4.1\" style=\"font-size:90%;\">52.43</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.6.1\"><span class=\"ltx_text\" id=\"S4.T6.1.6.1.1\" style=\"font-size:90%;\">Alpaca2 (7B)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.6.2\"><span class=\"ltx_text\" id=\"S4.T6.1.6.2.1\" style=\"font-size:90%;\">70.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.6.3\"><span class=\"ltx_text\" id=\"S4.T6.1.6.3.1\" style=\"font-size:90%;\">65.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.1.6.4\"><span class=\"ltx_text\" id=\"S4.T6.1.6.4.1\" style=\"font-size:90%;\">67.92</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.7.1\"><span class=\"ltx_text\" id=\"S4.T6.1.7.1.1\" style=\"font-size:90%;\">GPT-3.5 (12-way 1-shot)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.7.2\"><span class=\"ltx_text\" id=\"S4.T6.1.7.2.1\" style=\"font-size:90%;\">14.29</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.7.3\"><span class=\"ltx_text\" id=\"S4.T6.1.7.3.1\" style=\"font-size:90%;\">1.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.1.7.4\"><span class=\"ltx_text\" id=\"S4.T6.1.7.4.1\" style=\"font-size:90%;\">3.23</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.8.1\"><span class=\"ltx_rule\" style=\"width:100%;height:1.2pt;background:black;display:inline-block;\">\u00a0</span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T6.1.8.2\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T6.1.8.3\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T6.1.8.4\"></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>The performance of all models on the Political support relation type.</figcaption>\n</figure>",
            "capture": "Table 6: The performance of all models on the Political support relation type."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.15088v2_figure_1.png",
            "caption": "(a) Entity types"
        },
        "2": {
            "figure_path": "2403.15088v2_figure_2.png",
            "caption": "(b) Relation types"
        },
        "3": {
            "figure_path": "2403.15088v2_figure_3.png",
            "caption": "Figure 2: Annotation process for our corpus."
        },
        "4": {
            "figure_path": "2403.15088v2_figure_4.png",
            "caption": "Figure 3: Illustration of an annotation sample."
        }
    },
    "references": [
        {
            "1": {
                "title": "Spatiotemporal\ninformation extraction from a historic expedition gazetteer.",
                "author": "Mafkereseb Bekele, Rolf De By, and Gaurav Singh. 2016.",
                "venue": "ISPRS International Journal of Geo-Information, 5(12):221.",
                "url": "https://doi.org/10.3390/ijgi5120221"
            }
        },
        {
            "2": {
                "title": "Language models are few-shot\nlearners.",
                "author": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. 2020a.",
                "venue": "(arXiv:2005.14165).",
                "url": "http://arxiv.org/abs/2005.14165"
            }
        },
        {
            "3": {
                "title": "Language models are few-shot\nlearners.",
                "author": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. 2020b.",
                "venue": "(arXiv:2005.14165).",
                "url": "http://arxiv.org/abs/2005.14165"
            }
        },
        {
            "4": {
                "title": "Span-based joint entity\nand relation extraction with transformer pre-training.",
                "author": "Markus Eberts and Adrian Ulges. 2021.",
                "venue": "ArXiv:1909.07755 [cs].",
                "url": "https://doi.org/10.3233/FAIA200321"
            }
        },
        {
            "5": {
                "title": "Extended overview of clef hipe 2020: Named entity processing on\nhistorical newspapers.",
                "author": "Maud Ehrmann, Matteo Romanello, Alex Fluckiger, and Simon Clematide. 2020.",
                "venue": null,
                "url": null
            }
        },
        {
            "6": {
                "title": "Instructie: A chinese\ninstruction-based information extraction dataset.",
                "author": "Honghao Gui, Jintian Zhang, Hongbin Ye, and Ningyu Zhang. 2023.",
                "venue": "(arXiv:2305.11527).",
                "url": "http://arxiv.org/abs/2305.11527"
            }
        },
        {
            "7": {
                "title": "Lora: Low-rank adaptation of\nlarge language models.",
                "author": "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen. 2021.",
                "venue": "(arXiv:2106.09685).",
                "url": "http://arxiv.org/abs/2106.09685"
            }
        },
        {
            "8": {
                "title": "Rebel:\nRelation extraction by end-to-end language generation.",
                "author": "Pere-Llu\u00eds Huguet Cabot and Roberto Navigli. 2021.",
                "venue": "In Findings of the Association for Computational Linguistics:\nEMNLP 2021, page 2370\u20132381, Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.findings-emnlp.204"
            }
        },
        {
            "9": {
                "title": "Span-based\njoint entity and relation extraction with attention-based span-specific and\ncontextual semantic representations.",
                "author": "Bin Ji, Jie Yu, Shasha Li, Jun Ma, Qingbo Wu, Yusong Tan, and Huijun Liu. 2020.",
                "venue": "In Proceedings of the 28th International Conference on\nComputational Linguistics, page 88\u201399, Barcelona, Spain (Online).\nInternational Committee on Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.coling-main.8"
            }
        },
        {
            "10": {
                "title": "Research on information extraction methods for historical classics\nunder the perspective of digital humanities.",
                "author": "Zijing Ji, Zirui Chen, Lifan Han, and Xin Wang. 2021.",
                "venue": "Big Data Research, pages 1\u201321.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Kochet: A korean\ncultural heritage corpus for entity-related tasks.",
                "author": "Gyeongmin Kim, Jinsung Kim, Junyoung Son, and Heuiseok Lim. 2022.",
                "venue": "In Proceedings of the 29th International Conference on\nComputational Linguistics, page 3496\u20133505, Gyeongju, Republic of Korea.\nInternational Committee on Computational Linguistics.",
                "url": "https://aclanthology.org/2022.coling-1.308"
            }
        },
        {
            "12": {
                "title": "Few-shot relation\nextraction on ancient chinese documents.",
                "author": "Bo Li, Jiyu Wei, Yang Liu, Yuze Chen, Xi Fang, and Bin Jiang. 2021.",
                "venue": "Applied Sciences, 11(24):12060.",
                "url": "https://doi.org/10.3390/app112412060"
            }
        },
        {
            "13": {
                "title": "Entity-relation\nextraction as multi-turn question answering.",
                "author": "Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan, Duo Chai, Mingxin Zhou,\nand Jiwei Li. 2019.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, page 1340\u20131350, Florence, Italy.\nAssociation for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P19-1129"
            }
        },
        {
            "14": {
                "title": "Modeling\nmulti-granularity hierarchical features for relation extraction.",
                "author": "Xinnian Liang, Shuangzhi Wu, Mu Li, and Zhoujun Li. 2022.",
                "venue": "In Proceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, page 5088\u20135098, Seattle, United States. Association for\nComputational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.naacl-main.375"
            }
        },
        {
            "15": {
                "title": "Improving open information\nextraction with large language models: A study on demonstration uncertainty.",
                "author": "Chen Ling, Xujiang Zhao, Xuchao Zhang, Yanchi Liu, Wei Cheng, Haoyu Wang,\nZhengzhang Chen, Takao Osaki, Katsushi Matsuda, Haifeng Chen, and Liang Zhao.\n2023.",
                "venue": "(arXiv:2309.03433).",
                "url": "http://arxiv.org/abs/2309.03433"
            }
        },
        {
            "16": {
                "title": "P-tuning v2: Prompt tuning\ncan be comparable to fine-tuning universally across scales and tasks.",
                "author": "Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang.\n2021.",
                "venue": "CoRR, abs/2110.07602.",
                "url": "http://arxiv.org/abs/2110.07602"
            }
        },
        {
            "17": {
                "title": "P-tuning:\nPrompt tuning can be comparable to fine-tuning across scales and tasks.",
                "author": "Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie\nTang. 2022.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Papers), page 61\u201368,\nDublin, Ireland. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.acl-short.8"
            }
        },
        {
            "18": {
                "title": "Is prompt all you need? no.\na comprehensive and broader view of instruction learning.",
                "author": "Renze Lou, Kai Zhang, and Wenpeng Yin. 2023.",
                "venue": "(arXiv:2303.10475).",
                "url": "http://arxiv.org/abs/2303.10475"
            }
        },
        {
            "19": {
                "title": "Effective modeling\nof encoder-decoder architecture for joint entity and relation extraction.",
                "author": "Tapas Nayak and Hwee Tou Ng. 2020.",
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,\n34(05):8528\u20138535.",
                "url": "https://doi.org/10.1609/aaai.v34i05.6374"
            }
        },
        {
            "20": {
                "title": "An open corpus for named\nentity recognition in historic newspapers.",
                "author": "Clemens Neudecker. 2016.",
                "venue": "In Proceedings of the Tenth International Conference on\nLanguage Resources and Evaluation (LREC\u201916), pages 4348\u20134352,\nPortoro\u017e, Slovenia. European Language Resources Association (ELRA).",
                "url": "https://aclanthology.org/L16-1689"
            }
        },
        {
            "21": {
                "title": "Joint\nextraction of entities, relations, and events via modeling inter-instance and\ninter-label dependencies.",
                "author": "Minh Van Nguyen, Bonan Min, Franck Dernoncourt, and Thien Nguyen. 2022.",
                "venue": "In Proceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, page 4363\u20134374, Seattle, United States. Association for\nComputational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.naacl-main.324"
            }
        },
        {
            "22": {
                "title": "Automating the extraction of information from a historical text and building\na linked data model for the domain of ecology and conservation science.",
                "author": "Vatsala Nundloll, Robert Smail, Carly Stevens, and Gordon Blair. 2022.",
                "venue": "Heliyon, 8(10):e10710.",
                "url": "https://doi.org/https://doi.org/10.1016/j.heliyon.2022.e10710"
            }
        },
        {
            "23": {
                "title": "Training language models to\nfollow instructions with human feedback.",
                "author": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022.",
                "venue": "(arXiv:2203.02155).",
                "url": "http://arxiv.org/abs/2203.02155"
            }
        },
        {
            "24": {
                "title": "Llama 2: Open foundation and\nfine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine\nBabaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,\nDan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem\nCucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar\nHosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier\nMartinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\nSilva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\nRoss Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan\nZarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien\nRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023.",
                "venue": "(arXiv:2307.09288).",
                "url": "http://arxiv.org/abs/2307.09288"
            }
        },
        {
            "25": {
                "title": "Instructuie: Multi-task\ninstruction tuning for unified information extraction.",
                "author": "Xiao Wang, Weikang Zhou, Can Zu, Han Xia, Tianze Chen, Yuansen Zhang, Rui\nZheng, Junjie Ye, Qi Zhang, Tao Gui, Jihua Kang, Jingsheng Yang, Siyuan Li,\nand Chunsai Du. 2023.",
                "venue": "(arXiv:2304.08085).",
                "url": "http://arxiv.org/abs/2304.08085"
            }
        },
        {
            "26": {
                "title": "Zero-shot information\nextraction via chatting with chatgpt.",
                "author": "Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun\nXie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, and Wenjuan Han. 2023.",
                "venue": "(arXiv:2302.10205).",
                "url": "http://arxiv.org/abs/2302.10205"
            }
        },
        {
            "27": {
                "title": "Enriching pre-trained\nlanguage model with entity information for relation classification.",
                "author": "Shanchan Wu and Yifan He. 2019.",
                "venue": "(arXiv:1905.08284).",
                "url": "http://arxiv.org/abs/1905.08284"
            }
        },
        {
            "28": {
                "title": "Asking effective and\ndiverse questions: A machine reading comprehension based framework for joint\nentity-relation extraction.",
                "author": "Tianyang Zhao, Zhao Yan, Yunbo Cao, and Zhoujun Li. 2020.",
                "venue": "In Proceedings of the Twenty-Ninth International Joint\nConference on Artificial Intelligence, page 3948\u20133954, Yokohama, Japan.\nInternational Joint Conferences on Artificial Intelligence Organization.",
                "url": "https://doi.org/10.24963/ijcai.2020/546"
            }
        },
        {
            "29": {
                "title": "A survey of large language\nmodels.",
                "author": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,\nYushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,\nZikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.",
                "venue": "(arXiv:2303.18223).",
                "url": "http://arxiv.org/abs/2303.18223"
            }
        },
        {
            "30": {
                "title": "Joint extraction of\nentities and relations based on a novel tagging scheme.",
                "author": "Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, and Bo Xu. 2017.",
                "venue": "In Proceedings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), page 1227\u20131236,\nVancouver, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P17-1113"
            }
        },
        {
            "31": {
                "title": "Corpus of chinese dynastic histories: Gender analysis over two\nmillennia.",
                "author": "Sergey Zinin and Yang Xu. 2020.",
                "venue": "pages 785\u2013793.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.15088v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.2",
            "3.3",
            "3.3.1",
            "3.3.2"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.15088v2",
        "paper_title": "CHisIEC: An Information Extraction Corpus for Ancient Chinese History",
        "research_background": "### Motivation\n\nThe motivation behind this paper lies in the intersection of preserving historical and cultural heritage and advancing digital humanities. The researchers are driven by the need to leverage cutting-edge tools and technologies, such as Natural Language Processing (NLP), to structure and analyze historical documents. They aim to bridge the gap in existing resources for ancient Chinese historical documents by creating a comprehensive and high-quality information extraction corpus.\n\n### Research Problem\n\nThe core research problem addressed in the paper is the significant lack of robust, annotated corpuses that can adequately support the development of Information Extraction (IE) models for ancient Chinese historical documents. Current datasets, like those developed by Li et al. (2021) and Ji et al. (2021), are too small, encompassing only 1,600 and 4,000 pieces of data, respectively. This limitation impedes the progress of IE techniques within this domain, as these datasets are insufficient to train and validate contemporary deep learning models. \n\n### Relevant Prior Work\n\n1. **Kim et al. (2022):** This work involved annotating a mixed multilingual corpus of Korean cultural heritage entities.\n2. **Neudecker (2016) and Ehrmann et al. (2020):** These studies focused on entity annotation and recognition in European historical newspapers.\n3. **Bekele et al. (2016):** The research extracted spatial and temporal entities from the Brazilian historic expedition gazetteer.\n4. **Nundloll et al. (2022):** This work identified custom and domain entities in the annals of a historical Botany journal.\n5. **Li et al. (2021) and Ji et al. (2021):** Both attempted to build a corpus for information extraction from ancient Chinese historical documents but struggled with small data sizes (1,600 and 4,000 entries, respectively).\n\nThese prior works underscore the importance and challenges of developing structured annotated corpora for historical documents, and they highlight the specific gaps and needs within the context of ancient Chinese historical texts. \n\n### Conclusion\n\nAnchored by these motivations, this paper introduces the Chinese Historical Information Extraction Corpus (CHisIEC), aiming to fill the existing gaps and propel the development of IE models specialized for ancient Chinese history. This corpus is comprehensive, covering a vast time span of 1,830 years and includes detailed annotations suitable for Named Entity Recognition (NER) and Relation Extraction (RE) tasks.",
        "methodology": "#### Methodology: CHisIEC\n\nCHisIEC is a specialized corpus designed for the study of ancient Chinese history, offering a structured dataset drawn from historical texts. \n\n**Data Source:**\nThe raw data for this corpus is sourced from the Twenty-four Histories, a comprehensive compilation that encompasses the official histories of twenty-four Chinese dynasties. This monumental collection, also referred to as the \u201cOfficial History,\u201d chronicles over 4,000 years of Chinese historical events and narratives. The entirety of the Twenty-four Histories includes 3,213 volumes, which contain approximately 40 million tokens.\n\n**Selection of Texts:**\nOut of this vast repository, we selectively choose 22 volumes from 13 distinct books within the Twenty-four Histories to be labeled. These selected texts include prominent works such as: \n- The Records of the Grand Historian\n- The Book of Han\n- The Book of Tang\n- The History of Song\n- The History of Ming\n\nThese chosen texts cover a wide historical span of over 1830 years, reflecting significant events and historical periods.\n\n**Segmentation of Texts:**\nTo facilitate the labeling process, we then segment the selected texts into smaller, more manageable parts. Each segment is approximately 100 tokens in length. This segmentation yields a comprehensive dataset totaling around 150K characters.\n\nIn summary, CHisIEC is meticulously curated from the Twenty-four Histories, offering a rich and extensive corpus for the study of ancient Chinese history. Its careful selection and segmentation cater to a range of historical research needs while preserving the depth and breadth of the original texts.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiment Setup\n\n**Tasks**:\n- Named Entity Recognition (NER): Modeled as a sequence labeling task.\n- Relation Extraction (RE): Modeled as a relation classification task.\n\n**Baseline Models**:\n1. **SikuBERT**:\n   - **NER**: Utilized as an encoder with CRF as the decoder.\n   - **RE**: Utilized as an encoder with MLP+softmax as the classifier.\n2. **SikuRoBERTa**: Similar experiment setup as SikuBERT.\n3. **ChatGLM2-6B**: Fine-tuned using the P-Tuning v2 technique.\n4. **Alpaca2-7B**: Fine-tuned using LoRA.\n5. **GPT3.5**: Utilized In-Context Learning for tuning due to its closed-source nature.\n   - **NER**: Five random examples from the training set as demonstrations.\n   - **RE**: One sample from the training set per relation type as demonstrations (12-way 1-shot).\n\n**Datasets**:\n- The specifics of the dataset were not mentioned directly, but the experiments were conducted using an ancient Chinese corpus which was divided appropriately for training and evaluation.\n\n**Evaluation Metrics**:\n- The specific evaluation metrics used were not explicitly mentioned in the provided text. However, standard metrics for NER tasks typically include Precision, Recall, and F1-score, and for RE tasks, Precision, Recall, and F1-score per relation type.\n\n#### Main Experimental Results\nThe detailed experimental results have not been provided in the given text. However, the setup implies that alinear performance comparison across different models using the NER and RE tasks was conducted to assess the efficacy and challenges of information extraction from ancient Chinese historical documents using various language models and tuning methodologies. \n\nThe inclusion of several models (both open-source and closed-source) from different paradigms and the use of advanced fine-tuning and learning techniques underscore a rigorous evaluation to benchmark performance in this specialized domain. The results would likely include the effectiveness of each model in extracting information accurately from ancient Chinese texts, varying based on the complexity and the specific language model fine-tuning method used."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate the performance of various language models for Named Entity Recognition (NER) in ancient Chinese history.",
            "experiment_process": "The experiment involved using multiple baseline models including SikuBERT, SikuRoBERTa, ChatGLM2-6B, Alpaca2-7B, and GPT3.5. Different techniques such as LoRA, P-tuning, and fine-tuning were employed to train the models on the CHisIEC dataset. SikuBERT and SikuRoBERTa leveraged incremental training on ancient Chinese corpora, while ChatGLM2-6B and Alpaca2-7B used fine-tuning techniques like P-tuning and LoRA. GPT3.5 was fine-tuned solely with In-Context Learning. The evaluation metrics included micro F1, macro F1, and performance on each entity type.",
            "result_discussion": "PLMs outperformed LLMs in NER because they were incrementally trained in Ancient Chinese, providing them with a better understanding of the language. Fine-tuning proved to be more effective than partial modifications like LoRA and P-tuning. GPT-3.5 yielded promising results with just five examples, indicating the potential of in-context learning. It was noted that performances on entity types 'Official' and 'Book' were relatively ineffective due to era-specific and insufficient training data.",
            "ablation_id": "2403.15088v2.No1"
        },
        {
            "research_objective": "Examine the performance of various language models for Relation Extraction (RE) in ancient Chinese history.",
            "experiment_process": "The study employed the same set of models as in the NER task: SikuBERT, SikuRoBERTa, ChatGLM2-6B, Alpaca2-7B, and GPT3.5. The models were trained and fine-tuned specifically for RE tasks using micro F1 and accuracy as evaluation metrics. Baseline techniques involved using different training approaches for ChatGLM2-6B and Alpaca2-7B and in-context learning for GPT3.5.",
            "result_discussion": "ChatGLM2 and Alpaca2 performance was comparable to PLMs, with Alpaca2 achieving the best results. GPT-3.5 struggled due to the limited number of samples and generated relation types not within the predefined set. It was suggested that better prompts could improve GPT-3.5's results. The models showed relatively poor performance in the 'Political support' relation type due to its semantic complexity. Overall, pre-trained language models require ample training data to be effective, and large language models vary significantly in their performance across tasks. ChatGLM2 and Alpaca2 performed better in the RE task compared to the NER task, indicating that the nature of the task affects model performance.",
            "ablation_id": "2403.15088v2.No2"
        }
    ]
}