{
    "title": "Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning",
    "abstract": "Translation-tailored Large language models (LLMs) exhibit remarkable translation capabilities, even competing with supervised-trained commercial translation systems. However, off-target translation remains an unsolved problem, especially for low-resource languages, hindering us from developing accurate LLMs-based translation models.\nTo mitigate the off-target translation problem and enhance the performance of LLMs on translation, recent works have either designed advanced prompting strategies to highlight the functionality of translation instructions or exploited the in-context learning ability of LLMs by feeding few-shot demonstrations.\nHowever, these methods essentially do not improve LLM\u2019s ability to follow translation instructions, especially the language direction information.\nIn this work, we design a two-stage fine-tuning algorithm to improve the instruction-following ability (especially the translation direction) of LLMs.\nSpecifically, we first tune LLMs with the maximum likelihood estimation loss on the translation dataset to elicit the basic translation capabilities. In the second stage, we construct instruction-conflicting samples by randomly replacing the translation directions with a wrong one within the instruction, and then introduce an extra unlikelihood loss to learn those samples.\nExperiments on IWSLT and WMT benchmarks upon the LLaMA model spanning 16 zero-shot directions show that, compared to the competitive baseline \u2013 translation-finetuned LLama, our method could effectively reduce the off-target translation ratio (averagely -53.3%), thus improving translation quality with average +5.7 SacreBLEU and +16.4 BLEURT. Analysis shows that our method could preserve the model\u2019s general task performance on AlpacaEval.\nCode and models will be released at https://github.com/alphadl/LanguageAware_Tuning.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) have demonstrated excellent performance on a wide range of NLP tasks, including reasoning Wei et al. (2022  ###reference_b35###), summarization Wang et al. (2023  ###reference_b33###), translation Hendy et al. (2023  ###reference_b9###), understanding Zhong et al. (2023  ###reference_b47###), and evaluation Lu et al. (2023  ###reference_b20###), etc.\nLLMs exemplified by GPT-3 Brown et al. (2020  ###reference_b1###), OPT Zhang et al. (2022  ###reference_b44###), LLaMA Touvron et al. (2023a  ###reference_b29###), and LLAMA2 Touvron et al. (2023b  ###reference_b30###), leverage large-scale monolingual data through pertaining with the causal language modelling task and exhibit strong zero-shot capabilities with few demonstration examples.\nInstruction tuning Mishra et al. (2022  ###reference_b21###); Wei et al. (2021  ###reference_b34###) further elicits the capacity of LLMs to address general tasks directly with proper guidance, such as task definition.\nNevertheless, due to the huge cost to call the state-of-the-art LLM, like GPT-4 OpenAI (2023  ###reference_b23###), it is attractive to explore strategies for effectively fitting suitably-sized LLMs into specific tasks Xu et al. (2023  ###reference_b39###); Fu et al. (2023  ###reference_b6###).\nIn the field of zero-shot translation (ZST) Gu et al. (2019  ###reference_b8###); Chen et al. (2023  ###reference_b3###); Zan et al. (2023  ###reference_b42###), the goal of this task is to translate sentences from a source language to a target language, where 1) the direct mappings between source and target languages lack in the training data, or 2) the target or source language themselves have not appeared during training.\nAddressing the ZST problem is both vital and challenging, especially for paired-data-hungry low-resource languages.\nRecent research demonstrates that LLMs tuned on translation data can achieve good translation performance by configuring a suitable task instruction Zeng et al. (2023  ###reference_b43###); Liu et al. (2023  ###reference_b18###); Xu et al. (2023  ###reference_b39###).\nHowever, as illustrated in Figure LABEL:fig:intro_example, our preliminary study shows that, when tackling zero-shot directions, LLM heavily encounters the off-target problem, for example,\nin DeFr, the off-target ratio reaches 99.5%111The effectiveness of our method can be significantly shown consistently besides the EnJa direction, the reason for the relatively weak improvement in Japanese may be due to Llama\u2019s vocabulary overly compressing non-Western languages..\nWe attribute this problem to the reason that training LLMs with the fashion of predicting the next token may lead to overlooking the information contained in instructions.\nPrevious studies Peng et al. (2023  ###reference_b24###); Xu et al. (2023  ###reference_b39###) indicate that introducing more informative prompts during inference, such as preemptively translating prompts into the target language or incorporating few-shot demonstrated samples, can be beneficial.\nSennrich et al. (2023  ###reference_b28###) modify the decoding by introducing language-contrastive samples to constrain the decoding process, thus alleviating the off-target problem.\nDifferent from the above approaches that focus on maximizing the utilization of LLMs for translation, our motivation is to fundamentally improve the instruction-following ability (especially the language-aware translation direction) of LLMs themselves.\nIn this paper, we introduce a simple-and-effective two-stage fine-tuning algorithm to enhance the effect of instruction in translation-tailored LLMs. This is accomplished by introducing unlikelihood loss on instruction-conflicting samples in which the translation sequence pairs deviate from the prescribed tasks associated with the given instructions.\nIn the first stage, we fine-tune the LLMs using a multilingual translation dataset. This pre-tuning process serves the purpose of unlocking the translation capabilities inherent in LLMs.\nIn the second stage, we build upon the pre-tuned model by incorporating translation data along with instruction-conflicting samples. We create instruction-conflicting samples by randomly replacing the translation directions with a wrong one.\nThese data are used to further train the model, leveraging the unlikelihood training paradigm.\nOur approach can be viewed as emphasizing the effect of instructions, thereby guiding the model to produce translation in the correct language.\nIn the experiments, we apply our method to fine-tune the LLaMA model. The results reveal substantial reductions in the off-target translation ratio, with improvements of -92.2% and -29.9% on the IWSLT and WMT benchmarks, respectively.\nThis leads to notable enhancements in translation quality, as evidenced by increases of average +23.0/ +12.4 BLEURT and +5.2/ +6.1 SacreBLEU in IWSLT/ WMT datasets.\nAlso, our method maintains the translation capability on supervised directions.\nThe main contributions are as follows:\nWe reveal the heavy off-target problem in LLM-based zero-shot translation settings, and we attribute this problem to the weak instruction (translation direction) following ability.\nTo fundamentally improve the translation direction following ability, we introduce a two-stage fine-tuning algorithm for LLMs that leverages instruction-conflicting samples.\nExtensive experiments illustrate the effectiveness of our approach in mitigating the off-target problem and producing better translations. Analyses show that our method will not affect the general ability of LLM, e.g., general task performance on AlpacaEval."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminary",
            "text": "Instruction tuning aims to refine LLMs through fine-tuning a diverse collection of data characterized by explicit instructions. This refinement process significantly enhances the zero-shot performance on previously unseen tasks Wei et al. (2021  ###reference_b34###).\nEach instance in the instruction tuning dataset comprises three fundamental components:\n1) Instruction: This is a textual representation that describes NLP tasks in natural language.\n2) Input (optional): Supplementary contextual information that provides additional context for the given task.\n3) Output: The expected response that LLMs should generate.\nDuring the tuning process, the model is trained using a teacher-forcing approach Cho et al. (2014  ###reference_b4###). It models the distribution of output tokens conditioned on the instruction and, optionally, the input. This training methodology empowers the model to understand and follow instructions effectively.\nSubsequently, the instruction-tuned model is capable of directly performing unseen tasks by following the appropriate task instructions in a zero-shot manner.\nIn this study, our primary focus is translation-tailored LLMs, where we fine-tune LLMs on paired multilingual translation data.\nWelleck et al. (2020  ###reference_b37###) explores a novel approach that encourages the model to assign lower probabilities to improbable generations, in contrast to the traditional likelihood training, which focuses on the overall probability distribution of correct sequences.\nThe general training framework comprises two types of updates:\n1) Likelihood updates on ground-truth sequences, ensuring they are assigned high probabilities.\n2) Unlikelihood updates on negative candidate sequences, preventing them from receiving excessively high probabilities.\nWe extend this approach to the domain of zero-shot translation based on translation-tailored LLMs. We introduce instruction-conflicting samples for unlikelihood updates, thereby emphasizing the impact of translation instructions (especially the translation direction and language) and addressing off-target problems.\n###figure_1###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "To mitigate the off-target problem with unlikelihood training, we build the negative candidate samples by replacing the instruction with another different one while keeping the input and output not changing.\nWe call this type of samples instructiong-conflicting samples as the translation pairs deviate from the prescribed tasks associated with the given instructions.\nAs shown in Figure 2  ###reference_###, we sample a sample  from the instruction dataset , where  is \u201cTranslate the following sentences from German to English.\u201d,  is \u201c Er sagte, dass er eine WLAN-T\u00fcrklingel gebaut ha\u201d, and  is \u201cHe built a WiFi door bell, he sai\u201d.\nThen, we randomly select another sample of a different task , e.g. \u201cTranslate the following sentences from English to Chinese\u201d, and replace the original correct  to get the instruction-conflicting sample, e.g. \u201c[Instruction]: Translate the following sentences from English to Chinese. [Input]: Er sagte, dass er eine WLAN-T\u00fcrklingel gebaut ha\u201d in example.\nBased on the instruction-conflicting samples, we generalize the unlikelihood training to zero-shot translation of translation-tailored LLMs.\nWe feed instruction samples into the model trained after stage 1, optimizing the unlikelihood loss:\nwhere  is one of corresponding negative instructions for . Thus, the overall objective function in unlikelihood training consists of mixing the likelihood and unlikelihood loss:\nwhere  is the mixing hyper-parameter."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Pre-Tuning on Multilingual Translation Samples",
            "text": "To unlock the translation capabilities of LLM, we use the multilingual translation examples for the first stage pre-tuning.\nFormally, an LLM is pre-tuned with a collection of instruction samples  covers  language pairs. Here,  denotes a translation parallel corpus of the -th language pair.\nAs depicted in Figure 2  ###reference_###, in training stage 1, the model is trained to predict output based on provided instructions, such as \u201cTranslate the following sentences from English to Chinese.\u201d and \u201cTranslate the following sentences from German to English.\u201d, and corresponding input like \u201cDid you see it go?\u201d and \u201cEr sagte, dass er eine WLAN-T\u00fcrklingel gebaut habe.\u201d. The likelihood training objective is applied:\nwhere  denotes task instruction, input, and output respectively. In the context of translation samples,  is the source sentence, and  is the target sentence.\n represents the trainable model parameters.\nConsequently, the model gets some capability to execute translation tasks by adhering to provided instructions."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we conduct a series of experiments spanning 16 zero-shot translation directions to assess the effectiveness of our algorithm.\nWe consider the following two widely-used datasets:\nWMT: Following Jiao et al. (2023  ###reference_b12###); Liu et al. (2023  ###reference_b18###), we use the development sets from WMT2017 to WMT2020 for instruction tuning, including four language directions: EnZh and EnDe. The WMT dataset encompasses 51k translation sentence pairs.\nThen, we evaluate translation performance on WMT22 test sets, including EnCs, EnJa, EnRu, EnUk, FrDe. All these translation language pairs do not exist in the training set, thus allowing for the evaluation of zero-shot translation performance.\nIWSLT: We collect the IWSLT dataset and focus on translation performance between non-English languages.\nFor fine-tuning using multilingual translation data, we randomly select 12k sentence pairs from the train set of IWSLT 2017 Cettolo et al. (2017  ###reference_b2###), spanning six directions: EnDe, EnZh, EnKo.\nWe utilize Flores-200 Costa-juss\u00e0 et al. (2022  ###reference_b5###) devtest sets for evaluation on zero-shot translation directions, including ZhDe, ZhKo, DeKo. The Flores-200 comprises 1012 sentences from English Wikipedia covering multi-domain and then translated into about 200 languages by professional translators.\nWe leverage 7B size LLaMA as the backbone and consider the following baselines:\nLLaMA Touvron et al. (2023a  ###reference_b29###): LLaMA serves as the foundation model, having undergone training on a corpus of trillion tokens. We employ pretrained 7B size LLaMA directly for inference.\nLLaMA-MT: We fine-tune the LLaMA solely on multilingual translation samples, following the same procedure as the pre-tuning stage in our algorithm. Following Jiao et al. (2023  ###reference_b12###), we format translation sentence pairs into unified translation instructions.\nPost-Ins Liu et al. (2023  ###reference_b18###): Following Liu et al. (2023  ###reference_b18###), we switch the positions of instruction and input of prompt, where the model pays more attention to the instruction.\nPrompt in the target language (PTL):\nInstead of using the English prompt during inference, we translate the prompt into the target language during inference, which could provide more guidance information. Our inference leverages LLaMA-MT.\n-shot: In-context learning Brown et al. (2020  ###reference_b1###) has proven to be an effective way to prompt LLMs performance. We report the few-shot performance for comprehensive comparison, including 1-shot and 5-shot. LLaMA-MT is used for inference.\nSennrich et al. (2023  ###reference_b28###): Following Sennrich et al. (2023  ###reference_b28###), we propose to decode by contrasting the translation sentence with language-contrastive input and  0.5. Our inference relies on LLaMA-MT, employing a greedy decoding strategy.\nWe conduct experiments on the Huggingface Transformers Wolf et al. (2020  ###reference_b38###) toolkit. All models are trained on Tesla-A100 GPUs.\nDuring the pre-tuning phase, we set the learning rate (lr) to be 2e-5, the warmup ratio as 0.03, and the batch size at 128. For the IWSLT dataset, we performed training over 3 epochs, while for the WMT dataset, training was conducted for 1 epoch.\nDuring the second stage of training, we set the mixing parameter denoted as  to 0.05, the lr to 2e-6, the batch size to 8, and the training step to 100.\nWe use the final model for evaluation.\nWe adopt SacreBLEU Post (2018  ###reference_b25###) to evaluate the translation accuracy, where translations are generated with a beam size of 4, a temperature of 0.1, and a top_p of 0.9.\nBesides, we compute the ratio of wrong language translation in the generated outputs, i.e. off-target translation ratio (OTR), with publicly available language detector222https://fasttext.cc/docs/en/language-identification.html  ###reference_ification.html### Joulin et al. (2016a  ###reference_b13###, b  ###reference_b14###).\nFollowing Garcia et al. (2023  ###reference_b7###), we also use BLEURT Sellam et al. (2020  ###reference_b27###) to assess the translation quality with BLEURT-20 checkpoint333https://github.com/google-research/bleurt  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "We consider the following two widely-used datasets:\nWMT: Following Jiao et al. (2023  ###reference_b12###  ###reference_b12###); Liu et al. (2023  ###reference_b18###  ###reference_b18###), we use the development sets from WMT2017 to WMT2020 for instruction tuning, including four language directions: EnZh and EnDe. The WMT dataset encompasses 51k translation sentence pairs.\nThen, we evaluate translation performance on WMT22 test sets, including EnCs, EnJa, EnRu, EnUk, FrDe. All these translation language pairs do not exist in the training set, thus allowing for the evaluation of zero-shot translation performance.\nIWSLT: We collect the IWSLT dataset and focus on translation performance between non-English languages.\nFor fine-tuning using multilingual translation data, we randomly select 12k sentence pairs from the train set of IWSLT 2017 Cettolo et al. (2017  ###reference_b2###  ###reference_b2###), spanning six directions: EnDe, EnZh, EnKo.\nWe utilize Flores-200 Costa-juss\u00e0 et al. (2022  ###reference_b5###  ###reference_b5###) devtest sets for evaluation on zero-shot translation directions, including ZhDe, ZhKo, DeKo. The Flores-200 comprises 1012 sentences from English Wikipedia covering multi-domain and then translated into about 200 languages by professional translators.\nWe leverage 7B size LLaMA as the backbone and consider the following baselines:\nLLaMA Touvron et al. (2023a  ###reference_b29###  ###reference_b29###): LLaMA serves as the foundation model, having undergone training on a corpus of trillion tokens. We employ pretrained 7B size LLaMA directly for inference.\nLLaMA-MT: We fine-tune the LLaMA solely on multilingual translation samples, following the same procedure as the pre-tuning stage in our algorithm. Following Jiao et al. (2023  ###reference_b12###  ###reference_b12###), we format translation sentence pairs into unified translation instructions.\nPost-Ins Liu et al. (2023  ###reference_b18###  ###reference_b18###): Following Liu et al. (2023  ###reference_b18###  ###reference_b18###), we switch the positions of instruction and input of prompt, where the model pays more attention to the instruction.\nPrompt in the target language (PTL):\nInstead of using the English prompt during inference, we translate the prompt into the target language during inference, which could provide more guidance information. Our inference leverages LLaMA-MT.\n-shot: In-context learning Brown et al. (2020  ###reference_b1###  ###reference_b1###) has proven to be an effective way to prompt LLMs performance. We report the few-shot performance for comprehensive comparison, including 1-shot and 5-shot. LLaMA-MT is used for inference.\nSennrich et al. (2023  ###reference_b28###  ###reference_b28###): Following Sennrich et al. (2023  ###reference_b28###  ###reference_b28###), we propose to decode by contrasting the translation sentence with language-contrastive input and  0.5. Our inference relies on LLaMA-MT, employing a greedy decoding strategy.\nWe conduct experiments on the Huggingface Transformers Wolf et al. (2020  ###reference_b38###  ###reference_b38###) toolkit. All models are trained on Tesla-A100 GPUs.\nDuring the pre-tuning phase, we set the learning rate (lr) to be 2e-5, the warmup ratio as 0.03, and the batch size at 128. For the IWSLT dataset, we performed training over 3 epochs, while for the WMT dataset, training was conducted for 1 epoch.\nDuring the second stage of training, we set the mixing parameter denoted as  to 0.05, the lr to 2e-6, the batch size to 8, and the training step to 100.\nWe use the final model for evaluation.\nWe adopt SacreBLEU Post (2018  ###reference_b25###  ###reference_b25###) to evaluate the translation accuracy, where translations are generated with a beam size of 4, a temperature of 0.1, and a top_p of 0.9.\nBesides, we compute the ratio of wrong language translation in the generated outputs, i.e. off-target translation ratio (OTR), with publicly available language detector222https://fasttext.cc/docs/en/language-identification.html  ###reference_ification.html###  ###reference_ification.html### Joulin et al. (2016a  ###reference_b13###  ###reference_b13###, b  ###reference_b14###  ###reference_b14###).\nFollowing Garcia et al. (2023  ###reference_b7###  ###reference_b7###), we also use BLEURT Sellam et al. (2020  ###reference_b27###  ###reference_b27###) to assess the translation quality with BLEURT-20 checkpoint333https://github.com/google-research/bleurt  ###reference_###  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Main Results",
            "text": "We present the zero-shot translation performance comparison of our model and other baselines on WMT and IWSLT datasets, as depicted in Table 2  ###reference_### and Table 1  ###reference_###. Our model outperforms the considered baselines across 16 translation directions.\nCompared with LLaMA-MT, which is only tuned on multilingual translation data, our model significantly reduces the average OTR scores by -29.9% in WMT and -92.9% in IWSLT through unlikelihood training on instruction-conflicting samples.\nIn contrast to the baseline approaches that focus on mitigating off-target problems during inference, such as PTL, -shot, and ,\nour model demonstrates superior performance, achieving improvements up to +11.3/ +6.2 SacreBLEU, -28.0%/ -40.2% OTR, and +19.3/ +21.3 BLEURT in IWSLT/ WMT datasets.\nRegarding baseline adjustments during the tuning stage, our model achieves significant improvements over Post-Ins, average +1.8/ +4.0 SacreBLEU score, -45.8%/ -22.7% OTR score, and +9.3/ +8.5 BLEURT score in IWSLT/ WMT datasets.\nAdditionally, our model surpasses other robust baseline models in these evaluations."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Effect of Unlikelihood Training Steps",
            "text": "To provide insight into the impact of unlikelihood training steps, Figure 3  ###reference_###. a) presents the zero-shot translation performance on the IWSLT dataset.\nAs observed, the model produces fewer wrong language translations and higher quality translations with more unlikelihood training steps.\nFrom the figure, it can seen that The model achieves the best performance, denoted by near-zero OTR scores, after about 60 updates, and this performance is consistently maintained even with further training extending up to 100 steps."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Effect of",
            "text": "As mentioned in Section 3.2  ###reference_###, our algorithm has a mixing hyper-parameter  to balance MLE loss and UL loss.\nThis is an ablation to evaluate the effect of different .\n###figure_2### ###figure_3### Figure 3  ###reference_###. b) shows the performance on the IWSLT dataset.\nAs expected, the higher  highlights the UL loss, resulting in fewer wrong language translations. Models fine-tuned with  exceeding 0.04 are unlikely to produce translations in wrong language.\nHowever, when  is increased beyond 0.3, there is a slight decrease in translation quality (with BLEURT scores of 42.2 vs. 38.8). This decline may be attributed to potential overfitting on the unlikelihood loss. Future research efforts should be directed toward mitigating the effects of this potential overfitting issue.\nIn summary, our experimental results indicate that our method exhibits robustness to varying values of the mixing parameter, ."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Results with Different Size of LLMs",
            "text": "To investigate the influence of model size, we conducted experiments with 13B size LLaMA on the multilingual translation dataset, employing the same experimental setup as that of the 7B size model\nThe results are summarized in Table 3  ###reference_###. The 13B model consistently outperforms the 7B model in terms of both the reduction in wrong language translations (-22.4% average OTR) and the improvement in translation quality (+9.1 average BLEURT). This observation aligns with prior findings Kaplan et al. (2020  ###reference_b15###), which suggest that increasing the number of training parameters yields benefits.\nHowever, the off-target problem still exists in the 13B size LLaMA-MT model.\nOur model achieves a significantly lower off-target translation ratio(0.7% vs. 70.1% average OTR), leading to a higher quality translation (45.4 vs. 27.9 average BLEURT).\nThis result demonstrates that our algorithm remains effective with larger LLMs."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Results with Different Amounts of Translation Data",
            "text": "Figure 4  ###reference_### illustrates the zero-shot translation performance of our models, according to BLEURT and OTR score, with a pre-tuning multilingual translation dataset consisting of  samples.\nAlthough we consider the zero-shot translation performance, it also brings gains with more translation data.\nHowever, the benefits derived from augmenting the translation data size become negligible when the dataset exceeds 40k samples.\nAdditionally, our algorithm exhibits robustness to different translation data sizes and consistently achieves OTR scores close to zero across all four settings, consequently leading to significantly higher BLEURT scores."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Performance on Supervised Translation",
            "text": "As our algorithm primarily enhances zero-shot translation performance through the unlikelihood training on instruction-conflicting samples. This raises a question: does the supervised translation ability persist even after unlikelihood training?\nAs shown in Table 4  ###reference_###, we report the performance of LLaMA-MT and ours on IWSLT and WMT.\nRemarkably, our models successfully retain the supervised translation ability after unlikelihood training with instruction-conflicting samples.\nSpecifically, our final model achieves comparable results compared with LLaMA-MT (19.7 vs 20.0 in SacreBLEU score and 61.6 vs. 61.4 in BLEURT score).\n###figure_4###"
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "Effect on General Task Performance",
            "text": "Inspired by Jiao et al. (2023  ###reference_b12###), we consider improving the zero-shot translation capabilities of LLMs tuned on a mixed dataset, consisting of translation data and general task data.\nWe construct the instruction tuning dataset by combining Alpaca444https://github.com/tatsu-lab/stanford_alpaca  ###reference_ca### with IWSLT translation samples and using the same hyperparameters as the main experiments for training.\nFollowing AlpacaEval555https://github.com/tatsu-lab/alpaca_eval  ###reference_###, we assess the performance with GPT-4 OpenAI (2023  ###reference_b23###) as the evaluator, while taking the reproduced Alpaca as the reference model to compute the win rate %.\nAs shown in Table 5  ###reference_###, our model attains comparable general tasks performance with Alpaca-MT (43.3% vs. 45.4%), while boosting the zero-shot translation performance (+1.3 SacreBLEU, -42.3% OTR, and +0.8 BLEURT), confirming the effectiveness of our algorithm when employed with a general task dataset."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Due to the huge cost to call the state-of-the-art LLMs, such as GPT-4 OpenAI (2023  ###reference_b23###), there is a need to investigate how to effectively fit a smaller LLM into specific tasks, e.g., machine translation. Note that although there are some powerful sequence-to-sequence style large-scale pretrained machine translation models Liu et al. (2020  ###reference_b19###); Zan et al. (2022  ###reference_b41###), this paper mainly focuses on the decoder-only LLMs due to their flexible interaction modes and rich world knowledge.\nIn the field of LLMs-based translation, various approaches have been proposed to optimize translation performance.\nParrot Jiao et al. (2023  ###reference_b12###) proposes to fine-tune model on machine translation data with a hint incorporating extra requirements to regulate the translation process.\nTIM Zeng et al. (2023  ###reference_b43###) introduces translation samples in comparisons to compute additional preference loss for regularization, exhibiting superior translation ability in both supervised and zero-shot directions.\nALMA Xu et al. (2023  ###reference_b39###) proposes a two-stage approach that first fine-tunes on monolingual data of downstream languages followed by fine-tuning on high-quality translation data, which achieves significant improvement of translation quality.\nLiu et al. (2023  ###reference_b18###) presents the position of instruction matters, that just moving the location of the instruction closer to the output can alleviate the instruction forgetting issue.\nIn contrast, we focus on the off-target problem of zero-shot translation, where the model fails to follow translation instructions, generating sequences not in the target language.\nAdditionally, we show how instruction-conflicting samples can enhance the influence of instruction thus mitigating the off-target problem.\nUnlikelihood training Welleck et al. (2019  ###reference_b36###) aims to force the mode to assign a lower probability for unlikely tokens.\nThis method has been further explored in dialog tasks by Li et al. (2020  ###reference_b17###), who demonstrated its effectiveness in generating more consistent and coherent human-like dialog.\nNogueira dos Santos et al. (2020  ###reference_b22###) used the unlikelihood loss for ranking and proposed a generative information retrieval approach.\nHosseini et al. (2021  ###reference_b10###) proposed the combination of an unlikelihood objective with a reference-based setup for input sentences to model negation with pretrained BERT Kenton and Toutanova (2019  ###reference_b16###).\nHu et al. (2023  ###reference_b11###) take the semantic-similar or ambiguous\ntokens as negative information and acquire it via inherent uncertainty for the ASQP task.\nIn this work, we take instances in which the translation pairs conflict with the instruction as the negative sample for zero-shot translation.\nFurthermore, we consider the new case that enhances the ability of LLMs to better follow translation instructions and generate translations in the correct language."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We propose a simple two-stage finetuning strategy to enhance the instruction-following ability of LLM for translation.\nThe core procedure consists of two main steps: 1) creating instruction-conflicting samples by replacing the translation directions with incorrect ones, and 2) training on these samples using an additional unlikelihood loss.\nExperimental results on IWSLT and WMT, spanning 16 zero-shot translation directions, demonstrate the effectiveness of the proposed method, which reduces the off-target translation ratio and produces translations with higher quality.\nFurthermore, our approach exerts a negligible influence on other aspects of LLMs, such as supervised translation performance and general task performance."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T1.16\" style=\"width:433.6pt;height:828.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(80.3pt,-153.5pt) scale(1.58801798504807,1.58801798504807) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.16.16\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.17.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T1.16.16.17.1.1\" rowspan=\"2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.17.1.1.1\">Models</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S4.T1.16.16.17.1.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.17.1.2.1\">Cs-En</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S4.T1.16.16.17.1.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.17.1.3.1\">Ja-En</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S4.T1.16.16.17.1.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.17.1.4.1\">Ru-En</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S4.T1.16.16.17.1.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.17.1.5.1\">Uk-En</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S4.T1.16.16.17.1.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.17.1.6.1\">Fr-De</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.16.16.17.1.7\" rowspan=\"2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.17.1.7.1\">AVG</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.10.10.10\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.1.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.2.2.2.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.3.3.3.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.4.4.4.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.5.5.5.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.6.6.6.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.7.7.7.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.8.8.8.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.9.9.9.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.10.10.10.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.11.11.11\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" id=\"S4.T1.11.11.11.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"10\" id=\"S4.T1.11.11.11.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_text ltx_font_bold ltx_font_italic\" id=\"S4.T1.11.11.11.1.1\">SacreBLEU Score </span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T1.11.11.11.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.18.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.18.1.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.18.1.1.1\">LLaMA</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.18.1.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a01.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.18.1.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a01.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.18.1.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.18.1.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a02.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.18.1.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.18.1.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a02.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.18.1.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.18.1.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a03.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.18.1.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a01.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.18.1.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a01.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.18.1.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a0<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.18.1.12.1\">1.6</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.19.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.19.2.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.19.2.1.1\">LLaMA-MT</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.19.2.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">14.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.19.2.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">35.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.19.2.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a01.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.19.2.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">10.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.19.2.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">16.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.19.2.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">33.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.19.2.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a05.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.19.2.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">31.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.19.2.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a04.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.19.2.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a03.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.19.2.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.19.2.12.1\">15.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.20.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.20.3.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.20.3.1.1\">Post-Ins</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.20.3.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.20.3.2.1\">17.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.20.3.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.20.3.3.1\">37.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.20.3.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.20.3.4.1\">1.4</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.20.3.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.20.3.5.1\">11.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.20.3.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.20.3.6.1\">19.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.20.3.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.20.3.7.1\">34.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.20.3.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a08.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.20.3.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.20.3.9.1\">33.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.20.3.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">11.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.20.3.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a03.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.20.3.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.20.3.12.1\">17.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.21.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.21.4.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.21.4.1.1\">PTL</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.21.4.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">13.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.21.4.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">35.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.21.4.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a01.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.21.4.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">10.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.21.4.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">13.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.21.4.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">33.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.21.4.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.21.4.8.1\">14.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.21.4.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">31.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.21.4.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a08.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.21.4.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">10.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.21.4.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.21.4.12.1\">17.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.22.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.22.5.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.22.5.1.1\">1-shot</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.22.5.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">14.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.22.5.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">33.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.22.5.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a01.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.22.5.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a09.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.22.5.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">15.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.22.5.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">31.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.22.5.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a09.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.22.5.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">29.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.22.5.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a04.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.22.5.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a03.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.22.5.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.22.5.12.1\">15.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.23.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.23.6.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.23.6.1.1\">5-shot</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.23.6.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">15.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.23.6.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">32.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.23.6.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a01.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.23.6.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a08.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.23.6.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">16.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.23.6.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">30.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.23.6.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">11.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.23.6.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">28.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.23.6.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a04.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.23.6.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a03.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.23.6.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.23.6.12.1\">15.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.12.12.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.12.12.12.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.12.12.12.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a02.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.12.12.12.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">31.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.12.12.12.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.12.12.12.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a08.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.12.12.12.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a02.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.12.12.12.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">29.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.12.12.12.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a01.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.12.12.12.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">27.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.12.12.12.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a01.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.12.12.12.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.12.12.12.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.12.12.12.12.1\">10.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.24.7\" style=\"background-color:#E6FFE6;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.24.7.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.24.7.1.1\" style=\"background-color:#E6FFE6;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.24.7.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.24.7.2.1\" style=\"background-color:#E6FFE6;\">17.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.24.7.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.24.7.3.1\" style=\"background-color:#E6FFE6;\">36.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.24.7.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.24.7.4.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.24.7.4.1.1\">1.4</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.24.7.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.24.7.5.1\" style=\"background-color:#E6FFE6;\">10.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.24.7.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.24.7.6.1\" style=\"background-color:#E6FFE6;\">18.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.24.7.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.24.7.7.1\" style=\"background-color:#E6FFE6;\">34.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.24.7.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.24.7.8.1\" style=\"background-color:#E6FFE6;\">14.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.24.7.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.24.7.9.1\" style=\"background-color:#E6FFE6;\">32.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.24.7.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.24.7.10.1\" style=\"background-color:#E6FFE6;\">30.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.24.7.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.24.7.11.1\" style=\"background-color:#E6FFE6;\">23.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.24.7.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed_underline\" id=\"S4.T1.16.16.24.7.12.1\" style=\"background-color:#E6FFE6;\">21.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.13.13.13\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" id=\"S4.T1.13.13.13.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"10\" id=\"S4.T1.13.13.13.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_text ltx_font_bold ltx_font_italic\" id=\"S4.T1.13.13.13.1.1\">OTR Score % </span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T1.13.13.13.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.25.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.25.8.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.25.8.1.1\">LLaMA</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.25.8.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">99.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.25.8.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">82.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.25.8.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">98.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.25.8.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">39.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.25.8.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">96.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.25.8.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">69.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.25.8.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">97.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.25.8.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">59.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.25.8.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">89.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.25.8.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">98.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.25.8.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.25.8.12.1\">83.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.26.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.26.9.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.26.9.1.1\">LLaMA-MT</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.26.9.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">22.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.26.9.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.26.9.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">96.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.26.9.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a03.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.26.9.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">18.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.26.9.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.26.9.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">83.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.26.9.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.26.9.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">93.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.26.9.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">99.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.26.9.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.26.9.12.1\">41.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.27.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.27.10.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.27.10.1.1\">Post-Ins</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.27.10.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">11.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.27.10.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.27.10.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">86.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.27.10.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a03.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.27.10.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a04.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.27.10.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.27.10.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">71.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.27.10.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.27.10.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">68.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.27.10.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">99.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.27.10.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.27.10.12.1\">34.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.28.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.28.11.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.28.11.1.1\">PTL</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.28.11.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">31.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.28.11.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.28.11.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">98.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.28.11.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a03.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.28.11.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">32.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.28.11.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.28.11.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.28.11.8.1\">12.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.28.11.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.28.11.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">84.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.28.11.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">74.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.28.11.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.28.11.12.1\">33.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.29.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.29.12.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.29.12.1.1\">1-shot</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.29.12.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">24.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.29.12.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.29.12.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">98.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.29.12.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a04.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.29.12.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">22.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.29.12.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.29.12.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">50.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.29.12.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.29.12.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">98.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.29.12.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">99.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.29.12.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.29.12.12.1\">40.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.30.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.30.13.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.30.13.1.1\">5-shot</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.30.13.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">22.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.30.13.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.30.13.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">96.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.30.13.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a03.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.30.13.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">20.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.30.13.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.30.13.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">33.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.30.13.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.30.13.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">99.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.30.13.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">99.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.30.13.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.30.13.12.1\">37.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.14.14.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.14.14.14.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.14.14.14.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a05.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.14.14.14.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.14.14.14.3.1\">0.4</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.14.14.14.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.14.14.14.4.1\">83.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.14.14.14.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.14.14.14.5.1\">2.6</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.14.14.14.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a04.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.14.14.14.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.14.14.14.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">38.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.14.14.14.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.14.14.14.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">86.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.14.14.14.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">91.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.14.14.14.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.14.14.14.12.1\">31.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.31.14\" style=\"background-color:#E6FFE6;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.31.14.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.31.14.1.1\" style=\"background-color:#E6FFE6;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.31.14.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.31.14.2.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.31.14.2.1.1\">5.1</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.31.14.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.31.14.3.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a00.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.31.14.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.31.14.4.1\" style=\"background-color:#E6FFE6;\">88.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.31.14.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.31.14.5.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a03.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.31.14.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.31.14.6.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.31.14.6.1.1\">2.7</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.31.14.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.31.14.7.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.31.14.7.1.1\">0.4</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.31.14.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.31.14.8.1\" style=\"background-color:#E6FFE6;\">15.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.31.14.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.31.14.9.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.31.14.9.1.1\">0.0</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.31.14.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.31.14.10.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.31.14.10.1.1\">1.2</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.31.14.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.31.14.11.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.31.14.11.1.1\">2.2</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.31.14.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed_underline\" id=\"S4.T1.16.16.31.14.12.1\" style=\"background-color:#E6FFE6;\">12.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.15.15.15\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" id=\"S4.T1.15.15.15.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"10\" id=\"S4.T1.15.15.15.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_text ltx_font_bold ltx_font_italic\" id=\"S4.T1.15.15.15.1.1\">BLEURT Score % </span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T1.15.15.15.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.32.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.32.15.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.32.15.1.1\">LLaMA</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.32.15.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">26.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.32.15.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">46.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.32.15.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.32.15.4.1\">34.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.32.15.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">29.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.32.15.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">23.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.32.15.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">31.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.32.15.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">13.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.32.15.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">27.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.32.15.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">34.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.32.15.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">38.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.32.15.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.32.15.12.1\">30.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.33.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.33.16.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.33.16.1.1\">LLaMA-MT</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.33.16.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">58.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.33.16.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">71.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.33.16.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">27.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.33.16.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">54.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.33.16.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">57.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.33.16.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">72.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.33.16.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">35.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.33.16.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">70.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.33.16.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">28.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.33.16.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">21.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.33.16.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.33.16.12.1\">49.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.34.17\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.34.17.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.34.17.1.1\">Post-Ins</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.34.17.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">65.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.34.17.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.34.17.3.1\">71.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.34.17.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">28.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.34.17.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.34.17.5.1\">55.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.34.17.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.34.17.6.1\">66.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.34.17.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.34.17.7.1\">73.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.34.17.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">43.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.34.17.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">71.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.34.17.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">40.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.34.17.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">21.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.34.17.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.34.17.12.1\">53.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.35.18\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.35.18.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.35.18.1.1\">PTL</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.35.18.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">53.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.35.18.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">71.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.35.18.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">25.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.35.18.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">54.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.35.18.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">49.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.35.18.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">72.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.35.18.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">59.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.35.18.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">70.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.35.18.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">33.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.35.18.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">31.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.35.18.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.35.18.12.1\">52.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.36.19\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.36.19.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.36.19.1.1\">1-shot</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.36.19.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">56.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.36.19.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">70.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.36.19.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">26.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.36.19.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">52.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.36.19.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">55.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.36.19.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">71.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.36.19.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">44.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.36.19.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">69.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.36.19.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">27.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.36.19.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">21.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.36.19.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.36.19.12.1\">49.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.37.20\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.37.20.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.37.20.1.1\">5-shot</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.37.20.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">58.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.37.20.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">69.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.37.20.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">27.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.37.20.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">51.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.37.20.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">56.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.37.20.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">70.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.37.20.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">50.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.37.20.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">69.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.37.20.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">27.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.37.20.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">21.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.37.20.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.37.20.12.1\">50.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.16.16.16.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.16.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">35.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.16.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">69.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.16.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">19.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.16.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">53.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.16.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">39.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.16.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">70.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.16.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">33.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.16.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">69.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.16.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">22.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.16.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">17.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.16.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.16.16.16.12.1\">43.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.16.16.38.21\" style=\"background-color:#E6FFE6;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T1.16.16.38.21.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.38.21.1.1\" style=\"background-color:#E6FFE6;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.16.16.38.21.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.38.21.2.1\" style=\"background-color:#E6FFE6;\">67.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.16.16.38.21.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.38.21.3.1\" style=\"background-color:#E6FFE6;\">71.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.16.16.38.21.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.38.21.4.1\" style=\"background-color:#E6FFE6;\">28.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.16.16.38.21.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.38.21.5.1\" style=\"background-color:#E6FFE6;\">55.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.16.16.38.21.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.38.21.6.1\" style=\"background-color:#E6FFE6;\">66.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.16.16.38.21.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T1.16.16.38.21.7.1\" style=\"background-color:#E6FFE6;\">72.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.16.16.38.21.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.38.21.8.1\" style=\"background-color:#E6FFE6;\">61.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.16.16.38.21.9\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.38.21.9.1\" style=\"background-color:#E6FFE6;\">71.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.16.16.38.21.10\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.38.21.10.1\" style=\"background-color:#E6FFE6;\">68.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.16.16.38.21.11\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.16.16.38.21.11.1\" style=\"background-color:#E6FFE6;\">59.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.16.16.38.21.12\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed_underline\" id=\"S4.T1.16.16.38.21.12.1\" style=\"background-color:#E6FFE6;\">62.2</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.20.1\">Zero-shot translation performance achieved on WMT benchmark.</span> <span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.21.2\">Bold</span>: the better results. <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T1.22.3\">Underline</span>: average scores obtained for all directions.</figcaption>\n</figure>",
            "capture": "Table 1: Zero-shot translation performance achieved on WMT benchmark. Bold: the better results. Underline: average scores obtained for all directions."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T2.12\" style=\"width:390.3pt;height:725.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(59.6pt,-110.8pt) scale(1.43949994382884,1.43949994382884) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.12.12\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T2.6.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T2.6.6.6.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.6.6.6.7.1\">Models</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.1.1.1.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.1\">ZhDe</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.2.2.2.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.2.2.2.1\">ZhDe</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.3.3.3.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.3.3.3.3.1\">ZhKo</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.4.4.4.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.4.4.4.4.1\">ZhKo</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.5.5.5.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.5.5.5.1\">DeKo</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.6.6.6.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.6.6.6.6.1\">DeKo</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.6.6.6.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.6.6.6.8.1\">AVG</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.7.7.7\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" id=\"S4.T2.7.7.7.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\" id=\"S4.T2.7.7.7.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" id=\"S4.T2.7.7.7.1.1\">SacreBLEU Score </span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.7.7.7.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.13.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.13.1.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.13.1.1.1\">LLaMA</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.13.1.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.13.1.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.13.1.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.13.1.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.13.1.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.13.1.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.13.1.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a0<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.13.1.8.1\">0.3</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.14.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.14.2.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.14.2.1.1\">LLaMA-MT</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.14.2.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.14.2.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a01.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.14.2.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.14.2.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.14.2.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.14.2.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a02.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.14.2.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a0<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.14.2.8.1\">0.9</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.15.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.15.3.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.15.3.1.1\">Post-Ins</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.15.3.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a09.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.15.3.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a06.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.15.3.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.15.3.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a03.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.15.3.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a03.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.15.3.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.15.3.7.1\">4.1</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.15.3.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a0<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.15.3.8.1\">4.3</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.16.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.16.4.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.16.4.1.1\">PTL</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.16.4.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a09.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.16.4.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a01.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.16.4.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.16.4.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.16.4.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.16.4.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a01.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.16.4.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a0<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.16.4.8.1\">2.3</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.17.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.17.5.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.17.5.1.1\">1-shot</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.17.5.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a02.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.17.5.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a01.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.17.5.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.17.5.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.17.5.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.17.5.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a03.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.17.5.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a0<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.17.5.8.1\">1.5</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.18.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.18.6.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.18.6.1.1\">5-shot</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.18.6.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a06.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.18.6.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a02.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.18.6.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.18.6.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a01.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.18.6.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.18.6.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a05.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.18.6.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a0<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.18.6.8.1\">2.6</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.8.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.8.8.8.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.8.8.8.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.8.8.8.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.8.8.8.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.8.8.8.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.8.8.8.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.8.8.8.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a00.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.8.8.8.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a0<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.8.8.8.8.1\">0.3</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.19.7\" style=\"background-color:#E6FFE6;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.19.7.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.19.7.1.1\" style=\"background-color:#E6FFE6;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.19.7.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.19.7.2.1\" style=\"background-color:#E6FFE6;\">11.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.19.7.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T2.12.12.19.7.3.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.19.7.3.1.1\">7.1</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.19.7.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T2.12.12.19.7.4.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.19.7.4.1.1\">6.4</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.19.7.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T2.12.12.19.7.5.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.19.7.5.1.1\">3.5</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.19.7.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T2.12.12.19.7.6.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.19.7.6.1.1\">4.9</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.19.7.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T2.12.12.19.7.7.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a03.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.19.7.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T2.12.12.19.7.8.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold ltx_framed_underline\" id=\"S4.T2.12.12.19.7.8.1.1\">6.1</span></span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.9.9.9\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" id=\"S4.T2.9.9.9.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\" id=\"S4.T2.9.9.9.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<span class=\"ltx_text ltx_font_bold ltx_font_italic\" id=\"S4.T2.9.9.9.1.1\">OTR Score % </span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.9.9.9.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.20.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.20.8.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.20.8.1.1\">LLaMA</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.20.8.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">100.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.20.8.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">99.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.20.8.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">100.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.20.8.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">99.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.20.8.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">99.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.20.8.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">99.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.20.8.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.20.8.8.1\">99.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.21.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.21.9.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.21.9.1.1\">LLaMA-MT</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.21.9.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">92.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.21.9.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">99.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.21.9.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">100.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.21.9.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">99.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.21.9.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">99.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.21.9.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">63.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.21.9.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.21.9.8.1\">92.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.22.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.22.10.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.22.10.1.1\">Post-Ins</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.22.10.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">28.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.22.10.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">31.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.22.10.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">99.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.22.10.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">46.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.22.10.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">63.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.22.10.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a06.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.22.10.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.22.10.8.1\">46.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.23.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.23.11.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.23.11.1.1\">PTL</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.23.11.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">16.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.23.11.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">94.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.23.11.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">94.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.23.11.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">99.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.23.11.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">99.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.23.11.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">90.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.23.11.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.23.11.8.1\">82.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.24.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.24.12.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.24.12.1.1\">1-shot</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.24.12.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">13.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.24.12.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">23.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.24.12.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">13.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.24.12.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">15.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.24.12.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">20.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.24.12.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">26.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.24.12.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.24.12.8.1\">20.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.25.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.25.13.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.25.13.1.1\">5-shot</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.25.13.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">33.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.25.13.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">29.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.25.13.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">14.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.25.13.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">18.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.25.13.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">22.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.25.13.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">31.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.25.13.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.25.13.8.1\">24.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.10.10.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.10.10.10.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.10.10.10.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">82.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.10.10.10.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">94.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.10.10.10.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">99.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.10.10.10.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">97.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.10.10.10.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">97.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.10.10.10.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">44.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.10.10.10.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.10.10.10.8.1\">86.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.26.14\" style=\"background-color:#E6FFE6;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.26.14.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.26.14.1.1\" style=\"background-color:#E6FFE6;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.26.14.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T2.12.12.26.14.2.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.26.14.2.1.1\">0.8</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.26.14.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T2.12.12.26.14.3.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.26.14.3.1.1\">0.0</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.26.14.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T2.12.12.26.14.4.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.26.14.4.1.1\">0.6</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.26.14.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T2.12.12.26.14.5.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.26.14.5.1.1\">0.0</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.26.14.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T2.12.12.26.14.6.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.26.14.6.1.1\">0.3</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.26.14.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T2.12.12.26.14.7.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.26.14.7.1.1\">0.0</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.26.14.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T2.12.12.26.14.8.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold ltx_framed_underline\" id=\"S4.T2.12.12.26.14.8.1.1\">0.3</span></span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.11.11.11\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" id=\"S4.T2.11.11.11.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\" id=\"S4.T2.11.11.11.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" id=\"S4.T2.11.11.11.1.1\">BLEURT Score % </span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.11.11.11.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.27.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.27.15.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.27.15.1.1\">LLaMA</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.27.15.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">42.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.27.15.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">31.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.27.15.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.27.15.4.1\">46.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.27.15.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.27.15.5.1\">34.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.27.15.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">36.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.27.15.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.27.15.7.1\">34.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.27.15.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.27.15.8.1\">37.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.28.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.28.16.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.28.16.1.1\">LLaMA-MT</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.28.16.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">20.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.28.16.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">22.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.28.16.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">13.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.28.16.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">14.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.28.16.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">19.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.28.16.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">23.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.28.16.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.28.16.8.1\">18.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.29.17\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.29.17.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.29.17.1.1\">Post-Ins</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.29.17.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">40.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.29.17.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">48.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.29.17.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">14.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.29.17.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">25.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.29.17.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">32.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.29.17.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.29.17.7.1\">34.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.29.17.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.29.17.8.1\">32.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.30.18\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.30.18.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.30.18.1.1\">PTL</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.30.18.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">38.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.30.18.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">24.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.30.18.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">15.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.30.18.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">14.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.30.18.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">20.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.30.18.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">16.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.30.18.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.30.18.8.1\">21.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.31.19\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.31.19.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.31.19.1.1\">1-shot</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.31.19.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">14.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.31.19.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">23.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.31.19.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">13.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.31.19.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">15.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.31.19.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">20.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.31.19.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">26.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.31.19.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.31.19.8.1\">20.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.32.20\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.32.20.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.32.20.1.1\">5-shot</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.32.20.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">33.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.32.20.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">29.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.32.20.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">14.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.32.20.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">18.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.32.20.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">22.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.32.20.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">31.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.32.20.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.32.20.8.1\">24.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.12.12.12.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.12.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">15.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.12.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">17.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.12.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">14.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.12.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">11.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.12.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">16.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.12.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">10.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.12.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.12.12.12.8.1\">14.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.33.21\" style=\"background-color:#E6FFE6;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T2.12.12.33.21.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.33.21.1.1\" style=\"background-color:#E6FFE6;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.12.12.33.21.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.33.21.2.1\" style=\"background-color:#E6FFE6;\">46.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.12.12.33.21.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.33.21.3.1\" style=\"background-color:#E6FFE6;\">57.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.12.12.33.21.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T2.12.12.33.21.4.1\" style=\"background-color:#E6FFE6;\">34.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.12.12.33.21.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T2.12.12.33.21.5.1\" style=\"background-color:#E6FFE6;\">31.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.12.12.33.21.6\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.12.12.33.21.6.1\" style=\"background-color:#E6FFE6;\">48.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.12.12.33.21.7\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S4.T2.12.12.33.21.7.1\" style=\"background-color:#E6FFE6;\">32.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.12.12.33.21.8\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed_underline\" id=\"S4.T2.12.12.33.21.8.1\" style=\"background-color:#E6FFE6;\">41.8</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.16.1\">Zero-Shot translation performance on the IWSLT dataset.</span> <span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.17.2\">Bold</span>: the best results. <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T2.18.3\">Underline</span>: average scores obtained for all directions.</figcaption>\n</figure>",
            "capture": "Table 2: Zero-Shot translation performance on the IWSLT dataset. Bold: the best results. Underline: average scores obtained for all directions."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T3.8\" style=\"width:258.3pt;height:178.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-14.4pt,9.9pt) scale(0.9,0.9) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.8.8\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.6.6.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.6.6.6.7\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.6.6.6.7.1\">Models</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.6.6.6.8\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.6.6.6.8.1\">Size</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.1\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1\">ZhDe</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.2.2.2.2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.2.2.2.2.1\">ZhDe</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.3.3.3.3\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.3.3.1\">ZhKo</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.4.4.4.4\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.4.4.4.1\">ZhKo</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.5.5.5.5\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.5.5.5.5.1\">DeKo</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.6.6.6.6\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.6.6.6.6.1\">DeKo</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.6.6.6.9\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.6.6.6.9.1\">AVG</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.7.7.7\">\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.7.7.7.2\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.7.7.7.3\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\" id=\"S4.T3.7.7.7.1\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold ltx_font_italic\" id=\"S4.T3.7.7.7.1.1\">OTR Score % </span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.7.7.7.4\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.8.8.9.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.9.1.1\" rowspan=\"2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.9.1.1.1\">LLaMA-MT</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.9.1.2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.9.1.2.1\">7B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.9.1.3\" style=\"padding-top:1pt;padding-bottom:1pt;\">92.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.9.1.4\" style=\"padding-top:1pt;padding-bottom:1pt;\">99.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.9.1.5\" style=\"padding-top:1pt;padding-bottom:1pt;\">100.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.9.1.6\" style=\"padding-top:1pt;padding-bottom:1pt;\">99.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.9.1.7\" style=\"padding-top:1pt;padding-bottom:1pt;\">99.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.9.1.8\" style=\"padding-top:1pt;padding-bottom:1pt;\">63.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.9.1.9\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T3.8.8.9.1.9.1\">92.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.8.8.10.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.10.2.1\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.10.2.1.1\">13B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.10.2.2\" style=\"padding-top:1pt;padding-bottom:1pt;\">30.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.10.2.3\" style=\"padding-top:1pt;padding-bottom:1pt;\">68.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.10.2.4\" style=\"padding-top:1pt;padding-bottom:1pt;\">99.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.10.2.5\" style=\"padding-top:1pt;padding-bottom:1pt;\">99.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.10.2.6\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.10.2.7\" style=\"padding-top:1pt;padding-bottom:1pt;\">40.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.10.2.8\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T3.8.8.10.2.8.1\">70.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.8.8.11.3\">\n<td class=\"ltx_td\" id=\"S4.T3.8.8.11.3.1\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.11.3.2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.11.3.2.1\" style=\"background-color:#E6FFE6;\">7B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.11.3.3\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.11.3.3.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a00.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.11.3.4\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.11.3.4.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a00.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.11.3.5\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.11.3.5.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a00.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.11.3.6\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.11.3.6.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a00.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.11.3.7\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.11.3.7.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a00.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.11.3.8\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.11.3.8.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a00.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.11.3.9\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text\" id=\"S4.T3.8.8.11.3.9.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold ltx_framed_underline\" id=\"S4.T3.8.8.11.3.9.1.1\">0.3</span></span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.8.8.12.4\" style=\"background-color:#E6FFE6;\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.12.4.1\" rowspan=\"-2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.12.4.1.1\" style=\"background-color:#E6FFE6;\">Ours</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.12.4.2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.12.4.2.1\" style=\"background-color:#E6FFE6;\">13B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.12.4.3\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.12.4.3.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a01.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.12.4.4\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.12.4.4.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a00.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.12.4.5\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.12.4.5.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a02.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.12.4.6\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.12.4.6.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a00.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.12.4.7\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.12.4.7.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a00.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.12.4.8\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.12.4.8.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a00.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.12.4.9\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.12.4.9.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_framed_underline\" id=\"S4.T3.8.8.12.4.9.1.1\">0.7</span></span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.8.8.8\">\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.8.8.8.2\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.8.8.8.3\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\" id=\"S4.T3.8.8.8.1\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold ltx_font_italic\" id=\"S4.T3.8.8.8.1.1\">BLEURT Score % </span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.8.8.8.4\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.8.8.13.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.13.5.1\" rowspan=\"2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.13.5.1.1\">LLaMA-MT</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.13.5.2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.13.5.2.1\">7B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.13.5.3\" style=\"padding-top:1pt;padding-bottom:1pt;\">20.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.13.5.4\" style=\"padding-top:1pt;padding-bottom:1pt;\">22.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.13.5.5\" style=\"padding-top:1pt;padding-bottom:1pt;\">13.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.13.5.6\" style=\"padding-top:1pt;padding-bottom:1pt;\">14.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.13.5.7\" style=\"padding-top:1pt;padding-bottom:1pt;\">19.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.13.5.8\" style=\"padding-top:1pt;padding-bottom:1pt;\">23.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.13.5.9\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T3.8.8.13.5.9.1\">18.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.8.8.14.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.14.6.1\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.14.6.1.1\">13B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.14.6.2\" style=\"padding-top:1pt;padding-bottom:1pt;\">42.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.14.6.3\" style=\"padding-top:1pt;padding-bottom:1pt;\">36.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.14.6.4\" style=\"padding-top:1pt;padding-bottom:1pt;\">14.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.14.6.5\" style=\"padding-top:1pt;padding-bottom:1pt;\">14.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.14.6.6\" style=\"padding-top:1pt;padding-bottom:1pt;\">28.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.14.6.7\" style=\"padding-top:1pt;padding-bottom:1pt;\">31.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.14.6.8\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T3.8.8.14.6.8.1\">27.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.8.8.15.7\">\n<td class=\"ltx_td\" id=\"S4.T3.8.8.15.7.1\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.15.7.2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.15.7.2.1\" style=\"background-color:#E6FFE6;\">7B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.15.7.3\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.15.7.3.1\" style=\"background-color:#E6FFE6;\">46.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.15.7.4\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.15.7.4.1\" style=\"background-color:#E6FFE6;\">57.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.15.7.5\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.15.7.5.1\" style=\"background-color:#E6FFE6;\">34.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.15.7.6\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.15.7.6.1\" style=\"background-color:#E6FFE6;\">31.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.15.7.7\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.15.7.7.1\" style=\"background-color:#E6FFE6;\">48.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.15.7.8\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.15.7.8.1\" style=\"background-color:#E6FFE6;\">33.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.8.8.15.7.9\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed_underline\" id=\"S4.T3.8.8.15.7.9.1\" style=\"background-color:#E6FFE6;\">41.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.8.8.16.8\" style=\"background-color:#E6FFE6;\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.8.8.16.8.1\" rowspan=\"-2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text\" id=\"S4.T3.8.8.16.8.1.1\" style=\"background-color:#E6FFE6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.16.8.1.1.1\">Ours</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.8.8.16.8.2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.16.8.2.1\" style=\"background-color:#E6FFE6;\">13B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.8.8.16.8.3\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.16.8.3.1\" style=\"background-color:#E6FFE6;\">49.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.8.8.16.8.4\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.16.8.4.1\" style=\"background-color:#E6FFE6;\">62.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.8.8.16.8.5\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.16.8.5.1\" style=\"background-color:#E6FFE6;\">38.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.8.8.16.8.6\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.16.8.6.1\" style=\"background-color:#E6FFE6;\">31.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.8.8.16.8.7\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.16.8.7.1\" style=\"background-color:#E6FFE6;\">52.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.8.8.16.8.8\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.16.8.8.1\" style=\"background-color:#E6FFE6;\">37.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.8.8.16.8.9\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed_underline\" id=\"S4.T3.8.8.16.8.9.1\" style=\"background-color:#E6FFE6;\">45.4</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.12.1\">The impact of model size.</span> We report the BLEURT and OTR Scores on the IWSLT dataset. <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.13.2\">Bold</span>: the better results of LLaMA-MT and ours. <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T3.14.3\">Underline</span>: average scores obtained for all directions.</figcaption>\n</figure>",
            "capture": "Table 3: The impact of model size. We report the BLEURT and OTR Scores on the IWSLT dataset. Bold: the better results of LLaMA-MT and ours. Underline: average scores obtained for all directions."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T4\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T4.2\" style=\"width:222.6pt;height:68.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-5.9pt,1.8pt) scale(0.95,0.95) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T4.2.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S5.T4.2.2.2.3\" rowspan=\"2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.2.2.3.1\">Models</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S5.T4.1.1.1.1\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.1.1\">SacreBLEU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S5.T4.2.2.2.2\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.2.2.2.1\">BLEURT</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2.3.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T4.2.2.3.1.1\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.2.3.1.1.1\">IWSLT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T4.2.2.3.1.2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.2.3.1.2.1\">WMT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T4.2.2.3.1.3\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.2.3.1.3.1\">AVG</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T4.2.2.3.1.4\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.2.3.1.4.1\">IWSLT-4</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T4.2.2.3.1.5\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.2.3.1.5.1\">WMT-3</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T4.2.2.3.1.6\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.2.3.1.6.1\">AVG</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2.4.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T4.2.2.4.1.1\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.2.4.1.1.1\">LLaMA-MT</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.4.1.2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.2.4.1.2.1\">16.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.4.1.3\" style=\"padding-top:1pt;padding-bottom:1pt;\">23.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.4.1.4\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed_underline\" id=\"S5.T4.2.2.4.1.4.1\">20.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.4.1.5\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.2.4.1.5.1\">57.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.4.1.6\" style=\"padding-top:1pt;padding-bottom:1pt;\">65.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.4.1.7\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S5.T4.2.2.4.1.7.1\">61.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2.5.2\" style=\"background-color:#E6FFE6;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T4.2.2.5.2.1\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.2.5.2.1.1\" style=\"background-color:#E6FFE6;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.2.2.5.2.2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text\" id=\"S5.T4.2.2.5.2.2.1\" style=\"background-color:#E6FFE6;\">15.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.2.2.5.2.3\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.2.5.2.3.1\" style=\"background-color:#E6FFE6;\">23.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.2.2.5.2.4\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed_underline\" id=\"S5.T4.2.2.5.2.4.1\" style=\"background-color:#E6FFE6;\">19.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.2.2.5.2.5\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text\" id=\"S5.T4.2.2.5.2.5.1\" style=\"background-color:#E6FFE6;\">57.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.2.2.5.2.6\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.2.5.2.6.1\" style=\"background-color:#E6FFE6;\">65.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.2.2.5.2.7\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed_underline\" id=\"S5.T4.2.2.5.2.7.1\" style=\"background-color:#E6FFE6;\">61.6</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.6.1\">Supervised translation performance.</span> <span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.7.2\">Bold</span>: the best results. <span class=\"ltx_text ltx_framed_underline\" id=\"S5.T4.8.3\">Underline</span>: average scores.</figcaption>\n</figure>",
            "capture": "Table 4: Supervised translation performance. Bold: the best results. Underline: average scores."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T5\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T5.4\" style=\"width:254.8pt;height:51.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-6.7pt,1.4pt) scale(0.95,0.95) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T5.4.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T5.4.4.4\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" id=\"S5.T5.4.4.4.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T5.1.1.1.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.1.1\">Win Rate % </span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T5.2.2.2.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.2.2.2.2.1\">SacreBLEU </span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T5.3.3.3.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.3.3.3.3.1\">OTR % </span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T5.4.4.4.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.4.4.4.4.1\">BLEURT </span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T5.4.4.5.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T5.4.4.5.1.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.4.4.5.1.1.1\">Alpaca-MT</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.4.5.1.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.4.4.5.1.2.1\">45.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.4.5.1.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\u00a0\u00a03.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.4.5.1.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">47.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.4.5.1.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">28.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.4.6.2\" style=\"background-color:#E6FFE6;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S5.T5.4.4.6.2.1\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.4.4.6.2.1.1\" style=\"background-color:#E6FFE6;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.4.4.6.2.2\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S5.T5.4.4.6.2.2.1\" style=\"background-color:#E6FFE6;\">43.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.4.4.6.2.3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S5.T5.4.4.6.2.3.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.4.4.6.2.3.1.1\">5.0</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.4.4.6.2.4\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text\" id=\"S5.T5.4.4.6.2.4.1\" style=\"background-color:#E6FFE6;\">\u00a0\u00a0<span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.4.4.6.2.4.1.1\">4.8</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.4.4.6.2.5\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.4.4.6.2.5.1\" style=\"background-color:#E6FFE6;\">29.5</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.7.1\">Performance after combining with general tasks data.</span> We report the Win rate % compared with Alpaca and translation performance on IWSLT. <span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.8.2\">Bold</span>: the best results.</figcaption>\n</figure>",
            "capture": "Table 5: Performance after combining with general tasks data. We report the Win rate % compared with Alpaca and translation performance on IWSLT. Bold: the best results."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.14399v1_figure_1.png",
            "caption": "Figure 2: \nOverview of our fine-tuning framework for zero-shot translation. (a) In the first stage, we perform pre-tuning on LLMs using MLE loss on multilingual translation samples, focusing on unlocking the translation ability of LLMs. (b) Subsequently, we introduce instruction-conflicting samples by randomly substituting the instruction component with a different one. We then train the model with MLE loss \u2112M\u2062L\u2062Esuperscript\u2112\ud835\udc40\ud835\udc3f\ud835\udc38\\mathcal{L}^{MLE}caligraphic_L start_POSTSUPERSCRIPT italic_M italic_L italic_E end_POSTSUPERSCRIPT on translation data and incorporate an unlikelihood loss \u2112U\u2062Lsuperscript\u2112\ud835\udc48\ud835\udc3f\\mathcal{L}^{UL}caligraphic_L start_POSTSUPERSCRIPT italic_U italic_L end_POSTSUPERSCRIPT on the instruction-conflicting samples."
        },
        "2": {
            "figure_path": "2403.14399v1_figure_2.png",
            "caption": "Figure 2: \nOverview of our fine-tuning framework for zero-shot translation. (a) In the first stage, we perform pre-tuning on LLMs using MLE loss on multilingual translation samples, focusing on unlocking the translation ability of LLMs. (b) Subsequently, we introduce instruction-conflicting samples by randomly substituting the instruction component with a different one. We then train the model with MLE loss \u2112M\u2062L\u2062Esuperscript\u2112\ud835\udc40\ud835\udc3f\ud835\udc38\\mathcal{L}^{MLE}caligraphic_L start_POSTSUPERSCRIPT italic_M italic_L italic_E end_POSTSUPERSCRIPT on translation data and incorporate an unlikelihood loss \u2112U\u2062Lsuperscript\u2112\ud835\udc48\ud835\udc3f\\mathcal{L}^{UL}caligraphic_L start_POSTSUPERSCRIPT italic_U italic_L end_POSTSUPERSCRIPT on the instruction-conflicting samples."
        },
        "3": {
            "figure_path": "2403.14399v1_figure_3.png",
            "caption": "Figure 2: \nOverview of our fine-tuning framework for zero-shot translation. (a) In the first stage, we perform pre-tuning on LLMs using MLE loss on multilingual translation samples, focusing on unlocking the translation ability of LLMs. (b) Subsequently, we introduce instruction-conflicting samples by randomly substituting the instruction component with a different one. We then train the model with MLE loss \u2112M\u2062L\u2062Esuperscript\u2112\ud835\udc40\ud835\udc3f\ud835\udc38\\mathcal{L}^{MLE}caligraphic_L start_POSTSUPERSCRIPT italic_M italic_L italic_E end_POSTSUPERSCRIPT on translation data and incorporate an unlikelihood loss \u2112U\u2062Lsuperscript\u2112\ud835\udc48\ud835\udc3f\\mathcal{L}^{UL}caligraphic_L start_POSTSUPERSCRIPT italic_U italic_L end_POSTSUPERSCRIPT on the instruction-conflicting samples."
        },
        "4": {
            "figure_path": "2403.14399v1_figure_4.png",
            "caption": "Figure 2: \nOverview of our fine-tuning framework for zero-shot translation. (a) In the first stage, we perform pre-tuning on LLMs using MLE loss on multilingual translation samples, focusing on unlocking the translation ability of LLMs. (b) Subsequently, we introduce instruction-conflicting samples by randomly substituting the instruction component with a different one. We then train the model with MLE loss \u2112M\u2062L\u2062Esuperscript\u2112\ud835\udc40\ud835\udc3f\ud835\udc38\\mathcal{L}^{MLE}caligraphic_L start_POSTSUPERSCRIPT italic_M italic_L italic_E end_POSTSUPERSCRIPT on translation data and incorporate an unlikelihood loss \u2112U\u2062Lsuperscript\u2112\ud835\udc48\ud835\udc3f\\mathcal{L}^{UL}caligraphic_L start_POSTSUPERSCRIPT italic_U italic_L end_POSTSUPERSCRIPT on the instruction-conflicting samples."
        },
        "5": {
            "figure_path": "2403.14399v1_figure_5.png",
            "caption": "Figure 3: Ablation Studies. a) Ablation study on continued training steps. b) Ablation study on the mixing hyper-parameter \u03b1\ud835\udefc\\alphaitalic_\u03b1. This demonstrates the zero-shot translation performance following the second stage of training."
        },
        "6": {
            "figure_path": "2403.14399v1_figure_6.png",
            "caption": "Figure 3: Ablation Studies. a) Ablation study on continued training steps. b) Ablation study on the mixing hyper-parameter \u03b1\ud835\udefc\\alphaitalic_\u03b1. This demonstrates the zero-shot translation performance following the second stage of training."
        },
        "7": {
            "figure_path": "2403.14399v1_figure_7.png",
            "caption": "Figure 4: \nThe impact of fine-tuning translation data size. We report the BLEURT and OTR Scores on the IWSLT dataset. The x-axis is the fine-tuning data size n\ud835\udc5bnitalic_n."
        }
    },
    "references": [
        {
            "1": {
                "title": "Language models are few-shot learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.",
                "venue": "NeurIPS, 2020.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Overview of the IWSLT 2017 evaluation campaign.",
                "author": "Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian St\u00fcker, Katsuhito Sudoh, Koichiro Yoshino, and Christian Federmann.",
                "venue": "In IWSLT, 2017.",
                "url": null
            }
        },
        {
            "3": {
                "title": "On the off-target problem of zero-shot multilingual neural machine translation.",
                "author": "Liang Chen, Shuming Ma, Dongdong Zhang, Furu Wei, and Baobao Chang.",
                "venue": "In Findings of ACL, 2023.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation.",
                "author": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.",
                "venue": "In EMNLP, 2014.",
                "url": null
            }
        },
        {
            "5": {
                "title": "No language left behind: Scaling human-centered machine translation.",
                "author": "Marta R Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al.",
                "venue": "arXiv preprint, 2022.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Specializing smaller language models towards multi-step reasoning.",
                "author": "Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot.",
                "venue": "In ICML, 2023.",
                "url": null
            }
        },
        {
            "7": {
                "title": "The unreasonable effectiveness of few-shot learning for machine translation.",
                "author": "Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maxim Krikun, Melvin Johnson, and Orhan Firat.",
                "venue": "In ICML, 2023.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Improved zero-shot neural machine translation via ignoring spurious correlations.",
                "author": "Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li.",
                "venue": "In ACL, 2019.",
                "url": null
            }
        },
        {
            "9": {
                "title": "How good are gpt models at machine translation? a comprehensive evaluation.",
                "author": "Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla.",
                "venue": "arXiv preprint, 2023.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Understanding by understanding not: Modeling negation in language models.",
                "author": "Arian Hosseini, Siva Reddy, Dzmitry Bahdanau, R Devon Hjelm, Alessandro Sordoni, and Aaron Courville.",
                "venue": "In NAACL, 2021.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Uncertainty-aware unlikelihood learning improves generative aspect sentiment quad prediction.",
                "author": "Mengting Hu, Yinhao Bai, Yike Wu, Zhen Zhang, Liqi Zhang, Hang Gao, Shiwan Zhao, and Minlie Huang.",
                "venue": "In Findings of ACL, 2023.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Parrot: Translating during chat using large language models.",
                "author": "Wenxiang Jiao, Jen tse Huang, Wenxuan Wang, Xing Wang, Shuming Shi, and Zhaopeng Tu.",
                "venue": "In arXiv preprint, 2023.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Fasttext.zip: Compressing text classification models.",
                "author": "Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H\u00e9rve J\u00e9gou, and Tomas Mikolov.",
                "venue": "arXiv preprint, 2016a.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Bag of tricks for efficient text classification.",
                "author": "Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov.",
                "venue": "arXiv preprint, 2016b.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Scaling laws for neural language models.",
                "author": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.",
                "venue": "arXiv preprint, 2020.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding.",
                "author": "Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova.",
                "venue": "In NAACL, 2019.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Don\u2019t say that! making inconsistent dialogue unlikely with unlikelihood training.",
                "author": "Margaret Li, Stephen Roller, Ilia Kulikov, Sean Welleck, Y-Lan Boureau, Kyunghyun Cho, and Jason Weston.",
                "venue": "In ACL, 2020.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Instruction position matters in sequence generation with large language models.",
                "author": "Yijin Liu, Xianfeng Zeng, Fandong Meng, and Jie Zhou.",
                "venue": "arXiv preprint, 2023.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Multilingual denoising pre-training for neural machine translation.",
                "author": "Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer.",
                "venue": "TACL, 2020.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt.",
                "author": "Qingyu Lu, Baopu Qiu, Liang Ding, Kanjian Zhang, Tom Kocmi, and Dacheng Tao.",
                "venue": "arXiv preprint, 2023.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Cross-task generalization via natural language crowdsourcing instructions.",
                "author": "Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.",
                "venue": "In ACL, 2022.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Beyond [CLS] through ranking by generation.",
                "author": "Cicero Nogueira dos Santos, Xiaofei Ma, Ramesh Nallapati, Zhiheng Huang, and Bing Xiang.",
                "venue": "In EMNLP, 2020.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Gpt-4 technical report.",
                "author": "OpenAI.",
                "venue": "arXiv preprint, 2023.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Towards making the most of chatgpt for machine translation.",
                "author": "Keqin Peng, Liang Ding, Qihuang Zhong, Li Shen, Xuebo Liu, Min Zhang, Yuanxin Ouyang, and Dacheng Tao.",
                "venue": "In Findings of EMNLP 2023, 2023.",
                "url": null
            }
        },
        {
            "25": {
                "title": "A call for clarity in reporting BLEU scores.",
                "author": "Matt Post.",
                "venue": "In WMT, 2018.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Healthcare copilot: Eliciting the power of general llms for medical consultation.",
                "author": "Zhiyao Ren, Yibing Zhan, Baosheng Yu, Liang Ding, and Dacheng Tao.",
                "venue": "arXiv preprint, 2024.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Bleurt: Learning robust metrics for text generation.",
                "author": "Thibault Sellam, Dipanjan Das, and Ankur P Parikh.",
                "venue": "In ACL, 2020.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Mitigating hallucinations and off-target machine translation with source-contrastive and language-contrastive decoding.",
                "author": "Rico Sennrich, Jannis Vamvas, and Alireza Mohammadshahi.",
                "venue": "arXiv preprint, 2023.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Llama: Open and efficient foundation language models.",
                "author": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.",
                "venue": "arXiv preprint, 2023a.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.",
                "venue": "arXiv preprint, 2023b.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Oop: Object-oriented programming evaluation benchmark for large language models.",
                "author": "Shuai Wang, Liang Ding, Li Shen, Yong Luo, Bo Du, and Dacheng Tao.",
                "venue": "arXiv preprint, 2024a.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Wisdom: Improving multimodal sentiment analysis by fusing contextual world knowledge.",
                "author": "Wenbin Wang, Liang Ding, Li Shen, Yong Luo, Han Hu, and Dacheng Tao.",
                "venue": "arXiv preprint, 2024b.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Element-aware summarization with large language models: Expert-aligned evaluation and chain-of-thought method.",
                "author": "Yiming Wang, Zhuosheng Zhang, and Rui Wang.",
                "venue": "In ACL, 2023.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Finetuned language models are zero-shot learners.",
                "author": "Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le.",
                "venue": "In ICLR, 2021.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Chain-of-thought prompting elicits reasoning in large language models.",
                "author": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.",
                "venue": "NeurIPS, 2022.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Neural text generation with unlikelihood training.",
                "author": "Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston.",
                "venue": "In ICLR, 2019.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Neural text generation with unlikelihood training.",
                "author": "Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston.",
                "venue": "In ICLR, 2020.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Transformers: State-of-the-art natural language processing.",
                "author": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.",
                "venue": "In EMNLP Demonstrations, 2020.",
                "url": null
            }
        },
        {
            "39": {
                "title": "A paradigm shift in machine translation: Boosting translation performance of large language models.",
                "author": "Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla.",
                "venue": "arXiv preprint, 2023.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Take care of your prompt bias: Investigating and mitigating the prompt bias in factual knowledge extraction.",
                "author": "Ziyang Xu, Keqin Peng, Liang Ding, Dacheng Tao, and Xiliang Lu.",
                "venue": "In Proc. of COLING, 2024.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Vega-mt: The jd explore academy machine translation system for wmt22.",
                "author": "Changtong Zan, Keqin Peng, Liang Ding, Baopu Qiu, Boan Liu, Shwai He, Qingyu Lu, Zheng Zhang, Chuang Liu, Weifeng Liu, et al.",
                "venue": "In WMT, 2022.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Unlikelihood tuning on negative samples amazingly improves zero-shot translation.",
                "author": "Changtong Zan, Liang Ding, Li Shen, Yibin Lei, Yibing Zhan, Weifeng Liu, and Dacheng Tao.",
                "venue": "arXiv preprint, 2023.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Tim: Teaching large language models to translate with comparison.",
                "author": "Jiali Zeng, Fandong Meng, Yongjing Yin, and Jie Zhou.",
                "venue": "arXiv preprint, 2023.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Opt: Open pre-trained transformer language models.",
                "author": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.",
                "venue": "arXiv preprint, 2022.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Siren\u2019s song in the ai ocean: A survey on hallucination in large language models.",
                "author": "Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al.",
                "venue": "arXiv preprint, 2023.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Intention analysis prompting makes large language models a good jailbreak defender.",
                "author": "Yuqi Zhang, Liang Ding, Lefei Zhang, and Dacheng Tao.",
                "venue": "arXiv preprint, 2024.",
                "url": null
            }
        },
        {
            "47": {
                "title": "Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert.",
                "author": "Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao.",
                "venue": "arXiv preprint, 2023.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Rose doesn\u2019t do that: Boosting the safety of instruction-tuned large language models with reverse prompt contrastive decoding.",
                "author": "Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao.",
                "venue": "arXiv preprint, 2024.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.14399v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "6"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "research_context": {
        "paper_id": "2403.14399v1",
        "paper_title": "Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning",
        "research_background": "**Motivation:**\nThe paper is motivated by the need to address the challenges in zero-shot translation (ZST) using large language models (LLMs). Specifically, the paper aims to tackle the prevalent off-target problem that occurs when LLMs, such as GPT-3, OPT, LLaMA, and LLaMA2, fail to adhere accurately to translation directions due to their weak instruction-following capabilities. The high cost of using state-of-the-art LLMs like GPT-4 further motivates the exploration of more cost-effective, suitably-sized LLMs tailored for specific tasks such as translation.\n\n**Research Problem:**\nThe primary research problem addressed in this paper is the improvement of the instruction-following abilities of LLMs, particularly in the context of language-aware ZST. This involves minimizing the occurrences of off-target translations where the translated output is not in the intended target language. This problem is essential for low-resource languages where training data lack direct mappings or the languages themselves may not have appeared during the training phase.\n\n**Relevant Prior Work:**\n1. **LLM Capabilities:** Prior works have demonstrated the capabilities of LLMs in various NLP tasks, including reasoning (Wei et al., 2022), summarization (Wang et al., 2023), and translation (Hendy et al., 2023).\n2. **Instruction Tuning:** Instruction tuning has been shown to enhance LLMs' ability to handle general tasks with proper guidance (Mishra et al., 2022; Wei et al., 2021). This involves task definition and specific instructions for task execution.\n3. **Zero-Shot Translation:** Research in ZST (Gu et al., 2019; Chen et al., 2023; Zan et al., 2023) aims to translate languages without direct training data mappings or when the languages have not been part of the training data. This is particularly crucial for paired-data-hungry low-resource languages.\n4. **Translation Specific Tuning:** Recent research (Zeng et al., 2023; Liu et al., 2023; Xu et al., 2023) suggests that tuning LLMs on translation data with appropriate task instructions can significantly improve translation performance.\n5. **Addressing Off-Target Issues:** Previous studies (Peng et al., 2023; Xu et al., 2023) have explored introducing more informative prompts and language-contrastive samples (Sennrich et al., 2023) to mitigate the off-target problem by modifying the decoding process.\n\nIn summary, the paper aims to fundamentally enhance the instruction-following capabilities of LLMs for language-aware translation by proposing a two-stage fine-tuning algorithm that addresses the intrinsic off-target problem experienced in current ZST approaches.",
        "methodology": "**Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning**\n\n**Methodology:**\nTo address the off-target problem with unlikelihood training, we construct negative candidate samples by replacing the instruction with an unrelated one while keeping the input and output unchanged. These samples are termed *instruction-conflicting samples* since the translation pairs diverge from the tasks associated with their provided instructions.\n\nWe take a sample from the instruction dataset where:\n- **Instruction:** \"Translate the following sentences from German to English.\",\n- **Input:** \"Er sagte, dass er eine WLAN-T\u00fcrklingel gebaut ha\",\n- **Output:** \"He built a WiFi door bell, he said.\"\n\nNext, we randomly pick another sample from a different task, such as:\n**Instruction:** \"Translate the following sentences from English to Chinese\", \nand replace the original correct instruction to create an instruction-conflicting sample. For example:\n- **New Sample:** \"[Instruction]: Translate the following sentences from English to Chinese. [Input]: Er sagte, dass er eine WLAN-T\u00fcrklingel gebaut ha.\"\n\nUsing these instruction-conflicting samples, we extend unlikelihood training to the zero-shot translation of translation-tailored Large Language Models (LLMs). We input these instructional samples into the model trained after stage 1, optimizing the unlikelihood loss:\n\\[ \\mathcal{L}_{UL} = \\sum_{(x, y') \\in D_{N}} -\\log(1 - P(y'|x)) \\]\nwhere \\( y' \\) is one of the corresponding negative instructions for \\( x \\). Hence, the overall objective function in unlikelihood training incorporates both likelihood and unlikelihood losses:\n\\[ \\mathcal{L} = \\mathcal{L}_{L} + \\lambda \\mathcal{L}_{UL} \\]\nwhere \\( \\lambda \\) is the mixing hyper-parameter.\n\nBy leveraging instruction-conflicting samples and mixing these two types of losses, the methodology enhances the model's accuracy in translating tasks specifically tailored to the given instructions.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Datasets:\n- **WMT:** Development sets from WMT2017 to WMT2020 are used for instruction tuning, including four language directions: EnZh and EnDe, encompassing 51k translation sentence pairs. Evaluation is performed on WMT22 test sets, including EnCs, EnJa, EnRu, EnUk, FrDe.\n- **IWSLT:** 12k sentence pairs are randomly selected from the train set of IWSLT 2017, focusing on directions EnDe, EnZh, EnKo. Evaluation is conducted on zero-shot translation directions, including ZhDe, ZhKo, DeKo, using Flores-200 devtest sets.\n\n#### Baselines:\n- **LLaMA:** Pretrained 7B size LLaMA model used for direct inference without fine-tuning.\n- **LLaMA-MT:** Fine-tuned LLaMA on multilingual translation samples, formatted into unified translation instructions.\n- **Post-Ins:** Model with instruction and input of prompt positions switched for better instruction attention.\n- **Prompt in the Target Language (PTL):** Prompts translated into the target language during inference with LLaMA-MT.\n- **n-shot:** In-context learning (1-shot and 5-shot) for performance comparison, using LLaMA-MT for inference.\n- **Sennrich et al. (2023):** Translation with language-contrastive input and 0.5 decoding, using greedy decoding strategy with LLaMA-MT.\n\n#### Evaluation Metrics:\n- **SacreBLEU:** Used to evaluate translation accuracy with translations generated at a beam size of 4, a temperature of 0.1, and a top_p of 0.9.\n- **Off-target Translation Ratio (OTR):** Ratio of wrong language translations in generated outputs, assessed with a publicly available language detector.\n- **BLEURT:** Translation quality assessment using the BLEURT-20 checkpoint.\n\n#### Training Details:\n- **Pre-tuning Phase:** Learning rate (lr) of 2e-5, warmup ratio of 0.03, and batch size of 128; 3 epochs of training for the IWSLT dataset and 1 epoch for the WMT dataset.\n- **Second Training Stage:** Mixing parameter set to 0.05, lr of 2e-6, batch size of 8, and training step set to 100.\n- **Hardware:** All models are trained on Tesla-A100 GPUs using the Huggingface Transformers toolkit.\n\n#### Main Experimental Results:\n- The performance is measured across 16 zero-shot translation directions spanning both the WMT and IWSLT datasets.\n- Detailed results in terms of SacreBLEU score, OTR, and BLEURT are not explicitly provided in the text but are implied to be evaluated comprehensively using the mentioned metrics."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the effect of different mixing hyper-parameters on balancing Maximum Likelihood Estimation (MLE) loss and Unlikelihood (UL) loss in the fine-tuning algorithm to mitigate off-target translations.",
            "experiment_process": "The experiment evaluates different values of the mixing hyper-parameter using the IWSLT dataset. Various models were fine-tuned with different hyper-parameter values, testing their effects on reducing incorrect language translations and overall translation quality. The models were assessed for translation quality using BLEURT scores.",
            "result_discussion": "The results show that a higher value of the mixing hyper-parameter emphasizes the UL loss, leading to fewer incorrect language translations. Models fine-tuned with values exceeding 0.04 are less likely to produce incorrect translations. However, increasing this parameter beyond 0.3 slightly decreases translation quality due to potential overfitting on the UL loss. The experiments indicate the method's robustness to variations in the mixing parameter.",
            "ablation_id": "2403.14399v1.No1"
        },
        {
            "research_objective": "To investigate the influence of model size on translation performance.",
            "experiment_process": "Experiments were conducted using a 13B parameter LLaMA model on the multilingual translation dataset, maintaining the same experimental setup as the 7B parameter model. The performance of both models was compared in terms of incorrect language translation reduction and translation quality, measured by BLEURT scores.",
            "result_discussion": "The 13B model consistently outperforms the 7B model, achieving a -22.4% average reduction in wrong language translations and a +9.1 average BLEURT improvement. Despite these improvements, the off-target problem persists in the 13B model. However, the algorithm significantly lowers the off-target translation ratio (0.7% vs. 70.1%), resulting in better translation quality (45.4 vs. 27.9 average BLEURT). This demonstrates the algorithm's effectiveness with larger LLMs.",
            "ablation_id": "2403.14399v1.No2"
        },
        {
            "research_objective": "To examine the effect of different amounts of translation data on translation performance.",
            "experiment_process": "The zero-shot translation performance of the models was evaluated on a pre-tuning multilingual translation dataset using BLEURT and Off-Target Ratio (OTR) scores. The performance was assessed with varying dataset sizes, up to 40,000 samples. The impact of increasing data size on translation quality and off-target translations was analyzed.",
            "result_discussion": "The results illustrate that increasing the translation data size improves performance, but the gains become negligible beyond 40k samples. The algorithm shows robustness across different data sizes, consistently achieving OTR scores close to zero and significantly higher BLEURT scores in all settings.",
            "ablation_id": "2403.14399v1.No3"
        }
    ]
}