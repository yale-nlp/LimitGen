{
    "title": "Event Grounded Criminal Court View Generation with Cooperative (Large) Language Models",
    "abstract": "With the development of legal intelligence, Criminal Court View Generation has attracted much attention as a crucial task of legal intelligence, which aims to generate concise and coherent texts that summarize case facts and provide explanations for verdicts. Existing researches explore the key information in case facts to yield the court views. Most of them employ a coarse-grained approach that partitions the facts into broad segments (e.g., verdict-related sentences) to make predictions. However, this approach fails to capture the complex details present in the case facts, such as various criminal elements and legal events. To this end, in this paper, we propose an Event Grounded Generation (EGG) method for criminal court view generation with cooperative (Large) Language Models, which introduces the fine-grained event information into the generation. Specifically, we first design a LLMs-based extraction method that can extract events in case facts without massive annotated events. Then, we incorporate the extracted events into court view generation by merging case facts and events. Besides, considering the computational burden posed by the use of LLMs in the extraction phase of EGG, we propose a LLMs-free EGG method that can eliminate the requirement for event extraction using LLMs in the inference phase. Extensive experimental results on a real-world dataset clearly validate the effectiveness of our proposed method. Code is available at https://github.com/yuelinan/Codes-of-EGG.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "The remarkable success of deep neural networks has stimulated the exploration of legal intelligence applications (Luo et al., 2017  ###reference_b21###; Zhong et al., 2018  ###reference_b44###, 2020  ###reference_b45###; Wu et al., 2022  ###reference_b35###; Zhang et al., 2023  ###reference_b43###; Liu et al., 2023  ###reference_b19###). Among these applications, Criminal Court View Generation (Ye et al., 2018  ###reference_b37###; Yue et al., 2021b  ###reference_b41###) has garnered increasing attention as a foundational facet of legal intelligence. As depicted in Figure 1  ###reference_###(a), the objective of criminal court view generation is to produce a coherent text, referred to as a court view, which serves as a concise representation of the case facts and offers an explanation for the rendered verdicts, such as charges and sentencing. The automated generation of court views has the potential to alleviate the workload of legal professionals while providing legal assistance to laymen (Yue et al., 2021b  ###reference_b41###; Wu et al., 2022  ###reference_b35###).\n###figure_1### The existing approaches in the field can be categorized into two groups: domain-specific models (Ye et al., 2018  ###reference_b37###; Huang et al., 2020  ###reference_b12###) and large language models (LLMs) (Touvron et al., 2023  ###reference_b31###; OpenAI, 2023  ###reference_b24###). Several domain-specific models (Yue et al., 2021b  ###reference_b41###; Wu et al., 2022  ###reference_b35###) commonly generate court views by leveraging key information (e.g., crime circumstances (Yue et al., 2021b  ###reference_b41###, a  ###reference_b40###, 2024  ###reference_b39###)) extracted from the case facts using legal knowledge. For instance, C3VG (Yue et al., 2021b  ###reference_b41###), a court view generation model that has demonstrated promising results, explicitly categorizes crime circumstances in the case facts into two broad types: verdict-related circumstances and sentencing-related ones. Subsequently, it employs pre-trained language models (PLMs) (e.g., BART111https://github.com/yuelinan/C3VG/tree/main/bart_based_c3vg  ###reference_/bart_based_c3vg### (Lewis et al., 2020  ###reference_b14###)) to generate court views based on these two types of information. Nevertheless, the components comprising the case facts are highly intricate. As illustrated in Figure 1  ###reference_###(b), case facts encompass various criminal elements222https://en.wikipedia.org/wiki/Element_(criminal_law)  ###reference_minal_law)### (i.e., legal events), represented by the underlined tokens in the fact description. Consequently, the adoption of a coarse-grained domain-specific approach that partitions the facts into two segments proves to be inadequate.\nFurthermore, considering that court view generation is essentially a text generation task, it is plausible to fine-tune LLMs (Yue et al., 2023b  ###reference_b38###) (e.g., Baichuan-7B (BaiChuan-Inc, 2023  ###reference_b3###)) for court view generation. However, as evident from the experimental findings presented in Table 2  ###reference_###, simple fine-tuning of LLMs does not yield satisfactory results. This could be attributed to the intricacy of the fact descriptions, necessitating the incorporation of additional legal knowledge. In this regard, a straightforward approach is to substitute PLMs with LLMs in domain-specific models. Nonetheless, domain-specific models often involve the collaborative training of multiple PLMs, which poses a significant computational burden on LLMs.\nTo this end, in this paper, we aim to develop a method which incorporates fined-grained event information into the court view generation by leveraging the collaboration between LLMs and PLMs in domain-specific models.\nThe overview of our proposed method is present in Figure 1  ###reference_###(b) and is two-fold: (1) extracting the fine-grained event of the\ncase fact and (2) generating court views based on the identified events.\nHowever, it is a non-trivial problem.\nAlthough available legal event extraction datasets contain substantial annotated data (Feng et al., 2022  ###reference_b8###; Yao et al., 2022  ###reference_b36###), they primarily focus on annotating which information belongs to events in each legal document within specific case types. This approach not only necessitates extensive professional effort but also requires re-annotation of vast amounts of legal documents when encountering new case types, thereby serving as a major bottleneck for practical applications of legal event extraction.\nTherefore, it is crucial to devise a strategy that can extract events with minimal human annotation and demonstrate good generalization capabilities across different case types.\nTo tackle the challenge mentioned above, we propose an Event Grounded Generation (EGG) method for criminal court view generation with Cooperative (Large) Language Models following an extract-generate framework:\nIn the extraction phase, we design a LLMs-based event extractor.\nSpecifically, we first fine-tune LLMs with the publicly available legal QA dataset CJRC (Duan et al., 2019  ###reference_b7###) (an extractive QA dataset like SQuAD (Rajpurkar et al., 2018  ###reference_b26###)).\nThis fine-tuning process enables the LLMs to extract pertinent answers from the original text based on a given legal question (i.e., the prompt).\nAfter the extractor is trained, we label each case type with several event-related questions.\nFor example, as shown in Figure 1  ###reference_###(b), for a case type of Mobbing, we label the event-related questions (e.g. \u201cWhat is the cause of the crime?\u201d and \u201cWhat is the amount of compensation?\u201d.)\nImportantly, we label these questions only for the case type itself and not for individual case facts.\nWhen dealing with a specific case fact related to the crime of Mobbing, we utilize the pre-defined event-related questions for the Mobbing case type.\nBy prompting the trained LLMs-based event extractor with the labeled questions, we extract events for each question based on the given case fact. It is worth noting that our labeled event-related questions are not present in the CJRC dataset, thus making our extraction method a zero-shot event extraction approach.\nNext, we combine the question and answer to get the events (e.g., the fine-grained events information in Figure 1  ###reference_###(b)).\nIn summary, this approach only necessitates the annotation of relevant questions for each case type, with an average of 9 questions per case type. In comparison to previous methods, our proposed event extraction approach significantly reduces the annotation time required.\n###figure_2### In the generation phase, we splice the facts and events together to form a new text input, which is then fed into the PLMs-based generator to yield the court views.\nAdditionally, taking into consideration the computational burden posed by the use of LLMs in the extraction phase of EGG, we recognize the need to enhance its practical applicability for both laymen and professionals. To address this, we propose an LLMs-free EGG method, referred to as , which eliminates the requirement of events during the inference phase.\nSpecifically, in the training process, we still employ LLMs to extract events in the extraction phase. However, in the generation phase, instead of merging the event and fact as input to the generator, we leverage the event as auxiliary information to assist the model in generating the court view based solely on the fact. To achieve this, we encode the fact and event separately using the fact and event encoders. Subsequently, we design a contrastive learning module to facilitate the fact encoder in capturing co-occurrence signals with the event through contrastive constraints. Finally, we generate the court view based on the fact.\nImportantly, during the inference phase,  no longer requires any event information. It solely relies on the case fact to generate the court view without the need for event extraction. This modification aims to improve the practicality and usability of the EGG method for legal professionals and individuals without legal expertise.\nIn summary, the major contributions of this paper are:\nWe propose an Event Grounded Generation (EGG) method for criminal court view generation with cooperative (large) language models, which first introduces the fine-grained event information into the court view generation.\nWe propose a low data resource approach to achieve a zero-shot legal event extraction with LLMs.\nTo alleviate the computational burden in EGG during inference that employs LLMs, we propose a LLMs-free EGG method based on the contrastive constraint.\nExtensive experiments on a real-world dataset validate the effectiveness of our method by comparing it with several competitive methods."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Related Work",
            "text": "Court View Generation.\nThe remarkable success in neural networks provokes the legal intelligence (Zhong et al., 2018  ###reference_b44###, 2020  ###reference_b45###; Shao et al., 2021  ###reference_b28###; Ma et al., 2021  ###reference_b22###; Li et al., 2023  ###reference_b16###; Chen et al., 2023  ###reference_b4###). Among them, court view generation has achieved increasing attention (Ye et al., 2018  ###reference_b37###; Yue et al., 2021b  ###reference_b41###).\nSpecifically,\n(Ye et al., 2018  ###reference_b37###) were the first to formulate the task of court view generation and explored the use of charges to enhance the generation process, allowing the model to focus on verdict-related information within the case facts.\n(Huang et al., 2020  ###reference_b12###) proposed a court view generation approach that involved masking key tokens in a template and subsequently employing a question-answering (QA) method to fill in these masked tokens.\n(Wu et al., 2022  ###reference_b35###) integrated legal judgment prediction with court view generation, enabling the simultaneous generation of judgment results and court views.\n(Yue et al., 2021b  ###reference_b41###) designed an extract-generate framework that categorized case facts into two types, namely verdict-related and sentencing-related information, using an extractor. The generated court views were then based on the extracted information.\nDespite the promising results achieved by these existing methods, they have overlooked the incorporation of fine-grained event information present in case facts. This limitation highlights the need to consider and leverage event information for more comprehensive and accurate court view generation.\nLarge Language Model in Legal AI.\nLarge Language Models (LLMs) such as ChatGPT (OpenAI, 2023  ###reference_b24###) and LLaMA (Touvron et al., 2023  ###reference_b31###) have exhibited impressive performance across various complex tasks and have made a significant impact on society. In the realm of legal AI, researchers have been combining LLMs with legal tasks (Wang et al., 2023  ###reference_b33###; Yue et al., 2023a  ###reference_b42###; He et al., 2023  ###reference_b9###).\nOne notable example is Lawyer LLaMA (Huang et al., 2023  ###reference_b11###), which underwent continual pretraining on an extensive legal corpus to systematically acquire legal knowledge. The model was then fine-tuned using legal instruction data, enabling it to apply its legal knowledge to specific scenarios. This approach leverages the power of LLMs to enhance the effectiveness of legal AI tasks.\nAnother approach, ChatLaw (Cui et al., 2023  ###reference_b6###), explored the use of larger base models to improve the logical reasoning capabilities of legal models. By leveraging the increased capacity and capabilities of larger models, ChatLaw aimed to enhance the model\u2019s ability to perform complex legal reasoning tasks.\nPrivacy concerns in the legal domain are addressed by FedJudge (Yue et al., 2023b  ###reference_b38###), which adopts Federated Learning during the instruction tuning process. This approach ensures the privacy of legal data by training the model on local devices and only sharing aggregated updates, rather than sharing raw data.\nIn this paper, the focus is specifically on utilizing LLMs to achieve legal event extraction.\n###figure_3### Legal Event Extraction.\nIn the field of legal event extraction, numerous studies (Li et al., 2019  ###reference_b15###; Shen et al., 2020  ###reference_b29###; Li et al., 2020  ###reference_b17###; Yao et al., 2022  ###reference_b36###; Feng et al., 2022  ###reference_b8###) have involved annotating legal event types for each legal document. Notably, (Yao et al., 2022  ###reference_b36###) have annotated over 8,000 legal documents with 108 event types. However, this manual annotation process is labor-intensive and time-consuming. Furthermore, when encountering new legal event types, it becomes necessary to label additional data, making existing datasets less reliable.\nIt is crucial to develop an event extraction method that minimizes the reliance on extensive manual annotation."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Event Grounded Generation for Criminal Court View",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Problem Definition",
            "text": "Here, we explore the problem of criminal court view generation. We first clarify the definitions of the terms as follows:\nFact description  is the identified facts in a case including several events, where  denotes the i-th token.\nEvent set  consists of  events of the fact, where  contains  tokens and each event is a subsequence of the fact.\nCourt view is the summary of the fact which consists of the charge  and rationales . Among them, the rationale is concluded from the fact in order to determine and support the judgment results, such as sentencing.\nIn this work, we assume the charge is available, and we only focus on generating rationales in court views, where the charge can be easily obtained by the judge or the charge prediction systems (Zhong et al., 2018  ###reference_b44###; Yue et al., 2021a  ###reference_b40###; Zhang et al., 2023  ###reference_b43###).\nThen, based on the above definitions, our problem is defined as:\nProblem 1 (Court View Generation).\nGiven the case fact , our goal is first to extract several events  from the case fact, and then generate the rationales  in court views, where the gold events are unavailable."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Architecture of EGG",
            "text": "Our proposed Event Grounded Generation (EGG) for criminal court view method consists of two phases, cascading the event extractor and the court view generator.\nSpecifically, in the extraction phase, we first train a LLMs-based QA model which can extract a subsequence of the text input as the answer to the prompts (or questions).\nAfter the model is trained, we consider this model as the event extractor to select several events from the case fact by introducing annotated legal event-related questions.\nFinally, we employ a PLMs-based court view generator to generate court views by merging the fact and event as the new text input."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2. Court view generator",
            "text": "Previous models generate court views based solely on case fact. In this section, our court view generator designs a strategy to incorporate extracted event information into the fact to yield more plausible court views, where we adopt the BART (Lewis et al., 2020  ###reference_b14###) as our backbone by considering the advantages of the current PLMs. Specifically, we merge the event and fact descriptions to form new input  of the court view generator.\nIn practice, limited by the maximum length of the PLMs, we enforce the events to be placed before facts (i.e., ), where \u201c\u201d represents the process of mergers."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3. Training and Inference",
            "text": "In this section, we describe the training loss in our proposed method EGG. Specifically,\nin the extraction phase, we employ Eq(1  ###reference_###) to train our LLMs-based event extractor.\nIn the generation phase, we adopt the negative log-likelihood loss to optimize the generator:\nwhere  is the trainable BART parameters,  denotes the -th token of  and  is the tokens before .\nDuring the inference phase, given a description of case fact, we first use the LLMs-based event extractor to extract the events from the case fact. Then, we generate the court view based on both the facts and events."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. : LLMs-free EGG with contrastive constraints",
            "text": "Indeed, the use of LLMs for event extraction in the extraction phase of EGG can lead to increased computational burden during the inference phase. This limitation hampers the practical application of the model in real-world scenarios. To overcome this challenge, we propose an LLMs-free EGG method that employs contrastive constraints, enabling court view generation without the need for event information during the inference phase."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. Architecture of",
            "text": "During training,  follows the extractor-generator framework.\nSpecifically, in the extraction phase, similar to EGG, we still employ LLM to extract events.\nIn the generation phase, unlike the previous EGG of combining event and fact as inputs to the generator, we use event as a kind of auxiliary information to assist the model in generating court views based on fact. In particular, as shown in Figure 4  ###reference_###, given the fact and event, we first employ the fact encoder and event encoder to encode both fact  and event  as the corresponding representations  and , where  is the dimensional size.\nThen, we feed the fact representation into the decoder for court view generation. In practice, we use the encoder and decoder of BART to achieve the above implementation.\nSubsequently, to enable the fusion of event information into , we employ a novel contrastive learning strategy during the training phase. This strategy aims to teach the fact encoder to memorize the co-occurrence event signals within its parameters, allowing the fact encoder to inject event clues into fact representations during the inference phase.\nIn particular, during the training phase, as shown in Figure 4  ###reference_###, we adjust the parameters of the fact encoder based on the event encoder to maximize the mutual information between the case fact and event. To achieve this objective, for a training fact representation , we build its positive sample set using its corresponding event  (referred to ), i.e., ,\nand its negative sample set  where  denotes each batch of event samples.\nTo teach the fact encoder to memorize the co-occurrence event signals, we define the contrastive loss following the concept of InfoNCE (Oord et al., 2018  ###reference_b23###). The contrastive loss is formulated as:\nwhere  represents the similarity measure between the fact representation  and the event representation , and  is a temperature parameter that controls the sharpness of the probability distribution.\nBesides, we set the fact encoder and the event encoder to share parameters to save GPU memory. According to our experiments, separate encoders and shared encoders do not have a significant difference in the generation performance."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Training and Inference",
            "text": "In the training process, since  uses the same extractor as EGG, we use Eq(1  ###reference_###) to train the extractor. Besides, the final objective of the generator in  is defined as:\nwhere  is the adjusted hyperparameter.\nDuring the inference phase, since the fact encoder learns to capture co-occurrence signals with the event through contrastive constraints,  can ignore the event as the input, enabling the generation of contextually relevant court views based solely on the case fact.\nBy leveraging contrastive constraints, our proposed method eliminates the reliance on LLMs for event extraction in the inference phase. This approach significantly reduces the computational burden, making the model more suitable for real-world applications."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Experiments",
            "text": "To evaluate the effectiveness of EGG, we conduct experiments to answer the following research questions:\nRQ1: How effective are EGG and  in improving the performance of event extraction and court view generation?\nRQ2: How efficient is  during the inference phase?\nRQ3: What are the performances of EGG by the length of court views?\nRQ4: How do EGG and  perform in human evaluation?\nRQ5: What is the court view generated by EGG to a specific case fact?"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1. Datasets",
            "text": "In the extraction phase, we adopt the criminal cases in CJRC333https://github.com/china-ai-law-challenge/CAIL2019  ###reference_CAIL2019### (Duan et al., 2019  ###reference_b7###) as the training data, where we process CJRC into the format of an instruction dataset.\nFigure 2  ###reference_### is an example from the CJRC and the instruction dataset.\nIn the generation phase, following (Yue et al., 2021b  ###reference_b41###), we conduct experiments on CJO444https://github.com/bigdata-ustc/C3VG  ###reference_###, where CJO is collected from the published legal documents in China Judgments Online555https://wenshu.court.gov.cn  ###reference_wenshu.court.gov.cn###.\nDetailed dataset statistics are shown in Table 1  ###reference_###.\nAmong them, since there exist 62 types of cases, we ask three law expects to annotate questions for each case type, for a total of 558 questions."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2. Experimental Setup",
            "text": "In this section, we present the detailed experimental setup of our proposed EGG. First, in the extraction phase, we adopt Baichuan-7B (BaiChuan-Inc, 2023  ###reference_b3###) as the backbone of the LLMs-based event extractor . Then, we employ the LoRA to parameter-efficient fine-tune it on the instruction dataset.\nFor training, we adopt an AdamW optimizer (Loshchilov and Hutter, 2019  ###reference_b20###) with an initial learning rate of 1e-5, then we set the maximum sequence length as 512 and the batch size as 4. Besides, the rank of LoRA is set to 4.\nIn the generation phase, we employ BART (Lewis et al., 2020  ###reference_b14###) to generate the court views. We set the learning rate to 1e-4 and the batch size to 8, and  in  to 1.\nFor evaluation, we adopt macro-average F1 as our metric to evaluate the performance of the LLMs-based event extractor  in the test set of CJRC.\nBesides, since there exist no gold events in CJO, we assume that the better the generated court views perform, the more effective events are extracted.\nTo this end, to evaluate the performance of the generation, we adopt ROUGE (Lin, 2004  ###reference_b18###) and BLEU (Papineni et al., 2002  ###reference_b25###) as the metrics. Among them, we report F1 scores of ROUGE-1, ROUGE-2, and ROUGE-L, and we keep the result of BLEU-1, BLEU-2 and BLEU-N (i.e., an average score of BLEU-1, BLEU2, BLEU-3, and BLEU-4)."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "5.3. Comparison methods",
            "text": "In this section, to evaluate the generated court view, we employ three type of baselines. First, we compare EGG with several traditional baselines:\nAttS2S (Bahdanau et al., 2014  ###reference_b2###) is an attention-based sequence-to-sequence model, following an encoder-decoder framework.\nPGN (See et al., 2017  ###reference_b27###) employs a pointer network to solve the out of vocabulary (OOV) problem in the text generation.\nTransformer (Vaswani et al., 2017  ###reference_b32###) has been widely implemented to generate texts.\nLabel-AttS2S (Ye et al., 2018  ###reference_b37###) is designed to generate court views by introducing the charge semantics into AttS2S.\nC3VG (Yue et al., 2021b  ###reference_b41###) separates the case fact into two parts with an extract-generate framework to generate the court views.\nThe above baselines are implemented with GRU (Cho et al., 2014  ###reference_b5###) or transformer.\nFor a fair comparison, the results of the above baselines are directly taken from (Yue et al., 2021b  ###reference_b41###).\nBesides, since the pre-training models have promoted the text generation in recent years, we introduce several approaches based on the pre-training models:\nBART (Lewis et al., 2020  ###reference_b14###) is a Transformer-based pre-training sequence-to-sequence model, which achieves promising results in text generation.\nIn this paper, BART(Fact) denotes BART takes the case fact as the input. BART(Event) represents taking the extracted event as the input.\nC3VG with BART (Yue et al., 2021b  ###reference_b41###) implements C3VG with BART as the backbone.\nFinally, we also compare LLMs baselines with EGG:\nBaichuan-7B (BaiChuan-Inc, 2023  ###reference_b3###) is a large language model which achieves competitive results in Chinese intelligence tasks.\nBaichuan-7B(Fact) employs LoRA to fine-tune Baichuan-7B by taking the case fact as the input with the form of the instruction dataset. Among them, the Instruction Input is: \u201cAssuming you are a judge, please summarize the facts of the case: [the description of case facts]\u201d, and the Instruction Output is the court views."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "5.4. Performance on Event Extraction and Court View Generation (RQ1)",
            "text": ""
        },
        {
            "section_id": "5.4.1",
            "parent_section_id": "5.4",
            "section_name": "5.4.1. Results of event extraction",
            "text": "In this section, we report the macro-average F1 to evaluate the performance of the LLMs-based extraction model  in the test set of CJRC. After statistics, the macro-average F1 is 84.6 which performs better than the original macro-average F1 (82.9) reported in the paper of CJRC (Duan et al., 2019  ###reference_b7###) which employs BERT (Kenton and Toutanova, 2019  ###reference_b13###) to achieve the answer extraction. This observation demonstrates the effectiveness of LLMs on extraction.\nHowever, our goal is employing the trained  to predict the potential events in our court view data CJO. Therefore, the F1 scores in the test set of CJRC fail to illustrate the effectiveness of our extraction sufficiently.\nTo further evaluate the extracted events,\nwe consider that the better the generated court view performs, the more effective the extracted events will be. The corresponding results are shown in section 5.4.2  ###reference_.SSS2###.\n###figure_5###"
        },
        {
            "section_id": "5.4.2",
            "parent_section_id": "5.4",
            "section_name": "5.4.2. Results of court view generation",
            "text": "To validate the effectiveness of EGG, we first compare it with several baselines. As shown in Table 2  ###reference_###, we find all methods that exploit PLMs outperform the traditional baselines implemented with GRU or transformer, which demonstrates the effectiveness of PLMs.\nThen, we can observe that both EGG and  perform better than other baselines in most metrics, which indicates our EGG can generate more plausible court views.\nSpecifically, compared with C3VG with BART, which groups the original fact into two type paragraphs to generate court views, our EGG significantly outperforms it. This observation demonstrates that incorporating fine-grained events into court view generation is more effective than employing coarse-grained paragraphs.\nWe also implement BART by taking fact and event as the text input, respectively.\nFrom the results, we observe that BART(fact) surpasses BART(Event) by a large margin, illustrating that there exist several events are not extracted. These observations prove that it is necessary to design an incorporated strategy to combine the case facts with the event information to generate court views.\nBesides, we observe that Baichuan-7B without instruction tuning performs well on the legal task in the zero-shot setting, which indicates that Baichuan-7B has already possessed court view abilities through training on a large amount of data. However, its results are still worse than the fine-tuned model (Baichuan-7B(Fact)), which also shows the necessity of fine-tuning LLMs to the court view generation.\nAlthough Baichuan-7B(Fact) achieves promising results on BLEU than EGG, it still performs worse on ROUGE and Bert-S. Meanwhile, EGG does not fine-tune LLMs in the generation phase, which further illustrates the effectiveness of EGG.\nFinally, we analyse the difference between EGG and .\nFrom Table 2  ###reference_###, EGG performs better than , which indicates that it is more effective to combine case facts and events directly and explicitly on the data side than to introduce events implicitly into the model structure.\nHowever, from the point of view of inference speed and computational resources occupied by the model,  is faster and occupies fewer computational resources, yet achieves similar results to EGG.\nThis observation illustrates the effectiveness of  which employs the contrastive learning constraint to incorporate event information into the learning of factual representations. In section 5.5  ###reference_###, we will further illustrate the efficiency of inference in ."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "5.5. Efficiency of Inference in  (RQ2)",
            "text": "In this section, we present the results of our experiments comparing the inference speed of our proposed  with other baselines. The hardware setup for the experiments consists of 12 cores of Intel(R) Xeon(R) Gold 5317 CPU and a single 40G NVIDIA A100 Tensor Core GPU. The findings are summarized in Table 3  ###reference_###.\nOur proposed  achieves an impressive decoding speed, approximately 12 times the speed achieved by EGG, which utilizes LLM for event extraction. It is worth noting that EGG has the slowest inference speeds.\nBesides, although the difference between the number of parameters in EGG and Baichuan-7B is not significant, since there are multiple events for a single case, EGG often needs to perform multiple event extractions, and thus is slower than Baichuan-7B.\nThis observation highlights that  strikes a balance between efficiency and effectiveness, making it well-suited for resource-constrained users.\nThe results demonstrate that  offers a practical solution for legal event extraction, providing efficient performance while maintaining effectiveness. Its suitability for resource-constrained users makes it a valuable option in real-world applications."
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "5.6. Performance by the Length of Court Views (RQ3)",
            "text": "In this section, we focus on investigating the generation performance of court views based on their length. We sample examples from the test set of CJO, where the real court views have lengths ranging from 50 to 120 tokens. We then predict and evaluate the generated court views by comparing them with the outputs of EGG and C3VG with BART using ROUGE-L and BLEU-4 scores.\nThe findings, as illustrated in Figure 5  ###reference_###, reveal that both EGG and C3VG with BART experience a degradation in performance as the length of court views increases. However, we observe that our EGG achieves the best performance when the court view length is between 60 and 70 tokens, with both ROUGE-L and BLEU-4 scores surpassing 90.\nFurthermore, our method outperforms C3VG with BART across all court view lengths, indicating the effectiveness of incorporating fine-grained event information into court view generation. This suggests that by considering the specific event details in the generation process, our approach can produce more accurate and higher-quality court views compared to existing methods."
        },
        {
            "section_id": "5.7",
            "parent_section_id": "5",
            "section_name": "5.7. Human Evaluation (RQ4)",
            "text": "Table 2  ###reference_### highlights that both EGG and  exhibit lower BLEU scores compared to Baichuan-7B(fact), prompting the need to investigate the performance of generated court views. To gain further insights, a human evaluation is conducted on the court views generated by EGG and Baichuan-7B(fact).\nIn this evaluation, a total of 100 examples are sampled, and three annotators with expertise in both computer science and law are asked to evaluate the generated court views based on two metrics: Usefulness and Fluency. Each metric is scored on a scale from 1 (lowest) to 5 (highest), with specific scoring standards provided in Table LABEL:human. The experimental results are presented in Table 5  ###reference_###.\nThe results indicate that all models achieve promising scores in terms of Fluency, indicating that the generated court views are fluent and well-formed. Additionally, it is observed that EGG and  outperform Baichuan-7B(fact) in terms of Usefulness. This finding further illustrates the effectiveness of incorporating fine-grained event information into court view generation. By considering the specific event details, our models generate court views that are deemed more useful by human evaluators."
        },
        {
            "section_id": "5.8",
            "parent_section_id": "5",
            "section_name": "5.8. Case Study (RQ5)",
            "text": "An example of extracted events and generated court views is shown in Figure 6  ###reference_###.\nSpecifically, firstly, the type of case in this fact is intentional injury. Then, Event-related Questions show all questions designed for the crime of intentional injury.\nIt is worth noting that the designed questions are the same for any fact which belongs to intentional injury.\nIn the Answers, we present the answers extracted from the fact description according to the questions by the LLMs-based extractor. Afterward, we post-process the questions and answers to obtain the corresponding Events.\n###figure_6### Next, we present 7 court views generated by EGG and baselines:\nWe can find although C3VG generates court views well, it fails to generate the court views about obtaining the victim\u2019s understanding.\nConversely, EGG can generate more plausible court views.\nBesides, EGG also yields injured another person with a knife which has been described in fact description but not in the real court view. This observation indicates EGG can generate several key information, which is ignored by the real court view.\nBesides, compared to BART(Event), we find it generates several unfaithful court views (the underlined) which do not exist in the case fact, illustrating that it is unfeasible to generate court views based solely on events, and we need to combine the fact and event for the generation.\nThen, although BART(Fact) has generated court views well, there are several omissions it generates compared to other methods. For example, EGG yields the injuries of the defendant is minor, however, BART(Fact) only generates the defendant has injuries (i.e., intentionally injured another person\u2019s body).\nMoreover, we can get a fluent court view by directly prompting Baichuan-7B (without fine-tuning). However, we tend to obtain some redundant information (e.g., \u201cIf you do not accept this judgment, \u2026 , directly to the [Province] Intermediate People\u2019s Court \u2026\u201d). This information has nothing to do with the court view, and may even involve some private information, such as [Province] Intermediate People\u2019s Court, where we have a privacy treatment for [Province] to show this example. Meanwhile, Baichuan-7B(fact) is not as accurate as EGG in yielding information about surrender, where Baichuan-7B(fact) only describes the defendant\u2019s surrender, while EGG also describes how the defendant surrendered (\u201cthe victim learned that others had called the police and waited at the scene to be arrested\u201d).\nThese observations demonstrate that incorporating fine-grained event information into the court view generation is effective."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6. Discussion",
            "text": "Ethical Discussion.\nCourt view generation has gained significant attention as a core task in legal intelligence.\nBased on the experimental results, EGG demonstrates the ability to generate more accurate court views. Additionally,  achieves a balance between modeling effectiveness and inference efficiency, making it suitable for users with limited computational resources.\nHowever, it is important to note that our model does not replace the work of judges. Instead, our aim is to assist judges in organizing court views and alleviate their workload. The final court views must be determined and decided upon by the judges themselves (Wu et al., 2020  ###reference_b34###; Yue et al., 2021b  ###reference_b41###). Our work serves to provide judges with a tool to streamline the process of collating court views and reduce their workload stress.\nLimitations.\nWhen extracting events, we simply post-process the extracted answers and questions to obtain the corresponding events. However, when dealing with complex relationships among events, such as causality, a more advanced approach is needed. One possible solution is to construct an event graph that represents the relationships among events.\nAn event graph is a graphical representation where events are nodes, and the relationships between events are represented by edges. By incorporating this event graph into the court view generation process, the model can better capture and understand the complex relationships among events.\nWe will leave it as the future work."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7. Conclusion",
            "text": "In this paper, we proposed an Event Grounded Generation (EGG) method for criminal court view generation with cooperative (Large) Language Models, cascading the event extractor and the court view generator.\nTo be specific, EGG first employed a trained LLMs-based legal event extractor to select several events in the case fact without massive annotated events.\nThen, in the court view generator, we incorporated these events into the court view generation by merging the case fact and event as the new input.\nBesides, to alleviate the computational burden in EGG during inference that employs LLMs, we further proposed a LLMs-free EGG method based on the contrastive constraint. This enhancement enables court view generation without requiring event information during the inference phase.\nExperimental results on a real-world dataset clearly demonstrated the effectiveness of our proposed method.\nAcknowledgements. This research was supported by grants from the National Natural Science Foundation of China (Grants No. 62337001, 623B1020) and the Fundamental Research Funds for the Central Universities."
        }
    ],
    "url": "http://arxiv.org/html/2404.07001v3",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.3",
            "4",
            "4.1",
            "4.2"
        ],
        "main_experiment_and_results_sections": [
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "5.4.1",
            "5.4.2",
            "5.5",
            "5.6",
            "5.7",
            "5.8"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.4.1",
            "5.4.2",
            "5.5",
            "5.6",
            "5.7",
            "5.8"
        ]
    },
    "research_context": {
        "paper_id": "2404.07001v3",
        "paper_title": "Event Grounded Criminal Court View Generation with Cooperative (Large) Language Models",
        "research_background": "### Criminal Court View Generation with Cooperative (Large) Language Models\n\n**Introduction:**\n\n**Motivation:**\nThe paper is motivated by the need to optimize the generation of criminal court views, which are concise representations of case facts and explanations for rendered verdicts. This automation has the potential to reduce the workload of legal professionals and provide useful legal assistance to non-professionals.\n\n**Research Problem:**\nExisting methods in criminal court view generation either utilize domain-specific models or large language models (LLMs). However, domain-specific models often partition the case facts into broad categories, which proves inadequate due to the complexity of the information. Additionally, fine-tuning LLMs directly on this task has not yielded satisfactory results, likely because of the intricate nature of case facts and the lack of additional legal knowledge. There remains a significant computational burden in using multiple pre-trained language models (PLMs) collaboratively within these approaches.\n\n**Prior Work:**\nPrior work includes domain-specific models like C3VG, which use pre-defined crime circumstances categorized into verdict-related and sentencing-related information to generate court views. However, the approach is coarse-grained, insufficiently addressing the diverse elements of case facts. Large language models (LLMs) like Baichuan-7B have been considered for fine-tuning but face challenges due to the complexity of the task. Furthermore, datasets for legal event extraction exist (Feng et al., 2022; Yao et al., 2022) but require extensive annotation for each new case type, hindering practical applications.\n\n**Proposed Approach:**\nThe paper proposes an Event Grounded Generation (EGG) method, which integrates fine-grained event information into the court view generation process by leveraging collaborative (large) language models. This method adopts an extract-generate framework, refining the extraction process through an LLM-based extractor fine-tuned on a legal QA dataset (CJRC). The method only necessitates annotating event-related questions for each case type (e.g., Mobbing), enabling a zero-shot event extraction approach. Following extraction, the events and facts are spliced together to form inputs for a PLM-based generator to yield the court views. To alleviate computational burdens, an improved LLM-free version of EGG uses contrastive learning to capture co-occurrence signals, thereby eliminating the need for event extraction during inference.\n\n**Contributions:**\n1. Introducing the EGG method, incorporating fine-grained event information into criminal court view generation using cooperative (large) language models.\n2. Proposing a zero-shot legal event extraction approach with minimal human annotation.\n3. Developing an LLM-free EGG method to reduce computational demands during inference, enhancing practicality for both legal professionals and laymen.\n4. Validating the effectiveness of the approach through extensive experiments on a real-world dataset, comparing it with several competitive methods.",
        "methodology": "The proposed Event Grounded Generation (EGG) method for generating criminal court views involves two distinct phases: the extraction phase and the generation phase. Here is a detailed description of each phase's key components and innovations:\n\n1. **Extraction Phase:**\n   - **LLMs-based QA Model Training:** Initially, a large language model-based question answering (LLMs-based QA) model is trained. This model is designed to extract specific subsequences of a text input as responses to particular prompts or questions.\n   - **Event Extraction:** Once the QA model is adequately trained, it functions as an event extractor. It identifies and selects several relevant events from the case fact by responding to pre-defined, annotated legal event-related questions.\n\n2. **Generation Phase:**\n   - **PLMs-based Court View Generator:** A pre-trained language model (PLMs) is then employed as the court view generator. This generator creates court views by synthesizing the extracted events and the case facts into a cohesive and comprehensive new text input.\n\nBy cascading these two phases, the EGG method innovatively combines the capabilities of large language models for question answering with pre-trained language models for text generation, effectively producing detailed and contextually grounded criminal court views.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\nIn the extraction phase, the criminal cases from the China Judgments Research Challenge (CJRC) are used as the training data. The criminal cases from CJRC are processed into the format of an instruction dataset.\n\nIn the generation phase, experiments are conducted following the methodology of Yue et al. (2021b) on the China Judgments Online (CJO) dataset. This dataset is collected from the published legal documents available on China Judgments Online. \n\nSince there are 62 types of cases within the dataset, three law experts were asked to annotate questions for each type, resulting in a total of 558 annotated questions. \n\n**Datasets:**\n- Training Data: CJRC (China Judgments Research Challenge).\n- Evaluation Data: CJO (China Judgments Online).\n\n**Baselines and Metrics:**\nThe document does not detail the specific baselines and evaluation metrics used in this summarized section. However, it can be inferred that the baselines would be standard models and methods related to legal document analysis and generation, and the evaluation metrics would typically measure the accuracy and relevance of the generated views in the legal context.\n\nThe specific main experimental results are not provided in this summary. However, it would generally include performance measures (like precision, recall, or other legal text relevance metrics) compared against baselines, highlighting improvements or areas of success for the proposed methods."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the performance of the LLMs-based extraction model on event extraction from legal case data.",
            "experiment_process": "The macro-average F1 score of the LLMs-based extraction model was calculated on the test set of CJRC. Comparisons were made against previous results reported in the CJRC paper, which used BERT for answer extraction.",
            "result_discussion": "The LLMs-based model achieved a macro-average F1 of 84.6, outperforming the original CJRC paper's F1 of 82.9, demonstrating the effectiveness of LLMs on extraction. However, further evaluation on the court view data CJO is necessary to sufficiently illustrate the effectiveness of the extracted events.",
            "ablation_id": "2404.07001v3.No1"
        },
        {
            "research_objective": "Validate the effectiveness of the Event Grounded Generation (EGG) method compared to other baselines in court view generation.",
            "experiment_process": "EGG was compared to several baselines including GRU and transformer models, as well as methods utilizing pre-trained language models (PLMs). Performance was measured on various metrics and included implementations of BART with facts and events as input.",
            "result_discussion": "All methods using PLMs outperformed traditional baselines. EGG performed better than other baselines in most metrics, demonstrating the effectiveness of incorporating fine-grained events. BART(fact) significantly outperformed BART(Event), indicating that not all events were extracted. Baichuan-7B showed promising results, but fine-tuning improved performance, particularly in ROUGE and Bert-S metrics, further illustrating the effectiveness of EGG.",
            "ablation_id": "2404.07001v3.No2"
        },
        {
            "research_objective": "Compare the inference speed of different methods, particularly focusing on the efficiency of the proposed LLM-free EGG method.",
            "experiment_process": "Inference speed was compared across models using a hardware setup with 12-core Intel Xeon Gold 5317 CPU and a single 40G NVIDIA A100 Tensor Core GPU.",
            "result_discussion": "The proposed LLM-free method achieved a decoding speed approximately 12 times faster than EGG (which utilizes LLMs for event extraction). Although EGG had the slowest inference speeds, the newly proposed method balanced efficiency and effectiveness, making it suitable for resource-constrained users.",
            "ablation_id": "2404.07001v3.No3"
        },
        {
            "research_objective": "Investigate the performance of court view generation relative to the length of the court views generated.",
            "experiment_process": "Samples from the CJO test set with court views ranging from 50 to 120 tokens were evaluated using ROUGE-L and BLEU-4 scores, comparing results to predictions by EGG and C3VG with BART.",
            "result_discussion": "Both EGG and C3VG with BART showed performance degradation with longer court views. EGG achieved the best performance for court views between 60 and 70 tokens, surpassing 90 in both ROUGE-L and BLEU-4 scores. EGG consistently outperformed C3VG with BART, indicating the effectiveness of using fine-grained event information.",
            "ablation_id": "2404.07001v3.No4"
        },
        {
            "research_objective": "Conduct a human evaluation to assess the usefulness and fluency of generated court views.",
            "experiment_process": "Human evaluation involved 100 examples assessed by three annotators with computer science and law expertise. The generated court views from EGG and Baichuan-7B(fact) were rated on a scale from 1 to 5 for Usefulness and Fluency.",
            "result_discussion": "All models achieved high fluency scores, indicating well-formed court views. EGG and the proposed model outperformed Baichuan-7B(fact) in Usefulness, demonstrating that fine-grained event information led to more useful court views.",
            "ablation_id": "2404.07001v3.No5"
        },
        {
            "research_objective": "Perform a case study to demonstrate the effectiveness of incorporating fine-grained event information into court view generation.",
            "experiment_process": "An example case of intentional injury was used to illustrate extracted events and seven generated court views by EGG and other baselines. Comparisons were made between different methods, including C3VG and BART implementations.",
            "result_discussion": "EGG generated more plausible and comprehensive court views compared to baselines. It provided additional key information not found in the real court view and generated more accurate descriptions compared to methods relying solely on events. Baichuan-7B without fine-tuning added redundant and private information, whereas EGG gave detailed accounts, demonstrating its effectiveness in incorporating fine-grained events.",
            "ablation_id": "2404.07001v3.No6"
        }
    ]
}