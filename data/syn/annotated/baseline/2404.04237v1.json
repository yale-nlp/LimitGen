{
    "title": "Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents",
    "abstract": "The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks. This has enabled many downstream applications, such as LLM agents, to rely on their sophisticated reasoning to navigate complex task requirements. However, LLMs are known to unexpectedly falter in simple tasks and under seemingly straightforward circumstances - underscoring the need for better and more diverse evaluation setups to measure their true capabilities. To this end, we choose to study compositional and conditional reasoning, two cornerstones of human cognition, and introduce GroundCocoa - a lexically diverse benchmark connecting these reasoning skills to the real-world problem of flight booking. Our task involves aligning detailed user preferences with available flight options presented in a multiple-choice format. Results indicate a significant disparity in performance among current state-of-the-art LLMs with even the best performing models not exceeding 67% accuracy despite advanced prompting techniques.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Advanced reasoning in LLMs has significantly influenced and effected the rise to prominence of language agents adept at handling a diverse range of tasks. Among them, web agents have demonstrated potential in automating web-based tasks such as flight booking. While such success through widespread application of LLMs is indeed promising, it necessitates ongoing exploration of their core reasoning proficiencies. Conditional and compositional reasoning play a crucial role in our ability to understand and interact with complex systems through elaborate decision-making. Conditional reasoning involves the comprehension and application of logical rules that are typically structured in \u201cif-then\u201d formats. It is critical to personal decision-making in everyday life through an evaluation of potential scenarios and anticipation of consequences. Compositional reasoning, on the other hand, is the ability to combine solutions to simpler sub-problems, and integrate them in a structured manner to solve a more complex task. This cognitive process entails understanding the interplay between different sub-problems. Our paper focuses on assessing how well current LLMs encapsulate these essential cognitive functions, which are integral to both human intelligence and advanced artificial intelligence systems. To that end, we introduce GroundCocoa - a benchmark for evaluating compositional & conditional reasoning in a grounding task.\n\nGroundCocoa is made up of questions framed as user needs, set within a real-world inspired flight reservation scenario. User requirements might be many or could be highly convoluted - leading to higher compositional and conditional complexity respectively. We leverage a controllable method to create samples of varying complexity. Our data generation process consists of a 5-stage pipeline including online scraping, constraint generation, and symbolic logic to impose conditionality. In order to test for robustness, we allow requirements to freely condition on one another and impose no restrictions on their nature. Additionally, we isolate a subset of more atypical queries that contain unconventional user needs (e.g., \u201dI want at least 2 layovers\u201d) and evaluate their impact on model performance. Samples in GroundCocoa may also require reasoning about time (e.g., when constraints are imposed on arrival or departure times) and arithmetic (e.g., when constraints are imposed on ticket price), thus, integrating logical, temporal, mathematical, and compositional reasoning.\n\nThe statistics of our dataset are shown in Table 1. Our key findings are as follows: Accuracy among contemporary LLMs varies greatly, ranging from a little better than random guess to about 67% on a five-option multiple-choice question task. Conditional reasoning poses a significant challenge to all evaluated models, even on samples of relatively lower complexity.\n\nIncluding unconventional user requirements leads to a drop in accuracy in models, indicating a pretraining bias towards more typical needs."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Approach",
            "text": "In the process of generating a natural language user requirement for flight booking, we are faced with the following considerations:\n\nConditionality of Constraints: We wish to challenge contemporary models in their ability to reason through scenarios characterized by conditional complexity. This is done through mutual dependence of flight features which we refer to as slots. In the example illustrated in Figure 2, there is an inter-dependence between the values for price and ticket class. The interplay between constraints corresponding to the different slots is represented in logical form through a Product-of-Sums (POS) expression. A POS expression consists of multiple OR operations (sums) which are later combined through AND operations (products). The inclusion of OR operations between slots introduces conditional complexity to our user requirement, necessitating consideration of potential slot values in if-then scenarios. On the other hand, a greater number of AND conditions implies a higher number of variables that a model has to simultaneously reason over, resulting in increased compositional complexity.\n\nSatisfiability of POS Expression: While generating the logical form for a user requirement, we must ensure satisfiability of the generated POS expression. For this, we use SymPy - an open-source Python symbolic mathematics library which generates an optimal POS expression given a minterm table. This is further described in Section 2.2.\n\nFuzziness in Slot Values: Corresponding to each occurrence of a slot in the POS expression there has to be a unique constraint. For the example in Figure 2, the two constraints on the price slot are provided. These constraints are randomly imposed through specialized rule-based systems corresponding to each slot. However, these might cause the final user criteria to become impossible to satisfy even if the corresponding POS expression is satisfiable. Thus, for a generated user requirement, we check our flight data to ensure that there exists at least one route that satisfies the criteria and at least four that do not. This way, we ensure there is at least one positive and four negative options for a generated requirement.\n\nStatistics of GroundCocoa are shown in Table 1. We also include a separate validation set which may be used for tuning hyperparameters. Our five-stage data creation pipeline is detailed in the subsequent sections."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Flight Data Collection",
            "text": "We use a list of the top 50 busiest airports by passenger traffic derived from Wikipedia. Source and destination airports are chosen randomly from this list and input to Google Flights with the departure date set for April 17, 2024. A small number of flights are sampled from search results for each source-destination pair. The sampled flights are chosen from each of economy, business, and first class and, for each flight option, all the relevant details such as the number of layovers, price, departure and arrival times etc. are saved. A sample flight schema with all the elements is provided in Appendix A. The entire data collection process is handled through web scraping using Selenium Webdriver."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Product-of-Sums Generation",
            "text": "To generate a POS expression, we first randomly select a small number of flight features or slots. The complete set of slots for any given flight is as follows: {airline, ticket class, departure time, arrival time, total travel time, number of layovers, average carbon emission difference, travel date, price, layover locations, layover times}. We vary the number of slots between 2 and 6 in order to generate samples of differing complexity. We then randomly generate 2-3 \"minterms\" - the list of all input combinations of slots that generate a \"1\". A higher number of minterms results in a greater conditional complexity and leads to more convoluted user requirements. The slot symbols and generated minterms are input to SymPy which uses a redundant-group eliminating algorithm to output the smallest POS expression consistent with the minterm table."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Primitive Generation",
            "text": "Corresponding to each slot, we have developed a rule-based system that randomly imposes constraints on its values. These constraints are converted to natural language through templates. Since a POS expression may contain a negation, we generate two primitives at each turn - one for the constraint and one for its negation. A sample primitive for total travel time is shown in Table 2. At this stage, we also isolate samples that include any one of the following three primitives - (1) carbon emissions must be above the average for that route, (2) price of the flight must be above a minimum threshold, and (3) number of layovers on the route should be greater than a minimum. While this list is not exhaustive, such samples (henceforth referred to as \u201catypical\u201d queries) are able to successfully encapsulate contrarian needs that are unlikely to manifest often during pretraining."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "LLM Paraphrasing and Human Validation",
            "text": "We carry out LLM paraphrasing in two distinct steps described below. The exact prompts and an example of intermediate results are provided in Appendix D. We manually verify each query to ensure it is consistent with the primitives and make changes wherever necessary. Individual primitives are substituted into each sum term and combined using templated rules. Next, we combine the individual sum terms into a product (logical AND). This is done by merging the individual paraphrases of sum terms, separated by periods."
        },
        {
            "section_id": "2.5",
            "parent_section_id": "2",
            "section_name": "Option Matching",
            "text": "We match the generated user requirements with the flight data collected in Section 2.1. Each route between the source and destination represents a potential choice in our multiple-choice dataset. Choices are divided into subsets containing one positive (matching the user requirement) and four negative (not matching the user requirement) options. This is done to ensure that each multiple-choice question has only a single correct answer for ease of evaluation. Many such subsets may be created from a single user requirement and, consequently, our dataset consists of queries repeated multiple times with differing choices. Details on the number of unique queries and overall samples is provided in Table 1."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Results & Analysis",
            "text": "To measure performance on GroundCocoa, we test several models of different sizes ranging from open-source to closed-source, including Mixtral 8x7B - Instruct (Jiang et al., 2024) / Mistral 7B Instruct(Jiang et al., 2023), and Gemini Pro(Team et al., 2023). Results from our experiments are shown in Table 3.\n\nGroundCocoa presents a substantial challenge for each of the evaluated models. We assess the overall model performance and investigate the consequences of varying the complexity of user criteria and presenting relatively unconventional user needs."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Impact of Increasing Complexity",
            "text": "In our analysis, we observe the performance of the models tested on GroundCocoa across different levels of conditional and compositional complexity. In their recent work on assessing the limitations of transformer models on compositional tasks, Dziri et al. (2023) use computational graphs as approximations of the underlying reasoning processes in such models. They define the terms reasoning depth, the length of the deepest layer in the computational graph from the source nodes, and reasoning width, the mode of number of nodes in each layer - indicating the extent of multi-hop reasoning and compositional parallelism required to solve a given problem. Considering the characteristics of GroundCocoa, we focus on reasoning width - the number of variables a model has to simultaneously reason over for a given problem. Intuitively, this may be represented by the number of slots used during the generation of a particular sample as described in Section 2.2. However, keeping the number of rows in the minterm table constant while increasing the slots may often lead to lower conditional complexity as the number of slots is increased.\n\nIn order to effectively gauge the compositional and conditional complexity of a sample in our dataset, we define a dependency graph derived from the POS expression corresponding to that sample. Vertices represent slots and a dependency (edge) is created when a particular slot co-occurs with another slot within a sum term in the POS. A sample POS expression and its corresponding dependency graph are shown in Figure 3. The graph has 3 connected components with the largest connected component (LCC) of size 4. The maximum degree is 2 which corresponds to the two connections for nodes LayoverTime and TicketClass.\n\nGiven a fixed-schema for the flight options, the number of sum terms in the POS expression as well as the LCC in the dependency graph are indicative of the reasoning width and, in turn, the compositional complexity of the user criteria. The LCC is the length of the largest chain of slots - the possible values of which are dependent on one another through OR conditions (represented by edges in the dependency graph). This metric effectively reflects the breadth of parallel computation or reasoning width required to accurately infer the given user criteria. Since increased branching in the dependency graph suggests a greater conditional complexity in user criteria, we also analyze model performance with increasing maximum degree of the dependency graph. This gives us the extent of conditioning on a single slot value. In Figure 4, we observe the decline in model performance with increased complexity as indicated by these factors."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Quantifying Confusion in Answer Choices through Entropy",
            "text": "Numerous recent studies have explored how deep learning models, specifically transformer-based architectures, achieve success by exploiting shortcuts (Geirhos et al., 2020; Liu et al., 2022; Tang et al., 2023; Du et al., 2023) and relying on spurious correlations present in the training data (Zhang et al., 2023; Saparov & He, 2023; Saparov et al., 2023). Most recently, Dziri et al. (2023) utilized relative information gain of individual output elements in partially correct answers to explain surface pattern understanding in LLMs. In the same vein, we employ entropy as a metric to measure the confusion that might be caused due to conditions in the user query for a given flight option. We do this in an attempt to demystify how language models may succeed at some and fail at other queries with similar levels of complexity. In order to illustrate this, we take an example user requirement, and two hypothetical and simplified flight options as shown in Figure 5. Additionally, we show the reasoning path that must be navigated in each case for a successful outcome.\n\nWe observe how option B in our example leads to a more convoluted reasoning path, whereas the model is able to bypass considerable conditional overhead in the case of Option A. For the purpose of quantifying this more generally, we observe the compositional primitives (values attached to individual slots in the POS expression) in each sample and attach a binary value indicating if the primitive is satisfied. For the example in Figure 5, we show the primitives and the corresponding values of both options in Figure 6. We also show the probability of a primitive being satisfied and being unsatisfied by the flight option under consideration, as well as the final entropy.\n\nEntropy due to user criteria for each option can then be computed using the formula in Equation 1. Higher uncertainty leads to greater entropy in Option B as opposed to Option A, indicating a greater conditional overhead.\n\nIn our analysis, we take the entropy values of the correct answer choice for each sample. Entropy gives us yet another measure of conditional complexity from the perspective of the answer choices rather than just the query, and helps explain why a model might exhibit inconsistent results across user queries of similar complexity."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Robustness to Unconventional User Needs",
            "text": "Several contemporary studies have sought to examine the robustness of language models by studying their resilience to out-of-distribution data (Koh et al., 2021  ###reference_b22###; Wang et al., 2023  ###reference_b52###) or through adversarial attacks and input perturbations (Gardner et al., 2020  ###reference_b11###; Goel et al., 2021  ###reference_b13###; Subhash et al., 2023  ###reference_b44###; Sanyal et al., 2022  ###reference_b35###; Yuan et al., 2023  ###reference_b55###). In our work, we challenge models through atypical user requirements in order to assess bias from pretraining and robustness to unorthodox and nontraditional queries. We segregate queries into \u201dRegular\u201d and \u201dAtypical\u201d groups as described in Section 2.3  ###reference_###. In Table 3  ###reference_###, we contrast model performance on samples that describe such unconventional user needs versus those that do not. While most models in our testing show a decay in performance, the impact is more noticeable on better performing models."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Reasoning Challenges in NLP. Our work extends the existing line of research on evaluating natural language processing (NLP) systems on different facets of reasoning - most notably commonsense question-answering (Talmor et al., 2019  ###reference_b46###; Huang et al., 2019  ###reference_b19###), physical reasoning (Bisk et al., 2020  ###reference_b3###), social interaction (Sap et al., 2019  ###reference_b36###), mathematical reasoning (Cobbe et al., 2021  ###reference_b6###; Amini et al., 2019  ###reference_b1###; Miao et al., 2020  ###reference_b27###; Hendrycks et al., 2021b  ###reference_b16###), story completion (Zellers et al., 2019  ###reference_b56###), temporal reasoning (Zhou et al., 2019  ###reference_b59###; Tan et al., 2023  ###reference_b47###) abductive reasoning (Bhagavatula et al., 2020  ###reference_b2###) and pronoun resolution (Sakaguchi et al., 2021  ###reference_b34###) among others. Different from these benchmarks, GroundCocoa introduces a unique and substantial challenge for LLMs in the form of conditional and compositional reasoning.\nBenchmarks on Propositional Logic. GroundCocoa also aligns with the considerable body of work on evaluating logical reasoning in language models. The RuleTaker (Clark et al., 2021  ###reference_b5###) and ProofWriter (Tafjord et al., 2021  ###reference_b45###) datasets proposed a modern approach to evaluating logical reasoning through a task involving assignment of binary labels to candidate implications following a set of premises expressed in natural language. The datasets emulate a linear deductive chain of reasoning of varying depths given a set of facts and rules, with ProofWriter augmenting this task through intermediate conclusions and proof generation. LogicNLI (Tian et al., 2021  ###reference_b50###) provides a more comprehensive diagnostic benchmark involving reasoning through all seven fundamental logics (conjunction, disjunction, negation, implication, equation, universal and existential quantifiers). It contains an additional \u201dparadox\u201d label implying a situation where both the hypothesis as well as its negative proposition can be simultaneously entailed to the premise through different reasoning paths. This facilitates a non-linear reasoning, but is still limited to two contradictory reasoning paths. The FOLIO (Han et al., 2022  ###reference_b14###) dataset boasts a higher vocabulary size due to a hybrid annotation approach but again consists of linear reasoning chains. Along similar lines, ProntoQA (Saparov & He, 2022  ###reference_b37###) proposes a first-order logic benchmark using a linear ontology which might be fictional. This is done to prevent LLMs from predicting correct outcomes through spurious correlations in their pretraining corpus.\nThe benchmarks described here are primarily focused on the evaluation of deductive reasoning. In contrast, GroundCocoa offers a more realistic grounding task with an emphasis on if-then reasoning which leads to many candidate reasoning paths for each answer choice. While deductive reasoning may involve a broader range of logical structures, conditional reasoning is a subset which deals specifically with the relationships and implications of conditional statements. Our dataset consists of a large vocabulary size and context length per sample, leading to greater linguistic diversity, and a higher reasoning width than other benchmarks in logical reasoning. Questions are designed to test for robustness against rare and unconventional user requirements and bring to the fore model bias from pretraining data. Also, unlike most other benchmarks, we do not attempt to evaluate logical reasoning in isolation - our task might require abilities such as temporal or mathematical reasoning.\nCompositional Generalization. Samples in GroundCocoa consist of novel combination of primitives expressed as user requirements in a flight-booking task. Such reasoning falls under the umbrella of compositional generalization - an area that has garnered increasing interest in the scientific community recently. Hosseini et al. (2022  ###reference_b18###) highlight the relative generalization gap with in-context learning between in-distribution and out-of-distribution samples in various semantic parsing tasks. Dziri et al. (2023  ###reference_b9###) demonstrates how transformer-based LLMs may solve compositional tasks by reducing them to linearized subgraph matching. By establishing a computational graph for each problem, the authors are able to define computational complexity by metrics such as the reasoning depth and width which correspond to levels in multi-hop reasoning and average parallelism respectively. Unsurprisingly, increased task complexity leads to a rapid decay in model performance under various settings.\nOur findings largely concur with previous literature on compositional reasoning. However, results on GroundCocoa reveal that even the most advanced LLMs struggle at relatively low levels of compositional complexity when juxtaposed with conditional reasoning and grounding. While Dziri et al. (2023  ###reference_b9###) demonstrated their results using problems such as multi-digit multiplication, dynamic programming, and Einstein\u2019s puzzle - we release a new dataset that is anchored on a practical, real world use-case of parsing complex user criteria and grounding to a fixed schema representing a flight option. GroundCocoa contains a high semantic coverage and we posit that it would be of interest to the NLP community as a hard evaluation set to benchmark compositional generalization in LLMs.\nDialogue-State Tracking. Finally, while our task is reminiscent of a single turn in a dialogue state tracking system, it goes one step further to test a language model\u2019s grounding ability to match a flight schema with the user query. Most schema-guided dialogue datasets (Rastogi et al., 2020  ###reference_b33###; Lee et al., 2022  ###reference_b23###) consist of fixed slot values and the filtering of available options is handled through external systems (e.g. api\u2019s). Slot values in GroundCocoa are fuzzy due to conditional constraints on the primitives. In Figure 5  ###reference_###, TicketPrice may take on different values based on TicketClass. GroundCocoa consists of examples with varying levels of compositional complexity due to long and complex user requirements. This differentiates it from the majority of schema-guided dialogue datasets where the primary objective is goal and slot identification, and tagging of slot values. These tasks, while challenging in their own respect, do not engage a models\u2019 compositional reasoning ability to the same extent."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Modern LLMs have demonstrated remarkable advancements in many tasks including those that require compositional and conditional reasoning, such as mathematical problem solving and code generation. However, distinguishing genuine reasoning from rote learning and shallow understanding remains a central focus of research. While LLMs have become adept at answering complex questions, they often struggle when faced with unfamiliar task settings. Although the size of a problem affects performance, even less complex samples in our dataset challenge the best language models today.\n\nWe introduce a new benchmark dataset and provide a detailed analysis of the effects of increasing complexity, including advanced prompting techniques and robustness to atypical queries. Our results reveal a significant disparity in the performance of different language models, a distinction not commonly seen in other evaluation benchmarks, highlighting their abilities to tackle novel challenges. The data generation process is mostly automatic, with human validation at the final step. Along with the dataset and evaluation script, we release code for data generation, which can be extended to create more examples, increasing diversity and complexity. The task can be further complicated, with minor modifications, by incorporating queries with multiple answers and questions that require other forms of logical reasoning such as aggregation or existential quantification, which we leave for future work."
        }
    ],
    "url": "http://arxiv.org/html/2404.04237v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "4"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4",
            "2.5"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1",
            "3.2",
            "3.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.04237v1",
        "paper_title": "Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents",
        "research_background": "### Motivation\nThe paper is motivated by the need to evaluate the reasoning capabilities of language models (LLMs) like GPT-4, particularly in the context of complex, real-world tasks such as flight booking. The proliferation of LLM-driven web agents capable of automating tasks highlights the importance of ongoing scrutiny into these models' core reasoning proficiencies, especially since advanced reasoning skills are integral to effective decision-making and interaction with elaborate systems. Specifically, the paper underscores the necessity to assess conditional and compositional reasoning, key components of both human and artificial intelligence, to ensure these models can manage intricate sub-problems and logical conditions.\n\n### Research Problem\nThe research problem addressed in this paper is the assessment of LLMs' ability to handle conditional and compositional reasoning within the context of flight booking tasks. The work introduces a new benchmark, GroundCocoa, designed to measure these reasoning skills using questions framed as user needs within realistic flight reservation scenarios. The goal is to determine how well current LLMs encapsulate these cognitive functions, particularly under conditions of varying complexity and with unconventional user requirements.\n\n### Relevant Prior Work\nThe paper builds on a wide array of prior research in language models and web agents:\n1. **Advanced reasoning in LLMs**:\n   - Shen et al. (2024)\n   - Wu et al. (2023)\n   - Schick et al. (2024)\n   - Significant Gravitas\n   - Osika (2023)\n2. **Web agents and automation**:\n   - Hong et al. (2023)\n   - Furuta et al. (2023)\n   - Deng et al. (2024)\n   - Zheng et al. (2024)\n3. **Importance of conditional and compositional reasoning**:\n   - Oaksford & Chater (2010)\n   - Simon & Newell (1971)\n4. **Chain of Thought (COT) prompting and its effects**:\n   - Wei et al. (2022)\n   - Hendrycks et al. (2021b)\n   - Madaan & Yazdanbakhsh (2022)\n   - Nogueira et al. (2021)\n   - Qian et al. (2023)\n\nThese studies provide foundational insights and methodologies for evaluating and improving the reasoning competencies of language models, which the current paper extends by focusing on the complexities involved in flight booking tasks.",
        "methodology": "The proposed method for generating natural language user requirements for flight booking hinges on conditional and compositional reasoning. The method can be broken down into several key components and innovations:\n\n1. **Conditionality of Constraints**:\n   - The method challenges contemporary models to navigate scenarios featuring conditional complexity. It achieves this by introducing inter-dependence between flight features (referred to as slots), such as price and ticket class.\n   - This interplay of constraints is represented using a Product-of-Sums (POS) expression. POS expressions consist of multiple OR operations (sums) combined through AND operations (products). The OR operations introduce conditional complexity, while more AND conditions increase the number of variables the model must reason over, thereby enhancing compositional complexity.\n\n2. **Satisfiability of POS Expression**:\n   - The generation of logical forms for user requirements ensures the satisfiability of the resulting POS expressions.\n   - SymPy, an open-source Python symbolic mathematics library, is employed to generate the optimal POS expression from a minterm table. This part of the methodology ensures that the logical form is viable and maintains its logical integrity.\n\n3. **Fuzziness in Slot Values**:\n   - Each occurrence of a slot in a POS expression is paired with a unique constraint. These constraints are randomly imposed using specialized rule-based systems for each slot. This randomness can potentially make the user criteria impossible to satisfy even if the POS expression is satisfiable.\n   - To mitigate this, the constraint satisfaction is cross-verified with real flight data to ensure feasibility. Specifically, the system checks that at least one route meets the criteria and at least four do not, ensuring one positive and four negative options for each generated requirement.\n\n4. **Validation and Data Creation Pipeline**:\n   - Statistics of the GroundCocoa dataset and details of a separate validation set are provided to assist in tuning hyperparameters.\n   - The data creation pipeline is elaborated upon in subsequent sections, outlining a 5-stage process to generate the dataset.\n\nThe innovative aspects lie in the method\u2019s focus on ensuring logical consistency (POS satisfiability), managing slot value fuzziness, and the structured pipeline that helps generate robust user requirements for flight booking scenarios. This imposes a higher degree of reasoning on models, specifically targeting their capability to handle conditional dependencies and simultaneous multiple constraints.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### **Experiment Setup**\n- **Datasets:** The primary dataset used for evaluation is GroundCocoa.\n- **Models Tested:**\n  - LLAMA 2-chat\n  - Mixtral 8x7B - Instruct\n  - Mistral 7B Instruct\n  - Gemini Pro\n  - GPT-4 Turbo\n- **Evaluation Metrics:** The main evaluation metric involves measuring how well the models ground user requirements to each answer choice.\n- **Methods:**\n  - Chain-of-Thought (CoT) Reasoning: Two strategies are employed:\n    - **CoT-full:** Consists of 5 distinct explanations, one for each flight option given the user requirement. \n    - **CoT-partial:** Involves providing explanations for only two flight choices (1 positive and 1 negative) to manage context length better.\n\n#### **Main Experimental Results**\n- **General Observations:** GroundCocoa poses a significant challenge for all evaluated models, even with CoT prompting strategies.\n- **Model Performance Comparison:**\n  - **CoT-full Strategy:** Due to limitations such as context length, the method couldn't be used on LLAMA 2-chat 70B. However, models generally struggled with this strategy.\n  - **CoT-partial Strategy:** This method, with only two options and explanations, led to better results than CoT-full in 2 out of 3 experimented cases.\n  - **Best Performance:** GPT-4 Turbo achieved the best results with the CoT-partial strategy.\n- **Performance Variation:** There is a notable difference in performance between GPT-4 Turbo and the other models. This variation is significant and diverges from the more comparable results usually observed in standard benchmarks like MMLU, HellaSwag, ARC Reasoning Challenge, WinoGrande, and GSM-8K.\n\nIn summary, while CoT-partial generally improved results over CoT-full, GPT-4 Turbo with CoT-partial achieved the best performance, illustrating a substantial gap between GPT-4 Turbo and other models."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate the impact of increasing levels of conditional and compositional complexity on the performance of the GPT-4 Turbo model in the context of flight booking.",
            "experiment_process": "The study analyzed the performance of GPT-4 Turbo across varying levels of compositional and conditional complexity, using characteristics such as reasoning width (number of variables a model has to simultaneously reason over) and dependency graphs derived from POS expressions corresponding to dataset samples. Metrics like the number of sum terms in the POS expression and the Largest Connected Component (LCC) size in the dependency graph were utilized to gauge the level of complexity. The analysis focused on measuring model performance against these increasing metrics of complexity.",
            "result_discussion": "The results indicated a decline in model performance with increased complexity, as attributed to the number of slots and dependency graph metrics. The study found that a greater breadth of parallel computation and increased conditional complexity harmed the model's performance.",
            "ablation_id": "2404.04237v1.No1"
        },
        {
            "research_objective": "Assess the confusion in model predictions caused by user query conditions, quantified through entropy, to explain why language models exhibit inconsistent results.",
            "experiment_process": "The experiment utilized entropy to measure confusion caused by specific conditions in user queries for different flight options. By examining compositional primitives in each sample, binary values were assigned indicating whether each primitive was satisfied. Comparisons of entropy values were made between correct and wrong predictions of GPT-4 Turbo on the dataset GroundCocoa, with densities of these entropy values analyzed. Simplified examples were also provided to illustrate the reasoning paths and the entropy calculation method.",
            "result_discussion": "Findings highlighted that higher entropy correlated with greater confusion and more frequent wrong model predictions. This pattern illustrated that at higher conditional complexities, models struggled more, causing increased prediction errors even with queries of similar complexity.",
            "ablation_id": "2404.04237v1.No2"
        },
        {
            "research_objective": "Examine the robustness of language models to unconventional and atypical user requirements in the flight booking context.",
            "experiment_process": "The study segregated user queries into 'Regular' and 'Atypical' groups and contrasted model performance on these different sets as detailed in Section 2.3. Testing included models with and without Chain of Thought (CoT) prompting. The in-context example for CoT included specific primitives like ticket price above $1800 and carbon emissions above average. Performance decay was analyzed between these two groups using tabled data comparisons.",
            "result_discussion": "It was observed that while all models showed a decline in performance on atypical queries, the decline was more significant for higher-performing models like GPT-4 Turbo. However, the use of CoT prompting helped reduce the performance degradation to some extent.",
            "ablation_id": "2404.04237v1.No3"
        }
    ]
}