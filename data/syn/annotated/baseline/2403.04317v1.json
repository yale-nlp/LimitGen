{
    "title": "Online Adaptation of Language Models with a Memory of Amortized Contexts",
    "abstract": "Due to the rapid generation and dissemination of information, large language models (LLMs) quickly run out of date despite enormous development costs. Due to this crucial need to keep models updated, online learning has emerged as a critical necessity when utilizing LLMs for real-world applications. However, given the ever-expanding corpus of unseen documents and the large parameter space of modern LLMs, efficient adaptation is essential. To address these challenges, we propose Memory of Amortized Contexts (MAC), an efficient and effective online adaptation framework for LLMs with strong knowledge retention. We propose an amortized feature extraction and memory-augmentation approach to compress and extract information from new documents into compact modulations stored in a memory bank. When answering questions, our model attends to and extracts relevant knowledge from this memory bank. To learn informative modulations in an efficient manner, we utilize amortization-based meta-learning which substitutes the optimization process with a single forward pass of the encoder. Subsequently, we learn to choose from and aggregate selected documents into a single modulation by conditioning on the question, allowing us to adapt a frozen language model during test time without requiring further gradient updates. Our experiment demonstrates the superiority of MAC in multiple aspects, including online adaptation performance, time, and memory efficiency. Code is available at: https://github.com/jihoontack/MAC.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Language models (LMs; Brown et al., 2020; Touvron et al., 2023) have significantly accelerated progress in natural language processing (NLP) and thus become a core technology in various real-world applications used by millions of users, such as coding assistants (Chen et al., 2021), search engines (Xuan-Quy et al., 2023), and personal AI assistants (Gao et al., 2023). However, LMs are typically static artifacts, and as the world changes, the knowledge encoded in their parameters becomes outdated. This becomes especially problematic for large language models (LLMs), as multiple applications (e.g., Chatbots; Kim et al. 2021; OpenAI 2022) require the model to be up-to-date. Unfortunately, updating LLMs with new documents not only requires high computational demands (Jang et al., 2022a), but also carries with it the likely risk of catastrophic forgetting (McCloskey & Cohen, 1989; French, 1999; Kirkpatrick et al., 2017; Schwarz et al., 2018), thus making it a challenging problem.\n\nTo tackle this issue, multiple studies suggested online and continual learning frameworks for LMs, i.e., adapting the LM on a stream of new documents. Here, one line of work proposes to use retrieval-augmented models by saving the stream of documents into the memory and learning a retrieval system to select the most relevant document based on the input (Chen et al., 2017; Karpukhin et al., 2020). However, even large models often fail to update their memorized predictions when the retrieved document consists of counterfactual information (Longpre et al., 2021; Li et al., 2022; Si et al., 2023). Moreover, retrieval-augmented models may not be suited for edge computing as a large number of documents itself poses expensive computation and storage costs for model inference (Hu et al., 2023).\n\nDue to these limitations, another line of recent works suggests updating the model\u2019s knowledge by finetuning on a stream of documents to directly update the knowledge inside the LM (i.e., online finetuning; Lazaridou et al., 2021; Jang et al., 2022b). For instance, recently, Hu et al. (2023) showed that online finetuning can be effective when the model is auto-regressively finetuned with the re-weighted token importance, where they propose to meta-learn a token weighting LM. While effective, online finetuning schemes also face limitations such as a large computation for gradient calculation of LM, sensitivity of the online optimization hyper-parameter (Hu et al., 2023), and the aforementioned catastrophic forgetting problem.\n\nIn this paper, we instead ask: Can we tackle the limitations of retrieval augmented models and the online finetuning techniques by updating the model\u2019s parameters efficiently while retaining the knowledge learned from the online documents?\n\nTo this end, we suggest bridging the gap between the two frameworks through a complementary learning systems approach by introducing an end-to-end differentiable auxiliary retrieval augmentation system which can be run alongside the target LM. This system extracts knowledge from incoming documents, builds a memory approach and learns the automated selection of relevant information from the memory bank which is subsequently passed as additional modulations to the target model.\n\nWhile intuitive, optimizing such modulations also (i) requires a gradient computation and (ii) is non-trivial to define, since the optimization objective at test time must work in the absence of labels (e.g., no ground truth supervision information about the salient aspects of a document). To this end, we take inspiration from the amortization-based meta-learning literature (Garnelo et al., 2018b; Requeima et al., 2019), which learns to predict modulations through amortized optimization (i.e., substituting an optimization process with an encoder forward pass; Amos et al., 2023; Lorraine et al., 2023) and learns to extract the knowledge without the need for label-access at test time.\n\nContribution. We propose Memory of Amortized Contexts (MAC), an efficient and effective online learning framework for static LMs (see the overview in Figure 1). The core idea of MAC is to freeze the parameter of the LM and instead edit the LM by using a predicted Parameter Efficient FineTuning (PEFT) modulation, which captures relevant knowledge from hitherto unseen documents. Specifically, we utilize amortization-based meta-learning to compress a new document\u2019s information into a compact modulation where such modulation maximizes the task performance of the adapted LM (e.g., question-and-answer ability). Then, we learn to aggregate documents represented in feature space into a single modulation based on a given question. During the online adaptation stage (or test-time), we thus"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Amortization-based meta-learning.\nAmortization-based meta-learning, which encodes the given context to directly predict the task-specific model, has gained much attention due to its computational efficiency as it only requires a single encoder forward pass when adapting the model (Santoro et al., 2016  ###reference_b69###; Mishra et al., 2018  ###reference_b51###; Garnelo et al., 2018b  ###reference_b19###, a  ###reference_b18###). These approaches, typically when combined with modulation techniques, have achieved notable success in various applications, such as few-shot visual recognition (Requeima et al., 2019  ###reference_b64###; Bronskill et al., 2021  ###reference_b7###) and 3D reconstructions (Guo et al., 2023  ###reference_b21###; Kim et al., 2023  ###reference_b36###). Recently, such a paradigm has been extended to language domains where prior works facilitate hypernetworks to adapt LMs with given few-shot prompts (Phang et al., 2023  ###reference_b57###; Ivison et al., 2023  ###reference_b29###). In this paper, we extend the use of amortization-based meta-learning to extract the knowledge of the given document into a compact yet informative modulation for online adaptation.\nOnline learning.\nOnline learning, also referred to as continual or lifelong learning, is a task of adapting models to new data or task distributions (Thrun & Mitchell, 1995  ###reference_b76###). Recently, due to the emergence of real-world applications using large models, online learning became important for multiple domains, including vision (Titsias et al., 2020  ###reference_b77###), multi-modal learning (Garg et al., 2023  ###reference_b17###), and RL (Schwarz et al., 2018  ###reference_b70###). In the language domain, there have been various attempts to tackle online learning (Kuhn, 1988  ###reference_b40###; Yogatama et al., 2014  ###reference_b87###; Rei, 2015  ###reference_b62###) where recent studies focus more on online learning of LLMs, e.g., finetuning on a stream of documents (Lazaridou et al., 2021  ###reference_b42###), architectural constraints (Jang et al., 2022b  ###reference_b33###), and the use of replay buffers (Dhingra et al., 2022  ###reference_b13###). Among them, Hu et al. (2023  ###reference_b27###) found that online finetuning can be effective when an LM focuses on important tokens during the adaptation and proposed a gradient-based meta-learning approach to automatically learn a token importance weighting model. However, such gradient-based meta-learning schemes require a compute-expensive second-order gradient calculation (Finn et al., 2017  ###reference_b14###; Ren et al., 2018  ###reference_b63###). Moreover, online finetuning schemes can face multiple issues and challenges, including (i) inevitable forgetting of the learned knowledge, (ii) gradient computation of LLMs during adaptation, and (iii) high sensitivity to the online optimization hyperparameter (e.g., learning rate; Hu et al., 2023  ###reference_b27###). MAC does not suffer from such issues as the amortization is not only efficient but also does not introduce any hyperparameter for the adaptation, and knowledge is preserved in the memory.\nRetrieval augmentation for LMs.\nRetrieval augmentation of LMs with relevant information from external knowledge sources has served as an effective way to improve the performance of LMs on various NLP tasks (Guu et al., 2020  ###reference_b22###; Lazaridou et al., 2022  ###reference_b43###; Izacard et al., 2023  ###reference_b31###) by reducing hallucination and leveraging external knowledge which is not seen during pre-training. However, retrieval augmentation drastically increases computational cost (Xu et al., 2023  ###reference_b84###) as documents often consist of thousands of words, and the computational cost of large LM\u2019s is quadratic in the input length. In addition, its effectiveness is sensitive to the configuration of retrieved information (Liu et al., 2023  ###reference_b46###), and even negatively affects the performance of LMs when the retrieved information is counterfactual (Si et al., 2023  ###reference_b74###). Our approach is more efficient than retrieval augmentation as it amortizes the external knowledge to modulate LMs rather than directly incorporating it.\nMemory augmented LMs. Augmenting the neural network with memory banks has been explored in multiple domains, including supervised learning (Hochreiter & Schmidhuber, 1997  ###reference_b25###; Graves et al., 2008  ###reference_b20###), meta-learning (Santoro et al., 2016  ###reference_b69###; Shan et al., 2020  ###reference_b73###), and reinforcement learning (Oh et al., 2016  ###reference_b54###; Rolnick et al., 2019  ###reference_b66###). More recently, memory augmentation has also shown great promise for LMs where it significantly improves the performance and efficiency in various directions, e.g., extending context length with memory retrieval (Wu et al., 2022  ###reference_b83###; Wang et al., 2023  ###reference_b81###), personalization (Baek et al., 2023  ###reference_b2###), and model editing (Mitchell et al., 2022b  ###reference_b53###). Unlike these methods, which store the raw text or use the memory bank to train new LMs, MAC stores the compact modulation parameter and adapts the frozen-based LM, thereby utilizing large models without the heavy computation of training LMs."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Online Adaptation of Language Models with a Memory of Amortized Contexts",
            "text": "In this section, we present Memory of Amortized Contexts (MAC), an efficient and effective framework for online learning of Language Models (LMs). We first briefly describe our problem setup (Section 3.1  ###reference_###), and then core components, namely amortization and aggregation framework (Section 3.2  ###reference_###) and finally, efficient training and inference schemes for MAC (Section 3.3  ###reference_###). Algorithm 2  ###reference_### and 3  ###reference_### in the Appendix B  ###reference_### provide detailed training and online adaptation processes for our framework."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Problem setup: Online Adaptation",
            "text": "We consider a realistic online adaptation scenario from Hu et al. (2023  ###reference_b27###) where an static LM parameterized by  is adapted with an online stream of context documents . We then evaluate the adapted model\u2019s performance with a set of query input  with the corresponding label set , where the  query and label are drawn from a conditional distribution of the context document , i.e., . Here, note that the query  is not accessible during online adaptation; hence, retaining the learned information from  is critical for achieving good results. While the query input and label pair  can be in any format or task, we specifically focus on question and answering (QA) tasks by following Hu et al. (2023  ###reference_b27###), i.e.,  is a question and  is the corresponding answer based on the given information in , as it is straightforward to evaluate the LM\u2019s updated knowledge."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "MAC: Memory of Amortized Contexts",
            "text": "The stated goal of MAC is (i) the efficient adaptation of a given LM to unseen information (ii) while retaining previously learned knowledge, both from its original training stage as well as updates from prior examples in a stream of novel data. To this end, we propose to utilize amortization-based meta-learning (Garnelo et al., 2018a  ###reference_b18###, b  ###reference_b19###) of a memory-augmented system. Amortization-based meta-learning with modulations (Humplik et al., 2019  ###reference_b28###; Requeima et al., 2019  ###reference_b64###; Bateni et al., 2020  ###reference_b4###) learns to predict a task-specific modulation (i.e., a compact representation of a task) through amortizing the given context set sampled from the task distribution. This enables efficient adaptation using the learned amortization network, as it only requires a single forward pass to adapt a model, foregoing the cost of gradient computation. It is worth noting that this is also beneficial as the LM does not have access to the input and label pair  during the online adaptation, where we can design the amortization to find the modulation only with the given document . Furthermore, meta-learned modulations have been found to preserve the task information well (e.g., showing great potential for generating or classifying distributions of tasks; Schwarz & Teh, 2022  ###reference_b71###; Bauer et al., 2023  ###reference_b5###), hence, can effectively extract the document information. Based on this insight, we suggest meta-learning the amortization network so that it directly predicts a compact modulation for a new document rather than requiring a separate optimization step.\nLearning to amortize contexts. For a given context document  sampled from the training document set , we learn an amortization network parameterized by  to predict the modulation parameter  as follows:\nHere, we use a hypernetwork (Ha et al., 2017  ###reference_b23###) for  where we mainly follow the architectural design from Phang et al. (2023  ###reference_b57###) by using the T5 encoder-decoder architecture (Raffel et al., 2020  ###reference_b59###) with learnable tokens for the decoder, to have a consistent number of output tokens. Here, one can design the modulation with any type of PEFT scheme (e.g., LoRA (Hu et al., 2022  ###reference_b26###) or FiLM (Perez et al., 2018  ###reference_b56###)) among which we use P-Tuning v2 (i.e., predictions of the key-value of each attention layer; Liu et al., 2022  ###reference_b47###) by following Phang et al. (2023  ###reference_b57###).\nModulating LMs via aggregating amortized contexts. Given a memory bank of compressed documents in the form of modulations , we now learn to choose relevant information in the form of a modulation  for a given input . While one design choice is to select a single modulation (i.e., retrieve), this has two drawbacks: i) risk of selecting the wrong modulation, and ii) limited utilization of learned knowledge across different modulations. Moreover, it is worth noting that recent studies empirically shown the linear interpolation (or advanced merging) between the modulations trained from the same pre-trained LM can even perform better than individual modulation (coined \u201cmodel soup\u201d; Wortsman et al., 2022  ###reference_b82###; Zadouri et al., 2023  ###reference_b88###). In this regard, we thus aggregate the memory bank into a single modulation based on the given input. Formally, we learn a set aggregation network  that satisfies permutation invariance (i.e., invariance to the order of a memory bank) by utilizing cross-attention blocks (Vaswani et al., 2017  ###reference_b79###; Kim et al., 2019  ###reference_b37###; Xu et al., 2020  ###reference_b85###) to select :\nwhere  is the input encoder, and we use the same architectural design as the amortization network , albeit resorting to a reduced number of parameters for efficiency reasons.\nEnd-to-end training objective. To learn aggregation and amortization networks, we optimize both networks in an end-to-end fashion based on the loss function , i.e., negative log-likelihood of the given label :\nwhere  is the batch size of training query inputs and labels.\nHere, it is important to state that we make no updates to the static LM , which would carry the risk of catastrophic forgetting by overwriting important parameters.\nOnline adaptation stage. After training amortization and aggregation networks based on a given training set, we now consider the online adaptation scenario. Here, we consider a stream of  documents  given to the LM in a sequential manner, where the task input  is not accessible during adaptation. To this end, we propose to store the compact modulations into a memory bank  and later predict the modulation using the aggregation network to adapt the LM, i.e.,  where ."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Memory Efficient Training and Inference for MAC",
            "text": "Due to aforementioned challenges, the training of MACcan quickly become prohibitive. The following sections cover techniques to drastically reduce memory requirements.\nBackpropagation dropout.\nDuring the online adaptation stage, the aggregation network is required to predict the modulation based on the memory bank, which may consist of large numbers of modulations (examples extracted from thousands of novel documents in our experimental setup). To handle large batch inference, it is crucial to present similar examples during training to avoid distribution shift between training and online adaptation stage and ensure that memory selection is robust. To this end, we propose a memory-efficient way to increase the training context size  by computing gradient using only a subset of randomly chosen examples (ensuring unbiased gradient computation), thus allowing training with significantly larger memory sizes. More concretely, with probability , we perform amortization at training time with a stop-gradient operation, i.e., stopgrad\nwhere  is a hyper-parameter, thus reminiscent of backpropagation. It is important to note that this random sub-sampling yields unbiased approximation of the full gradient under amortization-based meta-learning schemes (Bronskill et al., 2021  ###reference_b7###), hence, does not hurt the overall performance.\nInput: , , , , subgroup cardinality\nOutput:\nHierarchical modulation aggregation. In addition, we furthermore use a hierarchical modulation aggregation to deal with the large memory banks. Let  and  be the output token number for each context and the number of amortized contexts, respectively. Then, the memory usage made by a single cross-attention layer becomes  (note that the input  is also mapped into  tokens). This indicates the aggregation process requires a memory cost that linearly scales with the size of the memory bank.\nTo alleviate memory consumption, we propose hierarchical modulation aggregation that uses a divide-and-conquer strategy (see Algorithm 1  ###reference_###). Specifically, for a given memory bank size of  with  tokens, we subgroup the total  tokens into  tokens each, thereby having  groups ( is the ceil function, i.e., the smallest integer which is greater than or equal to the given input). Then, we aggregate the modulations of individual subgroups into a single output to obtain  modulations. We repeat this procedure until it outputs a single modulation. Assuming no parallelization, one can compute this process by only utilizing the memory complexity of  where  is a hyperparameter.\nSomewhat remarkably, we found that hierarchical modulation aggregation can be used without any additional training or modification of the main training objective (Eq. 3  ###reference_###); it shows comparable results to the full set aggregation (i.e., no hierarchical aggregation). We note that similar observations are also highlighted by other hierarchical merging methods in the Transformer literature (Bolya et al., 2023  ###reference_b6###; Song et al., 2024  ###reference_b75###), indicating the useful inductive bias of the attention mechanism (Bahdanau et al., 2014  ###reference_b3###; Vaswani et al., 2017  ###reference_b79###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we provide an empirical evaluation of MAC, systematically verifying claims made throughout the manuscript and thus supporting the suitability of its constituent components. Specifically, we investigate the following questions: How does MAC perform compared to other online learning techniques for LMs? (Table 1  ###reference_### & Table 2  ###reference_###) Is MAC more efficient compared to online finetuning schemes in terms of memory and time? (Figure 3  ###reference_###) Does MAC show effective knowledge retention compared to other finetuning methods? (Figure 3  ###reference_###) Does proposed efficient training and inference schemes save memory usage? (Figure 5  ###reference_### & Figure 5  ###reference_###) Before answering each question, we outline the experimental protocol (see Appendix A  ###reference_### for more details).\n\nDatasets. For the experiment, we utilize three question-and-answering (QA) datasets including StreamingQA (Li\u0161ka et al., 2022  ###reference_b45###), SQuAD (Rajpurkar et al., 2016  ###reference_b61###), and ArchivalQA (Wang et al., 2022  ###reference_b80###), by following the prior work (Hu et al., 2023  ###reference_b27###). Here, unlike the original use of SQuAD and ArchivalQA (i.e., used for evaluating static LMs), we use these datasets for online adaptation (i.e., adapting on a stream of documents), hence, denote with an additional \u201c-Seq\u201d notation throughout the section.\n\nModels. We mainly consider GPT2 family (Radford et al., 2018  ###reference_b58###) as the static base LM by following the prior work (Hu et al., 2023  ###reference_b27###), where we additionally conduct the experiment on LLaMA-2 (Touvron et al., 2023  ###reference_b78###) to verify the scalability of MAC. For the amortization network, we consider the T5 model family (Raffel et al., 2020  ###reference_b59###) that are relatively smaller than the base LM. It is important to note that the output number of tokens of the amortization and aggregation networks is a hyper-parameter, where we use 24 for all architectures except for Distil-GPT2, which uses 12. Then, we map these tokens into each layer\u2019s modulation through a linear layer where we use P-tuning v2 (Liu et al., 2022  ###reference_b47###) as the modulation design.\n\nOnline adaptation setup. After training MAC (i.e., learning, , and  parameters) on a training dataset that consists of document and QA pairs, we evaluate the online adaptation performance on the stream of documents. Here, we use 1,665 documents to adapt the LM and then perform the evaluation after the adaptation, where QA pairs are sampled from the learned documents. Each document can consist of tokens up to 512 when using the Byte Pair Encoding (Sennrich et al., 2015  ###reference_b72###)."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Online Adaptation Performance",
            "text": "We first present the main result by comparing the online adaptation performance with other baselines. Here, we mainly compare with online finetuning schemes and additionally show that MAC can be jointly used with a retrieval augmentation method to further improve the performance.\n\nComparison with online finetuning methods.\nIn Table 1, we show the online adaptation performance of MAC and the online finetuning baselines. Overall, MAC significantly outperforms all the prior online finetuning methods by a large margin, leading to a better exact match (EM) and F1 score.\n\nFurthermore, it is worth mentioning that MAC is significantly efficient in both memory and adaptation time compared to other online finetuning methods; we remark that MAC does not require any gradient computation to update the model, while online finetuning needs the gradient to update the model. We emphasize that both types of efficiency are crucial for online learning LMs as i) the document corpus is expanding rapidly, and ii) it enables the user to use a larger model for a better generalization.\n\nImproving MAC with retrieval augmentation. In addition, we show that MAC can be further improved by using retrieval augmentations. Here, we note that the user requires more inference costs to use retrieval augmentations as prepending the retrieved document in front of the question quadratically increases the inference computation based on the document length due to the Attention mechanism (Vaswani et al., 2017).\n\nFor the experimental setup, we compare it with LMs that are pre-trained on QA training set and then appended with top-1 retrieved document for each question. As shown in Table 2, using BM25 with MAC significantly improves the performance by a large margin in all cases.\n\nWe conjecture that the aggregation process of MAC enables the utilization of shared information across the documents, thus improving performance over single document retrieval. We believe further extending MAC for joint usage with retrieval augmentation schemes will be an interesting future direction to explore, where one can extend the amortization and input network to enhance the aggregation of the modulations but also learn to well retrieve the documents."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Knowledge Retention of MAC",
            "text": "We now address one of our primary motivations for this study: a comparison of knowledge retention by analyzing the catastrophic forgetting of each method. To this end, we evaluate the F1 score retention ratio, which is determined by the decline in the F1 score of the initially adapted 200 documents during the optimization on a subsequent stream of documents. As shown in Figure 3, MAC shows a strong knowledge retention compared to other online finetuning methods: when adapting additional 1,400 documents, MAC retains the initial performance by 96.2%. These results indeed highlight i) the benefit of using a memory bank as a tool for preserving knowledge and ii) our aggregation mechanism well predicts the modulation even when the memory bank\u2019s cardinality increases throughout the adaptation process. It is also worth noting that online finetuning schemes somewhat suffer from preserving the newly learned knowledge, especially when the number of adapted documents increases, thus may limit the practical usage for real-world applications."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Memory Efficiency of Backpropagation Dropout and Hierarchical Modulation Aggregation",
            "text": "Next, we verify the proposed memory efficient techniques, namely the backpropagation dropout and the hierarchical modulation aggregation for training and inference, respectively. Here, we report the peak GPU utilization when using the proposed techniques to show the memory efficiency. Furthermore, we re-emphasize that such techniques are important for (i) scaling LMs to larger models and (ii) handling a large number of documents during online adaptation, which are both necessary for scaling.\n\nTraining memory efficiency.\nTo show the memory efficiency of the backpropagation dropout, we increase the number of amortized contexts during training time and vary the dropout ratio. As shown in Figure 5, increasing the dropout ratio can significantly handle more contexts under the same memory constraint. As a result, we found that simply using 0.5 is an effective choice when using large models (# parameters > 1B) as the training context size is small in such cases. For instance, when training LLaMA-2 (7B) model on StreamingQA dataset without this technique, one can only compute the loss with a single document (under 32 GB GPU), thus the aggregation network cannot learn the similarity between the modulations.\n\nInference memory efficiency. Here, we show that the hierarchical modulation aggregation can significantly reduce memory usage while effectively preserving the performance for the inference. To this end, we vary the cardinality of the subgroup and report the peak GPU memory usage and F1 score where we only measure the used memory by the modulation aggregation (i.e., excluding the LM cost). As shown in Figure 5, using the subgroup size of 4 can reduce the memory by 65.6% while still preserving 93.2% of the original accuracy. We remark that this technique can be applied even without additional training trick or regularization, demonstrating similar observations from the prior works that uses hierarchical aggregation (or merging) in the context of Transformers (Bolya et al., 2023; Song et al., 2024), yet MAC is the first to aggregate the modulations."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion and Conclusion",
            "text": "In this paper, we propose Memory of Amortized Contexts (MAC), an efficient yet effective online adaptation framework for static LMs with strong knowledge retention. MAC extracts the information of each context document into compact parameter-efficient finetuning modulations where we predict such modulations through the meta-learned amortization network. Then, we store the amortized contexts into the memory bank for strong knowledge retention and aggregate the modulations into a single output when the question input is given. We demonstrate that MAC benefits from multiple aspects including performance, adaptation time, and memory efficiency, and more importantly, exhibits superior knowledge retention of the newly learned documents when learning on a stream of documents.\n\nFuture work. We believe MAC can be extended to multiple applications that require online learning in an efficient manner, e.g., federated learning for LMs (Che et al., 2023 ###reference_b9###) and model editing (Mitchell et al., 2022a ###reference_b52###, b ###reference_b53###; Hartvigsen et al., 2023 ###reference_b24###). Furthermore, it would be an interesting future direction to explore when the memory bank size has a constraint for further storage efficiency. In this direction, various ideas can be explored to maximize knowledge retention, such as merging similar modulations (or applying model soup; Wortsman et al., 2022 ###reference_b82###; Zadouri et al., 2023 ###reference_b88###) to perform better in both provided documents while reducing the size of the memory bank."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Experimental Details",
            "text": "Training details. We mainly follow the training configuration suggested by (Hu et al., 2023  ###reference_b27###). For all datasets, we train 50 epoch by using Adam (Kingma & Ba, 2015  ###reference_b38###) optimizer where we warmup the learning rate for the first epoch (except for training DistilGPT2; Sanh et al., 2019  ###reference_b68###) then use a constant value throughout the training. Here, we use a learning rate of  for all models except for DistilGPT2 where it uses . The output token number of the amortized network  is 12 for DistilGPT2 and 24 for the rest. We apply backpropagation dropout for large models with more than 1 billion parameters where we use the ratio of . Additionally, we use 4bit quantization (Dettmers et al., 2023  ###reference_b12###) and ZeRO (Rajbhandari et al., 2020  ###reference_b60###) when training GPT2-XL (Radford et al., 2018  ###reference_b58###) and LLaMA-2 (Touvron et al., 2023  ###reference_b78###) where we also (4-bit) quantize the T5 encoder (Raffel et al., 2020  ###reference_b59###). It is important to note that the quantization should be applied to pre-trained networks, not the networks learned from the random initialization (e.g., amortization and aggregation network). We use a batch size of 64 for DistilGPT2 and 32 for the rest by using the gradient accumulation.\nEvaluation details. We follow the same evaluation protocol from (Hu et al., 2023  ###reference_b27###). For the online adaptation, we adapt the model on 1,665 stream of documents then perform QA evaluation. For the online finetuning baselines we follow Hu et al. (2023  ###reference_b27###) to find the best learning rate hyperparameter where we observed that the performance is somewhat quite sensitive to the choice. We mainly used  for all online finetuning methods except for CaMeLS which uses  in most cases. For the catastrophic forgetting analysis in Figure 3  ###reference_###, we fixed the learning rate to  for all online finetuning methods as we found the forgetting occurs more on larger learning rates. It is worth remarking that MAC does not require any additional hyperparameter during online finetuning.\nModel details. For the model details, we mainly describe the design choice of our amortization  and aggregation network . Note that input encoder  use the same architectural design as  while using a smaller sized network.\nFor the amortization network, we follow the design choice from (Phang et al., 2023  ###reference_b57###) and use the T5 encoder-decoder model (Raffel et al., 2020  ###reference_b59###) as the base architecture. Specifically, we learn a trainable tokens that is used for decoder input, so that the output number of tokens  are consistent. Then we have a individual two layered MLP for each output tokens.\nFor the network size, we use T5-small as the amortization  network for Distil-GPT2, T5-base for GPT2-Large, and T5-Large for both GPT2-XL and LLaMA-2 (7B) where the input network  uses a smaller model (T5-small for Distil-GPT2 and T5-base for the rest).\nFor the aggregation network, we simply use the cross attention block design suggested in (Vaswani et al., 2017  ###reference_b79###) where we use four blocks in total.\nDataset details. Here, we describe the dataset detail in the following.\nStreamingQA (Li\u0161ka et al., 2022  ###reference_b45###) The StreamingQA is composed of questions that are either created by annotators or produced using a large-scale language model.\nThese questions can be answered using a dynamic knowledge database of English WMT news articles, which have been timestamped and were published from 2007 to 2020, and these articles are also included in the dataset.\nFollowing the setups in (Hu et al., 2023  ###reference_b27###), we use 21k training questions, 1.7k validation questions, and 5k test questions, respectively. Also, the same number of documents with the questions is used for each split, during the experiments.\nFor the baselines that require QA pre-training (see Section 4  ###reference_###), we use 40k training questions and 4k validation questions, respectively.\nSQuAD (Rajpurkar et al., 2016  ###reference_b61###): The Stanford Question Answering Dataset (SQuAD) is composed of questions created by crowdworkers based on a collection of Wikipedia articles, where the answer to each question is a span contained in the corresponding article. Following the setups in (Hu et al., 2023  ###reference_b27###), we use 39.9k training questions, 5.6k validation questions, and 10.6k test questions, respectively. Next, we use 8.6k training documents, 1.2k validation documents, and 2.1k test documents, respectively.\nFor the baselines that require QA pre-training (see Section 4  ###reference_###), we use 40k training questions and 2.1k validation questions, respectively.\nArchivalQA (Wang et al., 2022  ###reference_b80###): The ArchivalQA dataset is constructed with synthetically generated questions from the sophisticatedly designed pipelines with language models.\nSpecifically, questions are generated from articles in the New York Times Annotated Corpus (Sandhaus, 2008  ###reference_b67###). Also, the answer to each question is a span contained in an article.\nFollowing the setups in (Hu et al., 2023  ###reference_b27###), we use 21.7k training questions, 5.3k validation questions, and 8.7k test questions, respectively. Next, we use 12.8k training documents, 3.0k validation documents, and 5.0k test documents, respectively.\nFor the baselines that require QA pre-training (see Section 4  ###reference_###), we use 12.4k training questions and 3k validation questions, respectively."
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Algorithm",
            "text": "Input: , , , , , learning rate\nOutput: , ,\nInput: Stream of document , test QA set , , , ,\nOutput: EM, F1"
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>\nComparison of the online adaptation performance between MAC\u00a0and online finetuning baselines. We report the exact match (EM) and F1 score by adapting the LM on a stream of documents and then performing QA based on the learned data.  denotes the adaptation results of CaMeLS\u2019s using a proxy token weighting model (i.e., a smaller model than the base LM) due to memory consumption, and OOM denotes unavailable results due to the running out-of-memory on a single NVIDIA A100 80GB GPU (even with a batch size of 1). The <span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.12.1\">bold</span> indicates the best result within the group.\n</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T1.10\">\n<tr class=\"ltx_tr\" id=\"S4.T1.10.9\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T1.10.9.1\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T1.10.9.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T1.10.9.3\"><span class=\"ltx_text\" id=\"S4.T1.10.9.3.1\" style=\"font-size:90%;\">StreamingQA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T1.10.9.4\"><span class=\"ltx_text\" id=\"S4.T1.10.9.4.1\" style=\"font-size:90%;\">SQuAD-Seq</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T1.10.9.5\"><span class=\"ltx_text\" id=\"S4.T1.10.9.5.1\" style=\"font-size:90%;\">ArchivalQA-Seq</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.8.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.8.6.7\"><span class=\"ltx_text\" id=\"S4.T1.8.6.7.1\" style=\"font-size:90%;\">Model (# params)</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.8.6.8\"><span class=\"ltx_text\" id=\"S4.T1.8.6.8.1\" style=\"font-size:90%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1\">\n<span class=\"ltx_text\" id=\"S4.T1.3.1.1.1\" style=\"font-size:90%;\">EM (</span><span class=\"ltx_text\" id=\"S4.T1.3.1.1.2\" style=\"font-size:90%;\">)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.4.2.2\">\n<span class=\"ltx_text\" id=\"S4.T1.4.2.2.1\" style=\"font-size:90%;\">F1 (</span><span class=\"ltx_text\" id=\"S4.T1.4.2.2.2\" style=\"font-size:90%;\">)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.5.3.3\">\n<span class=\"ltx_text\" id=\"S4.T1.5.3.3.1\" style=\"font-size:90%;\">EM (</span><span class=\"ltx_text\" id=\"S4.T1.5.3.3.2\" style=\"font-size:90%;\">)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.6.4.4\">\n<span class=\"ltx_text\" id=\"S4.T1.6.4.4.1\" style=\"font-size:90%;\">F1 (</span><span class=\"ltx_text\" id=\"S4.T1.6.4.4.2\" style=\"font-size:90%;\">)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.7.5.5\">\n<span class=\"ltx_text\" id=\"S4.T1.7.5.5.1\" style=\"font-size:90%;\">EM (</span><span class=\"ltx_text\" id=\"S4.T1.7.5.5.2\" style=\"font-size:90%;\">)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.8.6.6\">\n<span class=\"ltx_text\" id=\"S4.T1.8.6.6.1\" style=\"font-size:90%;\">F1 (</span><span class=\"ltx_text\" id=\"S4.T1.8.6.6.2\" style=\"font-size:90%;\">)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.10.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.10.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T1.10.10.1.1\" style=\"font-size:90%;\"><span class=\"ltx_text\" id=\"S4.T1.10.10.1.1.1\"></span> <span class=\"ltx_text\" id=\"S4.T1.10.10.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T1.10.10.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T1.10.10.1.1.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.10.1.1.2.1.1.1\">DistilGPT2</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.10.10.1.1.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.10.1.1.2.1.2.1\">(82M)</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T1.10.10.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.10.10.2\">\n<span class=\"ltx_text\" id=\"S4.T1.10.10.2.1\" style=\"font-size:90%;\">Uniform </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T1.10.10.2.2.1\" style=\"font-size:90%;\">(</span>Hu et\u00a0al.<span class=\"ltx_text\" id=\"S4.T1.10.10.2.3.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib27\" title=\"\">2023</a><span class=\"ltx_text\" id=\"S4.T1.10.10.2.4.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.10.3\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.10.3.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.10.3.2\" style=\"font-size:90%;\">1.62</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.10.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.10.4.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.10.4.2\" style=\"font-size:90%;\">3.76</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.10.5\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.10.5.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.10.5.2\" style=\"font-size:90%;\">1.24</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.10.6\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.10.6.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.10.6.2\" style=\"font-size:90%;\">2.54</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.10.7\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.10.7.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.10.7.2\" style=\"font-size:90%;\">4.86</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.10.8\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.10.8.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.10.8.2\" style=\"font-size:90%;\">4.08</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.10.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.10.11.1\">\n<span class=\"ltx_text\" id=\"S4.T1.10.11.1.1\" style=\"font-size:90%;\">Salient Spans </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T1.10.11.1.2.1\" style=\"font-size:90%;\">(</span>Hu et\u00a0al.<span class=\"ltx_text\" id=\"S4.T1.10.11.1.3.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib27\" title=\"\">2023</a><span class=\"ltx_text\" id=\"S4.T1.10.11.1.4.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.11.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.11.2.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.11.2.2\" style=\"font-size:90%;\">1.44</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.11.3\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.11.3.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.11.3.2\" style=\"font-size:90%;\">4.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.11.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.11.4.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.11.4.2\" style=\"font-size:90%;\">1.03</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.11.5\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.11.5.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.11.5.2\" style=\"font-size:90%;\">2.47</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.11.6\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.11.6.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.11.6.2\" style=\"font-size:90%;\">4.52</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.11.7\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.11.7.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.11.7.2\" style=\"font-size:90%;\">3.76</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.10.12\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.10.12.1\">\n<span class=\"ltx_text\" id=\"S4.T1.10.12.1.1\" style=\"font-size:90%;\">CaMeLS </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T1.10.12.1.2.1\" style=\"font-size:90%;\">(</span>Hu et\u00a0al.<span class=\"ltx_text\" id=\"S4.T1.10.12.1.3.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib27\" title=\"\">2023</a><span class=\"ltx_text\" id=\"S4.T1.10.12.1.4.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.12.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.12.2.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.12.2.2\" style=\"font-size:90%;\">1.62</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.12.3\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.12.3.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.12.3.2\" style=\"font-size:90%;\">5.79</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.12.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.12.4.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.12.4.2\" style=\"font-size:90%;\">1.47</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.12.5\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.12.5.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.12.5.2\" style=\"font-size:90%;\">3.08</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.12.6\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.12.6.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.12.6.2\" style=\"font-size:90%;\">4.62</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.12.7\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.12.7.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.12.7.2\" style=\"font-size:90%;\">6.19</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.10.13\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.10.13.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.13.1.1\" style=\"font-size:90%;background-color:#E6E6E6;\">MAC\u00a0(ours)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.13.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.13.2.1\" style=\"font-size:90%;background-color:#E6E6E6;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.13.2.2\" style=\"font-size:90%;background-color:#E6E6E6;\">5.59</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.13.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.13.3.1\" style=\"font-size:90%;background-color:#E6E6E6;\">10.18</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.13.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.13.4.1\" style=\"font-size:90%;background-color:#E6E6E6;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.13.4.2\" style=\"font-size:90%;background-color:#E6E6E6;\">2.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.13.5\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.13.5.1\" style=\"font-size:90%;background-color:#E6E6E6;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.13.5.2\" style=\"font-size:90%;background-color:#E6E6E6;\">6.85</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.13.6\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.13.6.1\" style=\"font-size:90%;background-color:#E6E6E6;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.13.6.2\" style=\"font-size:90%;background-color:#E6E6E6;\">7.55</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.13.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.13.7.1\" style=\"font-size:90%;background-color:#E6E6E6;\">10.58</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.10.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.14.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T1.10.14.1.1\" style=\"font-size:90%;\"><span class=\"ltx_text\" id=\"S4.T1.10.14.1.1.1\"></span> <span class=\"ltx_text\" id=\"S4.T1.10.14.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T1.10.14.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T1.10.14.1.1.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.14.1.1.2.1.1.1\">GPT2-Large</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.10.14.1.1.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.14.1.1.2.1.2.1\">(774M)</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T1.10.14.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.10.14.2\">\n<span class=\"ltx_text\" id=\"S4.T1.10.14.2.1\" style=\"font-size:90%;\">Uniform </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T1.10.14.2.2.1\" style=\"font-size:90%;\">(</span>Hu et\u00a0al.<span class=\"ltx_text\" id=\"S4.T1.10.14.2.3.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib27\" title=\"\">2023</a><span class=\"ltx_text\" id=\"S4.T1.10.14.2.4.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.14.3\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.14.3.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.14.3.2\" style=\"font-size:90%;\">4.74</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.14.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.14.4.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.14.4.2\" style=\"font-size:90%;\">7.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.14.5\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.14.5.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.14.5.2\" style=\"font-size:90%;\">3.64</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.14.6\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.14.6.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.14.6.2\" style=\"font-size:90%;\">4.97</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.14.7\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.14.7.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.14.7.2\" style=\"font-size:90%;\">7.66</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.14.8\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.14.8.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.14.8.2\" style=\"font-size:90%;\">8.71</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.10.15\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.10.15.1\">\n<span class=\"ltx_text\" id=\"S4.T1.10.15.1.1\" style=\"font-size:90%;\">Salient Spans </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T1.10.15.1.2.1\" style=\"font-size:90%;\">(</span>Hu et\u00a0al.<span class=\"ltx_text\" id=\"S4.T1.10.15.1.3.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib27\" title=\"\">2023</a><span class=\"ltx_text\" id=\"S4.T1.10.15.1.4.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.15.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.15.2.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.15.2.2\" style=\"font-size:90%;\">4.86</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.15.3\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.15.3.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.15.3.2\" style=\"font-size:90%;\">8.54</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.15.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.15.4.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.15.4.2\" style=\"font-size:90%;\">4.03</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.15.5\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.15.5.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.15.5.2\" style=\"font-size:90%;\">6.48</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.15.6\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.15.6.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.15.6.2\" style=\"font-size:90%;\">9.75</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.15.7\"><span class=\"ltx_text\" id=\"S4.T1.10.15.7.1\" style=\"font-size:90%;\">11.19</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.9.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.9.7.1\">\n<span class=\"ltx_text\" id=\"S4.T1.9.7.1.1\" style=\"font-size:90%;\">CaMeLS</span><span class=\"ltx_text\" id=\"S4.T1.9.7.1.2\" style=\"font-size:90%;\"> </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T1.9.7.1.3.1\" style=\"font-size:90%;\">(</span>Hu et\u00a0al.<span class=\"ltx_text\" id=\"S4.T1.9.7.1.4.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib27\" title=\"\">2023</a><span class=\"ltx_text\" id=\"S4.T1.9.7.1.5.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.9.7.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.9.7.2.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.9.7.2.2\" style=\"font-size:90%;\">5.35</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.9.7.3\"><span class=\"ltx_text\" id=\"S4.T1.9.7.3.1\" style=\"font-size:90%;\">10.60</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.9.7.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.9.7.4.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.9.7.4.2\" style=\"font-size:90%;\">4.97</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.9.7.5\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.9.7.5.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.9.7.5.2\" style=\"font-size:90%;\">8.63</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.9.7.6\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.9.7.6.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.9.7.6.2\" style=\"font-size:90%;\">9.92</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.9.7.7\"><span class=\"ltx_text\" id=\"S4.T1.9.7.7.1\" style=\"font-size:90%;\">12.41</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.10.16\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.10.16.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.16.1.1\" style=\"font-size:90%;background-color:#E6E6E6;\">MAC\u00a0(ours)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.16.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.16.2.1\" style=\"font-size:90%;background-color:#E6E6E6;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.16.2.2\" style=\"font-size:90%;background-color:#E6E6E6;\">7.25</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.16.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.16.3.1\" style=\"font-size:90%;background-color:#E6E6E6;\">13.31</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.16.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.16.4.1\" style=\"font-size:90%;background-color:#E6E6E6;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.16.4.2\" style=\"font-size:90%;background-color:#E6E6E6;\">6.43</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.16.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.16.5.1\" style=\"font-size:90%;background-color:#E6E6E6;\">11.42</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.16.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.16.6.1\" style=\"font-size:90%;background-color:#E6E6E6;\">11.84</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.16.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.16.7.1\" style=\"font-size:90%;background-color:#E6E6E6;\">15.26</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.10.17\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.17.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T1.10.17.1.1\" style=\"font-size:90%;\"><span class=\"ltx_text\" id=\"S4.T1.10.17.1.1.1\"></span> <span class=\"ltx_text\" id=\"S4.T1.10.17.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T1.10.17.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T1.10.17.1.1.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.17.1.1.2.1.1.1\">GPT2-XL</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.10.17.1.1.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.17.1.1.2.1.2.1\">(1.5B)</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T1.10.17.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.10.17.2\">\n<span class=\"ltx_text\" id=\"S4.T1.10.17.2.1\" style=\"font-size:90%;\">Uniform </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T1.10.17.2.2.1\" style=\"font-size:90%;\">(</span>Hu et\u00a0al.<span class=\"ltx_text\" id=\"S4.T1.10.17.2.3.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib27\" title=\"\">2023</a><span class=\"ltx_text\" id=\"S4.T1.10.17.2.4.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.17.3\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.17.3.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.17.3.2\" style=\"font-size:90%;\">5.11</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.17.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.17.4.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.17.4.2\" style=\"font-size:90%;\">7.48</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.17.5\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.17.5.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.17.5.2\" style=\"font-size:90%;\">6.10</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.17.6\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.17.6.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.17.6.2\" style=\"font-size:90%;\">6.78</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.17.7\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.17.7.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.17.7.2\" style=\"font-size:90%;\">8.61</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.17.8\"><span class=\"ltx_text\" id=\"S4.T1.10.17.8.1\" style=\"font-size:90%;\">10.78</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.10.18\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.10.18.1\">\n<span class=\"ltx_text\" id=\"S4.T1.10.18.1.1\" style=\"font-size:90%;\">Salient Spans </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T1.10.18.1.2.1\" style=\"font-size:90%;\">(</span>Hu et\u00a0al.<span class=\"ltx_text\" id=\"S4.T1.10.18.1.3.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib27\" title=\"\">2023</a><span class=\"ltx_text\" id=\"S4.T1.10.18.1.4.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.18.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.18.2.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.18.2.2\" style=\"font-size:90%;\">5.40</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.18.3\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.18.3.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.18.3.2\" style=\"font-size:90%;\">9.42</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.18.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.18.4.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.18.4.2\" style=\"font-size:90%;\">4.55</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.18.5\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.18.5.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.18.5.2\" style=\"font-size:90%;\">6.74</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.18.6\"><span class=\"ltx_text\" id=\"S4.T1.10.18.6.1\" style=\"font-size:90%;\">11.81</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.18.7\"><span class=\"ltx_text\" id=\"S4.T1.10.18.7.1\" style=\"font-size:90%;\">14.11</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.10.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.10.8.1\">\n<span class=\"ltx_text\" id=\"S4.T1.10.8.1.1\" style=\"font-size:90%;\">CaMeLS</span><span class=\"ltx_text\" id=\"S4.T1.10.8.1.2\" style=\"font-size:90%;\"> </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T1.10.8.1.3.1\" style=\"font-size:90%;\">(</span>Hu et\u00a0al.<span class=\"ltx_text\" id=\"S4.T1.10.8.1.4.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib27\" title=\"\">2023</a><span class=\"ltx_text\" id=\"S4.T1.10.8.1.5.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.8.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.8.2.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.8.2.2\" style=\"font-size:90%;\">6.55</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.8.3\"><span class=\"ltx_text\" id=\"S4.T1.10.8.3.1\" style=\"font-size:90%;\">11.67</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.8.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.8.4.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T1.10.8.4.2\" style=\"font-size:90%;\">6.70</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.8.5\"><span class=\"ltx_text\" id=\"S4.T1.10.8.5.1\" style=\"font-size:90%;\">10.15</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.8.6\"><span class=\"ltx_text\" id=\"S4.T1.10.8.6.1\" style=\"font-size:90%;\">13.87</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.8.7\"><span class=\"ltx_text\" id=\"S4.T1.10.8.7.1\" style=\"font-size:90%;\">15.74</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.10.19\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.10.19.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.19.1.1\" style=\"font-size:90%;background-color:#E6E6E6;\">MAC\u00a0(ours)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.19.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.19.2.1\" style=\"font-size:90%;background-color:#E6E6E6;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.19.2.2\" style=\"font-size:90%;background-color:#E6E6E6;\">8.99</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.19.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.19.3.1\" style=\"font-size:90%;background-color:#E6E6E6;\">15.38</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.19.4\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T1.10.19.4.1\" style=\"font-size:90%;background-color:#E6E6E6;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.19.4.2\" style=\"font-size:90%;background-color:#E6E6E6;\">7.10</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.19.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.19.5.1\" style=\"font-size:90%;background-color:#E6E6E6;\">12.55</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.19.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.19.6.1\" style=\"font-size:90%;background-color:#E6E6E6;\">14.01</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.19.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.19.7.1\" style=\"font-size:90%;background-color:#E6E6E6;\">17.12</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.10.20\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T1.10.20.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T1.10.20.1.1\" style=\"font-size:90%;\"><span class=\"ltx_text\" id=\"S4.T1.10.20.1.1.1\"></span> <span class=\"ltx_text\" id=\"S4.T1.10.20.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T1.10.20.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T1.10.20.1.1.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.20.1.1.2.1.1.1\">LLaMA-2</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.10.20.1.1.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.20.1.1.2.1.2.1\">(7B)</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T1.10.20.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.10.20.2\">\n<span class=\"ltx_text\" id=\"S4.T1.10.20.2.1\" style=\"font-size:90%;\">Uniform </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T1.10.20.2.2.1\" style=\"font-size:90%;\">(</span>Hu et\u00a0al.<span class=\"ltx_text\" id=\"S4.T1.10.20.2.3.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib27\" title=\"\">2023</a><span class=\"ltx_text\" id=\"S4.T1.10.20.2.4.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.20.3\"><span class=\"ltx_text\" id=\"S4.T1.10.20.3.1\" style=\"font-size:90%;\">12.43</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.20.4\"><span class=\"ltx_text\" id=\"S4.T1.10.20.4.1\" style=\"font-size:90%;\">13.54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.20.5\"><span class=\"ltx_text\" id=\"S4.T1.10.20.5.1\" style=\"font-size:90%;\">13.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.20.6\"><span class=\"ltx_text\" id=\"S4.T1.10.20.6.1\" style=\"font-size:90%;\">17.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.20.7\"><span class=\"ltx_text\" id=\"S4.T1.10.20.7.1\" style=\"font-size:90%;\">18.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.10.20.8\"><span class=\"ltx_text\" id=\"S4.T1.10.20.8.1\" style=\"font-size:90%;\">21.35</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.10.21\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.10.21.1\">\n<span class=\"ltx_text\" id=\"S4.T1.10.21.1.1\" style=\"font-size:90%;\">Salient Spans </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T1.10.21.1.2.1\" style=\"font-size:90%;\">(</span>Hu et\u00a0al.<span class=\"ltx_text\" id=\"S4.T1.10.21.1.3.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib27\" title=\"\">2023</a><span class=\"ltx_text\" id=\"S4.T1.10.21.1.4.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.21.2\"><span class=\"ltx_text\" id=\"S4.T1.10.21.2.1\" style=\"font-size:90%;\">13.33</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.21.3\"><span class=\"ltx_text\" id=\"S4.T1.10.21.3.1\" style=\"font-size:90%;\">18.97</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.21.4\"><span class=\"ltx_text\" id=\"S4.T1.10.21.4.1\" style=\"font-size:90%;\">13.74</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.21.5\"><span class=\"ltx_text\" id=\"S4.T1.10.21.5.1\" style=\"font-size:90%;\">18.66</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.21.6\"><span class=\"ltx_text\" id=\"S4.T1.10.21.6.1\" style=\"font-size:90%;\">18.97</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.21.7\"><span class=\"ltx_text\" id=\"S4.T1.10.21.7.1\" style=\"font-size:90%;\">22.75</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.10.22\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.10.22.1\">\n<span class=\"ltx_text\" id=\"S4.T1.10.22.1.1\" style=\"font-size:90%;\">CaMeLS </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T1.10.22.1.2.1\" style=\"font-size:90%;\">(</span>Hu et\u00a0al.<span class=\"ltx_text\" id=\"S4.T1.10.22.1.3.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib27\" title=\"\">2023</a><span class=\"ltx_text\" id=\"S4.T1.10.22.1.4.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"6\" id=\"S4.T1.10.22.2\"><span class=\"ltx_text\" id=\"S4.T1.10.22.2.1\" style=\"font-size:90%;\">\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013 OOM \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.10.23\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T1.10.23.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.23.1.1\" style=\"font-size:90%;background-color:#E6E6E6;\">MAC\u00a0(ours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.10.23.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.23.2.1\" style=\"font-size:90%;background-color:#E6E6E6;\">14.29</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.10.23.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.23.3.1\" style=\"font-size:90%;background-color:#E6E6E6;\">21.79</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.10.23.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.23.4.1\" style=\"font-size:90%;background-color:#E6E6E6;\">15.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.10.23.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.23.5.1\" style=\"font-size:90%;background-color:#E6E6E6;\">21.14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.10.23.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.23.6.1\" style=\"font-size:90%;background-color:#E6E6E6;\">20.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.10.23.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.10.23.7.1\" style=\"font-size:90%;background-color:#E6E6E6;\">23.90</span></td>\n</tr>\n</table>\n</figure>",
            "capture": "Table 1: \nComparison of the online adaptation performance between MAC\u00a0and online finetuning baselines. We report the exact match (EM) and F1 score by adapting the LM on a stream of documents and then performing QA based on the learned data.  denotes the adaptation results of CaMeLS\u2019s using a proxy token weighting model (i.e., a smaller model than the base LM) due to memory consumption, and OOM denotes unavailable results due to the running out-of-memory on a single NVIDIA A100 80GB GPU (even with a batch size of 1). The bold indicates the best result within the group.\n"
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>\nOnline adaptation performance of MAC\u00a0jointly using the retrieval augmentation under ArchivalQA-Seq dataset. We report the exact match (EM) and F1 score by adapting the LM on a stream of documents and then performing QA based on the learned data, while retrieval augmentation retrieves the top-1 document. The <span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.4.1\">bold</span> indicates the best results within the group.\n</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T2.2\" style=\"width:208.1pt;height:132pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-80.5pt,51.0pt) scale(0.563918717952831,0.563918717952831) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.2.2\">\n<tr class=\"ltx_tr\" id=\"S4.T2.2.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.2.2.2.3\"><span class=\"ltx_text\" id=\"S4.T2.2.2.2.3.1\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T2.2.2.2.4\"><span class=\"ltx_text\" id=\"S4.T2.2.2.2.4.1\" style=\"font-size:90%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.1.1\">\n<span class=\"ltx_text\" id=\"S4.T2.1.1.1.1.1\" style=\"font-size:90%;\">EM (</span><span class=\"ltx_text\" id=\"S4.T2.1.1.1.1.2\" style=\"font-size:90%;\">)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.2.2.2.2\">\n<span class=\"ltx_text\" id=\"S4.T2.2.2.2.2.1\" style=\"font-size:90%;\">F1 (</span><span class=\"ltx_text\" id=\"S4.T2.2.2.2.2.2\" style=\"font-size:90%;\">)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.2.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.2.3.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.2.2.3.1.1\" style=\"font-size:90%;\"><span class=\"ltx_text\" id=\"S4.T2.2.2.3.1.1.1\"></span> <span class=\"ltx_text\" id=\"S4.T2.2.2.3.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.2.2.3.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.2.2.3.1.1.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.3.1.1.2.1.1.1\">DistilGPT2</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.2.2.3.1.1.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.3.1.1.2.1.2.1\">(82M)</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T2.2.2.3.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.2.2.3.2\">\n<span class=\"ltx_text\" id=\"S4.T2.2.2.3.2.1\" style=\"font-size:90%;\">Contriever </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T2.2.2.3.2.2.1\" style=\"font-size:90%;\">(</span>Izacard et\u00a0al.<span class=\"ltx_text\" id=\"S4.T2.2.2.3.2.3.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib30\" title=\"\">2022</a><span class=\"ltx_text\" id=\"S4.T2.2.2.3.2.4.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.2.3.3\">\n<span class=\"ltx_text ltx_phantom\" id=\"S4.T2.2.2.3.3.1\" style=\"font-size:90%;\"><span style=\"visibility:hidden\">0</span></span><span class=\"ltx_text\" id=\"S4.T2.2.2.3.3.2\" style=\"font-size:90%;\">9.28</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.2.3.4\"><span class=\"ltx_text\" id=\"S4.T2.2.2.3.4.1\" style=\"font-size:90%;\">12.41</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.2.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.2.4.1\">\n<span class=\"ltx_text\" id=\"S4.T2.2.2.4.1.1\" style=\"font-size:90%;\">BM25 </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T2.2.2.4.1.2.1\" style=\"font-size:90%;\">(</span>Robertson et\u00a0al.<span class=\"ltx_text\" id=\"S4.T2.2.2.4.1.3.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib65\" title=\"\">2009</a><span class=\"ltx_text\" id=\"S4.T2.2.2.4.1.4.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.4.2\"><span class=\"ltx_text\" id=\"S4.T2.2.2.4.2.1\" style=\"font-size:90%;\">10.90</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.4.3\"><span class=\"ltx_text\" id=\"S4.T2.2.2.4.3.1\" style=\"font-size:90%;\">14.50</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.2.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.2.5.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.2.5.1.1\" style=\"font-size:90%;background-color:#E6E6E6;\">BM25 + MAC\u00a0(ours)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.5.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.2.5.2.1\" style=\"font-size:90%;background-color:#E6E6E6;\">12.22</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.5.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.2.5.3.1\" style=\"font-size:90%;background-color:#E6E6E6;\">16.05</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.2.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.2.6.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.2.2.6.1.1\" style=\"font-size:90%;\"><span class=\"ltx_text\" id=\"S4.T2.2.2.6.1.1.1\"></span> <span class=\"ltx_text\" id=\"S4.T2.2.2.6.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.2.2.6.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.2.2.6.1.1.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.6.1.1.2.1.1.1\">GPT2-Large</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.2.2.6.1.1.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.6.1.1.2.1.2.1\">(774M)</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T2.2.2.6.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.2.2.6.2\">\n<span class=\"ltx_text\" id=\"S4.T2.2.2.6.2.1\" style=\"font-size:90%;\">Contriever </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T2.2.2.6.2.2.1\" style=\"font-size:90%;\">(</span>Izacard et\u00a0al.<span class=\"ltx_text\" id=\"S4.T2.2.2.6.2.3.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib30\" title=\"\">2022</a><span class=\"ltx_text\" id=\"S4.T2.2.2.6.2.4.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.2.6.3\"><span class=\"ltx_text\" id=\"S4.T2.2.2.6.3.1\" style=\"font-size:90%;\">13.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.2.6.4\"><span class=\"ltx_text\" id=\"S4.T2.2.2.6.4.1\" style=\"font-size:90%;\">17.13</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.2.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.2.7.1\">\n<span class=\"ltx_text\" id=\"S4.T2.2.2.7.1.1\" style=\"font-size:90%;\">BM25 </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T2.2.2.7.1.2.1\" style=\"font-size:90%;\">(</span>Robertson et\u00a0al.<span class=\"ltx_text\" id=\"S4.T2.2.2.7.1.3.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib65\" title=\"\">2009</a><span class=\"ltx_text\" id=\"S4.T2.2.2.7.1.4.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.7.2\"><span class=\"ltx_text\" id=\"S4.T2.2.2.7.2.1\" style=\"font-size:90%;\">16.33</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.7.3\"><span class=\"ltx_text\" id=\"S4.T2.2.2.7.3.1\" style=\"font-size:90%;\">21.43</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.2.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.2.8.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.2.8.1.1\" style=\"font-size:90%;background-color:#E6E6E6;\">BM25 + MAC\u00a0(ours)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.8.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.2.8.2.1\" style=\"font-size:90%;background-color:#E6E6E6;\">22.54</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.8.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.2.8.3.1\" style=\"font-size:90%;background-color:#E6E6E6;\">28.10</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.2.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.2.9.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.2.2.9.1.1\" style=\"font-size:90%;\"><span class=\"ltx_text\" id=\"S4.T2.2.2.9.1.1.1\"></span> <span class=\"ltx_text\" id=\"S4.T2.2.2.9.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.2.2.9.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.2.2.9.1.1.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.9.1.1.2.1.1.1\">GPT2-XL</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.2.2.9.1.1.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.9.1.1.2.1.2.1\">(1.5B)</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T2.2.2.9.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.2.2.9.2\">\n<span class=\"ltx_text\" id=\"S4.T2.2.2.9.2.1\" style=\"font-size:90%;\">Contriever </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T2.2.2.9.2.2.1\" style=\"font-size:90%;\">(</span>Izacard et\u00a0al.<span class=\"ltx_text\" id=\"S4.T2.2.2.9.2.3.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib30\" title=\"\">2022</a><span class=\"ltx_text\" id=\"S4.T2.2.2.9.2.4.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.2.9.3\"><span class=\"ltx_text\" id=\"S4.T2.2.2.9.3.1\" style=\"font-size:90%;\">13.99</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.2.9.4\"><span class=\"ltx_text\" id=\"S4.T2.2.2.9.4.1\" style=\"font-size:90%;\">17.28</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.2.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.2.10.1\">\n<span class=\"ltx_text\" id=\"S4.T2.2.2.10.1.1\" style=\"font-size:90%;\">BM25 </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T2.2.2.10.1.2.1\" style=\"font-size:90%;\">(</span>Robertson et\u00a0al.<span class=\"ltx_text\" id=\"S4.T2.2.2.10.1.3.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib65\" title=\"\">2009</a><span class=\"ltx_text\" id=\"S4.T2.2.2.10.1.4.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.10.2\"><span class=\"ltx_text\" id=\"S4.T2.2.2.10.2.1\" style=\"font-size:90%;\">17.79</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.10.3\"><span class=\"ltx_text\" id=\"S4.T2.2.2.10.3.1\" style=\"font-size:90%;\">22.58</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.2.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.2.11.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.2.11.1.1\" style=\"font-size:90%;background-color:#E6E6E6;\">BM25 + MAC\u00a0(ours)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.11.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.2.11.2.1\" style=\"font-size:90%;background-color:#E6E6E6;\">24.23</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.11.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.2.11.3.1\" style=\"font-size:90%;background-color:#E6E6E6;\">29.93</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.2.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.2.2.12.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.2.2.12.1.1\" style=\"font-size:90%;\"><span class=\"ltx_text\" id=\"S4.T2.2.2.12.1.1.1\"></span> <span class=\"ltx_text\" id=\"S4.T2.2.2.12.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.2.2.12.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.2.2.12.1.1.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.12.1.1.2.1.1.1\">LLaMA-2</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.2.2.12.1.1.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.12.1.1.2.1.2.1\">(7B)</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T2.2.2.12.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.2.2.12.2\">\n<span class=\"ltx_text\" id=\"S4.T2.2.2.12.2.1\" style=\"font-size:90%;\">Contriever </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T2.2.2.12.2.2.1\" style=\"font-size:90%;\">(</span>Izacard et\u00a0al.<span class=\"ltx_text\" id=\"S4.T2.2.2.12.2.3.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib30\" title=\"\">2022</a><span class=\"ltx_text\" id=\"S4.T2.2.2.12.2.4.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.2.12.3\"><span class=\"ltx_text\" id=\"S4.T2.2.2.12.3.1\" style=\"font-size:90%;\">21.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.2.12.4\"><span class=\"ltx_text\" id=\"S4.T2.2.2.12.4.1\" style=\"font-size:90%;\">28.31</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.2.13\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.2.13.1\">\n<span class=\"ltx_text\" id=\"S4.T2.2.2.13.1.1\" style=\"font-size:90%;\">BM25 </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T2.2.2.13.1.2.1\" style=\"font-size:90%;\">(</span>Robertson et\u00a0al.<span class=\"ltx_text\" id=\"S4.T2.2.2.13.1.3.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04317v1#bib.bib65\" title=\"\">2009</a><span class=\"ltx_text\" id=\"S4.T2.2.2.13.1.4.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.13.2\"><span class=\"ltx_text\" id=\"S4.T2.2.2.13.2.1\" style=\"font-size:90%;\">25.11</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.13.3\"><span class=\"ltx_text\" id=\"S4.T2.2.2.13.3.1\" style=\"font-size:90%;\">31.30</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.2.14\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T2.2.2.14.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.2.14.1.1\" style=\"font-size:90%;background-color:#E6E6E6;\">BM25 + MAC\u00a0(ours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.2.2.14.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.2.14.2.1\" style=\"font-size:90%;background-color:#E6E6E6;\">31.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.2.2.14.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.2.14.3.1\" style=\"font-size:90%;background-color:#E6E6E6;\">40.11</span></td>\n</tr>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 2: \nOnline adaptation performance of MAC\u00a0jointly using the retrieval augmentation under ArchivalQA-Seq dataset. We report the exact match (EM) and F1 score by adapting the LM on a stream of documents and then performing QA based on the learned data, while retrieval augmentation retrieves the top-1 document. The bold indicates the best results within the group.\n"
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.04317v1_figure_1.png",
            "caption": "Figure 1: \nAn overview of the proposed Memory of Amortized Contexts (MAC): we amortize each context document into Parameter Efficient FineTuning (PEFT) modulation \u03d5italic-\u03d5\\phiitalic_\u03d5 and learn to aggregate modulations into a single target modulation \u03d5*superscriptitalic-\u03d5\\phi^{*}italic_\u03d5 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT based on the given question input \ud835\udc31\ud835\udc31{\\mathbf{x}}bold_x to adapt the frozen static Language Model (LM; \u03b8\ud835\ude8b\ud835\ude8a\ud835\ude9c\ud835\ude8esubscript\ud835\udf03\ud835\ude8b\ud835\ude8a\ud835\ude9c\ud835\ude8e\\theta_{\\mathtt{base}}italic_\u03b8 start_POSTSUBSCRIPT typewriter_base end_POSTSUBSCRIPT). For the online adaptation, we store the amortized contexts into a memory bank \u2133\u2133{\\mathcal{M}}caligraphic_M for strong knowledge retention, then adapt the LM via aggregating the memory bank based on the given question."
        },
        "2": {
            "figure_path": "2403.04317v1_figure_2.png",
            "caption": "Figure 2: Comparison of the adaptation memory and time efficiency between MAC and online finetuning baselines. We report the peak GPU memory allocation (GB) for adapting one document and the time (min) for adapting stream of 1,665 documents under the same memory usage. We use GPT2-XL on StreamingQA."
        },
        "3": {
            "figure_path": "2403.04317v1_figure_3.png",
            "caption": "Figure 2: Comparison of the adaptation memory and time efficiency between MAC and online finetuning baselines. We report the peak GPU memory allocation (GB) for adapting one document and the time (min) for adapting stream of 1,665 documents under the same memory usage. We use GPT2-XL on StreamingQA."
        },
        "4": {
            "figure_path": "2403.04317v1_figure_4.png",
            "caption": "Figure 4: Memory efficiency of the backpropagation dropout. We report the peak GPU memory allocation (GB) when training GPT2-XL on StreamingQA dataset under varying sizes of amortized contexts set size (K\ud835\ude9d\ud835\ude9b\ud835\ude8a\ud835\ude92\ud835\ude97superscript\ud835\udc3e\ud835\ude9d\ud835\ude9b\ud835\ude8a\ud835\ude92\ud835\ude97K^{\\mathtt{train}}italic_K start_POSTSUPERSCRIPT typewriter_train end_POSTSUPERSCRIPT). p\ud835\udc5dpitalic_p indicates the dropout ratio and \u2018min\u2019 denotes the full dropout except for the single document."
        },
        "5": {
            "figure_path": "2403.04317v1_figure_5.png",
            "caption": "Figure 4: Memory efficiency of the backpropagation dropout. We report the peak GPU memory allocation (GB) when training GPT2-XL on StreamingQA dataset under varying sizes of amortized contexts set size (K\ud835\ude9d\ud835\ude9b\ud835\ude8a\ud835\ude92\ud835\ude97superscript\ud835\udc3e\ud835\ude9d\ud835\ude9b\ud835\ude8a\ud835\ude92\ud835\ude97K^{\\mathtt{train}}italic_K start_POSTSUPERSCRIPT typewriter_train end_POSTSUPERSCRIPT). p\ud835\udc5dpitalic_p indicates the dropout ratio and \u2018min\u2019 denotes the full dropout except for the single document."
        }
    },
    "references": [
        {
            "1": {
                "title": "Tutorial on amortized optimization.",
                "author": "Amos, B. et al.",
                "venue": "Foundations and Trends\u00ae in Machine Learning, 2023.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Knowledge-augmented large language models for personalized contextual query suggestion.",
                "author": "Baek, J., Chandrasekaran, N., Cucerzan, S., Jauhar, S. K., et al.",
                "venue": "arXiv preprint arXiv:2311.06318, 2023.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Neural machine translation by jointly learning to align and translate.",
                "author": "Bahdanau, D., Cho, K., and Bengio, Y.",
                "venue": "arXiv preprint arXiv:1409.0473, 2014.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Improved few-shot visual classification.",
                "author": "Bateni, P., Goyal, R., Masrani, V., Wood, F., and Sigal, L.",
                "venue": "In IEEE Conference on Computer Vision and Pattern Recognition, 2020.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Spatial functa: Scaling functa to imagenet classification and generation.",
                "author": "Bauer, M., Dupont, E., Brock, A., Rosenbaum, D., Schwarz, J., and Kim, H.",
                "venue": "arXiv preprint arXiv:2302.03130, 2023.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Token merging: Your vit but faster.",
                "author": "Bolya, D., Fu, C.-Y., Dai, X., Zhang, P., Feichtenhofer, C., and Hoffman, J.",
                "venue": "In International Conference on Learning Representations, 2023.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Memory efficient meta-learning with large images.",
                "author": "Bronskill, J., Massiceti, D., Patacchiola, M., Hofmann, K., Nowozin, S., and Turner, R.",
                "venue": "In Advances in Neural Information Processing Systems, 2021.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Language models are few-shot learners.",
                "author": "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.",
                "venue": "In Advances in Neural Information Processing Systems, 2020.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Federated learning of large language models with parameter-efficient prompt tuning and adaptive optimization.",
                "author": "Che, T., Liu, J., Zhou, Y., Ren, J., Zhou, J., Sheng, V. S., Dai, H., and Dou, D.",
                "venue": "In Conference on Empirical Methods in Natural Language Processing, 2023.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Reading wikipedia to answer open-domain questions.",
                "author": "Chen, D., Fisch, A., Weston, J., and Bordes, A.",
                "venue": "In Annual Conference of the Association for Computational Linguistics, 2017.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Evaluating large language models trained on code.",
                "author": "Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al.",
                "venue": "arXiv preprint arXiv:2107.03374, 2021.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Qlora: Efficient finetuning of quantized llms.",
                "author": "Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L.",
                "venue": "In Advances in Neural Information Processing Systems, 2023.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Time-aware language models as temporal knowledge bases.",
                "author": "Dhingra, B., Cole, J. R., Eisenschlos, J. M., Gillick, D., Eisenstein, J., and Cohen, W. W.",
                "venue": "Transactions of the Association for Computational Linguistics, 10, 2022.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Model-agnostic meta-learning for fast adaptation of deep networks.",
                "author": "Finn, C., Abbeel, P., and Levine, S.",
                "venue": "In International Conference on Machine Learning, 2017.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Catastrophic forgetting in connectionist networks.",
                "author": "French, R. M.",
                "venue": "Trends in cognitive sciences, 3(4):128\u2013135, 1999.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Assistgpt: A general multi-modal assistant that can plan, execute, inspect, and learn.",
                "author": "Gao, D., Ji, L., Zhou, L., Lin, K. Q., Chen, J., Fan, Z., and Shou, M. Z.",
                "venue": "arXiv preprint arXiv:2306.08640, 2023.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Tic-clip: Continual training of clip models.",
                "author": "Garg, S., Farajtabar, M., Pouransari, H., Vemulapalli, R., Mehta, S., Tuzel, O., Shankar, V., and Faghri, F.",
                "venue": "arXiv preprint arXiv:2310.16226, 2023.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Conditional neural processes.",
                "author": "Garnelo, M., Rosenbaum, D., Maddison, C., Ramalho, T., Saxton, D., Shanahan, M., Teh, Y. W., Rezende, D., and Eslami, S. A.",
                "venue": "In International Conference on Machine Learning, 2018a.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Neural processes.",
                "author": "Garnelo, M., Schwarz, J., Rosenbaum, D., Viola, F., Rezende, D. J., Eslami, S., and Teh, Y. W.",
                "venue": "arXiv preprint arXiv:1807.01622, 2018b.",
                "url": null
            }
        },
        {
            "20": {
                "title": "A novel connectionist system for unconstrained handwriting recognition.",
                "author": "Graves, A., Liwicki, M., Fern\u00e1ndez, S., Bertolami, R., Bunke, H., and Schmidhuber, J.",
                "venue": "IEEE transactions on pattern analysis and machine intelligence, 2008.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Versatile neural processes for learning implicit neural representations.",
                "author": "Guo, Z., Lan, C., Zhang, Z., Lu, Y., and Chen, Z.",
                "venue": "In International Conference on Learning Representations, 2023.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Retrieval augmented language model pre-training.",
                "author": "Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.",
                "venue": "In International Conference on Machine Learning, 2020.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Hypernetworks.",
                "author": "Ha, D., Dai, A. M., and Le, Q. V.",
                "venue": "In International Conference on Learning Representations, 2017.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Aging with grace: Lifelong model editing with discrete key-value adaptors.",
                "author": "Hartvigsen, T., Sankaranarayanan, S., Palangi, H., Kim, Y., and Ghassemi, M.",
                "venue": "In Advances in Neural Information Processing Systems, 2023.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Long short-term memory.",
                "author": "Hochreiter, S. and Schmidhuber, J.",
                "venue": "Neural computation, 1997.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Lora: Low-rank adaptation of large language models.",
                "author": "Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.",
                "venue": "In International Conference on Learning Representations, 2022.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Meta-learning online adaptation of language models.",
                "author": "Hu, N., Mitchell, E., Manning, C. D., and Finn, C.",
                "venue": "In Conference on Empirical Methods in Natural Language Processing, 2023.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Meta reinforcement learning as task inference.",
                "author": "Humplik, J., Galashov, A., Hasenclever, L., Ortega, P. A., Teh, Y. W., and Heess, N.",
                "venue": "arXiv preprint arXiv:1905.06424, 2019.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Hint: Hypernetwork instruction tuning for efficient zero-shot generalisation.",
                "author": "Ivison, H., Bhagia, A., Wang, Y., Hajishirzi, H., and Peters, M.",
                "venue": "In Annual Conference of the Association for Computational Linguistics, 2023.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Unsupervised dense information retrieval with contrastive learning.",
                "author": "Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E.",
                "venue": "In Transactions on Machine Learning Research, 2022.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Few-shot learning with retrieval augmented language models.",
                "author": "Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E.",
                "venue": "Journal of Machine Learning Research, 2023.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models.",
                "author": "Jang, J., Ye, S., Lee, C., Yang, S., Shin, J., Han, J., Kim, G., and Seo, M.",
                "venue": "In Conference on Empirical Methods in Natural Language Processing, 2022a.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Towards continual knowledge learning of language models.",
                "author": "Jang, J., Ye, S., Yang, S., Shin, J., Han, J., Kim, G., Choi, S. J., and Seo, M.",
                "venue": "In International Conference on Learning Representations, 2022b.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Dense passage retrieval for open-domain question answering.",
                "author": "Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t.",
                "venue": "In Conference on Empirical Methods in Natural Language Processing, 2020.",
                "url": null
            }
        },
        {
            "35": {
                "title": "What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers.",
                "author": "Kim, B., Kim, H., Lee, S.-W., Lee, G., Kwak, D., Hyeon, J. D., Park, S., Kim, S., Kim, S., Seo, D., et al.",
                "venue": "In Conference on Empirical Methods in Natural Language Processing, 2021.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Generalizable implicit neural representations via instance pattern composers.",
                "author": "Kim, C., Lee, D., Kim, S., Cho, M., and Han, W.-S.",
                "venue": "In IEEE Conference on Computer Vision and Pattern Recognition, 2023.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Attentive neural processes.",
                "author": "Kim, H., Mnih, A., Schwarz, J., Garnelo, M., Eslami, A., Rosenbaum, D., Vinyals, O., and Teh, Y. W.",
                "venue": "In International Conference on Learning Representations, 2019.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Adam: A method for stochastic optimization.",
                "author": "Kingma, D. P. and Ba, J.",
                "venue": "In International Conference on Learning Representations, 2015.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Overcoming catastrophic forgetting in neural networks.",
                "author": "Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al.",
                "venue": "Proceedings of the National Academy of Sciences, 2017.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Speech recognition and the frequency of recently used words: A modified Markov model for natural language.",
                "author": "Kuhn, R.",
                "venue": "In Coling Budapest 1988 Volume 1: International Conference on Computational Linguistics, 1988.",
                "url": null
            }
        },
        {
            "41": {
                "title": "What learning systems do intelligent agents need? complementary learning systems theory updated.",
                "author": "Kumaran, D., Hassabis, D., and McClelland, J. L.",
                "venue": "Trends in cognitive sciences, 20(7):512\u2013534, 2016.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Mind the gap: Assessing temporal generalization in neural language models.",
                "author": "Lazaridou, A., Kuncoro, A., Gribovskaya, E., Agrawal, D., Liska, A., Terzi, T., Gimenez, M., de Masson d\u2019Autume, C., Kocisky, T., Ruder, S., et al.",
                "venue": "In Advances in Neural Information Processing Systems, 2021.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Internet-augmented language models through few-shot prompting for open-domain question answering.",
                "author": "Lazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N.",
                "venue": "arXiv preprint arXiv:2203.05115, 2022.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Faithfulness in natural language generation: A systematic survey of analysis, evaluation and optimization methods.",
                "author": "Li, W., Wu, W., Chen, M., Liu, J., Xiao, X., and Wu, H.",
                "venue": "arXiv preprint arXiv:2203.05227, 2022.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Streamingqa: a benchmark for adaptation to new knowledge over time in question answering models.",
                "author": "Li\u0161ka, A., Ko\u010disk\u1ef3, T., Gribovskaya, E., Terzi, T., Sezener, E., Agrawal, D., d\u2019Autume, C. d. M., Scholtes, T., Zaheer, M., Young, S., et al.",
                "venue": "In International Conference on Machine Learning, 2022.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Lost in the middle: How language models use long contexts.",
                "author": "Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P.",
                "venue": "arXiv preprint arXiv:2307.03172, 2023.",
                "url": null
            }
        },
        {
            "47": {
                "title": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.",
                "author": "Liu, X., Ji, K., Fu, Y., Tam, W. L., Du, Z., Yang, Z., and Tang, J.",
                "venue": "In Annual Conference of the Association for Computational Linguistics, 2022.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Entity-based knowledge conflicts in question answering.",
                "author": "Longpre, S., Perisetla, K., Chen, A., Ramesh, N., DuBois, C., and Singh, S.",
                "venue": "arXiv preprint arXiv:2109.05052, 2021.",
                "url": null
            }
        },
        {
            "49": {
                "title": "Att3d: Amortized text-to-3d object synthesis.",
                "author": "Lorraine, J., Xie, K., Zeng, X., Lin, C.-H., Takikawa, T., Sharp, N., Lin, T.-Y., Liu, M.-Y., Fidler, S., and Lucas, J.",
                "venue": "In IEEE International Conference on Computer Vision, 2023.",
                "url": null
            }
        },
        {
            "50": {
                "title": "Catastrophic interference in connectionist networks: The sequential learning problem.",
                "author": "McCloskey, M. and Cohen, N. J.",
                "venue": "The Psychology of Learning and Motivation, 1989.",
                "url": null
            }
        },
        {
            "51": {
                "title": "A simple neural attentive meta-learner.",
                "author": "Mishra, N., Rohaninejad, M., Chen, X., and Abbeel, P.",
                "venue": "In International Conference on Learning Representations, 2018.",
                "url": null
            }
        },
        {
            "52": {
                "title": "Fast model editing at scale.",
                "author": "Mitchell, E., Lin, C., Bosselut, A., Finn, C., and Manning, C. D.",
                "venue": "In International Conference on Learning Representations, 2022a.",
                "url": null
            }
        },
        {
            "53": {
                "title": "Memory-based model editing at scale.",
                "author": "Mitchell, E., Lin, C., Bosselut, A., Finn, C., and Manning, C. D.",
                "venue": "In International Conference on Machine Learning, 2022b.",
                "url": null
            }
        },
        {
            "54": {
                "title": "Control of memory, active perception, and action in minecraft.",
                "author": "Oh, J., Chockalingam, V., Lee, H., et al.",
                "venue": "In International Conference on Machine Learning, 2016.",
                "url": null
            }
        },
        {
            "55": {
                "title": "Introducing chatgpt.",
                "author": "OpenAI.",
                "venue": "https://openai.com/blog/chatgpt, 2022.",
                "url": null
            }
        },
        {
            "56": {
                "title": "Film: Visual reasoning with a general conditioning layer.",
                "author": "Perez, E., Strub, F., De Vries, H., Dumoulin, V., and Courville, A.",
                "venue": "In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.",
                "url": null
            }
        },
        {
            "57": {
                "title": "Hypertuning: Toward adapting large language models without back-propagation.",
                "author": "Phang, J., Mao, Y., He, P., and Chen, W.",
                "venue": "In International Conference on Machine Learning, 2023.",
                "url": null
            }
        },
        {
            "58": {
                "title": "Improving language understanding by generative pre-training.",
                "author": "Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.",
                "venue": "In preprint, 2018.",
                "url": null
            }
        },
        {
            "59": {
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer.",
                "author": "Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J.",
                "venue": "Journal of Machine Learning Research, 2020.",
                "url": null
            }
        },
        {
            "60": {
                "title": "Zero: Memory optimizations toward training trillion parameter models.",
                "author": "Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y.",
                "venue": "In International Conference for High Performance Computing, Networking, Storage and Analysis, 2020.",
                "url": null
            }
        },
        {
            "61": {
                "title": "Squad: 100,000+ questions for machine comprehension of text.",
                "author": "Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.",
                "venue": "In Conference on Empirical Methods in Natural Language Processing, 2016.",
                "url": null
            }
        },
        {
            "62": {
                "title": "Online representation learning in recurrent neural language models.",
                "author": "Rei, M.",
                "venue": "In Conference on Empirical Methods in Natural Language Processing, 2015.",
                "url": null
            }
        },
        {
            "63": {
                "title": "Learning to reweight examples for robust deep learning.",
                "author": "Ren, M., Zeng, W., Yang, B., and Urtasun, R.",
                "venue": "In International conference on machine learning, 2018.",
                "url": null
            }
        },
        {
            "64": {
                "title": "Fast and flexible multi-task classification using conditional neural adaptive processes.",
                "author": "Requeima, J., Gordon, J., Bronskill, J., Nowozin, S., and Turner, R. E.",
                "venue": "In Advances in Neural Information Processing Systems, 2019.",
                "url": null
            }
        },
        {
            "65": {
                "title": "The probabilistic relevance framework: Bm25 and beyond.",
                "author": "Robertson, S., Zaragoza, H., et al.",
                "venue": "Foundations and Trends\u00ae in Information Retrieval, 2009.",
                "url": null
            }
        },
        {
            "66": {
                "title": "Experience replay for continual learning.",
                "author": "Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T., and Wayne, G.",
                "venue": "In Advances in Neural Information Processing Systems, 2019.",
                "url": null
            }
        },
        {
            "67": {
                "title": "The new york times annotated corpus.",
                "author": "Sandhaus, E.",
                "venue": "Linguistic Data Consortium, Philadelphia, 2008.",
                "url": null
            }
        },
        {
            "68": {
                "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.",
                "author": "Sanh, V., Debut, L., Chaumond, J., and Wolf, T.",
                "venue": "arXiv preprint arXiv:1910.01108, 2019.",
                "url": null
            }
        },
        {
            "69": {
                "title": "Meta-learning with memory-augmented neural networks.",
                "author": "Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T.",
                "venue": "In International Conference on Machine Learning, 2016.",
                "url": null
            }
        },
        {
            "70": {
                "title": "Progress & compress: A scalable framework for continual learning.",
                "author": "Schwarz, J., Czarnecki, W., Luketina, J., Grabska-Barwinska, A., Teh, Y. W., Pascanu, R., and Hadsell, R.",
                "venue": "In International Conference on Machine Learning, 2018.",
                "url": null
            }
        },
        {
            "71": {
                "title": "Meta-learning sparse compression networks.",
                "author": "Schwarz, J. R. and Teh, Y. W.",
                "venue": "Transactions on Machine Learning Research, 2022.",
                "url": null
            }
        },
        {
            "72": {
                "title": "Neural machine translation of rare words with subword units.",
                "author": "Sennrich, R., Haddow, B., and Birch, A.",
                "venue": "In Annual Conference of the Association for Computational Linguistics, 2015.",
                "url": null
            }
        },
        {
            "73": {
                "title": "Meta-neighborhoods.",
                "author": "Shan, S., Li, Y., and Oliva, J. B.",
                "venue": "In Advances in Neural Information Processing Systems, 2020.",
                "url": null
            }
        },
        {
            "74": {
                "title": "Prompting gpt-3 to be reliable.",
                "author": "Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J., and Wang, L.",
                "venue": "In International Conference on Learning Representations, 2023.",
                "url": null
            }
        },
        {
            "75": {
                "title": "Hierarchical context merging: Better long context understanding for pre-trained LLMs.",
                "author": "Song, W., Oh, S., Mo, S., Kim, J., Yun, S., Ha, J.-W., and Shin, J.",
                "venue": "In International Conference on Learning Representations, 2024.",
                "url": null
            }
        },
        {
            "76": {
                "title": "Lifelong robot learning.",
                "author": "Thrun, S. and Mitchell, T. M.",
                "venue": "Robotics and Autonomous Systems, 1995.",
                "url": null
            }
        },
        {
            "77": {
                "title": "Functional regularisation for continual learning with gaussian processes.",
                "author": "Titsias, M. K., Schwarz, J., Matthews, A. G. d. G., Pascanu, R., and Teh, Y. W.",
                "venue": "In International Conference on Learning Representations, 2020.",
                "url": null
            }
        },
        {
            "78": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.",
                "venue": "arXiv preprint arXiv:2307.09288, 2023.",
                "url": null
            }
        },
        {
            "79": {
                "title": "Attention is all you need.",
                "author": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I.",
                "venue": "In Advances in Neural Information Processing Systems, 2017.",
                "url": null
            }
        },
        {
            "80": {
                "title": "Archivalqa: A large-scale benchmark dataset for open-domain question answering over historical news collections.",
                "author": "Wang, J., Jatowt, A., and Yoshikawa, M.",
                "venue": "In International ACM SIGIR Conference on Research and Development in Information Retrieval, 2022.",
                "url": null
            }
        },
        {
            "81": {
                "title": "Augmenting language models with long-term memory.",
                "author": "Wang, W., Dong, L., Cheng, H., Liu, X., Yan, X., Gao, J., and Wei, F.",
                "venue": "In Advances in Neural Information Processing Systems, 2023.",
                "url": null
            }
        },
        {
            "82": {
                "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.",
                "author": "Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et al.",
                "venue": "In International Conference on Machine Learning, 2022.",
                "url": null
            }
        },
        {
            "83": {
                "title": "Memorizing transformers.",
                "author": "Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C.",
                "venue": "In International Conference on Learning Representations, 2022.",
                "url": null
            }
        },
        {
            "84": {
                "title": "Recomp: Improving retrieval-augmented lms with compression and selective augmentation.",
                "author": "Xu, F., Shi, W., and Choi, E.",
                "venue": "arXiv preprint arXiv:2310.04408, 2023.",
                "url": null
            }
        },
        {
            "85": {
                "title": "Metafun: Meta-learning with iterative functional updates.",
                "author": "Xu, J., Ton, J.-F., Kim, H., Kosiorek, A. R., and Teh, Y. W.",
                "venue": "In International Conference on Machine Learning, 2020.",
                "url": null
            }
        },
        {
            "86": {
                "title": "Evaluation of chatgpt and microsoft bing ai chat performances on physics exams of vietnamese national high school graduation examination.",
                "author": "Xuan-Quy, D., Ngoc-Bich, L., Xuan-Dung, P., Bac-Bien, N., and The-Duy, V.",
                "venue": "arXiv preprint arXiv:2306.04538, 2023.",
                "url": null
            }
        },
        {
            "87": {
                "title": "Dynamic language models for streaming text.",
                "author": "Yogatama, D., Wang, C., Routledge, B. R., Smith, N. A., and Xing, E. P.",
                "venue": "Transactions of the Association for Computational Linguistics, 2014.",
                "url": null
            }
        },
        {
            "88": {
                "title": "Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning.",
                "author": "Zadouri, T., \u00dcst\u00fcn, A., Ahmadian, A., Ermi\u015f, B., Locatelli, A., and Hooker, S.",
                "venue": "arXiv preprint arXiv:2309.05444, 2023.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.04317v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.04317v1",
        "paper_title": "Online Adaptation of Language Models with a Memory of Amortized Contexts",
        "research_background": "The paper titled \"Online Adaptation of Language Models with a Memory of Amortized Contexts\" addresses the challenges of keeping language models (LMs) up-to-date with new information efficiently while preventing the issue of catastrophic forgetting, making it highly relevant for applications that require current knowledge, such as chatbots and AI assistants.\n\n### Motivation\nLanguage models have rapidly advanced natural language processing and are integral to many real-world applications. However, these models are static, and their knowledge becomes outdated as the world evolves. Updating them with new data is computationally intensive and risks catastrophic forgetting, making it challenging to keep them current.\n\n### Research Problem\nThe paper aims to overcome the computational drawbacks and catastrophic forgetting associated with updating language models with new information. Specifically, it seeks to address the limitations of two existing approaches: (1) retrieval-augmented models, which are computationally and storage expensive, and (2) online finetuning techniques, which require extensive gradient computations and are sensitive to optimization parameters.\n\n### Prior Work\n1. **Static Language Models:** These have been central to NLP applications but suffer from outdated knowledge (Brown et al., 2020; Touvron et al., 2023; Chen et al., 2021; Xuan-Quy et al., 2023; Gao et al., 2023).\n2. **Retrieval-Augmented Models:** These store incoming documents and use retrieval systems to fetch relevant information for input queries. However, they struggle with counterfactual information and are not well-suited for edge computing due to high resource demands (Chen et al., 2017; Karpukhin et al., 2020; Longpre et al., 2021; Li et al., 2022; Si et al., 2023; Hu et al., 2023).\n3. **Online Finetuning:** This approach continuously updates the LM with new documents. While promising, it is computationally expensive and still susceptible to catastrophic forgetting (Lazaridou et al., 2021; Jang et al., 2022b; Hu et al., 2023).\n\n### New Approach\nThis study introduces the Memory of Amortized Contexts (MAC), a framework that combines the benefits of retrieval-augmented models and online finetuning while mitigating their drawbacks. It proposes:\n- **An end-to-end differentiable auxiliary retrieval system:** This works alongside the target LM to manage incoming documents, build a memory bank, and select relevant information.\n- **Amortization-based meta-learning:** To optimize modulations for the LM without needing labeled data at test time.\n- **Backpropagation dropout and hierarchical modulation aggregation:** These techniques enhance memory efficiency during training and inference.\n\n### Contribution\nOverall, the proposed MAC framework efficiently updates LMs by using parameter-efficient fine-tuning modulations that capture and retain knowledge from new documents, thus outperforming baseline approaches on multiple evaluation criteria.",
        "methodology": "In the \"Methodology\" section, the paper introduces the Memory of Amortized Contexts (MAC), a novel framework designed for the online learning of Language Models (LMs). Here is a detailed description of the proposed method, including its key components and innovations:\n\n### Problem Setup\nThe problem setup is briefly described in Section 3.1. The objective is to enable Language Models to adapt online to new data efficiently. This setup includes continuous model learning and updating as new information becomes available, with a focus on maintaining performance without substantial computational overhead.\n\n### Core Components\n#### Amortization and Aggregation Framework (Section 3.2)\nThe core innovation of MAC lies in its amortization and aggregation framework. This framework aims to handle the challenges of online adaptation by:\n1. **Amortization:** Efficiently compressing and encoding relevant contexts into a memory structure that can be easily updated with new information. Essentially, this involves summarizing and storing the historical context to avoid full retraining of the model with every new piece of data.\n2. **Aggregation:** Integrating new contexts into the existing memory in a way that maintains useful information and relevance over time. This ensures that the model can make use of both the recent and older contexts effectively.\n\n### Efficient Training and Inference Schemes for MAC (Section 3.3)\nThe MAC framework includes specific training and inference schemes designed to optimize performance:\n1. **Training:** The training procedure ensures that the language model can efficiently incorporate new data into its memory. This includes strategies to update the model parameters and memory in a balanced manner to prevent overfitting to the latest data while still adapting to trends and significant changes.\n2. **Inference:** The online adaptation process during inference involves leveraging the amortized memory to provide real-time predictions that are informed by the integrated historical and current contexts. This allows for efficient and accurate real-time responses by the model.\n\nBy structuring the methodology around these core components, MAC aims to provide a balanced approach to online learning for LMs, emphasizing both efficiency and effectiveness. The framework leverages amortized contexts and structured aggregation to ensure that the model remains current and relevant without excessive computational demands.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n#### Experimental Setup:\n**Datasets:** \nFor the main experiment, three QA datasets were employed: \n1. **StreamingQA** (Li\u0161ka et al., 2022). \n2. **SQuAD-Seq** (Rajpurkar et al., 2016; adapted from SQuAD for sequential document adaptation).\n3. **ArchivalQA-Seq** (Wang et al., 2022; adapted from ArchivalQA for sequential document adaptation).\n\nThese datasets were adjusted for online adaptation purposes, following prior works (Hu et al., 2023).\n\n**Models:** \n- The **GPT2 family** (Radford et al., 2018) was used as the static base LM.\n- Additional experiments were conducted using **LLaMA-2** (Touvron et al., 2023) to test scalability.\n- The **T5 model family** (Raffel et al., 2020) served as the amortization network due to its smaller size relative to the base LM.\n\nFor training, MAC parameters (\u03c3, \u03c1, \u03bd) were trained on document and QA pairs. Post-training, the models were adapted to a stream of 1,665 documents, each containing up to 512 tokens (using Byte Pair Encoding).\n\n**Baselines:** \nThe main baselines included:\n1. Uniform (with uniform token weighting).\n2. Salient Spans (assigns uniform weight to tokens in salient spans).\n3. CaMeLS (uses a meta-learned token weighting LM).\nThese baselines were fine-tuned to optimize their adaptation learning rates. Additionally, the joint use of MAC with the retrieval augmentation scheme BM25 was compared with BM25 and Contriever (Izacard et al., 2022).\n\n**Online Adaptation Setup:** \nThe models adapted to a stream of documents, evaluated subsequently using sampled QA pairs from the documents.\n\n#### Evaluation Metrics: \n- Model performance compared to other online learning techniques.\n- Efficiency in terms of memory and time.\n- Knowledge retention efficacy.\n- Memory usage savings from the proposed training and inference schemes.\n\n#### Main Experimental Results:\nThe results are summarized as follows:\n\n1. **Performance Comparison:**\n   - MAC outperformed other online learning techniques for language models, indicating superior adaptation capabilities.\n\n2. **Efficiency in Memory and Time:**\n   - MAC exhibited greater efficiency compared to online finetuning baselines regarding memory usage and time.\n\n3. **Knowledge Retention:**\n   - MAC demonstrated effective knowledge retention, maintaining performance levels better than other finetuning methods.\n\n4. **Memory Usage Savings:**\n   - The proposed training and inference schemes with MAC resulted in significant memory usage savings, validating the efficiency of the design.\n\nIn conclusion, MAC offers improved performance and efficiency for online adaptation of language models compared to traditional finetuning methods and baselines, confirming the advantages of its design components."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To empirically evaluate the MAC framework, verifying its performance, memory and time efficiency, and knowledge retention capabilities in an online adaptation setting for large language models (LLMs).",
            "experiment_process": "The experiments use three QA datasets: StreamingQA, SQuAD-Seq, and ArchivalQA-Seq. The base static LLMs are from the GPT2 family, and also LLaMA-2, with T5 models for the amortization network. MAC is trained on training datasets of document-QA pairs, then evaluated on a stream of 1,665 documents. Comparisons are made with online finetuning methods like Uniform, Salient Spans, and CaMeLS, with adaptations based on autoregressive finetuning. Evaluated metrics include exact match (EM) and F1 scores after adaptation.",
            "result_discussion": "MAC significantly outperforms other online finetuning baselines with a higher EM and F1 score. Notably, it is more memory and time efficient, reducing memory usage by 68.0% and adaptation time by 90.31% compared to CaMeLS. Additionally, MAC can be further improved with retrieval augmentations like BM25, enhancing its performance significantly.",
            "ablation_id": "2403.04317v1.No1"
        },
        {
            "research_objective": "To compare the knowledge retention of MAC against other online finetuning methods, focusing on catastrophic forgetting during the continuous adaptation process.",
            "experiment_process": "The evaluation focuses on the F1 score retention ratio, measured by the performance drop from the initially adapted 200 documents while adapting additional 1,400 documents.",
            "result_discussion": "MAC exhibits strong knowledge retention, maintaining 96.2% of its initial performance, compared to 70.8% for CaMeLS. This highlights the benefit of MAC's memory bank and aggregation mechanism in preserving knowledge even with an increasing number of adapted documents, addressing limitations of other online finetuning methods.",
            "ablation_id": "2403.04317v1.No2"
        },
        {
            "research_objective": "To verify the memory efficiency of MAC's proposed techniques, specifically backpropagation dropout and hierarchical modulation aggregation, for training and inference.",
            "experiment_process": "For training, the backpropagation dropout technique is evaluated by varying the dropout ratio and the number of amortized contexts, measuring GPU utilization. For inference, the hierarchical modulation aggregation technique is tested by varying the subgroup size and reporting GPU memory usage and F1 score, focusing on the memory used by modulation aggregation excluding LLM costs.",
            "result_discussion": "Increasing the dropout ratio effectively manages more contexts under memory constraints, with a 32 GB GPU handling a single document loss for the LLaMA-2 (7B) model. The hierarchical modulation aggregation reduces memory usage by 65.6% while preserving 93.2% of the original accuracy. These techniques are crucial for scaling larger models and managing numerous documents in online adaptation without additional training tricks or regularizations.",
            "ablation_id": "2403.04317v1.No3"
        }
    ]
}