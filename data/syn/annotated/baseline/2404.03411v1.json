{
    "title": "Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?",
    "abstract": "Various jailbreak attacks have been proposed to red-team Large Language Models (LLMs) and revealed the vulnerable safeguards of LLMs. Some methods extend the jailbreak attack to Multimodal Large Language Models (MLLMs) by perturbing the visual input. However, the absence of a universal evaluation benchmark complicates the performance reproduction and fair comparison. There is a lack of comprehensive evaluation of closed-source state-of-the-art (SOTA) models, especially MLLMs, such as GPT-4V. To address these issues, this work builds a comprehensive jailbreak evaluation dataset with 1445 harmful questions covering 11 different safety policies. Based on this dataset, extensive red-teaming experiments are conducted on 11 different LLMs and MLLMs, including both SOTA proprietary models and open-source models. We then conduct a deep analysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate better robustness against jailbreak attacks compared to open-source LLMs and MLLMs. (2) Llama2 and Qwen-VL-Chat are more robust compared to other open-source models. (3) The transferability of visual jailbreak methods is relatively limited compared to textual jailbreak methods. The dataset and code can be found here 111https://anonymous.4open.science/r/red_teaming_gpt4-C1CE/README.md.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have shown superior performance in text generation. To avoid generating unobjectionable content learned from the web-scale training corpus, stringent safety regulations have been applied during the safety alignment (Ouyang et al., 2022; Touvron et al., 2023). However, many jailbreak attacks have been proven to be able to bypass these safeguards and successfully elicit harmful generations. For example, Zou et al. appends a trainable suffix to harmful behavior prompts, which makes the model generate targeted output rather than refusing. Apart from perturbing the textual input, there are also jailbreaking methods modifying the visual input such as trainable image noise (Carlini et al., 2023; Qi et al., 2023) to ignore the safety regulation and elicit unethical output. However, the lack of a universal evaluation benchmark and performance metrics makes the performance reproduction and a fair comparison hard to achieve. Besides, comprehensive evaluations of SOTA proprietary models against jailbreak attacks are still missing, especially MLLMs such as GPT-4V. It is hence still unknown how robust these proprietary models are against existing jailbreak attack methods.\n\nTo ensure a reproducible and universal evaluation, in this work, we first constructed a comprehensive jailbreak evaluation dataset with 1445 jailbreak questions covering 11 different safety policies. Then 32 jailbreak methods targeted at LLMs and MLLMs are collected in this study, which contains 29 textual jailbreak methods and 3 visual jailbreak methods. Based on this benchmark, we then deployed extensive red-teaming experiments on 11 different LLMs and MLLMs including both SOTA proprietary models such as GPT-4, and open-source models such as Llama2 and MiniGPT4. We find that GPT-4 and GPT-4V show much better robustness against both textual and visual jailbreak methods compared to open-source models. Besides, among open-source models, Llama2 and Qwen-VL-Chat demonstrate better robustness and Llama2 can even be more robust than GPT-4.\n\nThe contribution of our work can be summarized as follows: We provide a jailbreak evaluation benchmark with 1445 harmful behavior questions covering 11 different safety policies for both LLMs and MLLMs. We conduct red-teaming on both GPT-4 and GPT-4V and various SOTA open-source models with our evaluation benchmarks. We provide an in-depth analysis showing the robustness of both business proprietary and open-source multimodal large language models against existing jailbreak methods."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Red Teaming GPT4 Against Jailbreak Attacks",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Experimental Setup",
            "text": "Models. The experiments are conducted on both proprietary business multimodal LLMs and open-source multimodal LLMs. Specifically, gpt-4-vision-preview (referred to as GPT-4 below) is used to conduct jailbreak red-teaming based on visual input perturbations; gpt-4-1106-preview (referred to as GPT-4V) is used in jailbreak attacks based on textual input perturbations. Besides, four open-source LLMs and six open-source VLMs have been chosen as our red-teaming target. In total, there are 11 models used in our study, and detailed information is presented in Tab. 3 in Appendix.\n\nDataset. To build a comprehensive jailbreak benchmark, we have collected jailbreak behaviors and questions from existing literature. In total, 1445 different harmful behaviors and questions have been collected. The dataset covers 11 different usage policies followed by Meta\u2019s Purple LLaMA and OpenAI\u2019s GPT4.\n\nThreat Model. The primary focus of this study is to investigate the transferability of existing jailbreak methods. Open-source models act as surrogate models and are used to train input modifications for jailbreak. These modifications then are used to red-team closed-source models such as GPT-4, and other open-source models. Specifically, Guanaco-7B, Llama2-7B, and Vicuna-7B are used as surrogate models for textual jailbreak attacks. MiniGPT4-7B is used as the surrogate model for visual jailbreak attacks.\n\nEvaluation Metrics. Various evaluation metrics have been proposed to calculate the attack success rate (ASR) in existing literature. They can be classified into four main categories: refusal word detection, evaluation based on toxicity detection APIs, LLMs as judges, and human evaluation. In this work, two matrices are mainly used to calculate the ASR, i.e., refusal word detection, and LLMs as judges. We follow the setting in previous work and count an attack as successful if a set of pre-defined refusal words is not found in the generated content. The pre-defined refusal words can be found in Appendix D. Besides, LLaMA-Guard is used as the jailbreak judge to check whether the generated content is indeed harmful or not. As LLaMA-Guard is open-source and instruction-tuned on a large harmful corpus, it is a more frugal way compared to using GPT-4 as judges. The detailed instructions to use LLaMA-Guard are in Appendix D. We report the Llama-Guard metric in the main paper and present the full metrics in Appendix E."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Red Teaming against Textual Jailbreak",
            "text": "Hand-crafted Jailbreak Attacks use pre-defined jailbreak templates or process functions and insert harmful questions into the templates, then send the whole instruction to LLMs. These hand-crafted attacks can be further classified into template-based and function-based.\n\nTemplate-based methods normally design instruction templates to describe a specific scenario to mislead the LLMs and elicit harmful content, such as role-playing Wei et al. (2024 ###reference_b29###) and do-anything-now Wei et al. (2024 ###reference_b29###).\n\nFunction-based methods need extra pre- or post-process on the input of harmful questions and generated content, such as using base64 encoding and vowel removal.\n\nThis study systematically investigates 27 different hand-crafted jailbreak attack methods including 17 templated-based (e.g., refusal suppression and evil confidant) and 10 function-based methods (e.g., encoding the harmful questions using base64 and removing vowels from the questions). Detailed information about all these methods is provided in Appendix E ###reference_### and the full results are presented in Tab. 8 ###reference_###.\n\nAutomatic Jailbreak Attacks optimize a string as part of the jailbreak input to elicit harmful content. \n\nIn our work, Guanaco-7B, Llama2-7B, and Vicuna-7B are used as surrogate models."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Red Teaming against Visual Jailbreak",
            "text": "Various methods have been proposed to jailbreak multimodal LLMs via the visual modality, \\ie, perturbing the visual input by either manual functions or automatic optimization. This work adopts 2 different jailbreak methods in total, both being optimization-based methods: VisualAdv (Qi et al., 2023  ###reference_b21###), and ImageHijacks (Bailey et al., 2023  ###reference_b4###). VisualAdv optimizes an adversarial example on a few-shot harmful corpus to maximize the probability of generating harmful content. ImageHijacks optimizes the adversarial example to maximize the generation probability of affirmative response to harmful requests. We use MiniGPT-4 as surrogate models for VisualAdv and ImageHijacks."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "In our experiments, GPT-4 is more robust against textual jailbreak methods in most cases. Among open-source LLMs used in this work, Llama2-7B is the most robust model whereas Vicuna-7B is the most vulnerable one. This can be because Vicuna does not implement any specific safeguard fine-tuning and the dataset used for fine-tuning has not been rigorously filtered. Llama2-7B, on the other hand, deploys safety alignment fine-tuning and a series of red teaming to ensure safe response. Regarding visual jailbreak in our experiments, it is much harder to successfully jailbreak GPT-4V compared to other open-source MLLMs. Among open-source MLLMs, Qwen-VL-Chat is the most robust against jailbreak attacks whereas MiniGPT4-7B is the most vulnerable. This can also be attributed to the different LLMs upon which these two MLLMs are built. MiniGPT4-7B used in this study is based on Vicuna-7B, which is not safely fine-tuned. Qwen-VL-Chat is built on Qwen-Chat, which is fine-tuned on a curated dataset relevant to safety.\n\nThere is no single method for achieving the highest attack success rate across different target models. Among visual jailbreak methods, certain attacks achieve a higher success rate across MLLMs compared to others.\n\nIn our experiments, there is a significant gap between open-source models and GPT-4 in most testing scenarios. For example, certain attacks can obtain success on Vicuna-7B and on Guanaco-7B, whereas GPT-4 defends almost all its requests. The same gap applies to visual jailbreaks. However, this does not indicate that GPT-4 and GPT-4V have a perfect defense against jailbreak attacks. In our experiments, visual jailbreak on GPT-4V does not demonstrate more vulnerability compared to textual jailbreak methods. This is attributed to input filtering as these attacks do not alter the original harmful questions, and GPT-4V is still able to refuse the requests.\n\nThe transferability of visual jailbreak methods studied in this work is relatively limited. The improvement of success rates is limited compared to the baseline, and sometimes the success rates of transfer attacks are even lower. For example, when attacking Fuyu by specific visual methods and using MiniGPT4-7B as the surrogate model, the success rate is lower than the baseline result. Additionally, the transfer attack of visual jailbreak methods on GPT-4V is not effective. The main reason is that these methods do not alter the harmful questions. GPT-4V can directly detect the harmful content in the input and thus refuse to respond."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This study focuses on red-teaming both proprietary and open-source LLMs and MLLMs. We first collected existing jailbreak datasets and constructed a comprehensive evaluation benchmark covering 11 different usage policies. Based on the evaluation benchmark, we conducted red-teaming experiments across 11 different LLMs and MLLMs. We find that GPT-4 and GPT-4V are much more robust compared to open-source models and the gap between them is significant. Compared to text modality, current visual jailbreak methods are hard to succeed on GPT-4V. Future work includes incorporating more jailbreak methods and datasets."
        }
    ],
    "url": "http://arxiv.org/html/2404.03411v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2.1"
        ],
        "main_experiment_and_results_sections": [
            "2.1",
            "2.2",
            "2.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2.2",
            "2.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.03411v1",
        "paper_title": "Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?",
        "research_background": "### Paper\u2019s Motivation\n\nThe motivation of the paper stems from the need to thoroughly evaluate the safety of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs), particularly in the context of their robustness against jailbreak attacks. Despite stringent safety measures applied during their development, these models have been shown to be vulnerable to various types of jailbreak attacks, which can bypass their safeguards and provoke harmful outputs. This concern is compounded by the shortage of a universal benchmark and performance metrics, as well as the absence of comprehensive evaluations of state-of-the-art (SOTA) proprietary models, especially MLLMs like GPT-4V, against such jailbreak attacks.\n\n### Research Problem\n\nThis study aims to address two primary gaps in the existing literature and practical applications:\n1. The lack of a universal evaluation benchmark and performance metrics makes performance reproduction and fair comparisons challenging.\n2. The absence of comprehensive evaluations of SOTA proprietary models, particularly MLLMs like GPT-4V, against existing jailbreak attack methods, leaves a gap in understanding the robustness of these models.\n\n### Relevant Prior Work\n\nThe paper builds on a foundation of significant prior work in the area of LLM and MLLM safety and jailbreak attacks:\n- **Safety Alignment:** The work of Ouyang et al. (2022) and Touvron et al. (2023) highlights the application of stringent safety regulations to prevent models from generating harmful content, aligning them with safety policies learned during training.\n- **Textual Jailbreak Attacks:** Zou et al. presented a method of appending a trainable suffix to harmful prompts, making the model generate targeted output rather than refusing to comply with the harmful prompt.\n- **Visual Jailbreak Attacks:** Carlini et al. (2023) and Qi et al. (2023) demonstrated how modifying the visual input, such as through trainable image noise, can also bypass safety regulations and generate unethical outputs.\n\nThis study extends these foundational studies by creating a comprehensive evaluation benchmark and conducting extensive red-teaming experiments on a variety of both proprietary and open-source models, thus offering a structured and replicable evaluation method for assessing the robustness of these models against jailbreak attacks.",
        "methodology": "### Methodology\n\n#### Models\nThe experiments are designed to evaluate the robustness of various large language models (LLMs) against jailbreak attacks. Specifically, the study utilizes both proprietary (closed-source) and open-source multimodal LLMs. The key models used in this study include:\n\n- **GPT-4-Vision-Preview (GPT-4)**: Employed for conducting visual input perturbation-based jailbreak red-teaming.\n- **GPT-4-1106-Preview (GPT-4V)**: Used for textual input perturbation-based jailbreak red-teaming.\n\nAdditionally, four open-source LLMs and six open-source Vision-Language Models (VLMs) are targeted for red-teaming, summing up to 11 models in total. Detailed descriptions of these models can be found in the appendix (Tab. 3).\n\n#### Dataset\nTo establish a comprehensive benchmark for jailbreak attempts, harmful behaviors and questions were aggregated from existing literature sources such as AdvBench, SafeBench, Qi et al., the GPT-4 technical report, and ToxicChat. The compiled dataset comprises 1445 distinct harmful behaviors and questions, spanning 11 different usage policies, including categories like violence, hate speech, and illegal activities, as per the guidelines of Meta's Purple LLaMA and OpenAI's GPT-4. More detailed information is located in Appendix C.\n\n#### Threat Model\nThe main goal is to assess the transferability of known jailbreak methods. The study uses open-source models as surrogate models to train alterations in input for jailbreak purposes, such as suffixes used in GCG and image noise techniques described by Qi et al. These modifications are then applied to both closed-source models like GPT-4 and other open-source models. The specifics include:\n\n- **Textual Jailbreak Attacks**: Using Guanaco-7B, Llama2-7B, and Vicuna-7B as surrogate models.\n- **Visual Jailbreak Attacks**: Using MiniGPT4-7B as the surrogate model.\n\n#### Evaluation Metrics\nThe attack success rate (ASR) of jailbreak attempts is quantified using various evaluation metrics from the literature, which fall into four primary categories:\n\n1. **Refusal Word Detection**: Analyzing the generated content for predefined refusal words.\n2. **Toxicity Detection APIs**: Assessing generated content using toxicity detection services.\n3. **LLMs as Judges**: Evaluating generated content with LLMs functioning as adjudicators.\n4. **Human Evaluation**: Human reviewers assessing the harmfulness of content.\n\nFor this study, the primary metrics employed are refusal word detection and LLMs as judges. An attack is marked as successful if no predefined refusal words are detected in the generated outputs, as per the methodology in Zou et al. and Liu et al. The list of predefined refusal words is available in Appendix D.\n\n**LLaMA-Guard** is utilized as the main judge for verifying the harmfulness of generated content. LLaMA-Guard is an open-source model, fine-tuned on a substantial harmful corpus, offering a cost-effective alternative to using more computationally expensive models like GPT-4 for evaluation. Detailed instructions on employing LLaMA-Guard are also provided in Appendix D. The study reports the metrics derived from LLaMA-Guard in the main findings, with complete metric details presented in Appendix E.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Models:** \nThe main experiments involve both proprietary business multimodal LLMs and open-source multimodal LLMs. Specifically, the models used are:\n- **Closed-source models**:\n  - **gpt-4-vision-preview (GPT-4)** for red-teaming based on visual input perturbations.\n  - **gpt-4-1106-preview (GPT-4V)** for jailbreak attacks based on textual input perturbations.\n- **Open-source models**:\n  - Four open-source LLMs.\n  - Six open-source VLMs.\nIn total, 11 models are studied, with detailed information available in Appendix Tab. 3.\n\n**Dataset:** \nA comprehensive jailbreak benchmark dataset has been constructed using jailbreak behaviors and questions collected from multiple sources:\n- **Existing literature**: AdvBench, SafeBench, Qi et al., GPT-4 technical report, and ToxicChat.\n- The dataset contains 1445 different harmful behaviors and questions.\n- Covers 11 different usage policies from Meta\u2019s Purple LLaMA and OpenAI\u2019s GPT-4, including categories like Violence and Hate, Illegal Weapons, etc.\n\n**Threat Model:**\nThe study focuses on the transferability of existing jailbreak methods:\n- **Surrogate models for training input modifications**:\n  - **Textual jailbreak attacks**: Guanaco-7B, Llama2-7B, and Vicuna-7B.\n  - **Visual jailbreak attacks**: MiniGPT4-7B.\n- These modifications are then used to red-team both closed-source models like GPT-4 and other open-source models.\n\n**Evaluation Metrics:**\nTo measure the attack success rate (ASR), the following metrics are employed:\n- **Refusal word detection**: Attacks are deemed successful if pre-defined refusal words are absent in the generated content.\n- **LLMs as judges**: LLaMA-Guard is used to evaluate whether the generated content is harmful. LLaMA-Guard is instruction-tuned on a large harmful corpus and acts as a more resource-efficient alternative to using GPT-4 as judges.\n- Detailed metrics and instructions for LLaMA-Guard are provided in the appendices.\n\n### Main Experimental Results\n\nThe experimental results summarize the effectiveness of various jailbreak methods in terms of ASR, using defined evaluation metrics:\n- **Attack Success Rate (ASR)**: Measured by refusal word detection and validated by LLaMA-Guard to check the harmfulness of content.\n- The detailed results, including the comparison of ASR across different models and settings, are summarized in the appendix, with a focus on the insights derived from the use of LLaMA-Guard as the primary evaluation tool.\n\n(Note: Detailed results and metrics are documented extensively in Appendix E for further review.)"
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate the effectiveness of hand-crafted and automatic jailbreak attacks on LLMs and their ability to generate harmful content.",
            "experiment_process": "This study examines 27 different hand-crafted jailbreak attack methods, classified into template-based and function-based. Template-based methods design specific instruction templates like role-playing (e.g., refusal suppression and evil confidant), while function-based methods modify questions or responses (e.g., encoding using base64 and vowel removal). Detailed methods and results are provided in the appendix and Tab. 8. For automatic jailbreak attacks, two popular methods, GCG and AutoDAN, are used, which optimize strings within the jailbreak input to elicit harmful responses. Gson-7B, Llama2-7B, and Vicuna-7B serve as surrogate models. The performance results are presented in Tab. 1.",
            "result_discussion": "The study finds that both template-based and function-based hand-crafted jailbreak attacks, as well as automatic attack methods, can successfully induce harmful responses from LLMs. The detailed efficacy of these methods is tabulated, demonstrating how different attack techniques can bypass existing safety mechanisms.",
            "ablation_id": "2404.03411v1.No1"
        },
        {
            "research_objective": "Assess the robustness of multimodal LLMs against visual jailbreak attacks and their ability to produce harmful content when visual inputs are perturbed.",
            "experiment_process": "Three methods are employed to jailbreak multimodal LLMs using visual input perturbations: FigStep (a black-box typography method), and two optimization-based methods, VisualAdv and ImageHijacks. VisualAdv creates adversarial examples using a few-shot harmful corpus to maximize harmful output probabilities, while ImageHijacks does so to prompt affirmative harmful responses. MiniGPT-4 is employed as the surrogate model for these attacks. Results are shown in Tab. 2.",
            "result_discussion": "The results highlight that visual jailbreak attacks can effectively manipulate multimodal LLMs to generate harmful content. Among the methods tested, optimization-based approaches such as VisualAdv and ImageHijacks proved particularly potent in eliciting affirmative harmful responses from the model.",
            "ablation_id": "2404.03411v1.No2"
        }
    ]
}