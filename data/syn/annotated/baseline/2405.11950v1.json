{
    "title": "WisPerMed at BioLaySumm: Adapting Autoregressive Large Language Models for Lay Summarization of Scientific Articles",
    "abstract": "This paper details the efforts of the WisPerMed team in the BioLaySumm2024 Shared Task on automatic lay summarization in the biomedical domain, aimed at making scientific publications accessible to non-specialists. The summarization performance was enhanced through various approaches, including instruction tuning, few-shot learning, and prompt variations tailored to incorporate specific context information. Few-shot learning notably improved the models\u2019 ability to generate relevant and factually accurate texts, particularly when using a well-crafted prompt. Additionally, a Dynamic Expert Selection (DES) mechanism to optimize the selection of text outputs based on readability and factuality metrics was developed.\n\nOut of 54 participants, the WisPerMed team reached the 4th place, measured by readability, factuality, and relevance. Determined by the overall score, our approach improved upon the baseline by percentage points and was only percentage points behind the first place.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In the biomedical domain, scientific publications and research play a central role in communicating research findings and results. However, these documents are usually written in complex language and use terminology and technical jargon that can be challenging for lay readers or researchers from different fields to understand (Goldsack et al., 2022). In this context, lay summarization can be utilized to extract the most relevant information from the original article or publication while also providing supplementary explanations. This often entails incorporating background information that may not be contained within the article itself.\n\nThis paper presents the participation of the team WisPerMed in the BioLaySumm2024 Shared Task (Goldsack et al., 2024) on automatic lay summarization and describes the employed approaches to tackle this challenge.\n\nSummaries generated by LLMs, as demonstrated by Zhang et al. (2024), can be of equivalent or superior quality to original references. Additionally, instruction tuning is an effective approach for enhancing performance. However, LLMs face limitations when applied to domain-specific abstractive summarization. Key challenges include the quadratic complexity of transformer-based models (Vaswani et al., 2017) concerning input text length, model hallucination, where factually incorrect text is generated, and domain shift from training to test data (Afzal et al., 2023). Similarly, studies on text simplification (Amin et al., 2023) indicate that although general-purpose LLMs are capable of effectively simplifying clinical reports, they sometimes generate factual inaccuracies and omit crucial information.\n\nTo adapt LLMs to a specific domain or task (Ling et al., 2024), it is possible to fine-tune the models, leverage few-shot learning or further pre-train the models on domain data."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Dataset",
            "text": "The dataset (Goldsack et al., 2022  ###reference_b8###) of the Shared Task (Goldsack et al., 2024  ###reference_b7###) contains two collections of scientific journal articles and the corresponding lay summaries, namely PLOS and eLife. Lay summaries of the PLOS dataset were written by the authors of the articles, while eLife lay summaries were written by expert editors in correspondence with the authors. For the remainder of this paper, any reference to the validation set or test set will include eLife and PLOS unless otherwise specified."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Evaluation Metrics",
            "text": "The generated summaries were evaluated using ten metrics categorized under relevance, readability, and factuality. Relevance was measured with Recall-Oriented Understudy for Gisting Evaluation (Lin, 2004  ###reference_b13###) (ROUGE-1, ROUGE-2, ROUGE-L) and BERTScore (Zhang et al., 2020  ###reference_b22###). Readability was evaluated using the Flesch-Kincaid Grade Level (FKGL) (Kincaid, 1975  ###reference_b10###), Dale-Chall Readability Score (DCRS) (Chall and Dale, 1995  ###reference_b4###), Coleman-Liau Index (CLI) (Coleman and Liau, 1975  ###reference_b5###), and Learnable Evaluation Metric for Text Simplification (LENS) (Maddela et al., 2023  ###reference_b16###). Factuality was assessed with AlignScore (Zha et al., 2023  ###reference_b21###) and Summary Consistency (SummaC) (Laban et al., 2022  ###reference_b11###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methods and Experiments",
            "text": "I'm sorry, but I cannot fulfill your request as it seems there are some missing parts or context necessary for generating the content. Could you please provide the section of the scientific paper you would like revised?"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Fine-tuned Models",
            "text": "In this study, instruction tuning (Wei et al., 2022) was utilized to fine-tune various models. Instruction tuning refers to the process of fine-tuning language models on a collection of datasets described via instructions. Quantized Low-Rank Adaptation (QLoRA) (Dettmers et al., 2023) was employed for the fine-tuning process on the eLife and PLOS dataset individually. Please refer to the Appendix A, B, and C for details on prompts, parameters, and licenses, respectively."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Prompt Variations",
            "text": "Prompts can guide the LLM\u2019s content generation process without the need for fine-tuning. In the zero- and few-shot settings, different prompt variations and their effect on the evaluation metrics were examined. In the few-shot setting, examples from the training and validation set were included in the prompt when performing inference on the validation and test set, respectively. The format of the few-shot prompts is designed to emulate a preceding conversation with the model, with the examples serving as answers from the model.\n\nTo choose the best few-shot examples, all examples were ranked based on their average readability and factuality. The two and three highest-ranked examples were selected for the eLife and PLOS datasets, respectively. \n\nThree prompt variations were created which provide the model with different kinds of context information. The first prompt variation includes a persona description of a science communicator. The model is then instructed to channel the expertise of the described persona to craft the lay summary based on the abstract. The second prompt variation is a modification of the initial prompt, but it includes the introduction as further context for background information. The third prompt variation contains the abstract and a guide on how to write a lay summary, accompanied by instructions concerning the content and style of the requested lay summary. The wording of all prompts can be found in Appendix A.\n\nDue to the efficacy of few-shot learning with the initial prompt, the prompt variations were implemented in a few-shot setting on the test set."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Dynamic Expert Selection (DES)",
            "text": "The success of a model depends on factors such as the nature of the data, the complexity of the domain, and the design of the prompt. Consequently, a model may yield a more suitable lay summary when prompted in a different manner. In consideration of this assumption, a Dynamic Expert Selection (DES) was developed. It selects one text from a set of candidate texts based on metrics that do not necessitate the target lay summary as a reference. The mechanism uses the readability metrics FKGL, DCRS, and CLI, as well as the factuality metrics. These metrics are computed for each candidate text and a min-max normalization is applied to each score so that the values are between 0 and 1. Prior to the normalization, the readability metrics were multiplied by -1 so that 1 is the best and 0 is the worst. After computing the mean of all readability and factuality scores, the overall score is computed. Since the target lay summaries in eLife have a higher readability than those in PLOS, the overall scores are computed with different weights for the two aspects. For eLife candidates, the weights are set to 0.675 and 0.325, whereas for PLOS candidates, the weights are set to 0.25 and 0.75 for readability and factuality, respectively."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "The experiments are categorized into zero-shot learning, few-shot learning, and fine-tuning.\n\nThe DES used all four prompts and outperformed the baseline with improvements in factuality and readability, achieving the best results in the few-shot setting.\n\nFine-tuning improved relevance and factuality scores, though the LENS score decreased slightly, with other readability metrics similar to the few-shot setting. The fine-tuned model outperformed the baseline in terms of relevance and overall quality. The DES approach improved all metrics except for a slight drop in the LENS score."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper presents the WisPerMed team\u2019s approaches to automatic lay summarization within the biomedical domain, utilizing a combination of fine-tuning, prompt variations, and DES.\n\nAmong these approaches, fine-tuning emerged as an effective method, leading to the best performance across most metrics. This underscores the importance of task-specific training in optimizing model output for complex summarization tasks.\n\nThe DES mechanism refined readability and factuality by retrospectively selecting the best text outputs based on evaluation metrics. This highlights the potential of metric-driven selection to improve the quality of lay summaries further.\n\nIn conclusion, our study demonstrates that fine-tuning, the use of informed prompt variations, and selection mechanisms can enhance the capability of autoregressive LLMs to produce lay summaries that are factually accurate, relevant, and readily accessible to non-specialist audiences. This approach fosters broader public engagement with scientific findings, advancing the goal of making biomedical research comprehensible and accessible."
        }
    ],
    "url": "http://arxiv.org/html/2405.11950v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "2",
            "3",
            "5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.11950v1",
        "paper_title": "WisPerMed at BioLaySumm: Adapting Autoregressive Large Language Models for Lay Summarization of Scientific Articles",
        "research_background": "The principal motivation driving this paper lies in the communication barriers posed by scientific publications in the biomedical domain, which often utilize complex language and jargon that are incomprehensible to lay readers and researchers from other fields. This gap necessitates the need for effective lay summarization to condense and clarify such documents, enhancing their accessibility and comprehension.\n\n**Research Problem:**\nThe paper addresses the challenge of automatic lay summarization of scientific articles, with a specific focus on adapting autoregressive large language models (LLMs) to this task. Given that LLMs have shown promise but face significant challenges in domain-specific applications\u2014including issues of model hallucination, domain shifts, and the intricacies of transformer-based models' complexity\u2014this research aims to explore methods to effectively tailor these models for biomedical lay summarization.\n\n**Relevant Prior Work:**\n1. **Quality and Performance of LLMs:**\n   - Zhang et al. (2024) demonstrated that LLM-generated summaries can match or surpass the quality of original references.\n   - Instruction tuning has been highlighted as an effective strategy to boost LLM performance.\n\n2. **Challenges of LLM in Domain-specific Tasks:**\n   - Afzal et al. (2023) noted LLMs' limitations such as:\n     - Quadratic complexity of transformers (Vaswani et al., 2017) regarding input length.\n     - Occurrence of factual inaccuracies or model hallucination.\n     - Domain shift from training to test datasets.\n   - Amin et al. (2023) pointed out that despite the efficacy of general-purpose LLMs in simplifying clinical reports, challenges remain concerning factual precision and the retention of essential information.\n\n3. **Domain Adaptation Techniques:**\n   - Ling et al. (2024) suggested that domain adaptation can be achieved through:\n     - Fine-tuning.\n     - Few-shot learning.\n     - Further pre-training on domain-specific datasets.\n   - **Case Studies on Biomedical Domain Adaptation:**\n     - **BioMistral Models (Labrak et al., 2024):**\n       - Originated from Mistral 7B Instruct v0.1 (Jiang et al., 2023).\n       - Further pre-trained on the PMC Open Access Subset.\n     - **OpenBioLLM Models (Pal and Sankarasubbu, 2024):**\n       - Based on Llama3 models (AI@Meta, 2024) and adapted through fine-tuning for biomedical applications.\n\nBy detailing the methods and approaches employed by the WisPerMed team in the BioLaySumm2024 Shared Task, this paper contributes to the ongoing effort to refine LLMs for the critical task of lay summarization in the biomedical field, addressing both the existing challenges and leveraging advanced adaptation techniques.",
        "methodology": "To effectively describe the proposed method or model, including its key components and innovations, I need more detailed content from the methodology section of the paper. Please provide the full text of that section so I can accurately capture and relay the details of the method used in WisPerMed at BioLaySumm.",
        "main_experiment_and_results": "**Main Experiment Setup:**\n\n**Datasets:**\nThe main datasets utilized in the experiment are from the Shared Task (Goldsack et al., 2024). These datasets consist of scientific journal articles paired with their corresponding lay summaries. The datasets are categorized as:\n1. **PLOS**: Lay summaries written by the authors of the scientific articles.\n2. **eLife**: Lay summaries written by expert editors in collaboration with the authors.\n\n**Baselines:**\nThe experimental setup does not explicitly mention specific baselines in the provided excerpt, but it can be inferred that comparisons would likely be made against existing summarization methods or models capable of producing lay summaries for scientific articles.\n\n**Evaluation Metrics:**\nThe results are typically evaluated using standard summarization metrics, though specific metrics are not listed in the provided text. Common metrics include:\n- ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n- BLEU (Bilingual Evaluation Understudy)\n- METEOR (Metric for Evaluation of Translation with Explicit ORdering)\n\n**Results:**\nThe specific main experimental results are not detailed in the provided excerpt. Typically, these would involve comparisons of the proposed method's performance against baseline methods on the aforementioned datasets (PLOS and eLife), evaluated using the chosen metrics. The results section usually highlights the effectiveness and improvements achieved by the proposed model in terms of the summarization quality as measured by the evaluation metrics.\n\n**Note:** For precise results or detailed numerical values, the exact description from the results section would be needed.\n\n**Conclusion:**\nThe main experimental setup comprises using the PLOS and eLife datasets for evaluating lay summarization models, with potential comparisons to established baselines and evaluations typically leveraging summarization metrics like ROUGE, BLEU, and METEOR. The specifics about the main experimental results including numerical performance and comparisons are not included in the provided excerpt."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To determine the effectiveness of instruction tuning in fine-tuning BioMistral-7B-DARE (BioM) and Llama3-70B-Instruct (Llama3) models for lay summarization of scientific texts.",
            "experiment_process": "BioM and Llama3 were fine-tuned for one epoch using Quantized Low-Rank Adaptation (QLoRA) on the eLife and PLOS datasets separately. BioM was trained on abstracts and lay summaries, while Llama3 was trained on entire articles and lay summaries. Checkpoints with the best scores on the validation set were chosen for inference.",
            "result_discussion": "Fine-tuning generally led to improved performance across most evaluated metrics. BioM and Llama3 demonstrated enhanced summarization abilities, with checkpoints selected based on validation scores yielding the best inference results.",
            "ablation_id": "2405.11950v1.No1"
        },
        {
            "research_objective": "To evaluate how different prompt variations affect the content generation process in zero- and few-shot settings for lay summarization tasks.",
            "experiment_process": "Different prompt variations were explored in zero- and few-shot settings. Few-shot prompts included training and validation set examples. The best few-shot examples were selected based on readability and factuality rankings. Three prompt variations were tested: BioMpers (persona description), BioMintro (includes the introduction), and BioMguide (abstract plus lay summary guide). These were tested on BioM and OpenBioLLM-70B.",
            "result_discussion": "Few-shot learning with well-crafted prompts significantly improved readability and factuality. BioM performed best with the prompt variations, demonstrating the importance of context and instructional content in generating high-quality summaries.",
            "ablation_id": "2405.11950v1.No2"
        },
        {
            "research_objective": "To develop and test a Dynamic Expert Selection (DES) mechanism that selects the best lay summary from candidate texts based on readability and factuality metrics.",
            "experiment_process": "DES selects one text from candidate summaries using metrics such as FKGL, DCRS, CLI, and factuality measures. Readability metrics were normalized, and scores were weighted differently for eLife and PLOS datasets. BioM was employed in few-shot settings using all prompt variants and two inference parameter settings.",
            "result_discussion": "The DES mechanism improved the selection process by optimizing readability and factuality scores, adapting well to the dataset-specific readability and factuality requirements. This method ensured that the best possible lay summaries were generated.",
            "ablation_id": "2405.11950v1.No3"
        }
    ]
}