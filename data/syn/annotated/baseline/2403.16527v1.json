{
    "title": "Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art",
    "abstract": "Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to agricultural field robots, and from health care assistants to the entertainment industry. The majority of these systems are developed with modular sub-components for decision-making, planning, and control that may be hand-engineered or learning-based. While these existing approaches have been shown to perform well under the situations they were specifically designed for, they can perform especially poorly in rare, out-of-distribution scenarios that will undoubtedly arise at test-time.\n\nThe rise of foundation models trained on multiple tasks with impressively large datasets from a variety of fields has led researchers to believe that these models may provide \u201ccommon sense\u201d reasoning that existing planners are missing. Researchers posit that this common sense reasoning will bridge the gap between algorithm development and deployment to out-of-distribution tasks, like how humans adapt to unexpected scenarios.\n\nLarge language models have already penetrated the robotics and autonomous systems domains as researchers are scrambling to showcase their potential use cases in deployment. While this application direction is very promising empirically, foundation models are known to hallucinate and generate decisions that may sound reasonable, but are in fact poor. We argue there is a need to step back and simultaneously design systems that can quantify the certainty of a model\u2019s decision, and detect when it may be hallucinating.\n\nIn this work, we discuss the current use cases of foundation models for decision-making tasks, provide a general definition for hallucinations with examples, discuss existing approaches to hallucination detection and mitigation with a focus on decision problems, and explore areas for further research in this exciting field.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "It is an exciting time to be a machine learning and robotics researcher. A great deal of progress has been made in the last decade and a half with regards to the efficacy and efficiency of models for perception, decision-making, planning, and control (Soori et al., 2023; Janai et al., 2020). Broadly speaking, approaches to these problems fall under one of two umbrellas: hand-engineered model-based systems and data-driven learning-based models (Formentin et al., 2013). With some deployment scenario in mind, developers may hand-engineer rules (Hayes-Roth, 1985) or tune a controller (Borase et al., 2021) to be tested, or in the case of learning-based models, collect training data and craft some reward function to fit a model to an objective, given said data (Henderson et al., 2018). In practice, these methods work particularly well in the scenarios that they were specifically designed and trained for, but may produce undesirable results in previously unseen out-of-distribution cases (Wen et al., 2023). Designers may choose to add more rules, re-tune their controller, fine-tune their model to a more representative dataset, fix the reward function to handle edge cases, or even add a detector (which may itself be rule-based or data-driven) at test-time to identify out-of-distribution scenarios before calling on the decision-maker (Singer and Cohen, 2021; Schreiber et al., 2023; Chakraborty et al., 2023). However, even with these changes, there will always be other situations that designers had not previously considered which will come about during deployment, leading to sub-optimal performance or critical failures. Furthermore, the modifications made to the model may have unforeseen effects at test-time like undesired conflicting rules (Ekenberg, 2000) or catastrophic forgetting of earlier learned skills (Kemker et al., 2018). Informally, classical methods and data-driven approaches lack some form of common sense that humans use to adapt in unfamiliar circumstances (Fu et al., 2023a).\n\nRecently, researchers have explored the use of large (visual) language models, L(V)LMs, to fill this knowledge gap (Cui et al., 2024). These models are developed by collecting and cleaning an enormous natural language dataset, pre-training to reconstruct sentences on said dataset, fine-tuning on specific tasks (e.g., question-answering), and applying human-in-the-loop reinforcement learning to produce more reasonable responses (Achiam et al., 2023). Even though these models are another form of data-driven learning that attempt to maximize the likelihood of generated text conditioned on a given context, researchers have shown that they have the ability to generalize to tasks they have not been trained on, and reason about their decisions. As such, these foundation models are being tested in tasks like simulated decision-making (Huang et al., 2024b) and real-world robotics (Zeng et al., 2023) to take the place of perception, planning, and control modules.\n\nEven so, foundation models are not without their limitations. Specifically, these models have a tendency to hallucinate, i.e., generate decisions or reasoning that sound plausible, but are in fact inaccurate or would result in undesired effects in the world. This phenomenon has led to the beginning of a new research direction that attempts to detect when L(V)LMs hallucinate so as to produce more trustworthy and reliable systems. Before these large black-box systems are applied in safety-critical situations, there need to be methods to detect and mitigate hallucinations. Thus, this survey collects and discusses current hallucination mitigation techniques for foundation models in decision-making tasks, and presents potential research directions. Existing surveys particularly focus on presenting methods for hallucination detection and mitigation in question-answering (QA) (Ji et al., 2023; Rawte et al., 2023; Zhang et al., 2023d; Ye et al., 2023) or object detection tasks (Li et al., 2023c). There are also other works that provide examples of current use cases of L(V)LMs in autonomous vehicles (Yang et al., 2023b) and robotics (Zeng et al., 2023; Zhang et al., 2023a). Wang et al. (2023a) perform a deep analysis of the trustworthiness of a variety of foundation models and Chen and Shu (2024) provide a taxonomy of hallucinations within LLMs, but both exclude applications to general decision problems. To the best of our knowledge, we are the first to propose a general definition of hallucinations that can be flexibly tuned to any particular deployment setting, including commonly found applications to QA or information retrieval, and more recent developments in planning or control. Furthermore, there is no"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Foundation Models Making Decisions",
            "text": "Originally coined by Bommasani et al. (2022), the term foundation models refers to models that are \u201ctrained on broad data at scale such that they can be adapted to a wide range of downstream tasks.\u201d This approach is in contrast to works that design and train models on a smaller subset of data for the purpose of being deployed to a specific task Yang et al. (2024). The key difference is that foundation models undergo a pre-training procedure on a large-scale dataset containing information from a variety of possible deployment fields, through which they are expected to learn more general features and correspondences that may be useful at test-time on a broader set of tasks Zhou et al. (2023); Zhao et al. (2023). Examples of existing pre-trained foundation models span language Devlin et al. (2019); Brown et al. (2020); Touvron et al. (2023a), vision Caron et al. (2021); Oquab et al. (2024); Kirillov et al. (2023), and multi-modal Radford et al. (2021); Achiam et al. (2023) inputs. In this section, we give a brief overview of existing use cases for foundation models in robotics, autonomous vehicles, and other decision-making systems. We also succinctly point out hallucinations found in these works and leave a lengthier discussion in Section 3.2. Readers should refer to works from Yang et al. (2023b), Zeng et al. (2023), and Zhang et al. (2023a) for a deeper review of application areas."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Autonomous Driving",
            "text": "For the autonomous vehicle domain, researchers have formulated the use of language foundation models as a fine-tuning and prompt engineering problem. An external sub-system is usually designed with (1) a perception module to process signals from raw sensors, (2) a memory bank of prior important experiences and its corresponding similarity function to find alike scenarios, and (3) a prompt generator to convert current sensor data and relevant memories into natural language that can be input to the foundation model. Currently, works either fine-tune LLMs with a few examples, or directly apply the model in a zero-shot manner, on a QA task with driving related questions. By framing the task in a QA form, researchers have been able to provide context to the L(V)LM to probe for high-level natural language decisions Wen et al. (2023), path planning Mao et al. (2023); Sima et al. (2023), vehicle tracking and trajectory prediction Wu et al. (2023b); Keysan et al. (2023), descriptions of the surroundings of the vehicle Chen et al. (2023c); Xu et al. (2024), and low-level control Liu et al. (2023). Wen et al. (2024) propose DiLu, a framework consisting of reasoning, reflection, and memory modules that support an LLM in producing high-level decisions for autonomous driving, and they test their method within a driving simulator environment. Specifically, the reasoning module views the current observation of the vehicle, queries the memory module for any similar situations that were encountered in the past, and converts the experience into a prompt, which is input to the LLM. The prompt is formatted such that it elicits chain-of-thought reasoning Wei et al. (2022) from the LLM, which is shown to improve the accuracy of the model. The generated text output by the LLM is summarized by the reflection module, and is used to update the memory bank of experiences. A separate decision decoder model converts the summary into a discrete high-level decision (e.g., idle, turn right, accelerate, etc.) to take in the simulator. Agent-Driver from Mao et al. (2023) utilizes a tool library of functions that communicate with neural modules that are responsible for object detection, trajectory prediction, occupancy estimation, and mapping. The LLM is asked to reason about what information would be helpful to plan a path of the ego vehicle, and calls on functions from the tool library to iteratively build up relevant context. Like Wen et al. (2024), the authors use a memory bank of prior driving experiences to bolster the context provided to the LLM. With this context, the LLM predicts a suitable path for the ego vehicle to follow. If a collision is detected between the predicted trajectory and surrounding objects in the scene, the LLM undergoes self-reflection, like Reflexion Shinn et al. (2023), another hallucination mitigation technique, to fine-tune its prediction. Through a short study, the authors test the frequency of invalid, hallucinated outputs from the model, and find that their self-reflection approach results in zero invalid generations at test-time. Sima et al. (2023) build up context before predicting a path for the ego vehicle by asking a VLM questions about its perception of surrounding vehicles, predicting their behaviors, planning high-level ego vehicle decisions, converting to lower level discrete actions, and finally, estimating a coordinate-level trajectory. Their method, DriveLM-Agent, predicts paths from raw images in an end-to-end manner using a multi-modal approach, whereas Agent-Driver requires sensor modules to process context separately. Wu et al. (2023b) propose PromptTrack as a method to predict bounding boxes and trajectories of vehicles in multi-view camera scenes conditioned on a text prompt. PromptTrack is another end-to-end method that encodes multi-view images with an image encoder, decodes previously tracked boxes and current detections into new tracks, uses a language embedding branch to predict 3D bounding boxes, and updates the memory of current tracks using past & future reasoning branches. Rather than using ego vehicle point of view images for object tracking, Keysan et al. (2023) propose an approach to convert rasterized images from a birds-eye view of the scene into a prompt describing the past trajectories of vehicles with B\u00e9zier curves. This method combines vision and language encoders to generate the B\u00e9zier curve-based scene description, and elicits a language model to predict trajectories in a similar format. Works using foundation models for generating scene descriptions given multi-modal information frame the task as a QA problem. For example, Chen et al. (2023c) use a reinforcement learning (RL) agent pre-trained on the driving task in simulation to collect a dataset containing the vehicle\u2019s state, environment observation (assumed to be ground truth from simulator,) low-level action, and the ego\u2019s percentage of attention placed on"
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Robotics",
            "text": "Foundation models have been used in the robotics domain for object detection, affordance prediction, grounding, navigation, and communication. Ichter et al. (2023) address the issue of misalignment between a robot's capabilities and what a language model believes it is capable of performing. This gap, due to models not being trained with data from the specific robots they are deployed on, can lead to impractical outputs. The authors propose SayCan as a method to integrate the general knowledge of language models with the specific capabilities of a robot. An LLM is tasked with generating a list of smaller actions to complete a task, constrained by (1) the model's token probability distribution and (2) available robot skills, prioritizing actions with the highest combined probability of token generation and successful execution. \n\nPaLM-E, presented by Driess et al. (2023), is a multi-modal model that processes sensor inputs like images into token-space embeddings, integrating them with instruction embeddings for input into a PaLM language model. This setup enables the system to answer questions about the robot's environment or plan action sequences to complete tasks. Driess et al. emphasize the increased risk of hallucinations due to this multi-modal approach. \n\nInspired by success in using foundation models to generate programs, other works focus on deploying models to write low-level code for robots. Liang et al. (2023) introduce Code as Policies, where LLMs hierarchically generate interactive code and functions for robots. The models leverage existing function libraries or dynamically develop custom methods. Hu et al. (2024) propose the RoboEval benchmark to test robot-agnostic LLM-generated code via the CodeBotler platform, which provides abstract functions like \u201cpick\u201d and \u201cplace.\u201d RoboEval Temporal Logic (RTL) checks if the generated code meets task and temporal constraints, also testing robustness by using paraphrased prompts. This consistency-checking is discussed further in Section 4.3.1.\n\nIn robot navigation, LM-Nav uses a vision-language model (VLM) to predict a sequence of waypoints based on language commands, as shown by Shah et al. (2023). They apply in-context learning to teach a model like GPT-3 to extract landmarks from instructions. Using CLIP, LM-Nav matches these landmarks to waypoint images. Dynamic programming then optimizes the landmark path, aiming to maximize instruction completion likelihood."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Other Areas",
            "text": "There are also other works that apply foundation models for decision-making outside of the robotics and autonomous vehicle domains. For example, ReAct from Yao et al. (2023b ###reference_b144###) identifies that a key limitation of chain-of-thought reasoning Wei et al. (2022 ###reference_b131###) is that the model does not update its context or action based on observations from an environment. As such, chain-of-thought reasoning relies purely on the internal reasoning of the foundation model itself to predict actions to take, missing a crucial step in grounding its actions with their effects on the environment. Given a prompt, ReAct iterates between an internal reasoning step and acting in the environment to build up context relevant to the task. Yao et al. (2023b ###reference_b144###) showcase the promise of the method in a QA setting where the LLM can take actions to query information from an external knowledge base, as well as an interactive text-based game, ALFWorld Shridhar et al. (2021 ###reference_b114###).\n\nChen et al. (2023b ###reference_b14###) admit that ReAct is a powerful tool for dynamic reasoning and grounding, but is limited by the fact that the updated context from the Act step is only helpful for the particular task the model is currently deployed for. They propose Introspective Tips to allow an LLM to reason about its past successes and failures in a world to generate general tips that will be helpful across diverse instruction-following tasks. Specifically, tips are generated from the past experience of the model from a similar set of tasks, from expert demonstrations, and from several games that differ from the target task. By summarizing these experiences into more concise tips, Chen et al. (2023b ###reference_b14###) show that Introspective Tips outperform other methods in ALFWorld with both few- and zero-shot contexts.\n\nPark et al. (2023 ###reference_b94###) and Wang et al. (2023b ###reference_b128###) apply foundation models in more complex environments to push models to their limits to simulate realistic human behaviors and test lifelong learning. Park et al. (2023 ###reference_b94###) propose generative agents that produce believable, human-like interactions and decisions within a small town sandbox environment. They develop a module for individual agents in the simulation to store and retrieve memories, reflect about past and current experiences, and interact with other agents. Their generative agents use similar methods to ReAct and Introspective Tips to act based on a memory of experiences, but also interact and build relationships with other agents through dialogue. The authors show that the agents are able to effectively spread information, recall what has been said to others and stay consistent in future dialogue interactions, and coordinate events together. Sometimes, however, agents are found to hallucinate and embellish their responses with irrelevant details that may be attributed to the training dataset of outside, real-world knowledge.\n\nKwon et al. (2023 ###reference_b66###) explore the use of LLMs to act as a proxy for a hand-tuned reward function in RL tasks. This application is particularly motivated by decision-making tasks that are difficult to specify with a reward function, but can be explained textually with preferences of how a policy should generally act. Specifically, the LLM evaluator first undergoes in-context learning with examples of how it should decide the reward in several cases of the task that the agent will be deployed to. Then, during RL training, the LLM is provided a prompt with the trajectory of the agent within the episode, the resulting state from the simulator, and the original task objective from the user, and is asked to generate a binary reward for the agent (1 if success, 0 else). The binary reward is added to the experience replay, and the agent can be updated using any RL algorithm. Kwon et al. (2023 ###reference_b66###) find that a baseline in their work that predicts rewards with no in-context learning especially hallucinates with incoherent reasoning."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Hallucinations",
            "text": "Even with all their success in a multitude of deployment areas, foundation models still produce inconsistent outputs, or hallucinate, at test-time. Here, we provide a general definition for hallucinations that can be applied to any foundation model deployment task, including various autonomous systems. Additionally, we give examples of hallucinations encountered in literature and discuss how they come about during testing.\n\nA work evaluating the frequency at which LVLMs hallucinate in their descriptions of images finds that these models\u2019 outputs may include non-existent objects or additional irrelevant phrases (that may not even be possible to test for accuracy) Li et al. (2023c). For example, in a picture of food on a table, an LVLM hallucinates a non-existent beverage and predicts that the \u201ctable is neatly arranged, showcasing the different food items in an appetizing manner.\u201d Although the classification error and irrelevant generation in this example are not critical, earlier works warn of possible failures with more severe, high societal impact (e.g., biases in models leading to marginalizing users) Bommasani et al. (2022).\n\nChen et al. (2021) explore alignment failures of LLMs applied to code completion tasks. The authors evaluate the likelihood of these models generating defective code given different input prompts and discover that in-context learning using examples with buggy code has a higher chance of resulting in poor generations from the model on the actual task at hand. The study also identifies similar model biases towards race, gender, religion, and other representations. Furthermore, the authors find that their model, Codex, is able to generate code that could assist with developing insecure applications or malware, albeit in a limited manner. These findings have been corroborated by other foundation model code generation works in the robotics domain. For example, Wang et al. (2023b) describe that Voyager sometimes generates code with references to items that do not exist within MineDojo. Similarly, Hu et al. (2024) find that their model has a tendency to call functions with invalid objects or locations, pickup objects when it is already holding something, ask for help when no one is near, and other undesired behaviors.\n\nSeveral works focus on identifying cases of hallucinations in QA tasks. Although this application area is not the direct focus of this work, we present examples of hallucinations in this field as we can glean similar failure modes that could arise within decision-making systems. Common hallucinations in QA result in incorrect answers to questions. Another set of works identify hallucinations with contradictions among several sampled generations from an LLM, discussed further in Section 4.3.1 M\u00fcndler et al. (2024); Zhang et al. (2023b). Intuitively, if a context passed into a model results in conflicting generations, the model must be hallucinating some part of the output. Notice in this example, with relation to Definition 3.1, self-contradiction works test for consistency among multiple (hallucinated) generations, rather than with respect to a ground-truth knowledge-base that usually exists in QA tasks. As such, our definition can flexibly apply to different system setups by describing consistency, desired behavior, and relevancy respectively."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "What are hallucinations?",
            "text": "Across current literature on foundation models, there exist similar patterns and themes that can be used to develop a unified definition for hallucinations. With the majority of works studying this problem within QA tasks, where ground truth answers are available, several authors explain hallucinations as producing text that includes details/facts/claims that are fictional/misleading/fabricated rather than truthful or reliable. Works making use of a dedicated knowledge-base further describe hallucinations as generating nonsensical or false claims that are unsubstantiated or incorrectly cited. Some authors also present the idea that foundation models may sound syntactically correct, or coherent, while simultaneously being incorrect. Other works, who explore hallucinations of vision-language models (VLMs) in detecting and classifying objects within images, define hallucinations as generating object descriptions inconsistent with target images.\n\nA common theme among existing hallucination definitions for QA, information retrieval, and image captioning domains is that, while the generation may sound coherent, either the output is incorrect, or the model\u2019s reasoning behind the generated text is incorrect. However, these characteristics on their own do not completely encompass the hallucinations found in decision-making tasks, thus requiring additional nuances. Within papers that apply foundation models to decision-making tasks specifically, researchers have encountered similar problems of hallucinations impacting performance. Hallucinations are described as predicting an incorrect feasibility of an autonomous system when generating an explanation behind the uncertainty of an action to take. Studies find that language models may provide incoherent reasoning behind their actions and that these generative models also have a sense of high (false) confidence when generating incorrect or unreasonable plans. In the case of robot navigation and object manipulation, hallucinations refer to attempting to interact with non-existent locations or objects.\n\nMetric frameworks for various tasks can be outlined as:\n\n- Questing-Answering: Generations must align with database facts; tone of answer should be informative; answers should not include references to unrelated topics. \n- Image Captioning: Objects in description must appear in image; censor descriptions for inappropriate images; descriptions should not be embellished with details that cannot be confirmed.\n- Planning: Predicted sub-task must be feasible to solve; plans should maximize expected return; predicted sub-tasks and actions should not stray from the end goal with unnecessary steps.\n- Control: Predicted action must be possible to perform; predict actions to complete plan efficiently.\n\nIn the code generation task, terms like \u201calignment failure\u201d are used, with similar effects to those of hallucinations discussed above. More specifically, this is informally described as an outcome where a model is capable of performing a task but chooses not to. Issues arise when test-time prompts include even minor mistakes, leading models to continue to generate buggy outputs to match the input prompt.\n\nThese issues present across different applications suggest the need for a comprehensive definition of hallucinations that encompasses these diverse characteristics:\n\nA hallucination is a generated output from a model that conflicts with constraints or deviates from desired behavior in actual deployment, or is completely irrelevant to the task at hand, yet could be deemed syntactically plausible. There are three key aspects to consider:\n\n1. A generated output from a model.\n2. A deployment scenario to evaluate model outputs with:\n   - A list of constraints that must be consistent.\n   - A loose interpretation of a desired behavior.\n   - A set of topics relevant to the task.\n3. Metrics measuring consistency, desirability, relevancy, and syntactic soundness (plausibility) of generations.\n\nThis definition captures the qualities of hallucinations such as inconsistency, undesirability, irrelevancy, and plausibility, which can be applied flexibly to any deployment scenario by choosing appropriate metrics for each characteristic."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Examples",
            "text": "Regardless of the weather and driving conditions, difficulties are identified in detecting and identifying the traffic light state at an intersection, until the image has zoomed in on the light itself. Additional irrelevant (or completely false) details about other agents may be presented when not prompted. Describing temporal sequences (i.e., videos) and categorizing images by their direction within a panoramic view from the vehicle\u2019s perspective also pose challenges. Hallucinations arise in these complex environments due to the high variability in driving scenarios. Even after applying hallucination mitigation techniques like chain-of-thought reasoning, undesired outputs persist.\n\nIn evaluating LVLMs, models\u2019 outputs may include non-existent objects or additional irrelevant phrases that are not possible to test for accuracy. For example, in a picture of food on a table, a model hallucinates a non-existent beverage and predicts that the \u201ctable is neatly arranged, showcasing the different food items in an appetizing manner.\u201d Although errors in classification and generation in this case are not critical, there are warnings of potential failures with more severe societal impacts, such as biases leading to marginalizing users.\n\nResearch explores alignment failures of LLMs applied to code completion tasks by evaluating the likelihood of generating defective code given different input prompts. In-context learning with buggy code examples increases the chance of poor generations. Similar model biases towards race, gender, religion, and other representations are identified. Additionally, the model, Codex, can generate code that may aid in the development of insecure applications or malware, though in a limited scope. Such findings are corroborated in code generation works within the robotics domain.\n\nAuthors describe models generating code with references to non-existent items and actions that are inappropriate or contradictory. For example, models might call functions with invalid objects, attempt actions with inappropriate timing, or generate undesired behaviors like asking for help when no one is near.\n\nResearch in identifying cases of hallucinations in QA tasks highlights incorrectly generated answers. Such hallucinations are categorized into closed-domain (irrelevant information given context) and open-domain (incorrect claims without context). Fine-tuning helps reduce but does not eliminate hallucinations completely. Additional insights are drawn from contradictions among several sampled generations from an LLM. If a context leads to conflicting outputs, some form of hallucination is likely occurring. This aspect aligns with the concept of testing for consistency among multiple generations rather than against a fixed knowledge base, allowing flexibility across system setups by describing consistency, desired behavior, and relevancy."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Why do they happen?",
            "text": "There are several speculations as to how hallucinations come about during deployment. First and foremost, like any learning task, foundation models are sensitive to biases in training data (Rawte et al., 2023). Once a model is trained on a given large dataset, some facts may become out-of-date or stale at any point in time (Puthumanaillam et al., 2024). Furthermore, as the training set is embedded into a smaller encoding dimension, the knowledge within a large language model\u2019s frozen parameters is lossy, and models cannot feasibly be fine-tuned every time there is new data (Peng et al., 2023; Elaraby et al., 2023). Zhang et al. (2023b) recommend changing algorithm parameters at runtime, such as temperature (spread of probability distribution of next token), top-k sampling (narrows the set of next tokens to be considered), and beam search (choosing a set of possible beams, i.e., trajectories, of next tokens based on high conditional probabilities), but the process of tuning these parameters is expensive. To combat out-of-date training data, some works provide models with an external knowledge-base of information to pull facts from, with the hope of increasing model accuracy. Even with this up-to-date information, Zhang et al. (2023c) pose that there may exist a misalignment between the true capabilities of a model, and what a user believes the model is capable of, leading to poor prompt engineering. In fact, poor prompting is one of the most significant causes of hallucinations. Chen et al. (2021) find that poor quality prompts lead to poor quality generations, in the context of code completion. This phenomenon is attributed to the reconstruction training objective of large language models attempting to maximize the likelihood of next generated tokens, given context and past outputs, i.e., where context is an input to the model, there is an output sequence of tokens, and any generated token is conditioned on previously generated tokens. As the public datasets these models are trained on contain some fraction of undesirable generations (e.g., defective code), the models become biased to generate similar results under those inputs. Qiu et al. (2023) show that this limitation can actually be exploited to push foundation models to generate toxic sentences, or completely lie, by simply rewording the prompt. While foundation models condition generated tokens on ground-truth text without hallucinations at train time, during inference, the model chooses future tokens conditioned on previously (possibly hallucinated) generated text. As such, Chen et al. (2023d) and Varshney et al. (2023) state that generated outputs are more likely to contain hallucinations if prior tokens are hallucinated as well. Furthermore, Li et al. (2023a) find that, even if prompt context provided to a foundation model is relevant, the model may choose to ignore the information and revert to its own (possibly outdated or biased) parameterized knowledge. Overall, the hallucination detection task is highly complex with several possible sources of failures that need to be considered at test-time. Chen and Shu (2024) validate the complexity of the detection problem with studies identifying that human- and machine-based detectors have higher difficulty correctly classifying misinformation generated from large language models than those written by other people."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Detection and Mitigation Strategies",
            "text": "Hallucination detection and mitigation methods can be classified into three types (white-, grey-, and black-box) depending on the available inputs to the algorithm. Generally, given some context, a foundation model outputs a predicted sequence of tokens, the corresponding probabilities of each token, and embeddings of the generation from intermediate layers in the network. White-box hallucination detection methods assume access to all three output types, grey-box require token probabilities, and black-box only need the predicted sequence of tokens. Because not all foundation models provide access to their hidden states, or even the output probability distribution of tokens, black-box algorithms are more flexible during testing. In this section, we present existing detection and mitigation approaches clustered by input type. While several of these works show promise in QA and object detection settings, many of them require further validation on decision-making tasks, and we will point out these methods as they come about. Works in this section are summarized in Table 2."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "White-box Methods",
            "text": "Methods in this section require access to internal weights of the model for hallucination detection."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Hidden States",
            "text": "Some approaches utilize intermediate embeddings at different network layers. Azaria and Mitchell (2023) empirically find that language models attempt to correct themselves after outputting an untruthful claim. They hypothesize that the internal states of a model must have some understanding of whether the output is correct. Furthermore, the authors stray away from directly using the token probabilities, even though they have correlation with model accuracy Varshney et al. (2023), because the complete output sentence\u2019s probability is dependent on the length of the generation and appearance frequency of tokens.\n\nAzaria and Mitchell (2023) present SAPLMA, a simple classifier trained with supervised learning, that takes the activation values of a hidden layer of an LLM as input, and outputs the probability of the generated claim being true. SAPLMA is shown to be able to identify untruthful outputs, even when trained on a held-out dataset on a completely different topic than evaluated on.\n\nYao et al. (2023a) aim to test the resiliency of foundation models to varying prompts. They propose perturbing an input prompt with additional tokens so as to make an LLM under test produce a desired hallucination (e.g., modify the original query, \u201cWho won the US election,\u201d to get the LLM to generate, \u201cDonald Trump was the victor,\u201d while its original response was correctly stated as, \u201cJoe Biden was the victor\u201d). As the search space of possible tokens to add/replace when developing the adversarial prompt is massive, the work uses a gradient-based token replacing strategy. Specifically, they define an objective that attempts to find trigger tokens in the direction of the gradient of the likelihood that maximizes the probability of the model outputting the desired hallucination. With simple prompt modifications, the authors show that the white-box approach is able to induce the specified hallucinations.\n\nLUNA, introduced by Song et al. (2023), is a general framework that measures the trustworthiness of an LLM output containing four stages of evaluation: model construction, semantic binding, quality metrics, and practical application. The abstract model construction phase attempts to profile the LLM using its hidden states with either a discrete time Markov chain (DTMC) or a hidden Markov model (HMM) architecture. For example, when fitting a DTMC model, the authors encode the hidden states of the language model into a lower dimensional space, cluster them into abstract discrete states, and learn a transition function between said states. Semantic binding is used alongside quality metrics to identify the states and transitions that are trustworthy, and which ones are undesired. Finally, at inference time, as the model generates output tokens to a given prompt, the intermediate network layer embeddings are iteratively passed through the profiling model to identify when undesired transitions occur. The authors evaluate their framework\u2019s capability of detecting hallucinations within QA datasets."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Attention Weights",
            "text": "Attention weight matrices, prominent within transformer model architectures, signify the importance the model places on earlier tokens within a generation when predicting future tokens. OPERA, proposed by Huang et al. (2024a), is a hallucination detection method for LVLMs that utilizes the model\u2019s internal attention weights. When visualizing the attention matrix, the authors find peculiar column patterns that align with the beginning of a hallucinated phrase. These aggregation patterns usually occur on a non-substantial token like a period or quotation mark, but are deemed to have a large impact on the prediction of future tokens. As such, this finding led Huang et al. (2024a) to modify the beam search algorithm Freitag and Al-Onaizan (2017) by applying a penalty term to beams wherever an aggregation pattern is detected, and roll back the search to before the pattern arises. Their method is shown to reduce hallucinations and even eliminate possible repetitions in generations."
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Honesty Alignment",
            "text": "In addition to methods that require hidden states or attention matrices, we also include methods that fine-tune foundation models to better communicate their uncertainty to questions under white-box algorithms, as they require access to model weights for training. For example, Lin et al. (2022a) collect a calibration dataset of questions and answers from GPT-3 under multiple types of tasks (e.g., add/subtract and multiply/divide), and record how often each task is incorrectly answered. They aim to fine-tune the LLM to also output its certainty that the prediction is correct. Consequently, Lin et al. (2022a) fine-tune the model with data pairs of a question and the empirical accuracy on the task that the question originates from in the calibration dataset, such that the model is expected to similarly output a probability of accuracy at test-time. The authors show that the proposed verbalized probability in deployment does correlate with actual accuracy on the tasks. Yang et al. (2023a) take the method one step further by also training the model to refuse to answer questions with high uncertainty."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Grey-box Methods",
            "text": "Grey-box approaches leverage the probability distributions of tokens output from the model. These approaches can provide a more fine-grained understanding of model behavior and help in debugging model outputs. By examining token probabilities, researchers and developers can identify unexpected outputs and refine model behavior, ensuring more accurate and reliable language model performance. This method is especially useful in contexts where the model's decision-making process needs to be transparent and interpretable. Overall, grey-box approaches represent a valuable tool in the optimization and evaluation of language models."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Concept Probabilities",
            "text": "Empirically, Varshney et al. (2023) show that there is a negative correlation between hallucination rate and token probability (i.e., as a token\u2019s probability decreases within a sentence, the tendency to hallucinate increases). Thus, the authors rely on token probabilities to estimate uncertainty of concepts within a generated claim, and they check for correctness by cross-referencing a knowledge-base. Whenever a concept is found to be conflicting with a fact through verification questions, their method attempts to mitigate the error by prompting the LLM to replace the incorrect claim with the evidence. Although effective in the QA setting, Varshney et al. (2023) concede that, in the event token probabilities are not available, some form of heuristic must be used to detect hallucination candidates. Zhou et al. (2024) show that external models can be developed to automatically clean hallucinations. The authors tackle the issue of object hallucinations that LVLMs experience when describing the content of images. Through theoretical formulations, the authors show that LVLM responses tend to hallucinate in three settings: when described object classes appear frequently within a description, when a token output has low probability, and when an object appears closer to the end of the response. As such, their model, LURE, is a fine-tuned LVLM trained on a denoising objective with a training dataset that is augmented to include objects that appear frequently within responses, and replacing objects with low token probabilities or appearing close to the end of the response, with a placeholder tag. At inference time, tokens are augmented similarly to how they were changed to generate the training dataset, and the LURE LVLM is prompted to denoise hallucinations by filling in uncertain objects. SayCanPay, proposed by Hazra et al. (2024), builds off of the SayCan framework Ichter et al. (2023) to improve the expected payoff of following a plan specified by a language model. Within our hallucination definition, this goal translates to increasing the desirability of generations by improving the likelihood of the model achieving higher rewards. The authors propose three different strategies for planning: Say, SayCan, and SayCanPay. Say methods greedily choose next actions based only on token probabilities. SayCan approaches also take the success rate of the chosen action into consideration. Finally, SayCanPay additionally estimates the expected payoff from following the plan with some heuristic. Hazra et al. (2024) learn this Pay model with regression on an expert trajectory dataset. Combining all three models together minimizes the likelihood that a generated plan contains conflicting infeasible action calls, while maximizing the efficiency of the task completion."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Conformal Prediction",
            "text": "Another range of works estimate the uncertainty of a model output with conformal prediction so as to provide statistical guarantees on the likelihood of predictions being correct Shafer and Vovk (2008  ###reference_b110###). Quach et al. (2024  ###reference_b100###) propose conformal language modeling to build a set of possible candidate responses to a test prompt, while calibrating algorithm parameters on a held-out dataset of independent prompts and their corresponding admission functions, which check whether a model output meets the criteria of an input prompt. In their algorithm, the authors calibrate thresholds for three separate scoring functions that test for generation quality, similarity with other responses, and model confidence using \u201cLearn then Test\u201d Angelopoulos et al. (2022  ###reference_b3###).\n\nKumar et al. (2023  ###reference_b64###) similarly apply conformal prediction to LLMs, but for answering multiple choice questions. Specifically, the method first collects a calibration dataset of prompts and the normalized token probabilities of the correct token (i.e., A, B, C, or D) being chosen from the model. Then, during deployment, given a user-defined error rate and a prompt, their algorithm chooses the multiple choice answers with token probabilities that fall within the calibrated score on the held-out dataset.\n\nWhile the previous hallucination mitigation works presented using conformal prediction are solely applied to QA settings, Ren et al. (2023  ###reference_b105###) are the first to apply conformal prediction of foundation models to robotic tasks. The authors are motivated by a desire for language-conditioned robots to understand when they are uncertain about the next action to take, such that they can ask for help in those cases (while minimizing frequency of clarifications). Because LLM generations with different length sequences inherently produce different complete sentence probabilities, the authors propose framing the control task as a multiple-choice problem, like Kumar et al. (2023  ###reference_b64###). Their approach, KnowNo, prompts an LLM to generate a possible set of next actions to take in multiple choice form.\n\nLiang et al. (2024  ###reference_b75###) extend the KnowNo methodology by incorporating an introspective planning step using a previously constructed knowledge-base of experiences, which tends to (1) enhance quality of generated plans, and (2) improve interpretability of decisions. Specifically, introspective planning first constructs a knowledge-base containing training pairs of tasks, observations, and valid plans, which the LLM is prompted to generate explanations behind why they are reasonable. Each experience is stored with a key as an embedding of the original instruction. During inference, given a new test instruction, their method queries the database to find the key with the closest embedding to that of the new instruction. This previous experience and reasoning is fed into the model to generate a set of candidate plans to follow. Finally, the remainder of the algorithm follows the same process as KnowNo to calibrate and narrow down the prediction set to fall within a desired error rate.\n\nWang et al. (2024  ###reference_b129###) aim to provide additional guarantees on completing the task provided within the natural language instruction. To do so, the authors propose a novel task specification, LTL-NL, which combines linear-temporal-logic (LTL) descriptions with natural language from a user instruction, which the authors claim is easier to define than classical LTL specifications. Given this specification, a symbolic task planner chooses a sub-task to complete next and an LLM generates plans for each sub-task, respectively. Like Ren et al. (2023  ###reference_b105###) and Liang et al. (2024  ###reference_b75###), Wang et al. (2024  ###reference_b129###) apply conformal prediction to minimize the number possible actions to take next within some desired error rate. However, rather than directly asking a user for assistance when there is high uncertainty in the next action to take (or when there are environmental constraints), their method, HERACLEs, samples a new sub-task to complete from the task planner. If on the other hand, the task planner is unable to provide a new sub-task, HERACLEs requests help from the user. With experimentation, the authors find that their method achieves higher task completion rate on missions requiring more sub-tasks, outperforming baseline planners that do not utilize LTL specifications."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Black-box Methods",
            "text": "Black-box algorithms only rely on the input prompts and output predictions from the model, without making assumptions on the availability of the hidden state, nor the token probabilities."
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Analyzing Samples from Model",
            "text": "Several works examine sampling multiple responses from a language model (LLM) and measuring the similarity of information present in all samples. For instance, SelfCheckGPT, by Manakul et al. (2023), evaluates consistency among varied responses through different methods, one of which uses BERTScore to compute similarity between sentences from different outputs. Hallucinations are detected when the similarity is low across samples. Other approaches include using multiple-choice QA systems, training classifiers to predict contradictions, and prompting the LLM to evaluate mutual support among sampled responses. They find that sampling more responses enhances the estimation of a claim's validity but slows computation. Elaraby et al. (2023) focus on computing entailment among responses at the sentence level. Their method, HaloCheck, employs the SummaC entailment estimation method, offering a balanced prediction score. HaloCheck is efficient in score prediction.\n\nDu et al. (2023) propose an ensemble approach, where multiple agents debate the correct answer to a question using an iterative process, incorporating chain-of-thought reasoning to reduce hallucinations and improve accuracy. This method, though tested on static datasets, could be applied in simulation frameworks for debates on planning.\n\nPark et al. (2024) introduced CLARA for predicting ambiguous or infeasible instructions to robotic systems using LLMs. CLARA samples concepts from a prompt to create multiple inputs, examining output action similarity and checking goal feasibility. It requests clarification if uncertain but feasible. CLARA performs well in robotic tasks with real instructions yet occasionally hallucinates during uncertainty reasoning.\n\nSome approaches focus on identifying explicitly contradictory responses, enhancing response validity by removing conflicts. M\u00fcndler et al. (2024) propose finding important concepts, generating additional information on them, and evaluating sentence consistency. They revise conflicting sentences before output. Dhuliawala et al. (2023) utilize chain-of-thought reasoning, prompting an LLM to generate verification questions about responses and regenerating outputs if conflicts arise.\n\nIn image captioning, Li et al. (2023c) improve accuracy estimation for LLMs, proposing POPE to handle stability issues in accuracy metrics. POPE curates binary questions about image objects, flagging models hallucinating objects based on conflicting answers.\n\nXiong et al. (2024) explore LLMs expressing uncertainty verbally through prompt engineering, aiming for models to provide confidence levels. They implement chain-of-thought reasoning to predict confidence scores for sub-claims, integrating these for overall confidence assessment. Their hybrid approach combining verbalized uncertainty and self-contradiction detection enhances calibration error performance compared to individual methods."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Adversarial Prompting",
            "text": "Works specializing in adversarial prompting attempt to test the robustness of models to varying inputs that may coerce the model into producing out-of-distribution results. For example, Mehrabi et al. (2023) apply adversarial prompting to text-to-image foundation models, like Stable Diffusion, to generate offensive images. With respect to their framework, FLIRT is essentially testing the tendency of foundation models to hallucinate undesired generations in deployment. FLIRT uses an adversarial language model to predict a prompt to input to the image generator, scores the generated image for the presence of undesirable traits using an external classifier, re-prompts the adversary to produce a new instruction conditioned on the findings of the classifier, and repeatedly generates images until the adversary successfully prompts the test model to output an undesirable result. Mehrabi et al. (2023) define objective functions conditioned on the score output by external classifiers to maximize diversity of adversarial prompts and minimize toxicity to pass text filters that detect malicious inputs, while improving attack effectiveness.\n\nAnother work from Yu et al. (2023) presents the AutoDebug framework for automatically sampling and updating several prompts for use in adversarial testing of the language model. The authors specifically explore adversarial testing under the case that the model predicts a correct response when provided relevant context, but generates an incorrect prediction when the evidence is modified. They apply two different modification approaches: replacing tokens within the context to provide incorrect facts, and adding additional relevant facts to the prompt that may make it difficult to pick out the most important details. All in all, adversarial prompting is an effective method for identifying robustness of models to unseen inputs, which can be used to develop stronger input filters or fine-tune the model for decreased hallucination tendency."
        },
        {
            "section_id": "4.3.3",
            "parent_section_id": "4.3",
            "section_name": "4.3.3 Proxy Model",
            "text": "Certain black-box works rely on an external, proxy model to detect and mitigate hallucinations. One such method is used as a baseline within the SelfCheckGPT article Manakul et al. (2023 ###reference_b85###). As many language foundation models do not provide access to token probabilities, the authors use an open-source proxy LLM that does provide token probabilities as an estimate of the original output\u2019s probability. They find that using proxy LLMs for probability estimation and hallucination detection successfully is highly variable. The accuracy of detection is dependent on the complexity of the LLM itself, as well as the training data of the proxy LLM (i.e., models trained on independent datasets from the original LLM will have different generation patterns).\n\nWithin this section, we also include works that use an external trained classifier to detect hallucinations. For example, Chen et al. (2023d ###reference_b17###) curate a dataset of QA dialogue from LLM generated responses. They apply a composition of metrics to assess quality of responses, including a self-assessment from the LLM comparing the ground-truth and predicted text, human-labeled, and machine metrics (e.g., BERT score, F1 score, BLEU, etc.). Their hallucination discriminator, RelD, is trained on the dataset in multiple separate phases, each using a different objective: regression, multi-class classification, and finally binary classification. Through experiments, they find that RelD closely aligns with human evaluators\u2019 original predictions.\n\nSimilarly, Pacchiardi et al. (2024 ###reference_b92###) develop a black-box lie detector for LLMs. In their case, the authors hypothesize that models that output a lie will produce different behaviors in future responses, like Azaria and Mitchell (2023 ###reference_b4###). As such, at inference time, Pacchiardi et al. (2024 ###reference_b92###) prompt the LLM with several binary questions (that may be completely unrelated to the original response) and collect yes/no answers. All the responses are concatenated into a single embedding that is input to the logistic regression model to predict the likelihood that the response was untruthful. The authors find that the simple detector is mostly task- and model-agnostic once trained on a single dataset."
        },
        {
            "section_id": "4.3.4",
            "parent_section_id": "4.3",
            "section_name": "4.3.4 Grounding Knowledge",
            "text": "In knowledge grounding tasks, a language model is tasked with identifying evidence from an external knowledge-base that supports claims within a summary. Although seemingly irrelevant to decision-making scenarios, similar methods discussed in this section may be applied in planning tasks to identify observations that are most relevant to predicting the next action or to generate reasoning behind a specified plan.\n\nPURR, proposed by Chen et al. (2023a), is a denoising agent, like LURE, that is trained in an unsupervised fashion given evidence from online sources, a clean (correct) summary, and a noisy (hallucinated) summary. The model learns to denoise the incorrect summary to the clean statement. During deployment, given a possibly hallucinated claim, a question generation model queries online sources for evidence about the claim, and PURR generates a cleaned version of the original summary with said evidence.\n\nSome knowledge grounding approaches prompt LLMs to generate code to directly query information from databases. Li et al. (2024) are motivated by the limitations of existing knowledge-based hallucination mitigation methods; namely that (1) they utilize a fixed knowledge source for all questions, (2) generating retrieval questions with LLMs that interface with a database is not effective because they may not be trained on the particular programming language of the database, and (3) there is no correction capability that handles error propagation between knowledge modules. Consequently, the authors propose augmenting LLMs with heterogeneous knowledge sources to assist with summary generation. Specifically, in the event that the model is found to be uncertain about its generated statement through self-contradiction, their framework, chain-of-knowledge (CoK), chooses subsets of knowledge bases that may be helpful for answering the original question. Assuming each database has its own query generator, CoK queries for evidence and corrects rationales between different sources iteratively. Compared to chain-of-thought reasoning, CoK consistently produces more accurate answers with its iterative corrections.\n\nAnother source of potential conflict that leads to hallucinations is misalignment between a model\u2019s capabilities and the user\u2019s beliefs about what it can do. Zhang et al. (2023c) tackle this knowledge alignment problem and categorize alignment failures into four types: Semantic \u2014 an ambiguous term maps to multiple items in a database, Contextual \u2014 the user failing to explicitly provide constraints, Structural \u2014 user provides constraints that are not feasible in the database, Logical \u2014 complex questions that require multiple queries. Their proposed MixAlign framework interacts with the user to get clarification when the LLM is uncertain about its mapping from the user query to the database. With the original query, knowledge-base evidence, and user clarifications, the LLM formats its final answer to the user.\n\nPeng et al. (2023) aim to add plug-and-play modules to an LLM to make its outputs more accurate since these large foundation models cannot feasibly be fine-tuned whenever there is new information. Their work formulates the user conversation system as a Markov decision process (MDP) whose state space is an infinite set of dialogue states which encode the information stored in a memory bank, and whose discrete action space includes actions to call a knowledge consolidator to summarize evidence, to call an LLM prompt engine to generate responses, and to send its response to the user if it passes verification with a utility module. The proposed LLM-Augmenter has a memory storing dialogue history, evidence from the consolidator, set of output responses from an LLM, and utility module results. Its policy is trained in multiple phases with REINFORCE Williams (1992) starting with bootstrapping from a rule-based policy designed from domain experts, then learning from simulators, and finally, from real users. The authors find that access to ground-truth knowledge drastically improves QA results, and feedback from the utility module and knowledge consolidator help to provide more accurate answers to users.\n\nEvaluated in actual decision-making settings, Introspective Tips from Chen et al. (2023b) provide concise, relevant information to a language planner to learn to solve more efficiently. Intuitively, summaries that collect information over all past experiences may be long and contain unnecessary information. In contrast, tips are compact information with high-level guidance that can be learned from one\u2019s own experiences, from other demonstrations, and from other tasks in a similar setting. Chen et al. (2023b) show that providing low-level trajectories is less effective than tips on simulated planning tasks. Additionally, with expert demonstrations, the LLM learns faster with fewer failed trials than with just past experience alone. However, one limitation identified in the study is that the LLM underperforms in unseen, low-difficulty missions where it has issues generating general tips for zero-shot testing."
        },
        {
            "section_id": "4.3.5",
            "parent_section_id": "4.3",
            "section_name": "4.3.5 Constraint Satisfaction",
            "text": "There is also additional work in creating black-box algorithms for ensuring decision plans generated by foundation models meet user-defined goal specifications and system constraints, like their grey-box counterpart developed by Wang et al. (2024  ###reference_b129###).  \nBecause these models under test provide their results in text form, it is natural to apply formal method approaches (e.g., satisfiability modulo theory, SMT, solvers) to verify the satisfaction of generated plans.  \nFor example, Jha et al. (2023  ###reference_b54###) prompt an LLM planner with a problem formulated with first order constraints to predict a set of actions to complete the task.  \nThe output plan is input to an SMT solver to check for any infeasibilities in the program, and any counterexamples found are used to iteratively update the prompt and generate new plans.  \nThis counterexample approach is much faster than relying on combinatorial search methods that find a plan from scratch.  \nHowever, the quality of generated plans and the number of iterations before a successful plan is generated are heavily dependent on the LLM generator itself, with similar reasons to the proxy-model used by Manakul et al. (2023  ###reference_b85###).  \nAnother work from Hu et al. (2024  ###reference_b48###) develops a RoboEval benchmark to test generated plans on real robots, in a black-box manner.  \nLike Wang et al. (2024  ###reference_b129###), the authors introduce their own extension of LTL formulations, known as RTL, which specifies temporal logic at a higher, scenario-specific, level, while abstracting away constraints that are not dependent on available robot skills.  \nRTL and LTL-NL are easier to read and define than classic LTL methods.  \nRoboEval utilizes the provided RTL formulation of a problem, a simulator, and evaluator to systematically check whether the output meets requested goals.  \nFurthermore, to check for robustness of the model to varied instructions, Hu et al. (2024  ###reference_b48###) hand-engineer paraphrased sentences within an offline dataset that should ideally result in the same task completion.  \nPrimary causes of failures were found to be a result of generated code syntax/runtime errors, attempting to execute infeasible actions on the robot, and failing RTL checks.  \nLike adversarial prompting approaches, testing generated plans on robots in diverse scenarios enable researchers to design more robust systems that hallucinate less frequently at test-time."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Metrics and Evaluation Platforms",
            "text": "We now present common metrics, datasets, and simulation platforms leveraged when developing and evaluating the hallucination detection algorithms introduced in Section 4  ###reference_###.\nGiven a pair of responses, BERTScore computes the BERT Devlin et al. (2019  ###reference_b25###) embeddings of the sentences and calculates their cosine similarity.\nUsing a pre-trained BART model, which provides access to generated token probabilities, BARTScore sums over the log probability of each token generated while conditioning on context and previously output tokens.\nEssentially, BARTScore attempts to predict the quality of a generated text using BART as a proxy model.\nSummaC is a class of natural language inference models that predict entailment, contradiction, and neutral scores between pairs of sentences among a document and its summary.\nEach score is collected into a separate matrix split by metric type.\nThe authors propose two approaches, SummaCZS and SummaCConv, for aggregating scores of each sentence in the summary with respect to each sentence in the document.\nLike BARTScore, GPTScore relies on a pre-trained language model with access to token probabilities to estimate quality of outputs, but uses the GPT series of LLMs.\nThe creators of AlignScore pose that two pieces of text are aligned when all information present in one text exists in the other, and the texts do not contradict one another.\nConsequently, they train a classification model on labeled data with three types of labels: a binary classification of aligned or not, a multi-class prediction including a neutral label in addition to the binary classification labels, and a continuous score for a regression task.\nThe AlignScore metric computes a weighted score across all three prediction heads at test-time.\nOne common method of measuring uncertainty of a model\u2019s many generations is computing its entropy over all generated token probabilities.\nHowever, in cases where multiple sentences have the same semantic meaning but output different entropies, the aggregated measurement is not representative of the true uncertainty of the model.\nKuhn et al. (2023  ###reference_b63###) tackle this problem by clustering sentences into semantic classes and summing entropies of sentences from the same class together.\nCHAIR, used for measuring accuracy of descriptions of images, is the ratio of the number of hallucinated objects to all the objects mentioned in the description.\nTo identify the hallucinated objects within the description, the authors assume access to ground-truth object classes in the image.\nThe authors recognize that different instructions prompting for a description of an image may lead to different responses from the model with the same semantic meaning.\nIn this case, CHAIR gives different scores to both descriptions although they are alike.\nInstead, their proposed metric, POPE, asks binary questions about the existence of in-domain and out-of-domain objects in the image, which leads to more a more stable metric across different outputs.\nBDD-X is a multi-modal driving dataset consisting of K samples (i.e., video clips), each consisting of eight images with vehicle control actions and text annotations describing the scene and justifying actions.\nThe authors augment BDD-X into a QA dataset consisting of questions that ask about the current action of the vehicle, reasoning behind the action, and predicting future control signals.\nTo incorporate other questions a user might ask about the vehicle, surroundings, and other miscellaneous queries, they prompt ChatGPT to generate further questions.\nIn total, the DriveGPT4 dataset contains K samples.\nThe nuScenes dataset contains K driving videos, each running for  seconds, collected from roads in Boston and Singapore.\nEach frame includes six different RGB camera views, GPS, annotated D bounding boxes of various object classes, and semantically labeled rader, lidar, and map representations.\nLike DriveGPT4, NuScenes-QA is a visual QA dataset, but built on top of nuScenes.\nIt includes five different types of questions including checking the existence of objects, counting instances, detecting the object being referred to, identifying the action state of an object, and comparing two objects.\nOverall, the dataset holds K QA pairs across K scenes in nuScenes.\nTalk2Car is an earlier extension of the nuScenes dataset which aims to ignite further research into developing systems that bridge the gap between passengers and an autonomous vehicle through natural language.\nAnnotators provided approximately K text commands over  videos within the nuScenes training split which refer to an object in the scene.\nWhile Talk2Car is a pioneering work for object referral in real driving scenes through natural language, each annotated instruction only refers to one object.\nAs such, Wu et al. (2023a  ###reference_b135###) propose a new task definition, referring multi-object tracking (RMOT), which attempts to predict all objects that are referred to within a natural language input.\nThey augment the KITTI driving dataset Geiger et al. (2012  ###reference_b41###) with labeled D bounding boxes around objects that are referenced within a text prompt for K images.\nNuPrompt is another RMOT-based benchmark, but applied to nuScenes and with D bounding box labels.\nIt includes K languages prompts, with most prompts referring to anywhere between one and ten objects in a scene.\nThe authors argue that, while several datasets exist for anomaly detection or identification on roads, there is a gap in explaining the reason for categorizing an object as being risky, i.e., objects the model should pay attention to, like crosswalks, pedestrians, and traffic lights.\nAs such, DRAMA is a benchmark tackling identification of risky objects in a driving scene conditioned on natural language.\nDing et al. (2023  ###reference_b27###) extend DRAMA to further include suggestions on actions the ego vehicle can take to minimize risk, but the dataset is not public at this time.\nNuInstruct addresses two common limitations in existing driving datasets: they cover a limited subset of necessary tasks while driving (e.g., evaluating perception while ignoring planning), and disregard temporal and multi-view representations.\nBuilt on top of NuScenes, the dataset provides K samples of multi-view sequences with corresponding QA pairs spanning  subtasks within perception, prediction, planning, and risk detection.\nThe authors of DriveLM curate a similar comprehensive dataset from nuScenes and the CARLA driving simulator Dosovitskiy et al. (2017  ###reference_b30###) with open-ended and factual questions about importance rankings of nearby vehicles, planning actions, detecting lanes, and more.\nThe authors collect a text-based QA dataset from a proprietary driving simulator, generated from ChatGPT with ground-truth observations (e.g., relative locations of detected vehicles, ego vehicle control actions, etc.) from the simulator.\nHumanEval is a set of  handwritten programs, each with a function definition, docstring, program body, and unit tests.\nThe authors find there is great promise in using LLMs for code generation, but output quality is limited by length of context and buggy examples.\nThe authors build a new code generation benchmark specifically for robot tasks with  functions focused on spatial reasoning, geometric reasoning, and controls.\nThe Language-Table dataset contains K trajectories manually annotated with K unique instructions across simulated and real-world manipulator robots.\nThe multi-modal dataset consists of video sequences, corresponding actions at each time step, and language instructions describing the policy of the robot in hindsight.\nThe authors of the CLARA method developed a dataset to identify language goals from a user that are certain, ambiguous, and infeasible.\nCollected from three different types of robots (cooking, cleaning, and massage), SaGC is annotated with a floor-plan, descriptions of objects and people in view, a text goal, and a label of uncertainty.\nHotPotQA is a question-answering benchmark with K multi-hop questions (i.e., requiring multiple steps of reasoning to reach answer) collected from Wikipedia.\nThe dataset includes both questions that require finding relevant phrases from context paragraphs, and comparing two entities.\nIn contrast to HotPotQA, the developers of FEVER attempt to answer the question of whether a fact is supported by a knowledge-base.\nThe database contains K claims with annotated labels deciding if each claim is supported, refuted, or indeterminable from Wikipedia articles.\nNatural Questions is yet another QA dataset with sources from Wikipedia.\nThe authors release K training and K test samples of real (anonymized) queries into the Google search engine paired with a Wikipedia page and a long and short answer annotated by a person based on said article.\nLike HotPotQA, StrategyQA aims to develop a dataset of implicit multi-hop questions, but includes a greater variety categories of questions, and with less category imbalance.\nFurthermore, most of the questions in the dataset require three or more steps of decomposition and referencing to accurately solve.\nSeparate from the information retrieval task described in benchmarks above, Anantha et al. (2021  ###reference_b2###) develop a dataset, QreCC, for conversational QA.\nThey focus on reading comprehension, passage retrieval, and question rewriting tasks, with a total of K dialogues paired with K questions.\nZhao et al. (2024  ###reference_b157###) present a multi-model visual QA dataset of images, hallucinated descriptions, and non-hallucinated samples from the VG dataset Krishna et al. (2017  ###reference_b62###).\nThe developer presents a D car simulator, with driving scenarios ranging from a passing on a multi-lane highway, merging into a highway, merging and exiting from a roundabout, parking, and more.\nAn ego vehicle can be controlled with discrete (e.g., merge left, merge right, faster, etc.) or continuous (e.g., providing an explicit acceleration command) actions.\nGeared towards microscopic traffic simulation, SUMO allows researchers to design road networks, track traffic flow metrics, and control individual vehicles.\nCARLA is a D driving simulator built on top of Unreal Engine.\nExisting works benchmark their methods on CARLA for perception, planning, control, and QA tasks for its realism.\nThere is also capability to perform co-simulation with SUMO and CARLA simultaneously Wegener et al. (2008  ###reference_b130###).\nRavens is a D manipulator robot (UR5e) simulator built with PyBullet Coumans and Bai (2016\u20132021  ###reference_b22###) with tasks like block insertion, towers of hanoi, aligning boxes, assembling kits, etc. Each simulated task features a manipulator robot with a suction gripper sitting on a table workspace, with three camera views.\nBuilding on top of the TextWorld simulator, discussed in Section 5.3.3  ###reference_.SSS3###, ALFWorld aligns perception from the D robot simulation benchmark, ALFRED Shridhar et al. (2020  ###reference_b113###), with text-based, discrete actions like \u201cMoveAhead,\u201d \u201cRotateLeft,\u201d and \u201cOpen.\u201d\nProgPrompt is a benchmark of high-fidelity D data collected from a virtual home robot.\nIt includes three environments, each with  object instances.\nThese simulations are further used to create a dataset of  household robot tasks with a ground-truth set of actions to achieve each goal.\nRoboEval is a general platform for checking the correctness of code generated for a robot task.\nIt relies on a simulator, evaluator, and a set of defined tasks to perform evaluations on a simulated robot.\nWhile ProgPrompt captures more realistic scenarios in its high-fidelity D simulator, RoboEval is more tuned towards verifying code efficiently.\nMore recently, the developers of KnowNo also provide a tabletop simulator based on PyBullet, like Zeng et al. (2021  ###reference_b148###), for robot manipulation of blocks and bowls.\nProvided instructions vary in ambiguity by attribute, number, and spatial reasoning.\nTextWorld is a suite of text-based games that can be either hand-engineered or procedurally generated, where an agent directly receives text-based observations from an abstract world, and acts with natural language actions to complete a task.\nChevalier-Boisvert et al. (2019  ###reference_b18###) present a D top-down, grid-based simulator of instruction-following tasks with varying difficulty.\nSome tasks include simple navigation to a single goal, picking and placing objects with ambiguous references, and instructions that implicitly require multi-step reasoning to complete.\nThe simulator provides a partial observation of the space near the agent at every timestep.\nThe developers of MineDojo attempt to create a benchmark to test the continual learning of agents in an open-world setting.\nThey build an interface on top of Minecraft, a video game, to enable testing with diverse open-ended tasks, and provide access to an external knowledge-base of existing Minecraft tutorials and wiki discussions.\nMineDojo includes several thousands of tasks that are more complex that earlier works (and require multi-step reasoning).\nAs such, task completion is judged with a learned LVLM, which acts like a human evaluator.\nThe authors present a multi-agent conversational simulator where agents are controlled by language models.\nUsers may set up agents with a defined backstory and provide instructions when desired.\nEach agent has access to a memory of past experiences, and generates natural language actions to go to certain areas, communicate with others, complete chores, and more."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Metrics",
            "text": "Here, we list established metrics used for computing language similarity and accuracy of generated image descriptions.\nGiven a pair of responses, BERTScore computes the BERT Devlin et al. (2019  ###reference_b25###  ###reference_b25###) embeddings of the sentences and calculates their cosine similarity.\nUsing a pre-trained BART model, which provides access to generated token probabilities, BARTScore sums over the log probability of each token generated while conditioning on context and previously output tokens.\nEssentially, BARTScore attempts to predict the quality of a generated text using BART as a proxy model.\nSummaC is a class of natural language inference models that predict entailment, contradiction, and neutral scores between pairs of sentences among a document and its summary.\nEach score is collected into a separate matrix split by metric type.\nThe authors propose two approaches, SummaCZS and SummaCConv, for aggregating scores of each sentence in the summary with respect to each sentence in the document.\nLike BARTScore, GPTScore relies on a pre-trained language model with access to token probabilities to estimate quality of outputs, but uses the GPT series of LLMs.\nThe creators of AlignScore pose that two pieces of text are aligned when all information present in one text exists in the other, and the texts do not contradict one another.\nConsequently, they train a classification model on labeled data with three types of labels: a binary classification of aligned or not, a multi-class prediction including a neutral label in addition to the binary classification labels, and a continuous score for a regression task.\nThe AlignScore metric computes a weighted score across all three prediction heads at test-time.\nOne common method of measuring uncertainty of a model\u2019s many generations is computing its entropy over all generated token probabilities.\nHowever, in cases where multiple sentences have the same semantic meaning but output different entropies, the aggregated measurement is not representative of the true uncertainty of the model.\nKuhn et al. (2023  ###reference_b63###  ###reference_b63###) tackle this problem by clustering sentences into semantic classes and summing entropies of sentences from the same class together.\nCHAIR, used for measuring accuracy of descriptions of images, is the ratio of the number of hallucinated objects to all the objects mentioned in the description.\nTo identify the hallucinated objects within the description, the authors assume access to ground-truth object classes in the image.\nThe authors recognize that different instructions prompting for a description of an image may lead to different responses from the model with the same semantic meaning.\nIn this case, CHAIR gives different scores to both descriptions although they are alike.\nInstead, their proposed metric, POPE, asks binary questions about the existence of in-domain and out-of-domain objects in the image, which leads to more a more stable metric across different outputs."
        },
        {
            "section_id": "5.1.1",
            "parent_section_id": "5.1",
            "section_name": "5.1.1 Language Similarity",
            "text": "Given a pair of responses, BERTScore computes the BERT Devlin et al. (2019  ###reference_b25###  ###reference_b25###  ###reference_b25###) embeddings of the sentences and calculates their cosine similarity.\nUsing a pre-trained BART model, which provides access to generated token probabilities, BARTScore sums over the log probability of each token generated while conditioning on context and previously output tokens.\nEssentially, BARTScore attempts to predict the quality of a generated text using BART as a proxy model.\nSummaC is a class of natural language inference models that predict entailment, contradiction, and neutral scores between pairs of sentences among a document and its summary.\nEach score is collected into a separate matrix split by metric type.\nThe authors propose two approaches, SummaCZS and SummaCConv, for aggregating scores of each sentence in the summary with respect to each sentence in the document.\nLike BARTScore, GPTScore relies on a pre-trained language model with access to token probabilities to estimate quality of outputs, but uses the GPT series of LLMs.\nThe creators of AlignScore pose that two pieces of text are aligned when all information present in one text exists in the other, and the texts do not contradict one another.\nConsequently, they train a classification model on labeled data with three types of labels: a binary classification of aligned or not, a multi-class prediction including a neutral label in addition to the binary classification labels, and a continuous score for a regression task.\nThe AlignScore metric computes a weighted score across all three prediction heads at test-time.\nOne common method of measuring uncertainty of a model\u2019s many generations is computing its entropy over all generated token probabilities.\nHowever, in cases where multiple sentences have the same semantic meaning but output different entropies, the aggregated measurement is not representative of the true uncertainty of the model.\nKuhn et al. (2023  ###reference_b63###  ###reference_b63###  ###reference_b63###) tackle this problem by clustering sentences into semantic classes and summing entropies of sentences from the same class together."
        },
        {
            "section_id": "5.1.2",
            "parent_section_id": "5.1",
            "section_name": "5.1.2 Object Detection",
            "text": "CHAIR, used for measuring accuracy of descriptions of images, is the ratio of the number of hallucinated objects to all the objects mentioned in the description.\nTo identify the hallucinated objects within the description, the authors assume access to ground-truth object classes in the image.\nThe authors recognize that different instructions prompting for a description of an image may lead to different responses from the model with the same semantic meaning.\nIn this case, CHAIR gives different scores to both descriptions although they are alike.\nInstead, their proposed metric, POPE, asks binary questions about the existence of in-domain and out-of-domain objects in the image, which leads to more a more stable metric across different outputs."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Offline Datasets",
            "text": "In this section, we present relevant offline datasets used for evaluating the performance of hallucination detection and mitigation techniques in driving, robotic, and QA tasks.\nBDD-X is a multi-modal driving dataset consisting of K samples (i.e., video clips), each consisting of eight images with vehicle control actions and text annotations describing the scene and justifying actions.\nThe authors augment BDD-X into a QA dataset consisting of questions that ask about the current action of the vehicle, reasoning behind the action, and predicting future control signals.\nTo incorporate other questions a user might ask about the vehicle, surroundings, and other miscellaneous queries, they prompt ChatGPT to generate further questions.\nIn total, the DriveGPT4 dataset contains K samples.\nThe nuScenes dataset contains K driving videos, each running for  seconds, collected from roads in Boston and Singapore.\nEach frame includes six different RGB camera views, GPS, annotated D bounding boxes of various object classes, and semantically labeled rader, lidar, and map representations.\nLike DriveGPT4, NuScenes-QA is a visual QA dataset, but built on top of nuScenes.\nIt includes five different types of questions including checking the existence of objects, counting instances, detecting the object being referred to, identifying the action state of an object, and comparing two objects.\nOverall, the dataset holds K QA pairs across K scenes in nuScenes.\nTalk2Car is an earlier extension of the nuScenes dataset which aims to ignite further research into developing systems that bridge the gap between passengers and an autonomous vehicle through natural language.\nAnnotators provided approximately K text commands over  videos within the nuScenes training split which refer to an object in the scene.\nWhile Talk2Car is a pioneering work for object referral in real driving scenes through natural language, each annotated instruction only refers to one object.\nAs such, Wu et al. (2023a  ###reference_b135###  ###reference_b135###) propose a new task definition, referring multi-object tracking (RMOT), which attempts to predict all objects that are referred to within a natural language input.\nThey augment the KITTI driving dataset Geiger et al. (2012  ###reference_b41###  ###reference_b41###) with labeled D bounding boxes around objects that are referenced within a text prompt for K images.\nNuPrompt is another RMOT-based benchmark, but applied to nuScenes and with D bounding box labels.\nIt includes K languages prompts, with most prompts referring to anywhere between one and ten objects in a scene.\nThe authors argue that, while several datasets exist for anomaly detection or identification on roads, there is a gap in explaining the reason for categorizing an object as being risky, i.e., objects the model should pay attention to, like crosswalks, pedestrians, and traffic lights.\nAs such, DRAMA is a benchmark tackling identification of risky objects in a driving scene conditioned on natural language.\nDing et al. (2023  ###reference_b27###  ###reference_b27###) extend DRAMA to further include suggestions on actions the ego vehicle can take to minimize risk, but the dataset is not public at this time.\nNuInstruct addresses two common limitations in existing driving datasets: they cover a limited subset of necessary tasks while driving (e.g., evaluating perception while ignoring planning), and disregard temporal and multi-view representations.\nBuilt on top of NuScenes, the dataset provides K samples of multi-view sequences with corresponding QA pairs spanning  subtasks within perception, prediction, planning, and risk detection.\nThe authors of DriveLM curate a similar comprehensive dataset from nuScenes and the CARLA driving simulator Dosovitskiy et al. (2017  ###reference_b30###  ###reference_b30###) with open-ended and factual questions about importance rankings of nearby vehicles, planning actions, detecting lanes, and more.\nThe authors collect a text-based QA dataset from a proprietary driving simulator, generated from ChatGPT with ground-truth observations (e.g., relative locations of detected vehicles, ego vehicle control actions, etc.) from the simulator.\nHumanEval is a set of  handwritten programs, each with a function definition, docstring, program body, and unit tests.\nThe authors find there is great promise in using LLMs for code generation, but output quality is limited by length of context and buggy examples.\nThe authors build a new code generation benchmark specifically for robot tasks with  functions focused on spatial reasoning, geometric reasoning, and controls.\nThe Language-Table dataset contains K trajectories manually annotated with K unique instructions across simulated and real-world manipulator robots.\nThe multi-modal dataset consists of video sequences, corresponding actions at each time step, and language instructions describing the policy of the robot in hindsight.\nThe authors of the CLARA method developed a dataset to identify language goals from a user that are certain, ambiguous, and infeasible.\nCollected from three different types of robots (cooking, cleaning, and massage), SaGC is annotated with a floor-plan, descriptions of objects and people in view, a text goal, and a label of uncertainty.\nHotPotQA is a question-answering benchmark with K multi-hop questions (i.e., requiring multiple steps of reasoning to reach answer) collected from Wikipedia.\nThe dataset includes both questions that require finding relevant phrases from context paragraphs, and comparing two entities.\nIn contrast to HotPotQA, the developers of FEVER attempt to answer the question of whether a fact is supported by a knowledge-base.\nThe database contains K claims with annotated labels deciding if each claim is supported, refuted, or indeterminable from Wikipedia articles.\nNatural Questions is yet another QA dataset with sources from Wikipedia.\nThe authors release K training and K test samples of real (anonymized) queries into the Google search engine paired with a Wikipedia page and a long and short answer annotated by a person based on said article.\nLike HotPotQA, StrategyQA aims to develop a dataset of implicit multi-hop questions, but includes a greater variety categories of questions, and with less category imbalance.\nFurthermore, most of the questions in the dataset require three or more steps of decomposition and referencing to accurately solve.\nSeparate from the information retrieval task described in benchmarks above, Anantha et al. (2021  ###reference_b2###  ###reference_b2###) develop a dataset, QreCC, for conversational QA.\nThey focus on reading comprehension, passage retrieval, and question rewriting tasks, with a total of K dialogues paired with K questions.\nZhao et al. (2024  ###reference_b157###  ###reference_b157###) present a multi-model visual QA dataset of images, hallucinated descriptions, and non-hallucinated samples from the VG dataset Krishna et al. (2017  ###reference_b62###  ###reference_b62###)."
        },
        {
            "section_id": "5.2.1",
            "parent_section_id": "5.2",
            "section_name": "5.2.1 Driving",
            "text": "BDD-X is a multi-modal driving dataset consisting of K samples (i.e., video clips), each consisting of eight images with vehicle control actions and text annotations describing the scene and justifying actions.\nThe authors augment BDD-X into a QA dataset consisting of questions that ask about the current action of the vehicle, reasoning behind the action, and predicting future control signals.\nTo incorporate other questions a user might ask about the vehicle, surroundings, and other miscellaneous queries, they prompt ChatGPT to generate further questions.\nIn total, the DriveGPT4 dataset contains K samples.\nThe nuScenes dataset contains K driving videos, each running for  seconds, collected from roads in Boston and Singapore.\nEach frame includes six different RGB camera views, GPS, annotated D bounding boxes of various object classes, and semantically labeled rader, lidar, and map representations.\nLike DriveGPT4, NuScenes-QA is a visual QA dataset, but built on top of nuScenes.\nIt includes five different types of questions including checking the existence of objects, counting instances, detecting the object being referred to, identifying the action state of an object, and comparing two objects.\nOverall, the dataset holds K QA pairs across K scenes in nuScenes.\nTalk2Car is an earlier extension of the nuScenes dataset which aims to ignite further research into developing systems that bridge the gap between passengers and an autonomous vehicle through natural language.\nAnnotators provided approximately K text commands over  videos within the nuScenes training split which refer to an object in the scene.\nWhile Talk2Car is a pioneering work for object referral in real driving scenes through natural language, each annotated instruction only refers to one object.\nAs such, Wu et al. (2023a  ###reference_b135###  ###reference_b135###  ###reference_b135###) propose a new task definition, referring multi-object tracking (RMOT), which attempts to predict all objects that are referred to within a natural language input.\nThey augment the KITTI driving dataset Geiger et al. (2012  ###reference_b41###  ###reference_b41###  ###reference_b41###) with labeled D bounding boxes around objects that are referenced within a text prompt for K images.\nNuPrompt is another RMOT-based benchmark, but applied to nuScenes and with D bounding box labels.\nIt includes K languages prompts, with most prompts referring to anywhere between one and ten objects in a scene.\nThe authors argue that, while several datasets exist for anomaly detection or identification on roads, there is a gap in explaining the reason for categorizing an object as being risky, i.e., objects the model should pay attention to, like crosswalks, pedestrians, and traffic lights.\nAs such, DRAMA is a benchmark tackling identification of risky objects in a driving scene conditioned on natural language.\nDing et al. (2023  ###reference_b27###  ###reference_b27###  ###reference_b27###) extend DRAMA to further include suggestions on actions the ego vehicle can take to minimize risk, but the dataset is not public at this time.\nNuInstruct addresses two common limitations in existing driving datasets: they cover a limited subset of necessary tasks while driving (e.g., evaluating perception while ignoring planning), and disregard temporal and multi-view representations.\nBuilt on top of NuScenes, the dataset provides K samples of multi-view sequences with corresponding QA pairs spanning  subtasks within perception, prediction, planning, and risk detection.\nThe authors of DriveLM curate a similar comprehensive dataset from nuScenes and the CARLA driving simulator Dosovitskiy et al. (2017  ###reference_b30###  ###reference_b30###  ###reference_b30###) with open-ended and factual questions about importance rankings of nearby vehicles, planning actions, detecting lanes, and more.\nThe authors collect a text-based QA dataset from a proprietary driving simulator, generated from ChatGPT with ground-truth observations (e.g., relative locations of detected vehicles, ego vehicle control actions, etc.) from the simulator."
        },
        {
            "section_id": "5.2.2",
            "parent_section_id": "5.2",
            "section_name": "5.2.2 Code Generation and Robotics",
            "text": "HumanEval is a set of  handwritten programs, each with a function definition, docstring, program body, and unit tests.\nThe authors find there is great promise in using LLMs for code generation, but output quality is limited by length of context and buggy examples.\nThe authors build a new code generation benchmark specifically for robot tasks with  functions focused on spatial reasoning, geometric reasoning, and controls.\nThe Language-Table dataset contains K trajectories manually annotated with K unique instructions across simulated and real-world manipulator robots.\nThe multi-modal dataset consists of video sequences, corresponding actions at each time step, and language instructions describing the policy of the robot in hindsight.\nThe authors of the CLARA method developed a dataset to identify language goals from a user that are certain, ambiguous, and infeasible.\nCollected from three different types of robots (cooking, cleaning, and massage), SaGC is annotated with a floor-plan, descriptions of objects and people in view, a text goal, and a label of uncertainty."
        },
        {
            "section_id": "5.2.3",
            "parent_section_id": "5.2",
            "section_name": "5.2.3 Question-answering",
            "text": "HotPotQA is a question-answering benchmark with K multi-hop questions (i.e., requiring multiple steps of reasoning to reach answer) collected from Wikipedia.\nThe dataset includes both questions that require finding relevant phrases from context paragraphs, and comparing two entities.\nIn contrast to HotPotQA, the developers of FEVER attempt to answer the question of whether a fact is supported by a knowledge-base.\nThe database contains K claims with annotated labels deciding if each claim is supported, refuted, or indeterminable from Wikipedia articles.\nNatural Questions is yet another QA dataset with sources from Wikipedia.\nThe authors release K training and K test samples of real (anonymized) queries into the Google search engine paired with a Wikipedia page and a long and short answer annotated by a person based on said article.\nLike HotPotQA, StrategyQA aims to develop a dataset of implicit multi-hop questions, but includes a greater variety categories of questions, and with less category imbalance.\nFurthermore, most of the questions in the dataset require three or more steps of decomposition and referencing to accurately solve.\nSeparate from the information retrieval task described in benchmarks above, Anantha et al. (2021  ###reference_b2###  ###reference_b2###  ###reference_b2###) develop a dataset, QreCC, for conversational QA.\nThey focus on reading comprehension, passage retrieval, and question rewriting tasks, with a total of K dialogues paired with K questions.\nZhao et al. (2024  ###reference_b157###  ###reference_b157###  ###reference_b157###) present a multi-model visual QA dataset of images, hallucinated descriptions, and non-hallucinated samples from the VG dataset Krishna et al. (2017  ###reference_b62###  ###reference_b62###  ###reference_b62###)."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Simulation Platforms",
            "text": "Finally, we introduce common online simulators used to test hallucination detection methods for decision-making tasks.\nThe developer presents a D car simulator, with driving scenarios ranging from a passing on a multi-lane highway, merging into a highway, merging and exiting from a roundabout, parking, and more.\nAn ego vehicle can be controlled with discrete (e.g., merge left, merge right, faster, etc.) or continuous (e.g., providing an explicit acceleration command) actions.\nGeared towards microscopic traffic simulation, SUMO allows researchers to design road networks, track traffic flow metrics, and control individual vehicles.\nCARLA is a D driving simulator built on top of Unreal Engine.\nExisting works benchmark their methods on CARLA for perception, planning, control, and QA tasks for its realism.\nThere is also capability to perform co-simulation with SUMO and CARLA simultaneously Wegener et al. (2008  ###reference_b130###  ###reference_b130###).\nRavens is a D manipulator robot (UR5e) simulator built with PyBullet Coumans and Bai (2016\u20132021  ###reference_b22###  ###reference_b22###) with tasks like block insertion, towers of hanoi, aligning boxes, assembling kits, etc. Each simulated task features a manipulator robot with a suction gripper sitting on a table workspace, with three camera views.\nBuilding on top of the TextWorld simulator, discussed in Section 5.3.3  ###reference_.SSS3###  ###reference_.SSS3###, ALFWorld aligns perception from the D robot simulation benchmark, ALFRED Shridhar et al. (2020  ###reference_b113###  ###reference_b113###), with text-based, discrete actions like \u201cMoveAhead,\u201d \u201cRotateLeft,\u201d and \u201cOpen.\u201d\nProgPrompt is a benchmark of high-fidelity D data collected from a virtual home robot.\nIt includes three environments, each with  object instances.\nThese simulations are further used to create a dataset of  household robot tasks with a ground-truth set of actions to achieve each goal.\nRoboEval is a general platform for checking the correctness of code generated for a robot task.\nIt relies on a simulator, evaluator, and a set of defined tasks to perform evaluations on a simulated robot.\nWhile ProgPrompt captures more realistic scenarios in its high-fidelity D simulator, RoboEval is more tuned towards verifying code efficiently.\nMore recently, the developers of KnowNo also provide a tabletop simulator based on PyBullet, like Zeng et al. (2021  ###reference_b148###  ###reference_b148###), for robot manipulation of blocks and bowls.\nProvided instructions vary in ambiguity by attribute, number, and spatial reasoning.\nTextWorld is a suite of text-based games that can be either hand-engineered or procedurally generated, where an agent directly receives text-based observations from an abstract world, and acts with natural language actions to complete a task.\nChevalier-Boisvert et al. (2019  ###reference_b18###  ###reference_b18###) present a D top-down, grid-based simulator of instruction-following tasks with varying difficulty.\nSome tasks include simple navigation to a single goal, picking and placing objects with ambiguous references, and instructions that implicitly require multi-step reasoning to complete.\nThe simulator provides a partial observation of the space near the agent at every timestep.\nThe developers of MineDojo attempt to create a benchmark to test the continual learning of agents in an open-world setting.\nThey build an interface on top of Minecraft, a video game, to enable testing with diverse open-ended tasks, and provide access to an external knowledge-base of existing Minecraft tutorials and wiki discussions.\nMineDojo includes several thousands of tasks that are more complex that earlier works (and require multi-step reasoning).\nAs such, task completion is judged with a learned LVLM, which acts like a human evaluator.\nThe authors present a multi-agent conversational simulator where agents are controlled by language models.\nUsers may set up agents with a defined backstory and provide instructions when desired.\nEach agent has access to a memory of past experiences, and generates natural language actions to go to certain areas, communicate with others, complete chores, and more."
        },
        {
            "section_id": "5.3.1",
            "parent_section_id": "5.3",
            "section_name": "5.3.1 Driving",
            "text": "The developer presents a D car simulator, with driving scenarios ranging from a passing on a multi-lane highway, merging into a highway, merging and exiting from a roundabout, parking, and more.\nAn ego vehicle can be controlled with discrete (e.g., merge left, merge right, faster, etc.) or continuous (e.g., providing an explicit acceleration command) actions.\nGeared towards microscopic traffic simulation, SUMO allows researchers to design road networks, track traffic flow metrics, and control individual vehicles.\nCARLA is a D driving simulator built on top of Unreal Engine.\nExisting works benchmark their methods on CARLA for perception, planning, control, and QA tasks for its realism.\nThere is also capability to perform co-simulation with SUMO and CARLA simultaneously Wegener et al. (2008  ###reference_b130###  ###reference_b130###  ###reference_b130###)."
        },
        {
            "section_id": "5.3.2",
            "parent_section_id": "5.3",
            "section_name": "5.3.2 Robotics",
            "text": "Ravens is a D manipulator robot (UR5e) simulator built with PyBullet Coumans and Bai (2016\u20132021  ###reference_b22###  ###reference_b22###  ###reference_b22###) with tasks like block insertion, towers of hanoi, aligning boxes, assembling kits, etc. Each simulated task features a manipulator robot with a suction gripper sitting on a table workspace, with three camera views.\nBuilding on top of the TextWorld simulator, discussed in Section 5.3.3  ###reference_.SSS3###  ###reference_.SSS3###  ###reference_.SSS3###, ALFWorld aligns perception from the D robot simulation benchmark, ALFRED Shridhar et al. (2020  ###reference_b113###  ###reference_b113###  ###reference_b113###), with text-based, discrete actions like \u201cMoveAhead,\u201d \u201cRotateLeft,\u201d and \u201cOpen.\u201d\nProgPrompt is a benchmark of high-fidelity D data collected from a virtual home robot.\nIt includes three environments, each with  object instances.\nThese simulations are further used to create a dataset of  household robot tasks with a ground-truth set of actions to achieve each goal.\nRoboEval is a general platform for checking the correctness of code generated for a robot task.\nIt relies on a simulator, evaluator, and a set of defined tasks to perform evaluations on a simulated robot.\nWhile ProgPrompt captures more realistic scenarios in its high-fidelity D simulator, RoboEval is more tuned towards verifying code efficiently.\nMore recently, the developers of KnowNo also provide a tabletop simulator based on PyBullet, like Zeng et al. (2021  ###reference_b148###  ###reference_b148###  ###reference_b148###), for robot manipulation of blocks and bowls.\nProvided instructions vary in ambiguity by attribute, number, and spatial reasoning."
        },
        {
            "section_id": "5.3.3",
            "parent_section_id": "5.3",
            "section_name": "5.3.3 Other Simulators",
            "text": "TextWorld is a suite of text-based games that can be either hand-engineered or procedurally generated, where an agent directly receives text-based observations from an abstract world, and acts with natural language actions to complete a task.\nChevalier-Boisvert et al. (2019  ###reference_b18###  ###reference_b18###  ###reference_b18###) present a D top-down, grid-based simulator of instruction-following tasks with varying difficulty.\nSome tasks include simple navigation to a single goal, picking and placing objects with ambiguous references, and instructions that implicitly require multi-step reasoning to complete.\nThe simulator provides a partial observation of the space near the agent at every timestep.\nThe developers of MineDojo attempt to create a benchmark to test the continual learning of agents in an open-world setting.\nThey build an interface on top of Minecraft, a video game, to enable testing with diverse open-ended tasks, and provide access to an external knowledge-base of existing Minecraft tutorials and wiki discussions.\nMineDojo includes several thousands of tasks that are more complex that earlier works (and require multi-step reasoning).\nAs such, task completion is judged with a learned LVLM, which acts like a human evaluator.\nThe authors present a multi-agent conversational simulator where agents are controlled by language models.\nUsers may set up agents with a defined backstory and provide instructions when desired.\nEach agent has access to a memory of past experiences, and generates natural language actions to go to certain areas, communicate with others, complete chores, and more."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Future Directions",
            "text": "Here, we discuss some possible future directions in hallucination detection and mitigation techniques for foundation models to improve deployments to decision-making tasks.\nMost hallucination detection approaches are currently evaluated in offline QA settings for information retrieval or knowledge alignment, as seen in Table 2  ###reference_###.\nAs foundation models are increasingly used for more complex tasks, researchers should make an effort to adapt and evaluate earlier detection/mitigation approaches that were applied to QA problems in these modern applications.\nAlthough dissimilar in practice from QA settings, planning and control problems may be formulated in such a way that enables these earlier mitigation methods to be evaluated on decision-making tasks.\nFor example, as discussed in Section 2.1  ###reference_###, Chen et al. (2023c  ###reference_b15###) treat the autonomous driving task as a QA problem, which could be naturally extended to test other QA hallucination detection methods in the same setting.\nThis evaluation may lead to greater understanding of the general limitations of these models, as we draw parallels across diverse deployments.\nWhite- and grey-box detection methods may not generally be applicable in situations where the internal state or token probabilities are unavailable from the language model.\nThus, we predict black-box approaches will take precedence in the near future, as state-of-the-art LVLMs like GPT-4V already prohibit access to probability outputs.\nHowever, current black-box methods are limited with simplistic sampling techniques to gauge uncertainty, and proxy models may not be representative of the true state of the model under test.\nWorks like FLIRT (while only applied to image generation models) showcase the promise of black-box adversarial prompting approaches in generating undesirable results from models Mehrabi et al. (2023  ###reference_b87###).\nWe argue developing more aggressive black-box adversarial generative models, which explicitly optimize for producing inputs that may perturb the system outputs, is key to identifying the limits of a foundation model\u2019s knowledge.\nCurrently, foundation models are primarily deployed to decision-making tasks that likely have some relation to its training set.\nFor example, although complex, tasks like multi-agent communication, autonomous driving, and code generation will be present in training datasets.\nOn the other hand, dynamic environments like robot crowd navigation require identifying nuances in pedestrian behaviors which the model may not have explicitly seen during training.\nPushing the limits of foundation model deployments will allow researchers to find areas for growth in other applications.\nWith the explosion of LVLMs, which allow for explicit grounding of natural language and vision modalities, further exploration should be performed in evaluating their effectiveness in decision-making systems.\nWen et al. (2023  ###reference_b133###) take a step in the right direction towards testing black-box LVLMs in offline driving scenarios, but there is still work to be done in deploying these models in online settings.\nThis direction can shed light on the long-standing debate of whether modular or end-to-end systems should be preferred in a particular deployment setting.\nIn this survey, we provide a glimpse into the progress of research into evaluating hallucinations of foundation models for decision-making problems.\nWe begin by identifying existing usecases of foundation models in decision-making applications like autonomous driving and robotics, and find several works make note of undesired hallucinated generations in practice.\nBy referencing works that encounter hallucinations across diverse domains, we provide a flexible definition for hallucinations that researchers can leverage, regardless of the deployment scenario in mind.\nFinally, we give a taxonomy of hallucination detection and mitigation approaches for decision-making problems, alongside a list of commonly used metrics, datasets, and simulators for evaluation.\nWe find that existing methods range in varying assumptions of inputs and evaluation settings, and believe there is much room for growth in general, black-box hallucination detection algorithms for foundation models."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.2.1\">Examples of applying Definition\u00a0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S3.Thmdefinition1\" title=\"Definition 3.1. \u2023 3.1 What are hallucinations? \u2023 3 Hallucinations \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> to different tasks.</span>\nNote that developers may choose to only define a subset of hallucination metrics for their deployment depending on evaluation preferences.\n</figcaption>\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S3.T1.3\" style=\"width:433.6pt;height:23983.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(182.5pt,-10093.4pt) scale(6.31739755691394,6.31739755691394) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.3.1.1.1.1\" rowspan=\"2\" style=\"width:79.5pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.1.1.1.1\">Metric</p>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" id=\"S3.T1.3.1.1.1.2\">Problem Setting</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.2.2\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.3.1.2.2.1\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.2.2.1.1\">Questing-Answering</p>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.3.1.2.2.2\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.2.2.2.1\">Image Captioning</p>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.3.1.2.2.3\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.2.2.3.1\">Planning</p>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.3.1.2.2.4\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.2.2.4.1\">Control</p>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.3.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" id=\"S3.T1.3.1.3.1.1\" style=\"width:79.5pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.3.1.3.1.1.1\">Consistency</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" id=\"S3.T1.3.1.3.1.2\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.3.1.2.1\">Generations must align with database facts</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" id=\"S3.T1.3.1.3.1.3\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.3.1.3.1\">Objects in description must appear in image</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" id=\"S3.T1.3.1.3.1.4\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.3.1.4.1\">Predicted sub-task must be feasible to solve</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" id=\"S3.T1.3.1.3.1.5\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.3.1.5.1\">Predicted action must be possible to perform</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.4.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" id=\"S3.T1.3.1.4.2.1\" style=\"width:79.5pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.3.1.4.2.1.1\">Desired Behavior</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" id=\"S3.T1.3.1.4.2.2\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.4.2.2.1\">Tone of answer should be informative</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" id=\"S3.T1.3.1.4.2.3\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.4.2.3.1\">Censor descriptions for inappropriate images</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" id=\"S3.T1.3.1.4.2.4\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.4.2.4.1\">Plans should maximize expected return</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" id=\"S3.T1.3.1.4.2.5\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.4.2.5.1\">Predict actions to complete plan efficiently</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.5.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" id=\"S3.T1.3.1.5.3.1\" style=\"width:79.5pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.3.1.5.3.1.1\">Relevancy</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" id=\"S3.T1.3.1.5.3.2\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.5.3.2.1\">Answers should not include references to unrelated topics</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" id=\"S3.T1.3.1.5.3.3\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.5.3.3.1\">Descriptions should not be embellished with details that cannot be confirmed</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l\" colspan=\"2\" id=\"S3.T1.3.1.5.3.4\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.5.3.4.1\">Predicted sub-tasks and actions should not stray from the end goal with unnecessary steps</p></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.6.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb\" id=\"S3.T1.3.1.6.4.1\" style=\"width:79.5pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.3.1.6.4.1.1\">Plausibility</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb\" colspan=\"2\" id=\"S3.T1.3.1.6.4.2\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.6.4.2.1\">Generation is syntactically sound and believable</p></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_l\" colspan=\"2\" id=\"S3.T1.3.1.6.4.3\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.6.4.3.1\">Generated plan is reasonable and seems to attempt to accomplish goal</p></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 1: \nExamples of applying Definition\u00a03.1 to different tasks.\nNote that developers may choose to only define a subset of hallucination metrics for their deployment depending on evaluation preferences.\n"
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.56.1\">A summary of hallucination detection &amp; mitigation methods discussed in Section\u00a0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4\" title=\"4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</span>\nDeployment scenarios are split into question-answering (QA), information retrieval (IR), image captioning (IC), image generation (IG), &amp; planning (P) tasks.\nThe method ID includes the subsection the method appears in the paper and the order in which it appears in the subsection.\nBolded method IDs are deployed to decision-making tasks specifically.\nCustom datasets, custom simulators, &amp; real-world experiments for testing are abbreviated as CD, CS, &amp; RW, respectively.\n</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T2.54\" style=\"width:359.9pt;height:553.3pt;vertical-align:-1.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-189.5pt,290.9pt) scale(0.487017868908862,0.487017868908862) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.54.54\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.55.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.54.54.55.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.54.54.55.1.1.1\">Modality</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.54.54.55.1.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.54.54.55.1.2.1\">Method Type</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.54.54.55.1.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.54.54.55.1.3.1\">Method ID</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T2.54.54.55.1.4\">Application</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.54.54.55.1.5\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.54.54.55.1.5.1\">Deployment</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.54.54.55.1.6\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.54.54.55.1.6.1\">Evaluation Datasets or Simulators</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.56.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.54.54.56.2.1\">Detection</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.54.54.56.2.2\">Mitigation</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.1.2\" rowspan=\"8\"><span class=\"ltx_text\" id=\"S4.T2.1.1.1.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1.1.2.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.2.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.1.2.1.1.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T2.1.1.1.2.1.1.1.1.1\" style=\"width:6.9pt;height:46.1pt;vertical-align:-19.6pt;\"><span class=\"ltx_transformed_inner\" style=\"width:46.1pt;transform:translate(-19.58pt,0pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.1.2.1.1.1.1.1.1\">White-box</span>\n</span></span></span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.1.3\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T2.1.1.1.3.1\">Hidden States</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.1.4\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I1.i1\" title=\"item 1 \u2023 4.1.1 Hidden States \u2023 4.1 White-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.1.1.1</a></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.1.1.1.5\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.1.6\">QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.1.7\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.2.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.2.2\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I1.i2\" title=\"item 2 \u2023 4.1.1 Hidden States \u2023 4.1 White-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.1.1.2</a></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.2.1\"></td>\n<td class=\"ltx_td\" id=\"S4.T2.2.2.2.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.2.4\">QA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.2.5\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.3.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.3.3.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.3.3.3.2.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I1.i3\" title=\"item 3 \u2023 4.1.1 Hidden States \u2023 4.1 White-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.1.1.3</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.3.3.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.3.3.3.1.1\"></span></td>\n<td class=\"ltx_td\" id=\"S4.T2.3.3.3.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.3.3.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.3.3.3.4.1\">QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.3.3.5\">DecodingTrust\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib127\" title=\"\">2023a</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.57.3\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.57.3.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.57.3.2\">TruthfulQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Lin et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib77\" title=\"\">2022b</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.5.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.5.5.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.5.5.5.3.1\">Attention Weights</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.5.5.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.5.5.5.4.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I2.i1\" title=\"item 1 \u2023 4.1.2 Attention Weights \u2023 4.1 White-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.1.2.1</a></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.4.4.4.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.4.4.4.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.5.5.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.5.5.5.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.5.5.5\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.5.5.5.5.1\">IC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.5.5.6\">MSCOCO\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Lin et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib78\" title=\"\">2014</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.58.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.58.4.1\">Visual Genome\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Krishna et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib62\" title=\"\">2017</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.6.6.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.6.6.6.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.6.6.6.2.1\">Honesty Alignment</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.6.6.6.3\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I3.i1\" title=\"item 1 \u2023 4.1.3 Honesty Alignment \u2023 4.1 White-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.1.3.1</a></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.6.6.6.1\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.6.6.6.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.6.6.6.5\">QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.6.6.6.6\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.8.8.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.8.8.8.3\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I3.i2\" title=\"item 2 \u2023 4.1.3 Honesty Alignment \u2023 4.1 White-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.1.3.2</a></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.7.7.7.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.8.8.8.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.8.8.8.4\">QA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.8.8.8.5\">TriviaQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Joshi et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib57\" title=\"\">2017</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.10.10.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.10.10.10.3\" rowspan=\"15\"><span class=\"ltx_text\" id=\"S4.T2.10.10.10.3.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.10.10.10.3.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.10.10.10.3.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T2.10.10.10.3.1.1.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T2.10.10.10.3.1.1.1.1.1\" style=\"width:8.9pt;height:40.6pt;vertical-align:-17.8pt;\"><span class=\"ltx_transformed_inner\" style=\"width:40.7pt;transform:translate(-15.88pt,2.92pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T2.10.10.10.3.1.1.1.1.1.1\">Grey-box</span>\n</span></span></span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.10.10.10.4\" rowspan=\"6\"><span class=\"ltx_text\" id=\"S4.T2.10.10.10.4.1\">Concept Probabilities</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.10.10.10.5\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.10.10.10.5.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I4.i1\" title=\"item 1 \u2023 4.2.1 Concept Probabilities \u2023 4.2 Grey-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.2.1.1</a></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.9.9.9.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.9.9.9.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.10.10.10.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.10.10.10.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.10.10.10.6\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.10.10.10.6.1\">IR/QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.10.10.10.7\">HotpotQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Yang et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib142\" title=\"\">2018</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.59.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.59.5.1\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.11.11.11\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.11.11.11.2\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I4.i2\" title=\"item 2 \u2023 4.2.1 Concept Probabilities \u2023 4.2 Grey-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.2.1.2</a></td>\n<td class=\"ltx_td\" id=\"S4.T2.11.11.11.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.11.11.11.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.11.11.11.4\">IC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.11.11.11.5\">MSCOCO\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Lin et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib78\" title=\"\">2014</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.12\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.12.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.12.12.12.2.1\"><a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.16527v1#S4.I4.i3\" title=\"item 3 \u2023 4.2.1 Concept Probabilities \u2023 4.2 Grey-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.2.1.3</a></span></td>\n<td class=\"ltx_td\" id=\"S4.T2.12.12.12.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.12.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.12.12.12.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.12.4\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.12.12.12.4.1\">P</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.12.12.12.5\">Ravens\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib148\" title=\"\">2021</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.60.6\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.60.6.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.60.6.2\">BabyAI\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Chevalier-Boisvert et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib18\" title=\"\">2019</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.61.7\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.61.7.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.61.7.2\">VirtualHome\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Puig et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib96\" title=\"\">2018</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.14.14.14\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.14.14.14.3\" rowspan=\"9\"><span class=\"ltx_text\" id=\"S4.T2.14.14.14.3.1\">Conformal Prediction</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.14.14.14.4\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.14.14.14.4.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I5.i1\" title=\"item 1 \u2023 4.2.2 Conformal Prediction \u2023 4.2 Grey-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.2.2.1</a></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.13.13.13.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.13.13.13.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.14.14.14.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.14.14.14.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.14.14.14.5\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.14.14.14.5.1\">IR/QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.14.14.14.6\">MIMIC-CXR\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Johnson et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib56\" title=\"\">2019</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.62.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.62.8.1\">CNN/DM\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Hermann et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib47\" title=\"\">2015</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.63.9\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.63.9.1\">TriviaQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Joshi et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib57\" title=\"\">2017</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.16.16.16\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.16.16.16.3\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I5.i2\" title=\"item 2 \u2023 4.2.2 Conformal Prediction \u2023 4.2 Grey-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.2.2.2</a></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.15.15.15.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.16.16.16.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.16.16.16.4\">QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.16.16.16.5\">MMLU\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Hendrycks et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib46\" title=\"\">2021</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.18.18.18\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.18.18.18.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.18.18.18.3.1\"><a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.16527v1#S4.I5.i3\" title=\"item 3 \u2023 4.2.2 Conformal Prediction \u2023 4.2 Grey-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.2.2.3</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.17.17.17.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.17.17.17.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.18.18.18.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.18.18.18.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.18.18.18.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.18.18.18.4.1\">P</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.18.18.18.5\">KnowNo TableSim\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Ren et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib105\" title=\"\">2023</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.64.10\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.64.10.1\">RW</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.20.20.20\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.20.20.20.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.20.20.20.3.1\"><a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.16527v1#S4.I5.i4\" title=\"item 4 \u2023 4.2.2 Conformal Prediction \u2023 4.2 Grey-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.2.2.4</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.19.19.19.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.19.19.19.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.20.20.20.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.20.20.20.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.20.20.20.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.20.20.20.4.1\">P</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.20.20.20.5\">KnowNo TableSim\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Ren et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib105\" title=\"\">2023</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.65.11\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.65.11.1\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.22.22.22\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.22.22.22.3\"><a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.16527v1#S4.I5.i5\" title=\"item 5 \u2023 4.2.2 Conformal Prediction \u2023 4.2 Grey-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.2.2.5</a></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.21.21.21.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.22.22.22.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.22.22.22.4\">P</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.22.22.22.5\">CS</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.23.23.23\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.23.23.23.2\" rowspan=\"38\"><span class=\"ltx_text\" id=\"S4.T2.23.23.23.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.23.23.23.2.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.23.23.23.2.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T2.23.23.23.2.1.1.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T2.23.23.23.2.1.1.1.1.1\" style=\"width:6.9pt;height:43.5pt;vertical-align:-18.3pt;\"><span class=\"ltx_transformed_inner\" style=\"width:43.5pt;transform:translate(-18.26pt,0pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T2.23.23.23.2.1.1.1.1.1.1\">Black-box</span>\n</span></span></span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.23.23.23.3\" rowspan=\"19\"><span class=\"ltx_text\" id=\"S4.T2.23.23.23.3.1\">Analyzing Samples</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.23.23.23.4\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I6.i1\" title=\"item 1 \u2023 4.3.1 Analyzing Samples from Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.1.1</a></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.23.23.23.1\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.23.23.23.5\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.23.23.23.6\">IR/QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.23.23.23.7\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.25.25.25\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.25.25.25.3\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I6.i2\" title=\"item 2 \u2023 4.3.1 Analyzing Samples from Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.1.2</a></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.24.24.24.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.25.25.25.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.25.25.25.4\">IR/QA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.25.25.25.5\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.28.28.28\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.28.28.28.4\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.28.28.28.4.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I6.i3\" title=\"item 3 \u2023 4.3.1 Analyzing Samples from Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.1.3</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.26.26.26.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.26.26.26.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.27.27.27.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.27.27.27.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.28.28.28.5\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.28.28.28.5.1\">IR/QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.28.28.28.3\">GSMK\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Cobbe et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib20\" title=\"\">2021</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.66.12\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.66.12.1\">MMLU\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Hendrycks et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib46\" title=\"\">2021</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.67.13\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.67.13.1\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.30.30.30\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.30.30.30.3\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.30.30.30.3.1\"><a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.16527v1#S4.I6.i4\" title=\"item 4 \u2023 4.3.1 Analyzing Samples from Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.1.4</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.29.29.29.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.29.29.29.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.30.30.30.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.30.30.30.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.30.30.30.4\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.30.30.30.4.1\">P</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.30.30.30.5\">SaGC\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Park et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib93\" title=\"\">2024</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.68.14\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.68.14.1\">CS</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.69.15\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.69.15.1\">RW</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.32.32.32\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.32.32.32.3\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I6.i5\" title=\"item 5 \u2023 4.3.1 Analyzing Samples from Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.1.5</a></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.31.31.31.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.32.32.32.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.32.32.32.4\">IR/QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.32.32.32.5\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.34.34.34\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.34.34.34.3\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T2.34.34.34.3.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I6.i6\" title=\"item 6 \u2023 4.3.1 Analyzing Samples from Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.1.6</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.33.33.33.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T2.33.33.33.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.34.34.34.2\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T2.34.34.34.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.34.34.34.4\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T2.34.34.34.4.1\">IR/QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.34.34.34.5\">Quest\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Malaviya et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib83\" title=\"\">2023</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.70.16\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.70.16.1\">MultiSpanQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Li et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib70\" title=\"\">2022</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.71.17\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.71.17.1\">FActScore\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Min et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib88\" title=\"\">2023</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.72.18\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.72.18.1\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.35.35.35\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.35.35.35.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.35.35.35.2.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I6.i7\" title=\"item 7 \u2023 4.3.1 Analyzing Samples from Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.1.7</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.35.35.35.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.35.35.35.1.1\"></span></td>\n<td class=\"ltx_td\" id=\"S4.T2.35.35.35.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.35.35.35.4\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.35.35.35.4.1\">IC/QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.35.35.35.5\">MSCOCO\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Lin et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib78\" title=\"\">2014</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.73.19\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.73.19.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.73.19.2\">A-OKVQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Schwenk et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib109\" title=\"\">2022</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.74.20\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.74.20.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.74.20.2\">GQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Hudson and Manning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib51\" title=\"\">2019</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.36.36.36\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.36.36.36.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.36.36.36.2.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I6.i8\" title=\"item 8 \u2023 4.3.1 Analyzing Samples from Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.1.8</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.36.36.36.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.36.36.36.1.1\"></span></td>\n<td class=\"ltx_td\" id=\"S4.T2.36.36.36.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.36.36.36.4\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.36.36.36.4.1\">QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.36.36.36.5\">BIG-Bench\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Srivastava et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib120\" title=\"\">2023</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.37.37.37\">\n<td class=\"ltx_td\" id=\"S4.T2.37.37.37.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.37.37.37.1\">GSMK\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Cobbe et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib20\" title=\"\">2021</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.75.21\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.75.21.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.75.21.2\">MMLU\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Hendrycks et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib46\" title=\"\">2021</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.38.38.38\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.38.38.38.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.38.38.38.2.1\">Adversarial Prompting</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.38.38.38.3\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I7.i1\" title=\"item 1 \u2023 4.3.2 Adversarial Prompting \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.2.1</a></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.38.38.38.1\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.38.38.38.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.38.38.38.5\">IG</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.38.38.38.6\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.40.40.40\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.40.40.40.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.40.40.40.3.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I7.i2\" title=\"item 2 \u2023 4.3.2 Adversarial Prompting \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.2.2</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.39.39.39.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.39.39.39.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.40.40.40.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.40.40.40.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.40.40.40.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.40.40.40.4.1\">IR/QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.40.40.40.5\">Natural Questions\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Kwiatkowski et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib65\" title=\"\">2019</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.76.22\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.76.22.1\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.41.41.41\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.41.41.41.2\" rowspan=\"5\"><span class=\"ltx_text\" id=\"S4.T2.41.41.41.2.1\">Proxy Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.41.41.41.3\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I8.i1\" title=\"item 1 \u2023 4.3.3 Proxy Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.3.1</a></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.41.41.41.1\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.41.41.41.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.41.41.41.5\">IR/QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.41.41.41.6\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.42.42.42\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.42.42.42.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.42.42.42.2.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I8.i2\" title=\"item 2 \u2023 4.3.3 Proxy Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.3.2</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.42.42.42.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.42.42.42.1.1\"></span></td>\n<td class=\"ltx_td\" id=\"S4.T2.42.42.42.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.42.42.42.4\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.42.42.42.4.1\">IR/QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.42.42.42.5\">SQuAD\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Rajpurkar et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib103\" title=\"\">2016</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.77.23\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.77.23.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.77.23.2\">HotpotQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Yang et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib142\" title=\"\">2018</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.78.24\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.78.24.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.78.24.2\">TriviaQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Joshi et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib57\" title=\"\">2017</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.43.43.43\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.43.43.43.2\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I8.i3\" title=\"item 3 \u2023 4.3.3 Proxy Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.3.3</a></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.43.43.43.1\"></td>\n<td class=\"ltx_td\" id=\"S4.T2.43.43.43.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.43.43.43.4\">IR/QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.43.43.43.5\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.44.44.44\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.44.44.44.2\" rowspan=\"9\"><span class=\"ltx_text\" id=\"S4.T2.44.44.44.2.1\">Grounding Knowledge</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.44.44.44.3\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.44.44.44.3.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I9.i1\" title=\"item 1 \u2023 4.3.4 Grounding Knowledge \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.4.1</a></span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.44.44.44.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.44.44.44.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.44.44.44.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.44.44.44.5\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.44.44.44.5.1\">IR/QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.44.44.44.6\">Natural Questions\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Kwiatkowski et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib65\" title=\"\">2019</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.79.25\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.79.25.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.79.25.2\">StrategyQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Geva et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib42\" title=\"\">2021</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.80.26\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.80.26.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.80.26.2\">QRreCC\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Anantha et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib2\" title=\"\">2021</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.46.46.46\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.46.46.46.3\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.46.46.46.3.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I9.i2\" title=\"item 2 \u2023 4.3.4 Grounding Knowledge \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.4.2</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.45.45.45.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.45.45.45.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.46.46.46.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.46.46.46.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.46.46.46.4\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.46.46.46.4.1\">IR/QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.46.46.46.5\">LC-QuAD\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Trivedi et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib125\" title=\"\">2017</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.81.27\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.81.27.1\">KQA-Pro\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Cao et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib9\" title=\"\">2022</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.82.28\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.82.28.1\">ScienceQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Lu et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib81\" title=\"\">2022</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.48.48.48\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.48.48.48.3\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I9.i3\" title=\"item 3 \u2023 4.3.4 Grounding Knowledge \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.4.3</a></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.47.47.47.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.48.48.48.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.48.48.48.4\">IR/QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.48.48.48.5\">FuzzyQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib153\" title=\"\">2023c</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.50.50.50\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.50.50.50.3\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I9.i4\" title=\"item 4 \u2023 4.3.4 Grounding Knowledge \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.4.4</a></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.49.49.49.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.50.50.50.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.50.50.50.4\">IR/QA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.50.50.50.5\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.51.51.51\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.51.51.51.2\"><a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.16527v1#S4.I9.i5\" title=\"item 5 \u2023 4.3.4 Grounding Knowledge \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.4.5</a></td>\n<td class=\"ltx_td\" id=\"S4.T2.51.51.51.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.51.51.51.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.51.51.51.4\">P</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.51.51.51.5\">TextWorld\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">C\u00f4t\u00e9 et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib21\" title=\"\">2019</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.53.53.53\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.53.53.53.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.53.53.53.3.1\">Constraint Satisfaction</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.53.53.53.4\"><a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.16527v1#S4.I10.i1\" title=\"item 1 \u2023 4.3.5 Constraint Satisfaction \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.5.1</a></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.52.52.52.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.53.53.53.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.53.53.53.5\">P</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.53.53.53.6\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.54\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.54.54.54.2\"><a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.16527v1#S4.I10.i2\" title=\"item 2 \u2023 4.3.5 Constraint Satisfaction \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.5.2</a></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.54.54.54.1\"></td>\n<td class=\"ltx_td ltx_border_bb\" id=\"S4.T2.54.54.54.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.54.54.54.4\">P</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.54.54.54.5\">RoboEval\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Hu et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib48\" title=\"\">2024</a>)</cite>\n</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 2: \nA summary of hallucination detection & mitigation methods discussed in Section\u00a04.\nDeployment scenarios are split into question-answering (QA), information retrieval (IR), image captioning (IC), image generation (IG), & planning (P) tasks.\nThe method ID includes the subsection the method appears in the paper and the order in which it appears in the subsection.\nBolded method IDs are deployed to decision-making tasks specifically.\nCustom datasets, custom simulators, & real-world experiments for testing are abbreviated as CD, CS, & RW, respectively.\n"
        }
    },
    "image_paths": {},
    "references": [
        {
            "1": {
                "title": "GPT-4 Technical Report.",
                "author": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023.",
                "venue": "arXiv preprint arXiv:2303.08774.",
                "url": "https://arxiv.org/abs/2303.08774"
            }
        },
        {
            "2": {
                "title": "Open-Domain Question Answering Goes Conversational via Question Rewriting.",
                "author": "Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. 2021.",
                "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 520\u2013534. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.naacl-main.44"
            }
        },
        {
            "3": {
                "title": "Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control.",
                "author": "Anastasios N. Angelopoulos, Stephen Bates, Emmanuel J. Cand\u00e8s, Michael I. Jordan, and Lihua Lei. 2022.",
                "venue": "arXiv preprint arXiv:2110.01052.",
                "url": "https://arxiv.org/abs/2110.01052"
            }
        },
        {
            "4": {
                "title": "The Internal State of an LLM Knows When It\u2019s Lying.",
                "author": "Amos Azaria and Tom Mitchell. 2023.",
                "venue": "In Findings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 967\u2013976, Singapore. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-emnlp.68"
            }
        },
        {
            "5": {
                "title": "On the Opportunities and Risks of Foundation Models.",
                "author": "Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2022.",
                "venue": "arXiv preprint arXiv:2108.07258.",
                "url": "https://arxiv.org/abs/2108.07258"
            }
        },
        {
            "6": {
                "title": "A review of PID control, tuning methods and applications.",
                "author": "Rakesh P Borase, DK Maghade, SY Sondkar, and SN Pawar. 2021.",
                "venue": "International Journal of Dynamics and Control, 9:818\u2013827.",
                "url": "https://doi.org/10.1007/s40435-020-00665-4"
            }
        },
        {
            "7": {
                "title": "Language Models are Few-Shot Learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.",
                "venue": "In Proceedings of the 2020 Conference on Neural Information Processing Systems, pages 1877\u20131901, Vancouver, Canada. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf"
            }
        },
        {
            "8": {
                "title": "nuScenes: A Multimodal Dataset for Autonomous Driving.",
                "author": "Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. 2020.",
                "venue": "In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11618\u201311628. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/CVPR42600.2020.01164"
            }
        },
        {
            "9": {
                "title": "KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base.",
                "author": "Shulin Cao, Jiaxin Shi, Liangming Pan, Lunyiu Nie, Yutong Xiang, Lei Hou, Juanzi Li, Bin He, and Hanwang Zhang. 2022.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6101\u20136119, Dublin, Ireland. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.acl-long.422"
            }
        },
        {
            "10": {
                "title": "Emerging Properties in Self-Supervised Vision Transformers.",
                "author": "Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. 2021.",
                "venue": "In Proceedings of the 2021 IEEE/CVF Conference on International Conference on Computer Vision (ICCV), pages 9630\u20139640, Montreal, Canada. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/ICCV48922.2021.00951"
            }
        },
        {
            "11": {
                "title": "Structural Attention-based Recurrent Variational Autoencoder for Highway Vehicle Anomaly Detection.",
                "author": "Neeloy Chakraborty, Aamir Hasan, Shuijing Liu, Tianchen Ji, Weihang Liang, D. Livingston McPherson, and Katherine Driggs-Campbell. 2023.",
                "venue": "In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, page 1125\u20131134, London, England. International Foundation for Autonomous Agents and Multiagent Systems.",
                "url": "https://dl.acm.org/doi/abs/10.5555/3545946.3598754"
            }
        },
        {
            "12": {
                "title": "PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions.",
                "author": "Anthony Chen, Panupong Pasupat, Sameer Singh, Hongrae Lee, and Kelvin Guu. 2023a.",
                "venue": "arXiv preprint arXiv:2305.14908.",
                "url": "https://arxiv.org/abs/2305.14908"
            }
        },
        {
            "13": {
                "title": "Can LLM-Generated Misinformation Be Detected?",
                "author": "Canyu Chen and Kai Shu. 2024.",
                "venue": "In Proceedings of the 12th International Conference on Learning Representations, Vienna, Austria.",
                "url": "https://openreview.net/forum?id=ccxD4mtkTU"
            }
        },
        {
            "14": {
                "title": "Introspective Tips: Large Language Model for In-Context Decision Making.",
                "author": "Liting Chen, Lu Wang, Hang Dong, Yali Du, Jie Yan, Fangkai Yang, Shuang Li, Pu Zhao, Si Qin, Saravan Rajmohan, et al. 2023b.",
                "venue": "arXiv preprint arXiv:2305.11598.",
                "url": "https://arxiv.org/abs/2305.11598"
            }
        },
        {
            "15": {
                "title": "Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving.",
                "author": "Long Chen, Oleg Sinavski, Jan H\u00fcnermann, Alice Karnsund, Andrew James Willmott, Danny Birch, Daniel Maund, and Jamie Shotton. 2023c.",
                "venue": "arXiv preprint arXiv:2310.01957.",
                "url": "https://arxiv.org/abs/2310.01957"
            }
        },
        {
            "16": {
                "title": "Evaluating Large Language Models Trained on Code.",
                "author": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021.",
                "venue": "arXiv preprint arXiv:2107.03374.",
                "url": "https://arxiv.org/abs/2107.03374"
            }
        },
        {
            "17": {
                "title": "Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models.",
                "author": "Yuyan Chen, Qiang Fu, Yichen Yuan, Zhihao Wen, Ge Fan, Dayiheng Liu, Dongmei Zhang, Zhixu Li, and Yanghua Xiao. 2023d.",
                "venue": "In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, page 245\u2013255, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/3583780.3614905"
            }
        },
        {
            "18": {
                "title": "BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop.",
                "author": "Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. 2019.",
                "venue": "In Proceedings of the 7th International Conference on Learning Representations, New Orleans, LA, USA.",
                "url": "https://openreview.net/forum?id=rJeXCo0cYX"
            }
        },
        {
            "19": {
                "title": "PaLM: Scaling Language Modeling with Pathways.",
                "author": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023.",
                "venue": "Journal of Machine Learning Research, 24(240).",
                "url": "http://jmlr.org/papers/v24/22-1144.html"
            }
        },
        {
            "20": {
                "title": "Training Verifiers to Solve Math Word Problems.",
                "author": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021.",
                "venue": "arXiv preprint arXiv:2110.14168.",
                "url": "https://arxiv.org/abs/2110.14168"
            }
        },
        {
            "21": {
                "title": "TextWorld: A Learning Environment for Text-Based Games.",
                "author": "Marc-Alexandre C\u00f4t\u00e9, \u00c1kos K\u00e1d\u00e1r, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, et al. 2019.",
                "venue": "In Proceedings of the 7th Computer Games Workshop at the 27th International Conference on Artificial Intelligence, pages 41\u201375, Stockholm, Sweden. Springer International Publishing.",
                "url": "https://doi.org/10.1007/978-3-030-24337-1_3"
            }
        },
        {
            "22": {
                "title": "PyBullet, a Python module for physics simulation for games, robotics and machine learning.",
                "author": "Erwin Coumans and Yunfei Bai. 2016\u20132021.",
                "venue": "http://pybullet.org.",
                "url": null
            }
        },
        {
            "23": {
                "title": "A Survey on Multimodal Large Language Models for Autonomous Driving.",
                "author": "Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, et al. 2024.",
                "venue": "In Proceedings of the 1st Workshop on Large Language and Vision Models for Autonomous Driving at the 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 958\u2013979, Waikoloa, HI, USA. Institute of Electrical and Electronics Engineers.",
                "url": "https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Cui_A_Survey_on_Multimodal_Large_Language_Models_for_Autonomous_Driving_WACVW_2024_paper.html"
            }
        },
        {
            "24": {
                "title": "Talk2Car: Taking Control of Your Self-Driving Car.",
                "author": "Thierry Deruyttere, Simon Vandenhende, Dusan Grujicic, Luc Van Gool, and Marie-Francine Moens. 2019.",
                "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2088\u20132098, Hong Kong, China. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D19-1215"
            }
        },
        {
            "25": {
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.",
                "venue": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, MN, USA. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/N19-1423"
            }
        },
        {
            "26": {
                "title": "Chain-of-Verification Reduces Hallucination in Large Language Models.",
                "author": "Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023.",
                "venue": "arXiv preprint arXiv:2309.11495.",
                "url": "https://arxiv.org/abs/2309.11495"
            }
        },
        {
            "27": {
                "title": "HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving.",
                "author": "Xinpeng Ding, Jianhua Han, Hang Xu, Wei Zhang, and Xiaomeng Li. 2023.",
                "venue": "arXiv preprint arXiv:2309.05186.",
                "url": "https://arxiv.org/abs/2309.05186"
            }
        },
        {
            "28": {
                "title": "Holistic Autonomous Driving Understanding by Bird\u2019s-Eye-View Injected Multi-Modal Large Models.",
                "author": "Xinpeng Ding, Jinahua Han, Hang Xu, Xiaodan Liang, Wei Zhang, and Xiaomeng Li. 2024.",
                "venue": "arXiv preprint arXiv:2401.00988.",
                "url": "https://arxiv.org/abs/2401.00988"
            }
        },
        {
            "29": {
                "title": "A Survey on In-context Learning.",
                "author": "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023.",
                "venue": "arXiv preprint arXiv:2301.00234.",
                "url": "https://arxiv.org/abs/2301.00234"
            }
        },
        {
            "30": {
                "title": "CARLA: An Open Urban Driving Simulator.",
                "author": "Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. 2017.",
                "venue": "In Proceedings of the 1st Conference on Robot Learning, volume 78 of Proceedings of Machine Learning Research, pages 1\u201316, Mountain View, CA, USA. PMLR.",
                "url": "https://proceedings.mlr.press/v78/dosovitskiy17a.html"
            }
        },
        {
            "31": {
                "title": "PaLM-E: An Embodied Multimodal Language Model.",
                "author": "Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. 2023.",
                "venue": "In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 8469\u20138488, Honolulu, HI, USA. PMLR.",
                "url": "https://proceedings.mlr.press/v202/driess23a.html"
            }
        },
        {
            "32": {
                "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate.",
                "author": "Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2023.",
                "venue": "arXiv preprint arXiv:2305.14325.",
                "url": "https://arxiv.org/abs/2305.14325"
            }
        },
        {
            "33": {
                "title": "The logic of conflicts between decision making agents.",
                "author": "L Ekenberg. 2000.",
                "venue": "Journal of Logic and Computation, 10(4):583\u2013602.",
                "url": "https://doi.org/10.1093/logcom/10.4.583"
            }
        },
        {
            "34": {
                "title": "Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models.",
                "author": "Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang, Yu Wang, Shizhu Liu, Pingchuan Tian, Yuping Wang, and Yuxuan Wang. 2023.",
                "venue": "arXiv preprint arXiv:2308.11764.",
                "url": "https://arxiv.org/abs/2308.11764"
            }
        },
        {
            "35": {
                "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge.",
                "author": "Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. 2022.",
                "venue": "In Proceedings of the 2022 Conference on Neural Information Processing Systems, pages 18343\u201318362, New Orleans, LA, USA. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/74a67268c5cc5910f64938cac4526a90-Paper-Datasets_and_Benchmarks.pdf"
            }
        },
        {
            "36": {
                "title": "Model-based and data-driven model-reference control: A comparative analysis.",
                "author": "Simone Formentin, Klaske van Heusden, and Alireza Karimi. 2013.",
                "venue": "In Proceedings of the 2013 European Control Conference (ECC), pages 1410\u20131415, Zurich, Switzerland. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.23919/ECC.2013.6669388"
            }
        },
        {
            "37": {
                "title": "Beam Search Strategies for Neural Machine Translation.",
                "author": "Markus Freitag and Yaser Al-Onaizan. 2017.",
                "venue": "In Proceedings of the 1st Workshop on Neural Machine Translation, pages 56\u201360, Vancouver, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/W17-3207"
            }
        },
        {
            "38": {
                "title": "Drive Like a Human: Rethinking Autonomous Driving with Large Language Models.",
                "author": "Daocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai, Botian Shi, and Yu Qiao. 2023a.",
                "venue": "arXiv preprint arXiv:2307.07162.",
                "url": "https://arxiv.org/abs/2307.07162"
            }
        },
        {
            "39": {
                "title": "GPTScore: Evaluate as You Desire.",
                "author": "Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023b.",
                "venue": "arXiv preprint arXiv:2302.04166.",
                "url": "https://arxiv.org/abs/2302.04166"
            }
        },
        {
            "40": {
                "title": "Peer review of GPT-4 technical report and systems card.",
                "author": "Jack Gallifant, Amelia Fiske, Yulia A. Levites Strekalova, Juan S. Osorio-Valencia, Rachael Parke, Rogers Mwavu, Nicole Martinez, Judy Wawira Gichoya, Marzyeh Ghassemi, Dina Demner-Fushman, et al. 2024.",
                "venue": "PLOS Digital Health, 3(1):1\u201315.",
                "url": "https://doi.org/10.1371/journal.pdig.0000417"
            }
        },
        {
            "41": {
                "title": "Are we ready for autonomous driving? The KITTI vision benchmark suite.",
                "author": "Andreas Geiger, Philip Lenz, and Raquel Urtasun. 2012.",
                "venue": "In Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3354\u20133361, Providence, RI, USA. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/CVPR.2012.6248074"
            }
        },
        {
            "42": {
                "title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies.",
                "author": "Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021.",
                "venue": "Transactions of the Association for Computational Linguistics, 9:346\u2013361.",
                "url": "https://doi.org/10.1162/tacl_a_00370"
            }
        },
        {
            "43": {
                "title": "Rule-based systems.",
                "author": "Frederick Hayes-Roth. 1985.",
                "venue": "Communications of the ACM, 28(9):921\u2013932.",
                "url": "https://doi.org/10.1145/4284.4286"
            }
        },
        {
            "44": {
                "title": "SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge.",
                "author": "Rishi Hazra, Pedro Zuidberg Dos Martires, and Luc De Raedt. 2024.",
                "venue": "arXiv preprint arXiv:2308.12682.",
                "url": "https://arxiv.org/abs/2308.12682"
            }
        },
        {
            "45": {
                "title": "Deep Reinforcement Learning That Matters.",
                "author": "Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. 2018.",
                "venue": "In Proceedings of the 32nd AAAI Conference on Artificial Intelligence, pages 3207\u20133214, New Orleans, LA, USA. AAAI Press.",
                "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11694"
            }
        },
        {
            "46": {
                "title": "Measuring Massive Multitask Language Understanding.",
                "author": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.",
                "venue": "In Proceedings of the 9th International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=d7KBjmI3GmQ"
            }
        },
        {
            "47": {
                "title": "Teaching Machines to Read and Comprehend.",
                "author": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015.",
                "venue": "In Proceedings of the 2015 Conference on Neural Information Processing Systems, Montreal, Canada. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2015/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf"
            }
        },
        {
            "48": {
                "title": "Deploying and Evaluating LLMs to Program Service Mobile Robots.",
                "author": "Zichao Hu, Francesca Lucchetti, Claire Schlesinger, Yash Saxena, Anders Freeman, Sadanand Modak, Arjun Guha, and Joydeep Biswas. 2024.",
                "venue": "IEEE Robotics and Automation Letters, 9(3):2853\u20132860.",
                "url": "https://doi.org/10.1109/LRA.2024.3360020"
            }
        },
        {
            "49": {
                "title": "OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation.",
                "author": "Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. 2024a.",
                "venue": "arXiv preprint arXiv:2311.17911.",
                "url": "https://arxiv.org/abs/2311.17911"
            }
        },
        {
            "50": {
                "title": "Understanding the planning of LLM agents: A survey.",
                "author": "Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. 2024b.",
                "venue": "arXiv preprint arXiv:2402.02716.",
                "url": "https://arxiv.org/abs/2402.02716"
            }
        },
        {
            "51": {
                "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering.",
                "author": "Drew A. Hudson and Christopher D. Manning. 2019.",
                "venue": "In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6693\u20136702. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/CVPR.2019.00686"
            }
        },
        {
            "52": {
                "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances.",
                "author": "Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, et al. 2023.",
                "venue": "In Proceedings of The 6th Conference on Robot Learning, volume 205 of Proceedings of Machine Learning Research, pages 287\u2013318, Atlanta, GA, USA. PMLR.",
                "url": "https://proceedings.mlr.press/v205/ichter23a.html"
            }
        },
        {
            "53": {
                "title": "Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art.",
                "author": "Joel Janai, Fatma G\u00fcney, Aseem Behl, and Andreas Geiger. 2020.",
                "venue": "Foundations and Trends in Computer Graphics and Vision, 12(1-3):1\u2013308.",
                "url": "https://doi.org/10.1561/0600000079"
            }
        },
        {
            "54": {
                "title": "Counterexample Guided Inductive Synthesis Using Large Language Models and Satisfiability Solving.",
                "author": "Sumit Kumar Jha, Susmit Jha, Patrick Lincoln, Nathaniel D. Bastian, Alvaro Velasquez, Rickard Ewetz, and Sandeep Neema. 2023.",
                "venue": "In Proceedings of the 2023 IEEE Military Communications Conference (MILCOM), pages 944\u2013949, Boston, MA, USA. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/MILCOM58377.2023.10356332"
            }
        },
        {
            "55": {
                "title": "Survey of Hallucination in Natural Language Generation.",
                "author": "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023.",
                "venue": "ACM Computing Surveys, 55(12).",
                "url": "https://doi.org/10.1145/3571730"
            }
        },
        {
            "56": {
                "title": "MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports.",
                "author": "Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. 2019.",
                "venue": "Scientific Data, 6(1):317.",
                "url": "https://doi.org/10.1038/s41597-019-0322-0"
            }
        },
        {
            "57": {
                "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.",
                "author": "Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017.",
                "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601\u20131611, Vancouver, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P17-1147"
            }
        },
        {
            "58": {
                "title": "Measuring Catastrophic Forgetting in Neural Networks.",
                "author": "Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. 2018.",
                "venue": "In Proceedings of the 32nd AAAI Conference on Artificial Intelligence, pages 3390\u20133398, New Orleans, LA, USA. AAAI Press.",
                "url": "https://doi.org/10.1609/aaai.v32i1.11651"
            }
        },
        {
            "59": {
                "title": "Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving.",
                "author": "Ali Keysan, Andreas Look, Eitan Kosman, Gonca G\u00fcrsun, J\u00f6rg Wagner, Yu Yao, and Barbara Rakitsch. 2023.",
                "venue": "arXiv preprint arXiv:2309.05282.",
                "url": "https://arxiv.org/abs/2309.05282"
            }
        },
        {
            "60": {
                "title": "Textual Explanations for Self-Driving Vehicles.",
                "author": "Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata. 2018.",
                "venue": "In Proceedings of the 15th European Conference on Computer Vision (ECCV), pages 577\u2013593, Munich, Germany. Springer International Publishing.",
                "url": "https://doi.org/10.1007/978-3-030-01216-8_35"
            }
        },
        {
            "61": {
                "title": "Segment Anything.",
                "author": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, et al. 2023.",
                "venue": "In Proceedings of the 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 3992\u20134003, Paris, France. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/ICCV51070.2023.00371"
            }
        },
        {
            "62": {
                "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations.",
                "author": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. 2017.",
                "venue": "International Journal of Computer Vision, 123:32\u201373.",
                "url": "https://doi.org/10.1007/s11263-016-0981-7"
            }
        },
        {
            "63": {
                "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation.",
                "author": "Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.",
                "venue": "In Proceedings of the 11th International Conference on Learning Representations, Kigali, Rwanda.",
                "url": "https://openreview.net/forum?id=VD-AYtP0dve"
            }
        },
        {
            "64": {
                "title": "Conformal prediction with large language models for multi-choice question answering.",
                "author": "Bhawesh Kumar, Charlie Lu, Gauri Gupta, Anil Palepu, David Bellamy, Ramesh Raskar, and Andrew Beam. 2023.",
                "venue": "In Proceedings of the \u2018Neural Conversational AI Workshop - What\u2019s left to TEACH (Trustworthy, Enhanced, Adaptable, Capable and Human-centric) chatbots?\u2019 at the 40th International Conference on Machine Learning, Honolulu, HI, USA.",
                "url": "https://arxiv.org/abs/2305.18404"
            }
        },
        {
            "65": {
                "title": "Natural Questions: A Benchmark for Question Answering Research.",
                "author": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019.",
                "venue": "Transactions of the Association for Computational Linguistics, 7:452\u2013466.",
                "url": "https://doi.org/10.1162/tacl_a_00276"
            }
        },
        {
            "66": {
                "title": "Reward Design with Language Models.",
                "author": "Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. 2023.",
                "venue": "In Proceedings of the The 11th International Conference on Learning Representations, Kigali, Rwanda.",
                "url": "https://openreview.net/forum?id=10uNUgI5Kl"
            }
        },
        {
            "67": {
                "title": "SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization.",
                "author": "Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2022.",
                "venue": "Transactions of the Association for Computational Linguistics, 10:163\u2013177.",
                "url": "https://doi.org/10.1162/tacl_a_00453"
            }
        },
        {
            "68": {
                "title": "An environment for autonomous driving decision-making.",
                "author": "Edouard Leurent. 2018.",
                "venue": "https://github.com/eleurent/highway-env.",
                "url": null
            }
        },
        {
            "69": {
                "title": "Large Language Models with Controllable Working Memory.",
                "author": "Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar. 2023a.",
                "venue": "In Findings of the 61st Annual Meeting of the Association for Computational Linguistics, pages 1774\u20131793, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-acl.112"
            }
        },
        {
            "70": {
                "title": "MultiSpanQA: A Dataset for Multi-Span Question Answering.",
                "author": "Haonan Li, Martin Tomko, Maria Vasardani, and Timothy Baldwin. 2022.",
                "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1250\u20131260, Seattle, WA, USA. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.naacl-main.90"
            }
        },
        {
            "71": {
                "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models.",
                "author": "Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023b.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6449\u20136464, Singapore. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.emnlp-main.397"
            }
        },
        {
            "72": {
                "title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources.",
                "author": "Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing. 2024.",
                "venue": "In The 12th International Conference on Learning Representations, Vienna, Austria.",
                "url": "https://openreview.net/forum?id=cPgh4gWZlz"
            }
        },
        {
            "73": {
                "title": "Evaluating Object Hallucination in Large Vision-Language Models.",
                "author": "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. 2023c.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 292\u2013305, Singapore. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.emnlp-main.20"
            }
        },
        {
            "74": {
                "title": "Code as Policies: Language Model Programs for Embodied Control.",
                "author": "Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. 2023.",
                "venue": "In Proceedings of the 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 9493\u20139500, London, England. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/ICRA48891.2023.10160591"
            }
        },
        {
            "75": {
                "title": "Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty.",
                "author": "Kaiqu Liang, Zixu Zhang, and Jaime Fern\u00e1ndez Fisac. 2024.",
                "venue": "arXiv preprint arXiv:2402.06529.",
                "url": "https://arxiv.org/abs/2402.06529"
            }
        },
        {
            "76": {
                "title": "Teaching models to express their uncertainty in words.",
                "author": "Stephanie Lin, Jacob Hilton, and Owain Evans. 2022a.",
                "venue": "Transactions on Machine Learning Research.",
                "url": "https://openreview.net/forum?id=8s8K2UZGTZ"
            }
        },
        {
            "77": {
                "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods.",
                "author": "Stephanie Lin, Jacob Hilton, and Owain Evans. 2022b.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214\u20133252, Dublin, Ireland. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.acl-long.229"
            }
        },
        {
            "78": {
                "title": "Microsoft COCO: Common Objects in Context.",
                "author": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. 2014.",
                "venue": "In Proceedings of the 13th European Conference on Computer Vision (ECCV), pages 740\u2013755, Zurich, Switzerland. Springer International Publishing.",
                "url": "https://doi.org/10.1007/978-3-319-10602-1_48"
            }
        },
        {
            "79": {
                "title": "MTD-GPT: A Multi-Task Decision-Making GPT Model for Autonomous Driving at Unsignalized Intersections.",
                "author": "Jiaqi Liu, Peng Hang, Xiao Qi, Jianqiang Wang, and Jian Sun. 2023.",
                "venue": "In Proceedings of the 2023 IEEE International Conference on Intelligent Transportation Systems (ITSC), pages 5154\u20135161, Bilbao, Spain. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/ITSC57777.2023.10421993"
            }
        },
        {
            "80": {
                "title": "Microscopic Traffic Simulation using SUMO.",
                "author": "Pablo Alvarez Lopez, Michael Behrisch, Laura Bieker-Walz, Jakob Erdmann, Yun-Pang Fl\u00f6tter\u00f6d, Robert Hilbrich, Leonhard L\u00fccken, Johannes Rummel, Peter Wagner, and Evamarie Wiessner. 2018.",
                "venue": "In Proceedings of the 2018 International Conference on Intelligent Transportation Systems (ITSC), pages 2575\u20132582, Maui, HI, USA. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/ITSC.2018.8569938"
            }
        },
        {
            "81": {
                "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering.",
                "author": "Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022.",
                "venue": "In Proceedings of the 2022 Conference on Neural Information Processing Systems, pages 2507\u20132521, New Orleans, LA, USA. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf"
            }
        },
        {
            "82": {
                "title": "Interactive Language: Talking to Robots in Real Time.",
                "author": "Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. 2023.",
                "venue": "IEEE Robotics and Automation Letters.",
                "url": "https://doi.org/10.1109/LRA.2023.3295255"
            }
        },
        {
            "83": {
                "title": "QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations.",
                "author": "Chaitanya Malaviya, Peter Shaw, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14032\u201314047, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-long.784"
            }
        },
        {
            "84": {
                "title": "DRAMA: Joint Risk Localization and Captioning in Driving.",
                "author": "Srikanth Malla, Chiho Choi, Isht Dwivedi, Joon Hee Choi, and Jiachen Li. 2023.",
                "venue": "In Proceedings of the 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 1043\u20131052, Waikola, HI, USA. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/WACV56688.2023.00110"
            }
        },
        {
            "85": {
                "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models.",
                "author": "Potsawee Manakul, Adian Liusie, and Mark Gales. 2023.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9004\u20139017, Singapore. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.emnlp-main.557"
            }
        },
        {
            "86": {
                "title": "A Language Agent for Autonomous Driving.",
                "author": "Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone, and Yue Wang. 2023.",
                "venue": "arXiv preprint arXiv:2311.10813.",
                "url": "https://arxiv.org/abs/2311.10813"
            }
        },
        {
            "87": {
                "title": "FLIRT: Feedback Loop In-context Red Teaming.",
                "author": "Ninareh Mehrabi, Palash Goyal, Christophe Dupuy, Qian Hu, Shalini Ghosh, Richard Zemel, Kai-Wei Chang, Aram Galstyan, and Rahul Gupta. 2023.",
                "venue": "arXiv preprint arXiv:2308.04265.",
                "url": "https://arxiv.org/abs/2308.04265"
            }
        },
        {
            "88": {
                "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation.",
                "author": "Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12076\u201312100, Singapore. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.emnlp-main.741"
            }
        },
        {
            "89": {
                "title": "Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation.",
                "author": "Niels M\u00fcndler, Jingxuan He, Slobodan Jenko, and Martin Vechev. 2024.",
                "venue": "In Proceedings of the 12th International Conference on Learning Representations, Vienna, Austria.",
                "url": "https://openreview.net/forum?id=EmQSOi1X2f"
            }
        },
        {
            "90": {
                "title": "Introducing ChatGPT.",
                "author": "OpenAI. 2022.",
                "venue": "OpenAI blog.",
                "url": "https://openai.com/blog/chatgpt"
            }
        },
        {
            "91": {
                "title": "DINOv2: Learning Robust Visual Features without Supervision.",
                "author": "Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, et al. 2024.",
                "venue": "Transactions on Machine Learning Research.",
                "url": "https://openreview.net/forum?id=a68SUt6zFt"
            }
        },
        {
            "92": {
                "title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions.",
                "author": "Lorenzo Pacchiardi, Alex James Chan, S\u00f6ren Mindermann, Ilan Moscovitz, Alexa Yue Pan, Yarin Gal, Owain Evans, and Jan M. Brauner. 2024.",
                "venue": "In Proceedings of the 12th International Conference on Learning Representations, Vienna, Austria.",
                "url": "https://openreview.net/forum?id=567BjxgaTp"
            }
        },
        {
            "93": {
                "title": "CLARA: Classifying and Disambiguating User Commands for Reliable Interactive Robotic Agents.",
                "author": "Jeongeun Park, Seungwon Lim, Joonhyung Lee, Sangbeom Park, Minsuk Chang, Youngjae Yu, and Sungjoon Choi. 2024.",
                "venue": "IEEE Robotics and Automation Letters, 9(2):1059\u20131066.",
                "url": "https://doi.org/10.1109/LRA.2023.3338514"
            }
        },
        {
            "94": {
                "title": "Generative Agents: Interactive Simulacra of Human Behavior.",
                "author": "Joon Sung Park, Joseph O\u2019Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023.",
                "venue": "In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/3586183.3606763"
            }
        },
        {
            "95": {
                "title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback.",
                "author": "Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023.",
                "venue": "arXiv preprint arXiv:2302.12813.",
                "url": "https://arxiv.org/abs/2302.12813"
            }
        },
        {
            "96": {
                "title": "Virtualhome: Simulating household activities via programs.",
                "author": "Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. 2018.",
                "venue": "In Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8494\u20138502, Salt Lake City, UT, USA. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/CVPR.2018.00886"
            }
        },
        {
            "97": {
                "title": "A Moral Imperative: The Need for Continual Superalignment of Large Language Models.",
                "author": "Gokul Puthumanaillam, Manav Vora, Pranay Thangeda, and Melkior Ornik. 2024.",
                "venue": "arXiv preprint arXiv:2403.14683.",
                "url": "https://arxiv.org/abs/2403.14683"
            }
        },
        {
            "98": {
                "title": "NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario.",
                "author": "Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, and Yu-Gang Jiang. 2024.",
                "venue": "arXiv preprint arXiv:2305.14836.",
                "url": "https://arxiv.org/abs/2305.14836"
            }
        },
        {
            "99": {
                "title": "Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models.",
                "author": "Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, and Zhenzhong Lan. 2023.",
                "venue": "arXiv preprint arXiv:2307.08487.",
                "url": "https://arxiv.org/abs/2307.08487"
            }
        },
        {
            "100": {
                "title": "Conformal Language Modeling.",
                "author": "Victor Quach, Adam Fisch, Tal Schuster, Adam Yala, Jae Ho Sohn, Tommi S. Jaakkola, and Regina Barzilay. 2024.",
                "venue": "In Proceedings of the 12th International Conference on Learning Representations, Vienna, Austria.",
                "url": "https://openreview.net/forum?id=pzUhfQ74c5"
            }
        },
        {
            "101": {
                "title": "Learning Transferable Visual Models From Natural Language Supervision.",
                "author": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021.",
                "venue": "In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748\u20138763. PMLR.",
                "url": "https://proceedings.mlr.press/v139/radford21a.html"
            }
        },
        {
            "102": {
                "title": "Language models are unsupervised multitask learners.",
                "author": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.",
                "venue": "OpenAI blog.",
                "url": "https://openai.com/research/better-language-models"
            }
        },
        {
            "103": {
                "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text.",
                "author": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.",
                "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin, TX, USA. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D16-1264"
            }
        },
        {
            "104": {
                "title": "A Survey of Hallucination in Large Foundation Models.",
                "author": "Vipula Rawte, Amit Sheth, and Amitava Das. 2023.",
                "venue": "arXiv preprint arXiv:2309.05922.",
                "url": "https://arxiv.org/abs/2309.05922"
            }
        },
        {
            "105": {
                "title": "Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners.",
                "author": "Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et al. 2023.",
                "venue": "In Proceedings of The 7th Conference on Robot Learning, volume 229 of Proceedings of Machine Learning Research, pages 661\u2013682, Atlanta, GA, USA. PMLR.",
                "url": "https://proceedings.mlr.press/v229/ren23a.html"
            }
        },
        {
            "106": {
                "title": "Object Hallucination in Image Captioning.",
                "author": "Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018.",
                "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4035\u20134045, Brussels, Belgium. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D18-1437"
            }
        },
        {
            "107": {
                "title": "Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models.",
                "author": "Patrick Schramowski, Manuel Brack, Bj\u00f6rn Deiseroth, and Kristian Kersting. 2023.",
                "venue": "In Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 22522\u201322531, Vancouver, Canada. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/CVPR52729.2023.02157"
            }
        },
        {
            "108": {
                "title": "An Attentional Recurrent Neural Network for Occlusion-Aware Proactive Anomaly Detection in Field Robot Navigation.",
                "author": "Andre Schreiber, Tianchen Ji, D. Livingston McPherson, and Katherine Driggs-Campbell. 2023.",
                "venue": "In Proceedings of the 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 8038\u20138045, Detroit, MI, USA. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/IROS55552.2023.10341852"
            }
        },
        {
            "109": {
                "title": "A-OKVQA: A Benchmark for Visual Question Answering Using World Knowledge.",
                "author": "Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022.",
                "venue": "In Proceedings of the 17th European Conference on Computer Vision (ECCV), pages 146\u2013162, Tel Aviv, Israel. Springer Nature Switzerland.",
                "url": "https://doi.org/10.1007/978-3-031-20074-8_9"
            }
        },
        {
            "110": {
                "title": "A Tutorial on Conformal Prediction.",
                "author": "Glenn Shafer and Vladimir Vovk. 2008.",
                "venue": "Journal of Machine Learning Research, 9(12):371\u2013421.",
                "url": "http://jmlr.org/papers/v9/shafer08a.html"
            }
        },
        {
            "111": {
                "title": "LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action.",
                "author": "Dhruv Shah, B\u0142a\u017cej Osi\u0144ski, Brian Ichter, and Sergey Levine. 2023.",
                "venue": "In Proceedings of The 6th Conference on Robot Learning, volume 205 of Proceedings of Machine Learning Research, pages 492\u2013504, Atlanta, GA, USA. PMLR.",
                "url": "https://proceedings.mlr.press/v205/shah23b.html"
            }
        },
        {
            "112": {
                "title": "Reflexion: language agents with verbal reinforcement learning.",
                "author": "Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023.",
                "venue": "In Proceedings of the 2023 Conference on Neural Information Processing Systems, pages 8634\u20138652, New Orleans, LA, USA. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/1b44b878bb782e6954cd888628510e90-Paper-Conference.pdf"
            }
        },
        {
            "113": {
                "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks.",
                "author": "Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020.",
                "venue": "In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10737\u201310746. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/CVPR42600.2020.01075"
            }
        },
        {
            "114": {
                "title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning.",
                "author": "Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2021.",
                "venue": "In Proceedings of the 9th International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=0IOX0YcCdTn"
            }
        },
        {
            "115": {
                "title": "DriveLM: Driving with Graph Visual Question Answering.",
                "author": "Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Ping Luo, Andreas Geiger, and Hongyang Li. 2023.",
                "venue": "arXiv preprint arXiv:2312.14150.",
                "url": "https://arxiv.org/abs/2312.14150"
            }
        },
        {
            "116": {
                "title": "A framework for smart control using machine-learning modeling for processes with closed-loop control in Industry 4.0.",
                "author": "Gonen Singer and Yuval Cohen. 2021.",
                "venue": "Engineering Applications of Artificial Intelligence, 102:104236.",
                "url": "https://doi.org/https://doi.org/10.1016/j.engappai.2021.104236"
            }
        },
        {
            "117": {
                "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models.",
                "author": "Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023.",
                "venue": "In Proceedings of the 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 11523\u201311530, London, England. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/ICRA48891.2023.10161317"
            }
        },
        {
            "118": {
                "title": "LUNA: A Model-Based Universal Analysis Framework for Large Language Models.",
                "author": "Da Song, Xuan Xie, Jiayang Song, Derui Zhu, Yuheng Huang, Felix Juefei-Xu, and Lei Ma. 2023.",
                "venue": "arXiv preprint arXiv:2310.14211.",
                "url": "https://arxiv.org/abs/2310.14211"
            }
        },
        {
            "119": {
                "title": "Artificial intelligence, machine learning and deep learning in advanced robotics, a review.",
                "author": "Mohsen Soori, Behrooz Arezoo, and Roza Dastres. 2023.",
                "venue": "Cognitive Robotics, 3:54\u201370.",
                "url": "https://doi.org/https://doi.org/10.1016/j.cogr.2023.04.001"
            }
        },
        {
            "120": {
                "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models.",
                "author": "Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. 2023.",
                "venue": "Transactions on Machine Learning Research.",
                "url": "https://openreview.net/forum?id=uyTL5Bvosj"
            }
        },
        {
            "121": {
                "title": "Current Status and Issues of Traffic Light Recognition Technology in Autonomous Driving System.",
                "author": "Naoki Suganuma and Keisuke Yoneda. 2022.",
                "venue": "IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences, E105.A(5):763\u2013769.",
                "url": "https://doi.org/10.1587/transfun.2021WBI0002"
            }
        },
        {
            "122": {
                "title": "FEVER: a Large-scale Dataset for Fact Extraction and VERification.",
                "author": "James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018.",
                "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809\u2013819, New Orleans, Louisiana. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/N18-1074"
            }
        },
        {
            "123": {
                "title": "LLaMA: Open and Efficient Foundation Language Models.",
                "author": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a.",
                "venue": "arXiv preprint arXiv:2302.13971.",
                "url": "https://arxiv.org/abs/2302.13971"
            }
        },
        {
            "124": {
                "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b.",
                "venue": "arXiv preprint arXiv:2307.09288.",
                "url": "https://arxiv.org/abs/2307.09288"
            }
        },
        {
            "125": {
                "title": "LC-QuAD: A Corpus for Complex Question Answering over Knowledge Graphs.",
                "author": "Priyansh Trivedi, Gaurav Maheshwari, Mohnish Dubey, and Jens Lehmann. 2017.",
                "venue": "In Proceedings of the 2017 International Semantic Web Conference, pages 210\u2013218, Vienna, Austria. Springer International Publishing.",
                "url": "https://doi.org/10.1007/978-3-319-68204-4_22"
            }
        },
        {
            "126": {
                "title": "A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation.",
                "author": "Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023.",
                "venue": "arXiv preprint arXiv:2307.03987.",
                "url": "https://arxiv.org/abs/2307.03987"
            }
        },
        {
            "127": {
                "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.",
                "author": "Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. 2023a.",
                "venue": "In Proceedings of the 37th Conference on Neural Information Processing Systems Datasets and Benchmarks Track, New Orleans, LA, USA.",
                "url": "https://openreview.net/forum?id=kaHpo8OZw2"
            }
        },
        {
            "128": {
                "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models.",
                "author": "Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023b.",
                "venue": "arXiv preprint arXiv:2305.16291.",
                "url": "https://arxiv.org/abs/2305.16291"
            }
        },
        {
            "129": {
                "title": "Conformal Temporal Logic Planning using Large Language Models.",
                "author": "Jun Wang, Jiaming Tong, Kaiyuan Tan, Yevgeniy Vorobeychik, and Yiannis Kantaros. 2024.",
                "venue": "arXiv preprint arXiv:2309.10092.",
                "url": "https://arxiv.org/abs/2309.10092"
            }
        },
        {
            "130": {
                "title": "TraCI: an interface for coupling road traffic and network simulators.",
                "author": "Axel Wegener, Micha\u0142 Pi\u00f3rkowski, Maxim Raya, Horst Hellbr\u00fcck, Stefan Fischer, and Jean-Pierre Hubaux. 2008.",
                "venue": "In Proceedings of the 11th Communications and Networking Simulation Symposium, page 155\u2013163, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/1400713.1400740"
            }
        },
        {
            "131": {
                "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
                "author": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022.",
                "venue": "In Proceedings of the 2022 Conference on Neural Information Processing Systems, pages 24824\u201324837, New Orleans, LA, USA. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf"
            }
        },
        {
            "132": {
                "title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models.",
                "author": "Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao MA, Pinlong Cai, Min Dou, Botian Shi, Liang He, and Yu Qiao. 2024.",
                "venue": "In Proceedings of the 12th International Conference on Learning Representations, Vienna, Austria.",
                "url": "https://openreview.net/forum?id=OqTMUPuLuC"
            }
        },
        {
            "133": {
                "title": "On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving.",
                "author": "Licheng Wen, Xuemeng Yang, Daocheng Fu, Xiaofeng Wang, Pinlong Cai, Xin Li, Tao Ma, Yingxuan Li, Linran Xu, Dengke Shang, et al. 2023.",
                "venue": "arXiv preprint arXiv:2311.05332.",
                "url": "https://arxiv.org/abs/2311.05332"
            }
        },
        {
            "134": {
                "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning.",
                "author": "Ronald J. Williams. 1992.",
                "venue": "Machine Learning, 8(3\u20134):229\u2013256.",
                "url": "https://doi.org/10.1007/BF00992696"
            }
        },
        {
            "135": {
                "title": "Referring Multi-Object Tracking.",
                "author": "Dongming Wu, Wencheng Han, Tiancai Wang, Xingping Dong, Xiangyu Zhang, and Jianbing Shen. 2023a.",
                "venue": "In Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14633\u201314642, Vancouver, Canada. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/CVPR52729.2023.01406"
            }
        },
        {
            "136": {
                "title": "Language Prompt for Autonomous Driving.",
                "author": "Dongming Wu, Wencheng Han, Tiancai Wang, Yingfei Liu, Xiangyu Zhang, and Jianbing Shen. 2023b.",
                "venue": "arXiv preprint arXiv:2309.04379.",
                "url": "https://arxiv.org/abs/2309.04379"
            }
        },
        {
            "137": {
                "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs.",
                "author": "Miao Xiong, Zhiyuan Hu, Xinyang Lu, YIFEI LI, Jie Fu, Junxian He, and Bryan Hooi. 2024.",
                "venue": "In Proceedings of the 12th International Conference on Learning Representations, Vienna, Austria.",
                "url": "https://openreview.net/forum?id=gjeQKFxFpZ"
            }
        },
        {
            "138": {
                "title": "DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model.",
                "author": "Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee. K. Wong, Zhenguo Li, and Hengshuang Zhao. 2024.",
                "venue": "arXiv preprint arXiv:2310.01412.",
                "url": "https://arxiv.org/abs/2310.01412"
            }
        },
        {
            "139": {
                "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.",
                "author": "Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Shaochen Zhong, Bing Yin, and Xia Hu. 2024.",
                "venue": "ACM Transactions on Knowledge Discovery from Data.",
                "url": "https://doi.org/10.1145/3649506"
            }
        },
        {
            "140": {
                "title": "Alignment for Honesty.",
                "author": "Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. 2023a.",
                "venue": "arXiv preprint arXiv:2312.07000.",
                "url": "https://arxiv.org/abs/2312.07000"
            }
        },
        {
            "141": {
                "title": "LLM4Drive: A Survey of Large Language Models for Autonomous Driving.",
                "author": "Zhenjie Yang, Xiaosong Jia, Hongyang Li, and Junchi Yan. 2023b.",
                "venue": "arXiv preprint arXiv:2311.01043.",
                "url": "https://arxiv.org/abs/2311.01043"
            }
        },
        {
            "142": {
                "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.",
                "author": "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018.",
                "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369\u20132380, Brussels, Belgium. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D18-1259"
            }
        },
        {
            "143": {
                "title": "LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples.",
                "author": "Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li Yuan. 2023a.",
                "venue": "arXiv preprint arXiv:2310.01469.",
                "url": "https://arxiv.org/abs/2310.01469"
            }
        },
        {
            "144": {
                "title": "ReAct: Synergizing Reasoning and Acting in Language Models.",
                "author": "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023b.",
                "venue": "In Proceedings of the 11th International Conference on Learning Representations, Kigali, Rwanda.",
                "url": "https://openreview.net/forum?id=WE_vluYUL-X"
            }
        },
        {
            "145": {
                "title": "Cognitive Mirage: A Review of Hallucinations in Large Language Models.",
                "author": "Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. 2023.",
                "venue": "arXiv preprint arXiv:2309.06794.",
                "url": "https://arxiv.org/abs/2309.06794"
            }
        },
        {
            "146": {
                "title": "Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks.",
                "author": "Xiaodong Yu, Hao Cheng, Xiaodong Liu, Dan Roth, and Jianfeng Gao. 2023.",
                "venue": "arXiv preprint arXiv:2310.12516.",
                "url": "https://arxiv.org/abs/2310.12516"
            }
        },
        {
            "147": {
                "title": "BARTScore: Evaluating Generated Text as Text Generation.",
                "author": "Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.",
                "venue": "In Proceedings of the 2021 Conference on Neural Information Processing Systems, pages 27263\u201327277. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf"
            }
        },
        {
            "148": {
                "title": "Transporter Networks: Rearranging the Visual World for Robotic Manipulation.",
                "author": "Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, and Johnny Lee. 2021.",
                "venue": "In Proceedings of the 4th Conference on Robot Learning, volume 155 of Proceedings of Machine Learning Research, pages 726\u2013747. PMLR.",
                "url": "https://proceedings.mlr.press/v155/zeng21a.html"
            }
        },
        {
            "149": {
                "title": "Large Language Models for Robotics: A Survey.",
                "author": "Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, and Philip S. Yu. 2023.",
                "venue": "arXiv preprint arXiv:2311.07226.",
                "url": "https://arxiv.org/abs/2311.07226"
            }
        },
        {
            "150": {
                "title": "AlignScore: Evaluating Factual Consistency with A Unified Alignment Function.",
                "author": "Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11328\u201311348, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-long.634"
            }
        },
        {
            "151": {
                "title": "Large language models for human-robot interaction: A review.",
                "author": "Ceng Zhang, Junxin Chen, Jiatong Li, Yanhong Peng, and Zebing Mao. 2023a.",
                "venue": "Biomimetic Intelligence and Robotics, 3(4):100131.",
                "url": "https://doi.org/https://doi.org/10.1016/j.birob.2023.100131"
            }
        },
        {
            "152": {
                "title": "How Language Model Hallucinations Can Snowball.",
                "author": "Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A. Smith. 2023b.",
                "venue": "arXiv preprint arXiv:2305.13534.",
                "url": "https://arxiv.org/abs/2305.13534"
            }
        },
        {
            "153": {
                "title": "The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models.",
                "author": "Shuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang Wang. 2023c.",
                "venue": "arXiv preprint arXiv:2305.13669.",
                "url": "https://arxiv.org/abs/2305.13669"
            }
        },
        {
            "154": {
                "title": "BERTScore: Evaluating Text Generation with BERT.",
                "author": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020.",
                "venue": "In Proceedings of the 8th International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=SkeHuCVFDr"
            }
        },
        {
            "155": {
                "title": "Siren\u2019s Song in the AI Ocean: A Survey on Hallucination in Large Language Models.",
                "author": "Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023d.",
                "venue": "arXiv preprint arXiv:2309.01219.",
                "url": "https://arxiv.org/abs/2309.01219"
            }
        },
        {
            "156": {
                "title": "A Survey of Large Language Models.",
                "author": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023.",
                "venue": "arXiv preprint arXiv:2303.18223.",
                "url": "https://arxiv.org/abs/2303.18223"
            }
        },
        {
            "157": {
                "title": "Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization.",
                "author": "Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. 2024.",
                "venue": "arXiv preprint arXiv:2311.16839.",
                "url": "https://arxiv.org/abs/2311.16839"
            }
        },
        {
            "158": {
                "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT.",
                "author": "Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, et al. 2023.",
                "venue": "arXiv preprint arXiv:2302.09419.",
                "url": "https://arxiv.org/abs/2302.09419"
            }
        },
        {
            "159": {
                "title": "Analyzing and Mitigating Object Hallucination in Large Vision-Language Models.",
                "author": "Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. 2024.",
                "venue": "In Proceedings of the 12th International Conference on Learning Representations, Vienna, Austria.",
                "url": "https://openreview.net/forum?id=oZDJKTlOUe"
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.16527v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.3",
            "4.3.1",
            "4.3.2",
            "4.3.3",
            "4.3.4",
            "4.3.5"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.1.1",
            "5.1.2",
            "5.2",
            "5.2.1",
            "5.2.2",
            "5.2.3",
            "5.3",
            "5.3.1",
            "5.3.2",
            "5.3.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2.1",
            "2.2",
            "2.3",
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "4.2.1",
            "4.2.2",
            "4.3.1",
            "4.3.2",
            "4.3.3",
            "4.3.4",
            "4.3.5"
        ]
    },
    "research_context": {
        "paper_id": "2403.16527v1",
        "paper_title": "Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art",
        "research_background": "### Paper's Motivation\nThe motivation stems from the recent success and application of foundation models, including large (visual) language models (L(V)LMs), in various automated decision-making tasks. Despite their significant potential, these models have a known shortcoming: they can hallucinate, meaning they occasionally generate decisions or rationales that are plausible-sounding but factually incorrect or could lead to undesirable outcomes. Given that these models are increasingly being considered for safety-critical applications in fields such as robotics and autonomous vehicles, there is a pressing need for methods to detect and mitigate such hallucinations to ensure more trustworthy and reliable systems.\n\n### Research Problem\nThe research problem addressed in this paper is the detection and mitigation of hallucinations in foundation models, specifically within the context of decision-making tasks. While there has been some focus on hallucination detection in other domains like question-answering (QA) and object detection, there is a gap in summarizing state-of-the-art methods for decision-making and planning. This paper aims to propose a flexible definition of hallucinations that can be adapted to various deployment settings and to review the current techniques for detecting and mitigating such hallucinations in decision-making contexts.\n\n### Relevant Prior Work\n1. **Classical vs. Learning-Based Models**: The paper acknowledges the two broad categories of approaches in machine learning and robotics: hand-engineered model-based systems and data-driven learning-based models (Formentin et al., 2013). Both have their strengths but often fail in out-of-distribution scenarios (Wen et al., 2023).\n\n2. **Deployment Challenges**: Despite fine-tuning and modifications, models may struggle with unforeseen situations, leading to sub-optimal performance or even critical failures (Singer and Cohen, 2021; Schreiber et al., 2023; Chakraborty et al., 2023). Moreover, changes may have unintended side effects, such as conflicting rules or catastrophic forgetting (Kemker et al., 2018).\n\n3. **Foundation Models for Knowledge**: There is an emerging interest in using large (visual) language models to fill knowledge gaps and adapt in unfamiliar circumstances (Cui et al., 2024). These models have shown the ability to generalize to untrained tasks and justify their decisions, making them suitable for high-level tasks such as simulated decision-making (Huang et al., 2024b) and real-world robotics (Zeng et al., 2023).\n\n4. **Hallucination Detection and Mitigation**: Previous efforts have mainly focused on QA (Ji et al., 2023; Rawte et al., 2023; Zhang et al., 2023d; Ye et al., 2023) or object detection tasks (Li et al., 2023c). Wang et al. (2023a) analyzed the trustworthiness of various foundation models, and Chen & Shu (2024) proposed a taxonomy of hallucinations in LLMs but did not focus on decision-making tasks. \n\nThese works highlight the current capabilities and limitations of foundation models and the need for focused research on the specific issue of hallucination in decision-making contexts.",
        "methodology": "#### Methodology: \nThe proposed methodology for detecting and mitigating hallucinations in foundation models is classified into three distinct types based on the available inputs to the algorithm: white-box, grey-box, and black-box methods. \n\n1. **White-box Methods**:\n    - **Inputs Required**: Predicted sequences of tokens, corresponding probabilities of each token, and embeddings from intermediate layers in the network.\n    - **Application**: These methods are highly informative because they utilize comprehensive information from the model's internal states and outputs.\n\n2. **Grey-box Methods**:\n    - **Inputs Required**: Predicted sequences of tokens and their corresponding probabilities.\n    - **Application**: Grey-box methods strike a balance, providing more information than black-box methods but requiring less access than white-box methods. \n\n3. **Black-box Methods**:\n    - **Inputs Required**: Only the predicted sequence of tokens.\n    - **Application**: These methods are the most flexible, suitable for situations where the model does not grant access to its hidden states or output probability distributions, such as using interfaces like the ChatGPT web interface.\n\n**Context and Application**:\n- Foundation models typically output a sequence of predicted tokens, the associated probabilities for each token, and embeddings from intermediate layers. However, not all models provide full access to these outputs, necessitating different approaches to hallucination detection and mitigation.\n- The methods are evaluated primarily within QA and object detection settings, but there is a noted need for further validation within decision-making tasks.\n\n**Discussion and Review**:\n- Existing approaches within these classifications are discussed and summarized, highlighting their potential and identifying gaps particularly related to decision-making tasks.\n- The summarized works and their evaluations can be found in Table 2 of the referenced paper.\n\nThis detailed classification and discussion underscore the importance of matching the hallucination detection method with the level of access available to the model's outputs, offering a versatile framework for addressing hallucinations across various applications and interface constraints.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Datasets:\n\n1. **BDD-X**: \n   - Multi-modal driving dataset with video clips, vehicle control actions, and text annotations.\n   - Augmented to a QA dataset with questions on vehicle actions, reasoning, and predictions.\n   - Further augmented with generated questions from ChatGPT, creating the DriveGPT4 dataset.\n   \n2. **nuScenes**:\n   - Driving videos containing multi-view RGB camera views, GPS, bounding boxes, and semantic map representations.\n   - NuScenes-QA: A visual QA dataset derived from nuScenes focusing on object existence, counting, action states, and comparisons.\n   - Talk2Car: Refers to objects in the scene using natural language commands.\n   - NuPrompt: Extends NuScenes with RMOT annotations.\n   - DRAMA: Benchmark for identifying risky objects with suggested vehicle actions (extension dataset not public).\n\n3. **KITTI**: \n   - Augmented with RMOT labeled bounding boxes in text-referenced images.\n\n4. **SaGC**: \n   - Annotated robot tasks from cooking, cleaning, and massage, including text goals and uncertainty labels.\n\n5. **Other QA Datasets**:\n   - **HotPotQA**: Multi-hop questions from Wikipedia.\n   - **FEVER**: Verifies support/refute claims on Wikipedia.\n   - **Natural Questions**: Google search queries with Wikipedia-based answers.\n   - **StrategyQA**: Implicit multi-hop questions.\n   - **QreCC**: Conversational QA dataset with dialogues focusing on comprehension, retrieval, and rewriting.\n   - **Zhao et al. (2024)**: Multi-model visual QA dataset contrasting hallucinated and non-hallucinated descriptions.\n\n6. **Simulator-based Datasets**:\n   - **CARLA**: 3D driving simulator for perception, planning, control, and QA tasks.\n   - **ProgPrompt**: High-fidelity 3D robot tasks.\n   - **RoboEval**: Platform to check the correctness of robot task-generated code.\n\n#### Baselines:\n\n1. **BERTScore**: \n   - Computes cosine similarity of BERT embeddings between pairs of sentences.\n2. **BARTScore**:\n   - Uses a pre-trained BART model to sum log probabilities of generated tokens based on prior context.\n3. **SummaC**:\n   - Evaluates entailment, contradiction, and neutral scores between document sentences and summaries.\n4. **GPTScore**:\n   - Uses GPT series LLMs to estimate the quality of outputs via token probabilities.\n5. **AlignScore**:\n   - Classifies text alignment based on labeled data, predicting aligned/not aligned, neutrality, and continuous regression scores.\n6. **Uncertainty Measurement**:\n   - Approaches include entropy and sentence clustering based on semantic similarity to better compute uncertainty.\n\n#### Evaluation Metrics:\n\n- BERTScore, BARTScore, GPTScore, and SummaC scores for text quality and similarity assessment.\n- AlignScore: Weighted score across binary, multi-class, and regression heads.\n- Uncertainty-based methods: Entropy and clustering summing entropies within semantic classes.\n- CHAIR: Ratio of hallucinated to total mentioned objects in image descriptions.\n- POPE: Stability in object existence questioning across different outputs.\n\n#### Main Results:\n\n1. Hallucination detection methods utilizing BERTScore, BARTScore, GPTScore, and SummaC generally showed a strong capability in distinguishing between genuine and hallucinated responses in text generation tasks.\n2. AlignScore was notable for successfully rating text alignment by incorporating both binary and continuous scores.\n3. Uncertainty metrics based on entropy carried limitations when addressing semantically similar sentences with different entropies. The proposed clustering method improved representational accuracy of uncertainties.\n4. POPE demonstrated improved stability over CHAIR by focusing on binary presence/absence questions which reduced score variability across semantically similar descriptions.\n5. The newly developed multi-modal and QA datasets, such as DriveGPT4 and NuScenes-QA, facilitated more comprehensive evaluations across various dimensions, including perception, planning, and control tasks.\n6. Simulation platforms like CARLA, ProgPrompt, and RoboEval efficiently replicated real-world scenarios, validating the robustness of hallucination detection algorithms in controlled but realistic environments."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To test the effectiveness of the DiLu framework in producing high-level decisions for autonomous driving using a driving simulator environment.",
            "experiment_process": "The DiLu framework consists of reasoning, reflection, and memory modules supporting an LLM. The reasoning module views the vehicle's current observation, queries the memory module for similar past situations, and converts the experience into a prompt for input to the LLM. The prompt elicits chain-of-thought reasoning to improve model accuracy. The LLM's generated text output is summarized by the reflection module and used to update the memory bank. A separate decision decoder converts the summary into a discrete high-level decision to be executed in the simulator.",
            "result_discussion": "The researchers found that the DiLu framework could produce high-level decisions, recognizing safety-critical scenarios and suggesting more conservative driving actions. However, the framework had difficulty detecting traffic light states, a limitation similar to other vision models.",
            "ablation_id": "2403.16527v1.No1"
        },
        {
            "research_objective": "To evaluate the effectiveness of the self-reflection approach in reducing hallucinated outputs from the Agent-Driver model during autonomous driving tasks.",
            "experiment_process": "The Agent-Driver model utilizes a tool library that communicates with neural modules for object detection, trajectory prediction, occupancy estimation, and mapping. The LLM reasons about helpful information needed for path planning of the ego vehicle, iteratively building up context by calling functions from the tool library. The model employs a memory bank of prior experiences. If a predicted trajectory collides with surrounding objects, the LLM undergoes self-reflection to fine-tune its prediction.",
            "result_discussion": "The authors found that their self-reflection approach eradicates invalid (hallucinated) generations during test-time, achieving zero invalid outputs.",
            "ablation_id": "2403.16527v1.No2"
        },
        {
            "research_objective": "To determine whether the multi-modal DriveLM-Agent method can effectively predict ego vehicle paths from raw images in an end-to-end manner compared to Agent-Driver.",
            "experiment_process": "DriveLM-Agent frames the task into several sub-tasks: questioning a VLM about its perception of surrounding vehicles, predicting behaviors, planning high-level decisions, converting to lower-level discrete actions, and finally estimating a coordinate-level trajectory. This approach contrasts with Agent-Driver, which requires sensor modules to process context separately.",
            "result_discussion": "The DriveLM-Agent method successfully performed these tasks, leveraging a multi-modal approach to predict paths effectively. However, detailed comparative performance metrics with Agent-Driver were not specified.",
            "ablation_id": "2403.16527v1.No3"
        },
        {
            "research_objective": "To measure and improve the robustness of SayCan's action generation for robots by integrating methodical constraints using SayCanPay.",
            "experiment_process": "SayCanPay enhances SayCan by incorporating three strategies for planning: Say, SayCan, and SayCanPay. Say methods choose actions based only on token probabilities, SayCan approaches consider the success rate, and SayCanPay also estimates the expected payoff with heuristic-based learning. Models were trained using regression on expert trajectory datasets.",
            "result_discussion": "The SayCanPay method maximized task efficiency while minimizing conflicting infeasible actions. It showed improved performance over basic Say and SayCan methods.",
            "ablation_id": "2403.16527v1.No4"
        },
        {
            "research_objective": "To test the viability of CLARA in predicting robotic system response feasibility and handling uncertainty in user instructions.",
            "experiment_process": "CLARA samples multiple sets of concepts from the original prompt, orders them, and inputs them to the language model. It computes average similarity in an embedding space to identify uncertainties. For feasibility, it provides the robot's possible action space, observation, and goal to determine practicality. If uncertain but feasible, clarification is requested from the user.",
            "result_discussion": "CLARA achieved a reasonable success rate on robotic tasks but faced challenges during uncertainty reasoning and feasibility predictions, leading to occasional hallucinations.",
            "ablation_id": "2403.16527v1.No5"
        }
    ]
}