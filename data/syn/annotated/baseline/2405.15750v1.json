{
    "title": "Filtered Corpus Training (FiCT) Shows that Language Models can Generalize from Indirect Evidence",
    "abstract": "This paper introduces Filtered Corpus Training, a method that trains language models (LMs) on corpora with certain linguistic constructions filtered out from the training data, and uses it to measure the ability of LMs to perform linguistic generalization on the basis of indirect evidence. We develop filtered corpora that target a wide range of linguistic phenomena. Our approach focuses on evaluating how language models generalize from indirect evidence, suggesting that they are capable of successful linguistic generalizations.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Language models (LMs) play an increasingly large role in natural language processing systems and have become capable of producing surprisingly fluent and grammatical text. However, the mechanisms underlying the acquisition and use of such linguistic proficiency remain largely unknown. In particular, the degree that language learning relies on memorization versus generalization remains a topic of investigation (Hupkes et al., 2023).\n\nThe reliance of LMs on large amounts of training data raises the suspicion that they do not generalize in a \u2018human-like manner\u2019 (McCoy et al., 2019; Hu et al., 2020; Oh and Schuler, 2023b), but it is hard to address such questions with traditional evaluation metrics such as perplexity. \n\nThis paper introduces Filtered Corpus Training (FiCT) as a method for measuring the linguistic generalization abilities of language models. FiCT involves training models on corpora that have been filtered to remove specific linguistic constructions, thereby testing the models\u2019 ability to generalize beyond their training data. For example: we can train a model on a corpus that has never seen subjects modified by a prepositional phrase (e.g. \u201cA sketch of lights { doesn\u2019t / *don\u2019t }\u2026\u201d), and then ask whether it can judge the grammaticality of such sentences. If a model has learned that verbs must agree with the head noun of the subject noun phrase (NP), and that NPs can be modified by PPs (e.g. from seeing these in object but not subject position), it should be capable of generalizing to the unseen PP-modified subjects.\n\nThis method enables us to ask whether models can form relevant linguistic generalizations from indirect evidence, or whether they require direct evidence (e.g. examples of constructions during training; Warstadt and Bowman, 2022; Mueller and Linzen, 2023). Furthermore, by carefully controlling for the number of parameters, we can investigate the inductive biases of language models, which allows us to give more detailed answers about their linguistic capabilities.\n\nThe results of using FiCT show that the impact of filtered corpus training on grammaticality judgments is quite low, suggesting that language models are able to form sophisticated linguistic generalizations on the basis of only indirect evidence."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Surprisal Theory",
            "text": "Language modeling performance can be measured using perplexity, indicating a model\u2019s fit to a corpus distribution. Intuitively, one might expect that lower perplexity leads to more human-like linguistic behavior. This connection has been explored in detail in the context of surprisal theory (Hale, 2001; Levy, 2008): encountering a highly surprising token results in a longer reading time. Initial findings indicate that lower perplexity, as measured by language models, leads to better reading time predictions (Fossum and Levy, 2012; Goodkind and Bicknell, 2018; Wilcox et al., 2020), although affected by model architecture (Hao et al., 2020), cross-lingual effects (Kuribayashi et al., 2021), and syntactic ambiguity (Arehalli et al., 2022). It has been shown, however, that lower perplexity only results in better predictive power up to around 2 billion training tokens (Oh and Schuler, 2023a): after this point LMs become too powerful at predicting low-frequency constructions (Oh et al., 2024). The present paper also explores the connection between perplexity and human-like linguistic behavior and will find a dissociation with perplexity."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Targeted Syntactic Evaluations",
            "text": "Perplexity should be augmented with other evaluations that specifically target the models\u2019 ability to generalize in a human-like way. Such investigations often draw on psycholinguistic paradigms, treating language models as participants in order to learn what such models \u201cknow\u201d about specific linguistic phenomena (Futrell et al., 2019; Ettinger, 2020). A common paradigm in this body of literature, usually referred to as \u201ctargeted syntactic evaluations\u201d (Linzen et al., 2016; Jumelet and Hupkes, 2018; Marvin and Linzen, 2018; Gauthier et al., 2020; Newman et al., 2021) involves comparing language models\u2019 preferences between minimal pairs of sentences: a model is deemed to understand a phenomenon if it assigns a higher probability to the grammatical alternation."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Linguistic Generalization",
            "text": "While targeted syntactic evaluations provide insight into a model\u2019s linguistic competence, they do not show how a model acquires this notion of grammaticality. In this paper, we focus on two kinds of linguistic generalization. Structural generalization (Hupkes et al., 2023) asks: can language models make grammaticality judgments in syntactically more complex constructions than seen during training?\n\nOne line of work approaches this question from a fine-tuning perspective: by fine-tuning a model on a particular set of constructions, we can measure the impact this has on other linguistic constructions (Prasad et al., 2019; Weber et al., 2024). Lexical generalization asks whether models can generalize a seen construction to new lexical items that it has not seen in that construction (Kim and Linzen, 2020). \n\nIn order to gain a causal perspective on how the training data influences model performance, we retrain models from scratch on filtered corpora. This methodology has been deployed in earlier work to investigate how LMs learn the licensing conditions of negative polarity items from different contexts (Jumelet et al., 2021; Weber et al., 2021). Warstadt (2022) investigates the poverty of the stimulus debate through the lens of filtered corpora, focusing on the phenomenon of subject auxiliary inversion. Finally, Misra and Mahowald (2024) investigate rare adjective-noun constructions and manipulate training corpora to understand how models acquire an understanding of rare constructions. \n\nWhereas most of these studies focus on a particular linguistic construction, our work applies the approach to a wide range of phenomena."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Filtered Corpus Training (FiCT)",
            "text": "This section introduces the logic of the FiCT method before detailing the specific filters used in our experiments. The benchmark is designed to test subject-verb number agreement in the presence of an intervening distractor in a prepositional phrase, as illustrated in Figure 1. The agr-pp-mod filter removes all sentences containing this dependency structure. The resulting filtered corpus will still include PPs modifying nouns in other contexts, such as object position. If a learner understands a general 'rule' for subject-verb agreement and has seen PP-modified objects, it should generalize to agreement with PP-modified subjects, even when not encountered during training.\n\nA similar filter targets sentences where the distractor occurs in a relative clause in subject position, removing all sentences containing this structure. An example is \u201cThe boys that aren\u2019t disturbing Natalie dream.\u201d A model can generalize from its general 'rule' for subject-verb agreement to learn about relative clause structure from object-position clauses.\n\nWe utilize the list of negative polarity items (NPIs) provided by Jumelet et al. (2021) to filter sentences. The npi-only filter removes sentences with an NPI following 'only' (e.g., \u201cOnly students have ever complained about morning classes\u201d), npi-sent-neg eliminates sentences with a negation and an NPI, and npi-sim-ques removes questions with NPIs in them. Models can generalize NPI licensing conditions for specific environments from others remaining in the data.\n\nSuperlative quantifiers (e.g., at least, at most) cannot be embedded under negation. For example, an actor arrived at at most six lakes vs. *No actor arrived at at most six lakes. We handle this by replacing the superlative quantifier under negation with a relative quantifier or removing the negation. Superlative quantifiers are identified using morphological feature annotations, and we filter such constructions appearing in object position.\n\nWeak quantifiers can occur in existential there constructions, while strong quantifiers cannot (e.g., There are many people here vs. *There are all people here). We filter weak quantifiers in subject position under an existential there. Only the 5 weak quantifiers (a(n), no, some, few, many) are filtered, allowing models to generalize from other weak quantifiers.\n\nThere are four filters related to binding principles: binding-c-command, binding-case, binding-domain, and binding-reconstruction. These filters address various aspects of Chomsky's Principle A. The filters target sentences where pronouns appear in specific contexts, as described by the benchmarks, without removing sentences where similar principles apply elsewhere. For instance, the binding-c-command filter removes evidence of c-command relationships for anaphora licensing in relative clauses but not elsewhere.\n\nA filter targeting determiner-noun agreement addresses cases where an adjective appears between a demonstrative determiner and a noun (e.g., These/*This red cars). This filter removes demonstrative determiners followed by an adjective and a noun, so the model can infer number agreement from remaining determiner/noun pairs.\n\nThe language model performance regarding subject-verb agreement is tested with regular and irregular plurals. The filter removes all sentences with nominal subjects appearing in the respective benchmarks. A learner can succeed if it forms a notion of grammatical number based on usage in other contexts and generalizes this to the filtered nouns.\n\nAnother filter deals with determiner-noun agreement for demonstrative determiners directly adjacent to nouns, removing nouns from the benchmark preceded by demonstratives. Models can learn number agreement between other determiners and nouns.\n\nPassive constructions can only be formed by transitive verbs. To address this, we remove sentences containing words from a list used in these benchmarks in passive form. The filter operates by eliminating sentences where these words appear in passive voice."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Logic of the Method",
            "text": "The core methodological basis of this paper is what we call Filtered Corpus Training, or FiCT. This involves comparing the performance of otherwise identical learners that are trained on data which differs in some interesting way. In this paper, the FiCT methodology is primarily used to test if LMs are capable of extrapolating linguistic rules learned from environments in training data to unseen environments.\n\nIn order to ensure that the specified environments are not seen in the training data, we use filters to remove sentences with the specified environments from a naturalistic corpus. By comparing models trained on the ablated data and models trained on the full, naturalistic corpus, we can potentially determine if, how, and when language models are able to make such generalizations. Figure 1 illustrates the logic of our method. The sentence pair \u201cA sketch of lights {doesn\u2019t / *don\u2019t} appear\u201d contains a subject with a prepositional phrase (PP) modifying a noun, itself with a noun that differs in number from the main subject. We filter from the training corpus all sentences with subjects containing PP modifiers, and then compare the ability to make the correct grammaticality judgments on this pair between a model trained on the full corpus and this filtered corpus. This difference in performance we call \\( \\Delta \\) (formally defined in \u00a74). A model that has not seen PP-modified subjects could still make the correct judgments by forming the following generalizations: verbs agree with the head noun of the subject, and noun phrases with PP modifiers (which can be seen in object, but not subject position) are headed by the main noun. Low \\( \\Delta \\) would then provide evidence that the model has developed such generalizations.\n\nThe filters used in the present investigation are listed in Table 1, along with some descriptive summary statistics for each. These filters utilized part-of-speech, morphological features, and syntactic dependency annotations generated via the use of Stanza (Qi et al., 2020), an off-the-shelf package that uses pretrained neural models to generate grammatical annotations within the framework of Universal Dependencies (UD) (Nivre et al., 2017, 2020). We now describe the filters in more detail."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Corpus Filters",
            "text": "In general, we favor \u201cstronger\u201d filters, i.e., those that include false positives (and so filter out more training data), since our goal is to ensure that the LM has not seen a given construction during training. In what follows,  means that there is a dependency from  to  with label .\n\nThe benchmark targeted by this filter tests subject-verb number agreement in the presence of an intervening distractor in a prepositional phrase. The filter removes all sentences containing the dependency structure. The resulting filtered corpus will still contain PPs modifying nouns in other contexts (e.g., object position). If a learner has formed a general \u2018rule\u2019 for subject-verb agreement and seen PP-modified objects, it should be able to generalize to agreement with PP-modified subjects, even when it hasn\u2019t seen them during training.\n\nThis filter targets sentences where the distractor occurs in a relative clause in subject position, removing all sentences containing the structure, e.g., \u201cThe boys that aren\u2019t disturbing Natalie dream.\u201d A model might generalize from its general \u2018rule\u2019 for subject-verb agreement and learn about relative clause structure from relative clauses in object position.\n\nWe use the list of negative polarity items (NPIs) provided by Jumelet et al. (2021) and filter as follows: npi-only removes all sentences with an NPI occurring after \u2018only\u2019 (e.g., \u201cOnly students have ever complained about morning classes\u201d), npi-sent-neg removes sentences with a negation and an NPI, and npi-sim-ques removes questions with NPIs in them. In each of these cases, the model can generalize NPI licensing conditions for a particular environment from other environments that are still present.\n\nSuperlative quantifiers (e.g., at least, at most) cannot be embedded under negation: An actor arrived at at most six lakes vs. *No actor arrived at at most six lakes. We cannot detect superlative quantifiers based on dependency information alone, so we use morphological feature annotations. Next, we filter all such constructions that appear in object position. It is less clear for this filter how a model can still infer the grammaticality from other constructions that are not covered by the filter.\n\nWeak quantifiers can occur in the scope of existential there constructions, whereas strong quantifiers cannot: There are many people here vs. *There are all people here (Milsark, 1974). We filter all weak quantifiers occurring in subject position under an existential there. However, we only filter 5 weak quantifiers (a(n), no, some, few, many), which still allows a model to generalize from other weak quantifiers to infer the grammaticality conditions. Furthermore, weak vs. strong quantification plays a role in other linguistic phenomena as well, a fact which a learner could leverage.\n\nFour filters target binding-related grammatical principles. The implementations are generally similar: they target sentences where a reflexive or non-reflexive pronoun occurs in the specific context(s) illustrated by the corresponding principles, narrowly construed, while leaving in sentences where the same or similar principle is applied in a different environment.\n\nFor example, one filter removes evidence of the use of the c-command relationship in anaphora licensing in relative clauses but not elsewhere, as in sentences like Mary\u2019s brother hurt himself (but not *Mary\u2019s brother hurt herself). Such benchmarks assume a straightforward one-to-one relationship between certain names and their grammatical gender. While such a relationship may not actually be borne out in practice today, the corpora used in this investigation likely do adhere to such a formulation. The other three filters operate in similar ways.\n\nOne of the filters targeting determiner-noun agreement focuses on cases where an adjective occurs between a demonstrative determiner and a noun, e.g., These/*This red cars. We create a filter that removes all occurrences of a demonstrative determiner followed by an adjective and a noun. A model can then still infer the number agreement from determiner/noun pairs without an intervening adjective.\n\nAnother filter targets determiner-noun agreement for demonstrative determiners with the determiner directly adjacent to the noun. We create a filter based on all nouns preceded by a demonstrative determiner. A model can still infer the number agreement between determiner and noun from other nouns and learn the number information of the filtered nouns from other agreement tasks like subject-verb agreement.\n\nPassive constructions can only be formed by transitive verbs. The passive filter operates by removing sentences that contain words on a word list in a specific linguistic environment. Concretely, this word list consists of the verbs that are used in passive form, and the filter removes sentences where such words appear in passive voice."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Structural Generalization",
            "text": "In the following filters, a particular structural configuration has been completely removed from the corpus, and a model must generalize to it from similar/related configurations. The benchmark targeted by this filter tests subject-verb number agreement in the presence of an intervening distractor in a prepositional phrase. The resulting filtered corpus will still contain PPs modifying nouns in other contexts (e.g. object position). If a learner has formed a general \u2018rule\u2019 for subject-verb agreement, and seen PP-modified objects, it should be able to generalize to agreement with PP-modified subjects, even when it hasn\u2019t seen them during training.\n\nThis filter is similar to the previous one, but targets sentences where the distractor occurs in a relative clause in subject position, removing all sentences containing the structure, e.g. \u201cThe boys that aren\u2019t disturbing Natalie dream.\u201d A model might generalize again from its general \u2018rule\u2019 for subject-verb agreement and learn about relative clause structure from relative clauses in object position.\n\nWe use the list of negative polarity items (NPIs) provided by Jumelet et al. (2021) and filter as follows: npi-only removes all sentences with an NPI occurring after \u2018only\u2019 (e.g. \u201cOnly students have ever complained about morning classes\u201d), npi-sent-neg removes sentences with a negation and an NPI, and npi-sim-ques removes questions with NPIs in them. In each of these cases the model can generalize NPI licensing conditions for a particular environment from other environments that are still present.\n\nSuperlative quantifiers (e.g., at least, at most) cannot be embedded under negation: An actor arrived at at most six lakes vs. *No actor arrived at at most six lakes. We cannot detect superlative quantifiers based on dependency information alone, so we use morphological feature annotations. Next, we filter all such constructions that appear in object position. It is less clear for this filter how a model can still infer the grammaticality from other constructions that are not covered by the filter.\n\nWeak quantifiers can occur in the scope of existential there constructions, whereas strong quantifiers cannot: There are many people here vs. *There are all people here (Milsark, 1974). We filter all weak quantifiers occurring in subject position under an existential there. However, we only filter the 5 weak quantifiers that are common (a(n), no, some, few, many), which still allows a model to generalize from other weak quantifiers to infer the grammaticality conditions. Furthermore, weak vs. strong quantification plays a role in other linguistic phenomena as well, a fact which a learner could leverage.\n\nFour filters, binding-c-command, binding-case, binding-domain, and binding-reconstruction target various facets of Chomsky (1993)\u2019s Principle A. The implementations of all four filters are generally similar: they target sentences where a reflexive or non-reflexive pronoun occurs in specific contexts while leaving in sentences where the same or similar principle is applied in a different environment. For example, the binding-c-command filter removes evidence of the use of the c-command relationship in anaphora licensing in relative clauses, but not elsewhere, as in sentences like Mary\u2019s brother hurt himself (but not *Mary\u2019s brother hurt herself). The other three benchmarks operate in similar ways.\n\nOne of the filters targeting determiner-noun agreement focuses on cases where an adjective occurs between a demonstrative determiner and a noun, e.g. These/*This red cars. We create a filter that removes all occurrences of a demonstrative determiner followed by an adjective and a noun. A model can then still infer the number agreement from determiner/noun pairs without an intervening adjective."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Lexical Generalization",
            "text": "In the following filters, we do not exclude an entire configuration but focus on a subset of lexical items. This allows a model to generalize indirectly to a specific occurrence of the configuration from other occurrences, rather than relying on direct co-occurrences. The filters emphasize lexical generalization, as they are concerned with particular lexical items rather than specific syntactic constructions. \n\nWe target benchmarks that test language model performance on subject-verb agreement, examining \"regular\" plurals like dress/dresses and \"irregular\" plurals like goose/geese. The filter eliminates sentences with nominal subjects where the noun appears in these benchmarks. A learner on this filtered corpus can still address the benchmark if it develops an understanding of grammatical number, recognizes the number aspect of these nouns based on their usage in other contexts, and then generalizes the observed subject-verb agreement for other nouns to these nouns.\n\nAnother filter focuses on determiner-noun agreement for demonstrative determiners (e.g., These/*This books) with the determiner directly adjacent to the noun. We create a filter based on nouns that are preceded by a demonstrative determiner. A model can still deduce the number agreement between determiner and noun from other nouns and learn the number information of filtered nouns from other agreement tasks like subject-verb agreement.\n\nPassive constructions are only formed by transitive verbs. We examine this by removing transitive verbs in passive constructions and replacing them with intransitive verbs: John is insulted by Mary vs. *John is smiled by Mary. The passive filter removes sentences containing specific verbs used in these benchmarks in passive form whenever these words appear in passive voice."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "The base train, validation, and test corpora are the English Wikipedia corpora released by Gulordava et al. (2018), with the train corpus consisting of 3.05M sentences (83M tokens, with a vocabulary size of 50000 plus an unknown and EOS token). The 15 filtered corpora are derived from this base corpus by discarding all sentences that are targeted by the filter. The number of sentences and tokens discarded by each filter varied; for specifics, refer to Table 1. Then, as an additional control, the fifteen filtered corpora plus the original, full training corpus were uniformly downsampled to 2.4M lines, corresponding to the size of the original training corpus. It is worth noting that the number of tokens did vary, as reflected in the rightmost column of Table 1: this is explained by the fact that certain filters target longer sentences more often.\n\nWe use four metrics\u2014three standard and one novel\u2014as the primary means of evaluation for all models. The first is perplexity over the (unfiltered) test corpus of Gulordava et al. (2018). \n\nOur final evaluation metric looks at the probability deltas between grammatical and ungrammatical sentences."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "We present our results along the four metrics: perplexity, TSE accuracy, accuracy delta, and probability delta."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Perplexity",
            "text": "As noted, while we downsampled all corpora to the same number of lines, the number of tokens varies between different training corpora. Previous research has shown a clear negative relationship between the number of tokens seen in training and test corpus perplexity. This effect is also present in our data, as indicated by the correlations observed."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "TSE Accuracy on BLiMP",
            "text": "Mean overall accuracy on a given benchmark where filtered corpus targeted a benchmark showed that the difference was not statistically significant. In other words, we find no difference in the models\u2019 ability to make grammaticality judgments when trained on filtered data that forces them to perform subtle generalizations, despite differences in perplexity."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Accuracy Delta",
            "text": "A table of the accuracy deltas, averaged across all random seeds, can be found in Figure 2. Focusing on the cases where the absolute value of the deltas was appreciably larger than zero, we note that most deltas are generally negative but fairly close to zero, with a few outliers, such as the models trained on the existential-there, agr-pp-mod, and npi-only corpora. These results suggest that, overall, learners are usually able to use various sorts of indirect evidence to acquire correct grammatical generalizations when direct evidence has been made unavailable, as otherwise, we could expect much larger deltas across the board.\n\nFor the cases where absolute value of the deltas was appreciably larger than zero, it is observed that this performance varies depending on the specific linguistic phenomenon being tested. For example, some models perform better on the benchmarks associated with the agr-re-irr-sv and the npi-only corpora, while others show better performance with agr-pp-mod and quantifier-existential-there. This is true even for phenomena that are seemingly relatively similar; for example, the agr-pp-mod and agr-re-irr-sv-agr filters are extremely similar, in that they both test long distance agreement in the present of a clausal distractor intervening between the subject and the verb; they differ only in the nature of that distractor.\n\nYet, as noted, models trained on the agr-re-irr-sv corpus have, on average, a different performance on the associated benchmarks than those trained on the agr-pp-mod corpus. As in the previous section, we can make this precise by analyzing all of the accuracy deltas where the absolute value is larger than zero. This suggests that perplexity does not predict the ability of a model to perform linguistic generalizations from indirect evidence."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Probability Delta",
            "text": "In order to gain a more fine-grained insight into the impact of corpus filtering, we examine the results at an item-level. For this, we make use of the metric, which expresses a model\u2019s magnitude of a grammaticality judgment.\n\nWe focus on three salient filters that each yielded distinct results: i) Subject-Verb Agreement for PP-modified subjects; ii) NPI Only; and iii) Binding Case. This demonstrates that corpus filtering has the effect of moving scores closer to the origin, meaning the model becomes less certain in its grammaticality judgment. The resulting score for a benchmark is then dependent on the scores of the full model: a sufficient margin makes it robust to the decrease and allows it to correctly assign a higher probability to the grammatical item.\n\nTo investigate this observation across all benchmarks, we assess the difference in going from full to Filtered. This difference represents the absolute impact of filtering on the TSE task.\n\nSubtle differences are present, however, for a number of filters where the score increases after filtering. Finally, we examine the robustness of a model\u2019s grammaticality judgments: does filtering have a significant impact on the distribution of judgments? For this, we compute the Pearson correlation of before and after filtering for each filter benchmark. A model is robust to filtering if this correlation remains high."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Our findings contribute to a growing body of research that suggests a dissociation between perplexity and more targeted evaluations of linguistic competence in artificial learners (Hu et al., 2020  ###reference_b15###). In a carefully controlled setting and for a wide range of phenomena, we demonstrate that the training objective of minimizing perplexity does not predict linguistic generalization. This raises interesting questions on the relation between perplexity and grammaticality judgments (Lau et al., 2017  ###reference_b21###). \nOur study also builds on the insights of numerous other works that use artificial learners as models for understanding human language acquisition, and gaining better insights into the inductive biases of such learners (Warstadt and Bowman, 2020  ###reference_b40###; Mueller and Linzen, 2023  ###reference_b29###; Weber et al., 2024  ###reference_b44###). The present study conducts for a wide range of phenomena what Warstadt (2022  ###reference_b39###) calls a \u201cproof-of-concept [of a] large-scale controlled ablation study on the input to model learners,\u201d and finds that direct attestation of linguistic evidence is not strictly necessary for the development of sophisticated linguistic generalizations. Rather, learners can leverage much more indirect sources of evidence to arrive at the correct generalizations. Where earlier work has focused on specific linguistic constructions, such as subject auxiliary inversion (Warstadt, 2022  ###reference_b39###), relative clauses (Prasad et al., 2019  ###reference_b36###), and negative polarity items (Jumelet et al., 2021  ###reference_b17###; Weber et al., 2021  ###reference_b43###), the results of this paper essentially confirm a similar result for a much wider array of syntactic and semantic phenomena. While in many cases the ablations we performed did clearly negatively affect the performance of our artificial learners on the relevant linguistic evaluations, the magnitude of this effect was generally quite small for all but a small handful of the linguistic phenomena we analyzed. In general, even when tested on the specific benchmarks corresponding to the environments that were ablated from their input, models still perform considerably better than chance. Thus, our research provides evidence in favor of the indirect evidence hypothesis. Notably, we find that this is true not only for filters where there are fairly obvious sources of indirect evidence (as enumerated in \u00a73  ###reference_###), but also for filters where potential sources of indirect evidence for a correct generalization are much less clear (such as the superlative-quantifier filter). This suggests that there may be complex mechanisms by which certain linguistic generalizations can be derived via highly indirect means. Thus, our results open a door to future research that can provide a more thorough account of the source of these generalizations, with potentially significant ramifications for linguistics. \nAs just discussed, the primary contribution of this paper has been the development of the FiCT method and the use of it to demonstrate LMs\u2019 successful generalization from indirect evidence across a wide range of linguistic phenomena. This success raises a very natural follow-up question: what explains this successful generalization behavior? While a complete answer to this question must await future work, a detailed look at the NPI cases can provide insight into what an answer may look like. Jumelet et al. (2021  ###reference_b17###) used a filtered corpus method to test LMs\u2019 understanding of negative polarity items, but then also did a further analysis to examine the basis upon which the models made their grammaticality judgments. In particular: they found (via probing classifiers) that LMs were successfully recognizing the monotonicity of a linguistic environment and (via a novel correlation method) that these judgments of monotonicity were highly correlated with the LMs\u2019 judgment of NPI acceptability, reflecting human acceptability judgments (Deni\u0107 et al., 2021  ###reference_b5###; Chemla et al., 2011  ###reference_b3###). This example suggests two paths forward for explaining the generalization observations in the present paper. On the one hand, in the same way that the monotonicity explanation was inspired by human generalization, detailed explanations of individual cases of generalization can be developed with human behavior as an initial inspiration. On the other hand, in the same way that this paper extends the filtered corpus training method to a much wider range of phenomena, one can attempt to generalize these forms of explanation on the breadth axis as well. We leave these exciting pursuits to future work."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduced the Filtered Corpus Training methodology and applied it to a wide range of linguistic constructions. Our results demonstrate that all of our language models exhibit a strong ability to generalize from indirect evidence, even for models of relatively low parameter count trained on relatively small data. Future work will (i) extend this approach to models of different sizes and pretraining corpora, (ii) perform deeper analyses of the bases on which the models do make their generalizations, and (iii) analyze other forms of lexical and structural generalization through the lens of filtered corpus training."
        }
    ],
    "url": "http://arxiv.org/html/2405.15750v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.15750v1",
        "paper_title": "Filtered Corpus Training (FiCT) Shows that Language Models can Generalize from Indirect Evidence",
        "research_background": "**Motivation:**\nThe motivation for this study stems from an ongoing debate in the natural language processing (NLP) community regarding the extent to which language models (LMs) rely on memorization versus generalization when acquiring linguistic proficiency. Even though LMs, such as those based on Transformer and LSTM architectures, have demonstrated the ability to generate fluent and grammatical text, the underlying mechanisms by which these models achieve this proficiency remain largely unexplored. Concerns have been raised that LMs might not generalize in a \"human-like manner\" due to their dependency on extensive training datasets. Traditional evaluation metrics such as perplexity are insufficient to address these concerns, necessitating the development of new methodologies for measuring the generalization capabilities of LMs.\n\n**Research Problem:**\nThe research problem tackled in this paper revolves around the ability of language models to generalize from indirect evidence, rather than merely relying on direct evidence from their training data. The paper aims to investigate whether LMs can form relevant linguistic generalizations without explicit exposure to specific linguistic constructions during training. To this end, the authors introduce the Filtered Corpus Training (FiCT) methodology, which involves training LMs on specially filtered corpora that exclude certain linguistic structures. This allows an assessment of the models' capability to generalize to these unseen structures, thereby providing insights into their generalization mechanisms.\n\n**Relevant Prior Work:**\nThe paper builds upon several lines of previous research:\n\n1. **Human-like Generalization in LMs:** Works by McCoy et al. (2019) and Hu et al. (2020) have raised concerns that LMs do not generalize in a human-like manner, often requiring direct examples of constructions in their training datasets to perform accurately.\n   \n2. **Generalization vs. Memorization:** Studies, such as those by Hupkes et al. (2023), have investigated the balance between memorization and generalization in language learning, contributing to the theoretical backdrop of the current study.\n\n3. **Evaluation of Linguistic Generalization:** Warstadt and Bowman (2022) and Mueller and Linzen (2023) have examined whether LMs need direct evidence of constructions for effective language learning and generalization, aligning closely with the questions addressed in this paper.\n\n4. **BLiMP Benchmark:** The study utilizes linguistic phenomena evaluated by the BLiMP benchmark (Warstadt et al., 2020), which provides a set of linguistic tasks aimed at evaluating the grammatical capabilities of LMs.\n\nBy integrating these strands of research, the paper aims to advance our understanding of how LMs generalize from the linguistic data they are trained on, thereby bridging gaps left by traditional evaluation metrics like perplexity.",
        "methodology": "## Methodology: Filtered Corpus Training (FiCT)\n\n### Introduction\n\nThis section elucidates the logic behind the FiCT method and delves into the specific filters employed in our experiments. The detailed experimental setup is provided in an additional section. Code, data, and models are openly accessible at the provided GitHub repository link. The primary aim of these filters is to test language models' generalization capabilities in various grammatical contexts.\n\n### Subject-Verb Number Agreement\n\n1. **AGR-PP-MOD Filter**:\n    - Targets sentences containing a specific dependency structure within prepositional phrases (PPs) modifying nouns in other contexts, excluding object positions.\n    - Example: \"The boy with the balloons\".\n    - Objective: If a model has learned subject-verb agreement rules from PP-modified objects, it should generalize to PP-modified subjects.\n\n2. **Relative Clause Filter**:\n    - Filters sentences where a distractor occurs in a relative clause in the subject position.\n    - Example: \u201cThe boys that aren\u2019t disturbing Natalie dream\u201d.\n    - Objective: Models might generalize subject-verb agreement rules and apply them to relative clause structures.\n\n### Negative Polarity Items (NPIs)\n\n- **NPI-Only Filter**:\n    - Removes sentences with an NPI after 'only'.\n    - Example: \u201cOnly students have ever complained about morning classes\u201d.\n- **NPI-Sent-Neg Filter**:\n    - Removes sentences with a negation and an NPI.\n- **NPI-Sim-Ques Filter**:\n    - Removes questions containing NPIs.\n\n### Superlative Quantifiers\n\n- Filters involve replacing superlative quantifiers under negation with relative quantifiers or removing the negation in sentences.\n    - Example: from \"An actor arrived at at most six lakes\" to \"No actor arrived at at most six lakes\".\n- Uses morphological feature annotations rather than dependency information.\n\n### Weak Quantifiers in Existential Constructions\n\n1. **Existential There Filter**:\n    - Filters sentences with weak quantifiers under an existential \"there\".\n    - Example: \"There are many people here\" vs. \"*There are all people here\".\n    - Specific weak quantifiers filtered: a(n), no, some, few, many.\n\n### Binding Constraints\n\n1. **Binding-C-Command Filter**:\n    - Targets sentences utilizing c-command relationship in anaphora licensing within relative clauses.\n    - Example: removing structures like \"Mary\u2019s brother hurt himself\".\n\n2. **Other Filters (Binding-Case, Binding-Domain, and Binding-Reconstruction)**:\n    - Target similar principles in different syntactic environments without affecting other occurrences.\n\n### Determiner-Noun Agreement\n\n1. **Det-Adj-Noun Filter**:\n    - Removes instances where a demonstrative determiner is followed by an adjective and a noun.\n    - Example: \"These/*This red cars\".\n\n2. **Det-Noun Filter**:\n    - Targets sentences with demonstrative determiners directly preceding a noun.\n    - Example: \"These/*This books\".\n\n### Subject-Verb Agreement\n\n1. **AGR-RE-IRR-SV Filter**:\n    - Focuses on both regular and irregular plurals within nominal subjects.\n    - Removes sentences with specific nouns used in the benchmarks (e.g., \"goose/geese\").\n    - Designed to see if models can generalize subject-verb agreement to these nouns from other contexts.\n\n### Passive Constructions\n\n- **Passive Filter**:\n    - Removes sentences where specific verbs used in passive constructions appear.\n    - Example: Filtering sentences like \"John is insulted by Mary\".\n\n### Summary\n\nThe FiCT methodology involves the deliberate application of linguistic filters to assess the ability of language models to generalize grammatical rules. Various filters are crafted to target specific linguistic phenomena, removing certain contextual uses while leaving others intact, to test the models' learning and generalization capabilities.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Datasets:**\nThe primary dataset is the English Wikipedia corpora provided by Gulordava et al. (2018). It includes:\n- **Training data:** 3.05M sentences (83M tokens), vocabulary size of 50000 plus an unknown and EOS token.\n- **Validation and test data:** Derived from the same English Wikipedia corpora.\n\nFrom this base corpus:\n- These corpora were uniformly downsampled to 2.4M lines to manage the dataset size.\n\n**Models:**\nTwo architectures were used, each model type being trained five times with different random seeds, totaling 160 models:\n- **LSTMs:**\n  - Two layers, 1024 embedding and hidden dimensions.\n  - Tied output and embedding layer weights.\n  - Dropout of 0.1.\n  - 68.0M trainable parameters.\n- **Decoder-only Transformers:**\n  - Feed-forward and hidden layer dimensions of 768.\n  - Eight attention heads and eight hidden layers.\n  - 67.1M trainable parameters.\n\n**Training Setup:**\n- **Hardware:** Single A40 GPU.\n- **Training Duration:** 40 epochs.\n- **Optimization Algorithm:** AdamW.\n- **Scheduler:** Linear with initial learning rate as specified in Appendix A.\n- **Batch Size:** 32.\n- Models evaluated at the end of each epoch; the one with the best validation perplexity was selected.\n\n**Evaluation Metrics:**\nFour metrics were used to evaluate the models:\n1. **Perplexity:** Measured on the unfiltered test corpus from Gulordava et al. (2018).\n2. **Accuracy on BLiMP Benchmarks:** \n   - Assessed via the \u201cfull-sentence\u201d method.\n   - Models were evaluated across 67 benchmarks in the BLiMP challenge set (Warstadt et al., 2020).\n3. **Probability Deltas:** The difference in probabilities assigned to grammatical vs. ungrammatical sentences.\n\n**Main Experimental Results:**\n- Both LSTMs and Transformers showed generalization capabilities from indirect evidence.\n\nIn summary, the main experiment setup successfully demonstrates that language models can generalize using standardized evaluation metrics and controlled model training conditions."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To test if language models (LMs) are capable of extrapolating linguistic rules learned from environments in training data to unseen environments.",
            "experiment_process": "The method involves comparing the performance of otherwise identical learners trained on data which differ by the absence of certain linguistic constructions. Filters are used to remove sentences with specified environments from a naturalistic corpus. By comparing models trained on the ablated data and models trained on the full corpus, the study checks for the models' ability to generalize. For example, the 'agr-pp-mod' filter removes all sentences containing subject-verb agreement structures with a distractor in a prepositional phrase. The filtered and unfiltered models' performances on specific grammatical benchmarks are then compared.",
            "result_discussion": "The outcomes suggest that models trained on filtered data can still make correct grammatical judgments by forming generalizations such as verbs agreeing with the head noun of the subject, even when they have not seen the exact construction during training.",
            "ablation_id": "2405.15750v1.No1"
        },
        {
            "research_objective": "To determine if LMs can generalize the licensing conditions of Negative Polarity Items (NPIs) from environments that remain in the training data.",
            "experiment_process": "Three filters were applied to the dataset: 'npi-only' removes all sentences with an NPI following 'only,' 'npi-sent-neg' removes sentences with a negation and an NPI, and 'npi-sim-ques' removes questions with NPIs in them. The model's performance on benchmarks related to NPIs was then evaluated to see if it generalizes from remaining environments.",
            "result_discussion": "The results show that models can generalize NPI licensing conditions for a particular environment from other environments still present in the training data. This indicates that they are capable of inferring correct contexts for NPIs without direct exposure.",
            "ablation_id": "2405.15750v1.No2"
        },
        {
            "research_objective": "To test if LMs can generalize grammatical rules from remaining contexts after filtering out specific quantifier-related constructions.",
            "experiment_process": "The study filters sentences with superlative quantifiers under negation and weak quantifiers in subject position under existential 'there' constructions. Two conditions of the superlative quantifier phenomenon were targeted: either replacing the superlative quantifier with a relative quantifier or removing the negation. The model's ability to infer the grammaticality of sentences with weak and strong quantifiers was evaluated.",
            "result_discussion": "The conclusions indicate that models can leverage the presence and conditioning of weak and strong quantifiers across various linguistic phenomena, suggesting their ability to generalize based on existing data even when specific examples are filtered out.",
            "ablation_id": "2405.15750v1.No3"
        },
        {
            "research_objective": "To evaluate if LMs can generalize binding principles in different linguistic contexts.",
            "experiment_process": "Four filters targeted BLiMP's binding-related benchmarks: 'binding-c-command,' 'binding-case,' 'binding-domain,' and 'binding-reconstruction.' Each specifically aimed at environments where specific binding principles apply, while leaving alternative applications intact. The resulting model's performance on these benchmarks was compared to fully-trained models.",
            "result_discussion": "The findings suggest that the models indeed generalize the binding principles from situations that remained in the training data to those filtered out, reinforcing their capability to infer broader grammatical constraints.",
            "ablation_id": "2405.15750v1.No4"
        },
        {
            "research_objective": "To test generalization capabilities for determiner-noun agreement across linguistically varied contexts.",
            "experiment_process": "Two filters were implemented: one removing occurrences of demonstrative determiners followed by an adjective and a noun, and another stripping out nominal subjects involved in subject-verb agreement tasks with both regular and irregular plurals. The model's ability to infer number agreement without direct training examples was then assessed.",
            "result_discussion": "Results show that models can generalize number agreement rules from existing data to the filtered contexts, sustaining accuracy in identifying correct determiner-noun and subject-verb agreements.",
            "ablation_id": "2405.15750v1.No5"
        },
        {
            "research_objective": "To explore if LMs can derive rules for passive constructions from remaining non-exemplars after filtering specific verb usage.",
            "experiment_process": "The study incorporated a filter that removed sentences containing certain verbs in passive constructions (those actively used in BLiMP benchmarks). The filtered and unfiltered models were then compared based on their handling of passive vs. active verb contexts.",
            "result_discussion": "Outcomes revealed models' ability to recognize passive construction rules and generalize these from non-verbal forms still present in the corpus, indicating robust inferential mechanisms within the models.",
            "ablation_id": "2405.15750v1.No6"
        }
    ]
}