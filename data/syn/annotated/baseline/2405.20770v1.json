{
    "title": "Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent",
    "abstract": "Over the past two years, the use of large language models (LLMs) has advanced rapidly. While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some well-designed textual perturbations. In this paper, we introduce a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM. Our method comprises two main components: a) Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence while defending against attacks; b) Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs. Remarkably, the defense agent demonstrates robust defensive capabilities even without learning from adversarial examples. Extensive experiments on both open-source and closed-source LLMs demonstrate that our method effectively defends against adversarial attacks, thereby enhancing adversarial robustness.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) have garnered significant attention due to their impressive performance across a wide range of natural language tasks. The pre-trained LLMs, such as Meta\u2019s LLAMA and OpenAI\u2019s ChatGPT, have become essential foundations for AI applications in various sectors such as healthcare, education, and visual tasks. Despite their widespread use and convenience, concerns about the security of these models are increasing. Specifically, LLMs have been shown to be vulnerable to adversarial textual examples, which involve subtle modifications to textual content that maintain the same meaning for humans but completely change the prediction results to LLMs, often with severe consequences.\n\nIn this paper, focusing on adversarial textual attacks targeting LLM-based classification tasks, we propose a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which utilizes the LLM as a defense agent for adversarial purification. Specifically, LLAMOS comprises two components: Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence, and Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs. LLAMOS serves as a pre-processing method aiming to eliminate harmful information from potentially attacked textual inputs before feeding them into the target LLM for classification.\n\nOur contributions are summarized as follows:\n\nWe propose a novel defense technique named LLAMOS, which aims to purify the adversarial textual examples before feeding them into the target LLM. To the best of our knowledge, we are the first to employ an LLM agent to enhance the adversarial robustness of LLMs. The defense agent in LLAMOS is a plug-and-play module, serving as a pre-processing step. Notably, it operates without retraining of target LLM, rendering it efficient and user-friendly. We conduct extensive experiments to empirically demonstrate that the proposed method can effectively defend against adversarial attacks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminary",
            "text": "There are several methods to evaluate the adversarial robustness of large language models (LLMs). These methods aim to understand how LLMs behave under adversarial conditions and measure their resilience to such attacks. Adversarial attacks on neural networks are crafted by applying small, intentional perturbations to the input data that lead to incorrect outputs, revealing potential vulnerabilities. By examining these vulnerabilities, researchers can enhance their understanding of the limits of current models and work towards developing more robust systems.\n\nIn recent assessments, adversarial robustness is often quantitatively measured using metrics that consider the success rate of attacks and the performance degradation of the models under attack. Researchers use various benchmark datasets to systematically test the robustness of LLMs, providing insights into the model's ability to withstand adversarial input alterations. These insights can be crucial for deploying LLMs in safety-critical applications where reliability is paramount.\n\nThe evaluation experiments typically involve using adversarial examples and testing how well models can maintain performance in their presence. These experiments give insights into the models' strengths and weaknesses, guiding future development efforts to create more robust language understanding systems.\n\nUnderstanding and improving adversarial robustness not only involves identifying weaknesses in current models but also drives the advancement of machine learning algorithms capable of resisting such attack vectors. This area of research is vital for the development of LLMs that can be used confidently in real-world applications."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Adversarial Attcks on LLMs",
            "text": "Given a target LLM with task instruction, input , and correct output , the adversarial attacks aim to find the adversarial examples  that can fool the target LLM on classification tasks. The adversarial examples  can be obtained by the LLM itself with different system prompts, where  represents textual perturbations from a series of candidate sets for modifications, which are made at the character level, word level, or sentence level. In a specific instance, the system prompt of  can be: \u201cAnalyze the tone of this statement and respond with either \u2018positive\u2019 or \u2018negative\u2019.\u201d and the system prompt of corresponding  can be: \u201cYour task is to generate a new sentence that keeps the same semantic meaning as the original one but be classified as a different label.\u201d There are more details in Appendix B."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Evaluations of LLMs Robustness",
            "text": "To evaluate the effectiveness of the defense method, we follow the setting from Wang et al. (2021); Xu et al. (2024), using the attack success rate (ASR) and traditional robust accuracy (RA) on the adversarial examples as measures of the robustness of the defense method. The lower the ASR, the higher the RA, indicating greater model robustness."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "We propose a novel defense technique called LLAMOS, which purifies adversarial examples before feeding them into the target LLM. The overall pipeline of LLAMOS is outlined in Section 3.1. Subsequently, we further augment the defense agent using in-context learning as discussed in Section 3.2. Finally, in Section 3.3, we present the design of the adversarial system, incorporating the defense agent, attack agent, and target LLM."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Overview of LLAMOS",
            "text": "To defend against adversarial textual attacks targeting LLM-based classification tasks, we propose Large Language Model Sentinel (LLAMOS) that employs the LLM as a defense agent. LLAMOS comprises two components: Agent instruction and Defense guidance. Next, we introduce the overall pipeline in sequential order.\n\nIn this paper, we utilize the existing LLMs denoted by target LLMs as classifiers. Given an adversarial example, the target LLM outputs the incorrect label, while after processing, the example can be predicted as the correct label. To achieve this, we design prompts for generating a defense agent as described in the following.\n\n# Defense Agent Instruction\n\nTo begin, let me provide a brief overview of the input text: [Input Description]. The classification task for these sentences is [Task Description]. However, be aware that these sentences might be susceptible to adversarial attacks, which could lead to an incorrect label. Note that not all sentences will be affected by the attacks. Your task is to generate a new sentence that replaces the original one, which must satisfy the following conditions: [Defense Goal].\n\n# Defense Guidance\n\nYou can complete the task using the following guidance: [Defense Guidance]. \n\nInput: [Input]. Now, let\u2019s start the defense process and only output the generated sentence.\n\nInput Description. The format of Input varies significantly across different datasets, necessitating tailored input formats to correspond with the specific structure and content of each dataset, details in Table 10. For instance, the SST-2 dataset typically consists of a single sentence per data point. On the other hand, the MNLI dataset is structured to include pairs of sentences labeled as premise and hypothesis.\n\nTask Description. Similar to the input descriptions, the tasks associated with each dataset are distinct. As illustrated earlier, SST-2 focuses on determining the sentiment of a given sentence, making it a straightforward classification challenge. Conversely, MNLI presents a more complex task of natural language inference, where the relationship between a pair of sentences must be discerned and classified correctly. Detailed input and task descriptions are provided in Table 9.\n\nDefense Goal. Based on traditional defense methods, we designed the defense goal for LLAMOS as follows: \u201c1. Keeping the semantic meaning of the new sentence the same as the original one; 2. For natural examples, the new sentence should remain unchanged. For adversarial examples, modify the sentence so that it is classified as the correct label, effectively reversing the adversarial effect.\u201d\n\nDefense Guidance. The defense guidance offers specific instructions to the defense agent on how to modify the input text to ensure effective defense and accurate outputs from the target LLM. In designing our guidance, we considered attacks at various levels, including character, word, and sentence levels, which are presented in Table 1. These guidances are not rigidly fixed; they can be fine-tuned according to specific tasks.\n\nAfter the defense agent generates the new sentence, the example is input into the target LLM for classification."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Enhencing the Defense Agent with In-Context Learning",
            "text": "In the initial defense agent, the defense guidance relies on common sense, which may result in poor performance against some special attacks, even when the attacker adds obvious characters. To address this limitation, we introduce in-context learning (Dong et al., 2022) to further optimize the defense agent. The prompts of in-context learning are described in the following.\n\n# In-Context Learning\n\nThe new sentence still contains a lot of harmful content caused by adversarial attacks, such as [Specific Guidance]. Please consider these contents and output a new sentence for me. \nInput: [Input]. Now, let\u2019s start the defense process and only output the generated sentence.\n\nThe specific guidance is designed to assist the defense agent in better understanding an attack and generating a new sentence capable of effectively defending against the attack. These guidelines can be fine-tuned to address specific attacks and can be incorporated into the defense agent as needed. Through in-context learning, the defense agent can be continuously optimized, significantly enhancing its performance almost without adding any additional costs."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Adversarial System with Multiple LLMs",
            "text": "In this section, we devise an adversarial system involving multiple LLMs. Given that our method introduces a defense agent against attackers, a natural idea is to then create an attack agent to counter the defender. The attack agent is tasked with generating adversarial examples from purified examples to deceive the target LLM once more. To accomplish this, we design prompts for generating an attack agent, as described in the following.\n\nTo begin, let me provide a brief overview of the input text: [Input Description]. The classification task for these sentences is [Task Description]. Your task is to generate a new sentence that replaces the original one, which must satisfy specific attack instructions.\n\nFor example, the original sentence [Purified Example] is classified as [Correct Label]. You should generate a new sentence which is classified as the incorrect label. \n\nThe prompt structure of the attack agent and the defense agent is similar, differing mainly in particular details. The attack instruction includes changing at most two letters in the sentence. Finally, a specific example is provided to help the attack agent better understand the task.\n\nThen, we combine the defense agent and attack agent to form an adversarial system. In the adversarial system, the purified examples can be attacked again by the attack agent, and likewise, the adversarial examples can also be purified by the defense agent. They continuously counter each other, much like adversarial training (Goodfellow et al., 2015)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Adversarial Attack. Deep neural networks (DNNs) are vulnerable to adversarial examples, which are generated by adding small, human-imperceptible perturbations to natural examples, but completely change the prediction results to DNNs. With the rapidly increasing applications of LLMs, security concerns have emerged as a critical area of research, with researchers increasingly focusing on adversarial attacks targeting LLMs. For LLMs, attackers manipulate a small amount of text to change the output of the target LLM while maintaining the semantic information for humans. Presently, addressing the security issues surrounding LLMs is of paramount importance and requires urgent attention.\n\nAdversarial Defense. There are two main defense techniques on traditional DNNs, including adversarial training. Unlike traditional DNNs, retraining LLMs is nearly impossible due to cost issues. Therefore, most methods enhance the robustness of LLMs through other strategies. Even with fine-tuning, training the LLMs will still consume a significant cost.\n\nTo counteract the adversarial challenges faced by LLMs, we propose a novel LLM defense technique named LLAMOS to purify the adversarial textual examples before feeding them into the target LLM, aiming to improve the robustness of the entire system.\n\nLarge Language Model Agent. The LLM agent is a new research direction that has emerged in recent years. This novel type of agent is capable of interacting with humans in natural language, leading to a significant increase in applications across fields such as chatbots, natural sciences, robotics, and workflows. Furthermore, LLMs have demonstrated promising zero-shot/few-shot planning and reasoning capabilities across various configurations, covering specific environments and reasoning tasks. In this paper, we introduce a new variant of the LLM agent designed specifically to purify adversarial textual examples generated by attacks."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we conduct extensive experiments on GLUE datasets to evaluate the effectiveness of the proposed method (LLAMOS). Specifically, our method significantly reduces the attack success rate (ASR) by up to 37.86% with GPT-3.5 and 45.59% with LLAMA-2, respectively."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Experimental Setup",
            "text": "Datasets. The experiments are conducted on six tasks in GLUE datasets, including SST-2, RTE, QQP, QNLI, MNLI-mm, MNLI-m. The detailed descriptions are provided in Appendix A.\n\nEvaluation Metrics. We evaluate the performance of defense methods using two metrics: attack success rate (ASR) and robust accuracy (RA). These metrics are derived from testing on adversarial examples, where a lower ASR or a higher RA indicates greater model robustness.\n\nTraining Details. The experiments in this paper are conducted using GPT-3.5 (OpenAI, 2023) with \u2018GPT-3.5-Turbo-0613\u2019 version and LLAMA-2 (Touvron et al., 2023b) with \u2018LLAMA-2-7b\u2019 version. For GPT-3.5, we purchase OpenAI\u2019s API service and conduct testing experiments with the \u2018openai\u2019 package in Python. For LLAMA-2, we deploy it locally on NVIDIA RTX A6000 and utilize the available checkpoint published by MetaAI from HuggingFace."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Results",
            "text": "Evaluation of LLAMOS Performance on Robust Accuracy (RA). We evaluate the RA of LLAMOS on the SST-2 dataset with LLAMA-2 against three types of PromptAttack: character, word, and sentence attacks. In Table 4, the accuracies in the first two columns represent the standard accuracy and robust accuracy without defense, while the last three columns represent the robust accuracy with LLAMOS. Under strong attacks, the classification accuracy of the target LLM decreased from 92.18% to 30.42%. LLAMOS can effectively defend against adversarial textual attacks, significantly improving the robust accuracy. Specifically, the lowest robust accuracy reaches 86.96%.\n\nAdditionally, we conduct more comprehensive experiments across nine types of attacks and six tasks with GPT-3.5, as shown in Table 5. LLAMOS can effectively defend against character-level attacks, achieving results on C1 and C3 that closely match the standard accuracy.\n\nEvaluation of LLAMOS Performance with In-Context Learning (ICL). The C3-based attack (Xu et al., 2024) is a very obvious attack that adds up to two extraneous characters to the end of the sentence, as shown in Table 8. However, our method only achieves robust accuracies of 89.06% for the C3 attack and 66.41% for the C3-FS attack, respectively. To further improve the robustness, we introduce ICL to enhance the performance of the defense agent. As shown in Table 6, the defense agent with ICL significantly improves the robust accuracy against the C3 attack, achieving a robust accuracies of 97.66% for the C3 attack and 92.19% for the C3-FS attack, respectively.\n\nAnalysis of Adversarial System. We conduct experiments with an adversarial system and evaluate the robust accuracy defense against adversarial examples generated by the attack agent over multiple iterations. As shown in Table 7, the defense agent initially achieves a robust accuracy of 96.09% in the first round of confrontation. However, after the purified examples are re-attacked by the attack agent, the robust accuracy decreases to 56.25%. The defense agent then purifies these adversarial examples again, leading to an increase in robust accuracy, but it will decrease once more by subsequent attacks. This continual fluctuation in robust accuracy is a common phenomenon in adversarial training (Goodfellow et al., 2015). Upon reviewing the generated texts, we observe that after several rounds of confrontation, both the defense agent and attack agent may generate the same sentences as previous ones, resulting in a potential infinite loop, as shown in Table 8. This is an interesting phenomenon that requires further investigation, particularly strategies to disrupt such loops."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Discussion",
            "text": "The Advantages of LLAMOS. As emphasized by the experimental results presented in Section 5.2 and Table 12, LLAMOS significantly enhances performance across various tasks and attacks with LLAMA-2 and GPT-3.5. Additionally, the defense agent in LLAMOS is a plug-and-play module, serving as a pre-processing step. Through in-context learning (ICL), the defense agent can be continuously optimized to defend against emerging attacks. This addresses a major challenge in adversarial robustness: Due to significant differences between different attacks, models trained on specific attacks often fail to generalize to other unseen attacks. The model necessitates continuous adaptation to emerging attacks. However, fine-tuning parameters requires substantial costs, and new attack techniques continually make it impractical to train models for emerging threats. In contrast, our method can effectively enhance robustness through ICL without adjusting the parameters of the LLMs, which is a significant advantage.\n\nThe Challenges in LLM-based Defense. The defense agent is tasked with purifying adversarial examples, but it is difficult to distinguish between natural examples and adversarial examples in some cases. As shown in Table 12.4, the attacker altered the original meaning by inserting \u2018not\u2019, rendering the adversarial example indistinguishable from a natural example, resulting in the defense agent failing to generate the correct sentence. Although we hope that the defense agent can observe sentences like humans, it presents a huge challenge. Unlike the attacker or humans, the defense agent lacks access to the original label of the input sentence. Furthermore, although the defense agent can effectively defend against adversarial attacks, it cannot prevent subsequent attacks, as illustrated in Table 7. For instance, malicious LLMs can embed specific system prompts to influence the output, which is unbeknownst to users; they can add \u2018:)\u2019 to each input sentence for prediction. In this case, the defense agent also fails to defend. \n\nThis issue is an important problem in traditional adversarial training, and no method has completely resolved this. Nonetheless, as previously discussed, LLMs offer advantages not available to traditional DNNs, and we have naturally solved one challenge in adversarial robustness, which is that the model can adapt to new attacks. Hence, future advancements may resolve adversarial issues of attack and defense within LLM frameworks, representing a challenging but promising research direction.\n\nLimitations. It is well-known that training large language models (LLMs) requires significant resources and generates substantial carbon emissions, thereby burdening the planet. However, the long-term inference costs of LLMs far exceed the training costs. Chien et al. show that for ChatGPT-like services, inference dominates emissions due to its large user base, in one year producing 25 times the carbon emissions of training GPT-3. Our method introduces a defense agent for additional inference, which inevitably increases carbon emissions during the inference process, thereby exacerbating the negative impact on the climate, which is a limitation of our work. However, considering the nascent stage of LLM development, the trustworthiness issue is equally crucial. Therefore, we have dedicated significant effort to this area, but of course, we aspire to find future solutions that adequately address the environmental and climate challenges posed by AI.\n\nImpact Statements. This paper presents research aimed at enhancing the robustness of large language models (LLMs). With the rapidly increasing applications of LLMs, their security and trustworthiness have become a critical concern. Our work focuses on this significant issue and contributes positively to potential societal impacts."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we propose LLAMOS, a novel LLM-based defense technique designed to purify adversarial examples before feeding them into the target LLM. The defense agent within LLAMOS operates as a plug-and-play module that functions effectively as a pre-processing step without requiring retraining of the target LLM. We conduct extensive experiments across various tasks and attacks with LLAMA-2 and GPT-3.5. The results demonstrate that LLAMOS can effectively defend against adversarial attacks. Furthermore, we discuss certain existing shortcomings and challenges, which we aim to address in future research."
        }
    ],
    "url": "http://arxiv.org/html/2405.20770v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "4"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.3",
            "5.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.20770v1",
        "paper_title": "Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent",
        "research_background": "**Motivation:**\n\nThe motivation behind this paper stems from the growing concerns about the security vulnerabilities of Large Language Models (LLMs) despite their widespread use in various sectors such as healthcare, education, and visual tasks. The major performance capabilities of pre-trained LLMs like Meta\u2019s LLAMA and OpenAI\u2019s ChatGPT are overshadowed by their susceptibility to adversarial textual examples, which are subtle modifications that can lead to incorrect model predictions with potentially severe consequences.\n\n**Research Problem:**\n\nThe specific research problem addressed in this paper is how to robustly defend LLM-based classification tasks against adversarial textual attacks. Existing defense strategies, such as Adversarial Fine-Tuning (AFT), are computationally intensive and cost-prohibitive, making them less accessible and practical for many users. Additionally, due to the discrete nature of text and the combinatorial complexity of possible adversarial modifications, current methods often struggle with generalization and efficiency. Therefore, there is a need to develop an efficient, user-friendly defense mechanism that can protect LLMs from adversarial attacks without substantial retraining costs.\n\n**Relevant Prior Work:**\n\n1. **Impressive Performance of LLMs:**\n   - Large Language Models have shown remarkable capabilities in numerous natural language tasks (Minaee et al., 2024).\n   - Foundational models like Meta\u2019s LLAMA and OpenAI\u2019s ChatGPT have become critical in various industries (Kasneci et al., 2023; Thirunavukarasu et al., 2023).\n\n2. **Security Vulnerabilities:**\n   - Previous studies have highlighted the vulnerability of LLMs to adversarial textual examples (Wang et al., 2023a; Xu et al., 2024).\n\n3. **Adversarial Fine-Tuning (AFT) Methods:**\n   - Strategies involve fine-tuning the LLMs with adversarial examples to enhance robustness (Shen et al., 2023; Wang et al., 2023c), either through in-context learning (Dong et al., 2022; Xiang et al., 2024) or optimizing the parameters of pre-trained LLMs using adversarial examples (Dettmers et al., 2024; Li et al., 2024b).\n   - AFT methods face challenges such as high computational costs, combinatorial complexity, and difficulties in generalization (Hu et al., 2022; Li et al., 2023).\n\n4. **Adversarial Defense Techniques:**\n   - Previous methods required re-training and substantial computational resources for the target LLMs, thus presenting a barrier for wide adoption among ordinary users.\n\nIn conclusion, the paper introduces LLAMOS, an innovative method for adversarial defense that leverages an LLM as an agent for adversarial purification. This technique serves as a pre-processing step, effectively defending against adversarial attacks without necessitating costly retraining of the target LLM, thus addressing the critical challenges identified in prior work.",
        "methodology": "**Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent**\n\n**Methodology:**\n\nWe introduce an innovative defense technique called **LLAMOS** (Large Language Model-based Adversarial Purification), which aims to enhance the robustness of large language models against adversarial attacks. This method focuses on purification of adversarial examples by utilizing an LLM-based defense agent before these examples are presented to the target LLM.\n\nThe process of LLAMOS is systematically laid out as follows:\n\n1. **LLAMOS Pipeline (Section 3.1 ###reference_###):**\n   The primary workflow of LLAMOS involves a defense pipeline where adversarial examples go through a purification process handled by the LLM-based defense agent. This purification step ensures that the input to the target LLM is free of adversarial manipulations, thereby safeguarding the performance and integrity of the model.\n\n2. **Augmentation with In-Context Learning (Section 3.2 ###reference_###):**\n   To bolster the capability of the defense agent, we incorporate in-context learning techniques. This augmentation allows the defense agent to adapt dynamically to various contexts, enhancing its effectiveness in recognizing and mitigating adversarial threats.\n\n3. **Design of Adversarial System (Section 3.3 ###reference_###):**\n   The adversarial system is designed to integrate three key components:\n   - **Defense Agent:** This agent is responsible for the purification of inputs, making use of sophisticated LLM capabilities.\n   - **Attack Agent:** Acts as the source of adversarial examples, challenging the robustness of the system by generating inputs designed to fool the target LLM.\n   - **Target LLM:** The main model whose robustness against adversarial examples is being fortified through the intervention of the defense agent.\n\nIn essence, our LLAMOS methodology represents a significant step forward in defending large language models from adversarial attacks, enhancing their resilience and robustness through a well-structured and innovative purification process.",
        "main_experiment_and_results": "In the main experiment of the study, the researchers evaluate the effectiveness of the proposed method, LLAMOS, using the GLUE datasets. To measure performance, they focus on the attack success rate (ASR). The results indicate that LLAMOS significantly reduces the ASR, achieving a reduction of up to 37.86% when using GPT-3.5 and up to 45.59% when using LLAMA-2."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Examine the interaction between defense and attack agents in an adversarial system involving multiple large language models.",
            "experiment_process": "An adversarial system was devised involving multiple LLMs, including a defense agent and an attack agent. The attack agent generates adversarial examples from purified examples to deceive the target LLM. Specific prompts were crafted for both agents with instructions to change minimal characters while either defending or attacking. The interplay between these agents was tested to observe the continuous counteractive process, akin to adversarial training.",
            "result_discussion": "The adversarial system demonstrated a continual fluctuation in robust accuracy with repeated interactions between the defense and attack agents. Initially, the defense agent achieved 96.09% robust accuracy, but this decreased to 56.25% after the attack agent's re-attack. This cycle indicated a potential infinite loop caused by both agents generating repetitive sentences, an aspect warranting further investigation.",
            "ablation_id": "2405.20770v1.No1"
        },
        {
            "research_objective": "Evaluate the LLAMOS method's effectiveness in reducing the Attack Success Rate (ASR) and improving Robust Accuracy (RA) against various adversarial attacks.",
            "experiment_process": "Multiple evaluation metrics were employed to assess LLAMOS on various datasets and scenarios. The ASR was tested against PromptAttack-EN and PromptAttack-FS-EN on the GLUE datasets with GPT-3.5, and the SST-2 dataset with LLAMA-2. RA was evaluated against character, word, and sentence attacks on SST-2 with LLAMA-2, and additionally tested across nine attack types and six tasks with GPT-3.5. An in-context learning (ICL) method was introduced to further enhance robustness against the C3-based attack.",
            "result_discussion": "LLAMOS significantly reduced ASR and improved RA across multiple datasets. It showed an average ASR reduction of about 29% on the GLUE datasets and up to 45.59% on SST-2. The RA improvements were notable, with lowest robust accuracy reaching 86.96% under strong attacks. Introducing ICL further improved robust accuracies dramatically against specific attacks, with up to 97.66% accuracy. The experiments demonstrated LLAMOS\u2019s robust defense capabilities against diverse adversarial textual attacks.",
            "ablation_id": "2405.20770v1.No2"
        }
    ]
}