{
    "title": "OPSD: an Offensive Persian Social media Dataset and its baseline evaluations",
    "abstract": "Data-driven models have become instrumental in predictive analytics, especially in the field of text classification. Our study focuses on evaluating different machine learning algorithms to determine their efficacy in categorizing textual data.\n\n### Methods\nWe compiled a dataset consisting of labeled comments, enabling us to apply various machine learning techniques for classification tasks. Preprocessing steps included tokenization, removing stopwords, and stemming to prepare the data for analysis. Feature extraction was performed using term frequency-inverse document frequency (TF-IDF) to convert text data into numerical format suitable for machine learning models. \n\n### Naive Bayes\nNaive Bayes, based on Bayes' theorem, assumes that features are statistically independent. Despite its simplicity, it has shown to perform effectively in text classification tasks. We applied Multinomial Naive Bayes in this study due to its suitability for classification with discrete features like word counts.\n\n### Results and Analysis\nThe models' performance was evaluated using accuracy, precision, recall, and F1 score metrics. The results from Multinomial Naive Bayes indicated robust performance, particularly in terms of precision and recall. \n\n### Conclusion\nThe experiment underscores the capability of certain machine learning algorithms in handling text classification effectively. Future work will involve expanding the dataset and exploring additional preprocessing techniques to enhance model accuracy and generalization.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Social media platforms have experienced a surge in popularity in recent years, providing users with a platform to express their opinions across various forms of media. Among these forms, textual data holds significant importance as a means of conveying meaning and ideas. Unfortunately, it is not uncommon for certain individuals to incorporate abusive language within their sentences, resulting in offensive comments. Detecting such instances manually would be a time-consuming and expensive endeavor. However, thanks to advancements in Natural Language Processing (NLP) applications, it has become possible to automatically identify and classify these comments as offensive or non-offensive. This task can be approached as a classification problem, requiring a suitable dataset consisting of sentences of this nature. Regrettably, to the best of our knowledge, there are only few datasets available for offensive comment detection. Leveraging these NLP applications can prove instrumental in filtering offensive content from social media platforms, particularly for children or individuals who prefer not to be exposed to such material.\n\nIn this study, we collected approximately 5,000 and 17,000 comments from two widely used social media platforms, namely Instagram and Twitter, respectively. To ensure accurate labeling of the data, a meticulous three-phase annotation process was employed. In addition, a collection of 170,000 unlabeled data points in Persian was gathered using offensive keywords and hashtags. The primary contributions of this study can be summarized as follows:\n\nIntroduction of the offensive Persian Social Media Dataset (OPSD).\nInvestigation of the impact of masked language modeling on pre-trained language models using the unlabeled portion of the dataset.\nComprehensive experiments were conducted on the annotated dataset, involving transformer-based language models.\n\nThe remaining sections of this article are organized as follows: Section 2 reviews previous studies relevant to datasets in the field. Section 3 explains the data collection process in detail. Section 4 discusses the annotation methodology employed. Section 5 presents the extensive experiments conducted and the establishment of baselines for the dataset using transformer-based language models. Finally, Section 6 presents the concluding remarks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "In this section, we first review prior work on offensive\nlanguage datasets in the English language and then\npresent the related literature on offensive datasets in the Persian language."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "English Offensive Datasets",
            "text": "The OLID [1  ###reference_b1###] dataset is designed for the detection (Subtask-A), categorization (Subtask-B), and identification of offensive language (Subtask-C). Subtask-A focuses on binary classification with \u2019NOT\u2019 and \u2019OFF\u2019 labels. \u2019NOT\u2019 indicates posts without any offensive content, while \u2019OFF\u2019 refers to posts that include insults, threats, profanity, or swear words. The OLID dataset consists of 14,100 samples, with 13,240 samples for training and 860 samples for testing set.\nThe Semi-Supervised Offensive Language\nIdentification Dataset (SOLID) [2  ###reference_b2###] was introduced to address the scarcity of offensive language instances in the OLID dataset and the time-consuming annotation process. SOLID utilized a semi-supervised approach, starting with the OLID dataset, to collect new offensive data. This novel dataset overcomes the limitations of existing datasets and provides a substantial collection of English tweets for offensive language identification. SOLID demonstrates improved offensive language detection capabilities and includes a comprehensive analysis of various types of offensive tweets. The hierarchical labeling schema used in SOLID and related works is based on the OLID dataset.\nHSOL [3  ###reference_b3###] is a dataset created for the purpose of hate speech detection. The authors began by utilizing a hate speech lexicon, which consisted of words and phrases identified as hate speech by internet users and compiled by Hatebase.org. By using the Twitter API, they searched for tweets that contained terms from the lexicon. This process resulted in a sample of tweets collected from 33,458 Twitter users. The authors then extracted the timeline of each user, resulting in a corpus of 85.4 million tweets. From this corpus, a random sample of 25,000 tweets that contained lexicon terms was selected for manual coding by CrowdFlower (CF) workers. The workers were tasked with labeling each tweet into one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Persian Offensive Datasets",
            "text": "The Persian Offensive Language Identification Dataset (POLID) is one of the pioneering datasets in this domain, developed by [4  ###reference_b4###]. The dataset encompasses a diverse range of text data, including tweets, Instagram comments, and user reviews sourced from popular Iranian web applications such as Digikala and Snappfood. The annotation process for POLID involved a semi-automatic approach. Initially, a basic list of common swear words was used to label each text as either \u201dOFF\u201d (offensive) if it contained any of these words, or \u201dNOT\u201d (inoffensive) if it did not. Subsequently, the authors manually reviewed and corrected the labels assigned to the text data. POLID consists of 4,988 text entities, with 2,453 labeled as inoffensive and 2,535 as offensive.\nMozafari [5  ###reference_b5###] employed both random and lexicon-based sampling methods to collect tweets from June to August 2020. In the random sampling approach, tweets were selected randomly and evaluated by two experts. The analysis revealed that only a maximum of 2% of the selected tweets contained offensive content, resulting in an imbalanced sampling. On the other hand, the lexicon-based sampling involved the utilization of a word list to filter tweets, ensuring a more unbiased representation across different topics. The random sampling yielded 320,000 tweets, while the lexicon-based sampling resulted in 200,000 tweets. Approximately 6,000 of the collected tweets were manually annotated.\nThe PerBOLD dataset [6  ###reference_b6###] focuses on the acquisition and annotation process for building a dataset aimed at automatic detection of offensive speech on Instagram. Two approaches were employed to collect textual data: a user-based approach targeting controversial users\u2019 pages and a news agencies-based approach focusing on comments published on news agency pages. Due to restrictions imposed by Instagram, data extraction was conducted partially from the platform itself and partially from the picuki website. The collected data underwent an annotation process, taking into consideration the subjective nature of offensiveness and the necessity for accurate tagging. A sample of approximately 30,000 comments was randomly selected and labeled by three annotators, with the involvement of a linguist expert to resolve any inconsistencies. The annotators classified comments as offensive, non-offensive, or advertisement, while offensive comments were further categorized into various types such as curse, insult, sexist, racist, and more. This work underscores the significance of precise tagging for effective machine learning classifiers. Overall, the paper highlights the challenges and methodologies involved in creating a dataset for offensive speech detection on Instagram.\nPars-OFF [7  ###reference_b7###] is a comprehensive annotated corpus comprising 10,563 data samples for offensive language detection in Persian. To ensure balance in the dataset, a combination of similarity-based and keyword-based data selection techniques were employed during the collection of tweets. Additionally, this paper presents an evaluation of traditional machine learning approaches and Transformer-based models on the Pars-OFF dataset as a baseline. The BERT+fastText model achieved the highest performance, attaining an impressive F1-Macro score of 89.57%."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Data collection",
            "text": "In this section, we introduce OPSD and delve into how the dataset was collected and annotated. This section describes the data collection process for OPSD (Twitter), OPSD (Instagram), and also OPSD (Unlabeled), as demonstrated in Subsections 3.1  ###reference_###, 3.2  ###reference_###, and 3.3  ###reference_###, respectively."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "OPSD (Twitter)",
            "text": "To collect data from Persian Twitter, a list of keywords was compiled by inspecting trending hashtags and topics known to contain a higher percentage of offensive content. Over 40,000 records were extracted by searching for these keywords. An initial pre-processing step was conducted to remove records from company support accounts and eliminate duplicates. From each keyword, a subset containing more than 16,000 records was sampled. Finally, these samples were aggregated to form a dataset of 17,000 records."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "OPSD (Instagram)",
            "text": "The data collection process for OPSD (Instagram) involved crawling over 13,000 records from the Instagram platform. Specifically, a set of 100 comments from different posts was extracted from the accounts of an IT service company. Upon examination, it was discovered that a significant amount of the collected data was unusable. Therefore, an initial pre-processing step was carried out to remove records that contained less than one Persian word, after removing emojis, emoticons, hashtags, mentions, links, and all non-Persian text. Due to the diverse nature of the posts and the evenly distributed number of collected data from each post, a sample of 5,000 records was extracted from the crawled data."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "OPSD (Unlabeled)",
            "text": "The OPSD (Unlabeled) dataset was collected following a similar procedure as described in Subsection 3.1  ###reference_###. In this case, a collection of over 140,000 tweets were crawled from Twitter. An initial pre-processing step similar to previous datasets was applied. Finally, these records formed the OPSD (Unlabeled) dataset."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Annotation",
            "text": "The OPSD dataset is annotated with three labels, which are defined as follows:\nPOS (Positive): A comment is labeled as POS if it does not contain any destructive concepts, such as hate speech, offensive language, or abusive content.\nT-NEG (Targeted Negative): Comments with the T-NEG label specifically target individuals, companies, or organizations with hateful and offensive content.\nNT-NEG (Non-Targeted Negative): These comments contain hateful and offensive terms without any identifiable target."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Annotation Process",
            "text": "The annotation process for this study consists of three phases, involving five different annotators. In the first phase, the data were divided into five parts, and each part was annotated by two annotators. The Cohen\u2019s kappa score was then calculated between the two annotators. If the Cohen\u2019s kappa score was below 75%, indicating poor agreement, the process moved to the second phase.\nIn the second phase, a consensus meeting was conducted to discuss the reasons behind the poor results. Based on the discussions, the annotation guidelines were modified as necessary. The annotators then re-labeled the data on which they had disagreed in the previous phase.\nOnce the inter-annotator agreement reached a minimum threshold of 75%, the process moved to the third phase. In this phase, any remaining points of disagreement were resolved by a third annotator. The final label for each data point was determined based on the majority vote among the annotators.\nFigure 1  ###reference_### provides an overview of the annotation process.\n###figure_1###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Annotation Reliability",
            "text": "The annotation reliability in this study was assessed using Cohen\u2019s kappa coefficient. Cohen\u2019s kappa measures the agreement between annotators beyond what would be expected by chance.\nCohen\u2019s kappa is calculated as follows:\nwhere  is the percentage of observed agreement between annotators and  is the percentage of agreement expected by chance.\nTwo cases of labels were considered for calculating Cohen\u2019s kappa. The first case involved the 3-class labeling, where each class of data was assigned a unique label. For the NEG+ case, which was used for binary classification, the NT-NEG and T-NEG labels were combined.\nIt is important to note that we combined two separate OPSD datasets, namely OPSD (Twitter) and OPSD (Instagram), and subsequently annotated the combined OPSD dataset.\nTable 1  ###reference_### provides the Kappa scores for the OPSD dataset. The Phase column represents the first and the second phase of the labeling process. The Part column refers to five parts of the dataset, where each was annotated by two different annotators. The Annotator column indicates the five different annotators involved in the labeling process. The next three columns denote the number of comments labelled for each class. The Total and Common columns show the number of comments in each part, and the number of comments for which the annotators agreed on the label, respectively. Additionally, the Difference column indicates the number of differences in labeling between the two annotators. Lastly, the last column shows the Kappa score, considering the three classes, for each part of the dataset.\nThe average agreement scores per annotation phase are presented in Table 2  ###reference_###. This table provides information on the number of instances for each label in each phase, the number of agreements and disagreements between annotators, and the average kappa score for the five parts of the dataset.\nTable 3  ###reference_###, presents a selection of example samples from the dataset, showcasing the Persian text associated with each label.\nLabel\nComment\nPOS\nFor about a year now, the internet quality of Irancell has deteriorated significantly, to the point that I no longer use it.\nPOS\nWhat is this previous debt on the Hamrah Aval (MCI) bill? For each billing cycle that you have an internet plan, the bill shows the outstanding balance from the previous month, which you pay along with the current month\u2019s charges.\nPOS\nHamrah Aval (MCI) has become quite burdensome with its expensive packages lately \u2026 Come on, have a little mercy on people\u2019s situation.\nNT-NEG\nI see the gathering of foolish rulers here, don\u2019t they have your national code to ask you to enter it?\nNT-NEG\nIs your internet speed also sluggish?\nNT-NEG\nHe\u2019s expelled his essence, yet you\u2019re still recharging; you pour out more of him, the stench of his putrefaction spreads, and it contaminates everything around.\nT-NEG\nThe shameless ones have deleted the unlimited package.\nT-NEG\nI need to go to the bathroom; the package doesn\u2019t work without electricity. My phone\u2019s battery is at 4%, the modem is off, and Irancell hasn\u2019t left any balance. In the parking lot, the gate doesn\u2019t open without power, and my car is inside the parking lot, and I can\u2019t do anything. Curse your head and your nonsense.\nT-NEG\nYour driving is awful, people of Tabriz\u2026 I don\u2019t know about other cities, but I hope it\u2019s not as bad as this. If it is, even the ISIS government would be better for you."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Data Analysis",
            "text": "As previously mentioned, a total of 21,165 comments were collected and categorized into Positive, Targeted Negative, and Non-Targeted Negative. The distribution of these tags among the comments is illustrated in Figure 2  ###reference_### and the specific counts for each class are provided in Table 4  ###reference_###\n###figure_2### After a pre-processing step, which involved normalization and removal of numbers, URLs, email addresses, and special characters, the distribution of comments length (number of tokens) excluding emojis is presented in Figure 3  ###reference_###. The distribution of comment lengths reveals a prominent skew towards shorter texts, which coincides with the prevailing observation concerning comments in social media platforms.\n###figure_3### Moreover, the unprocessed OPSD (unlabeled) data also aligns with the same trend, as illustrated in Figure 4  ###reference_###.\n###figure_4###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We trained a classifier and compared multiple models using both supervised and unsupervised approaches. In supervised methods, we utilize machine learning and Transformer-based models, while in unsupervised methods, we employ a masked language model (MLM).\n\nThe dataset used in our classification experiments contains 21,165 entries and is split into a training set (80%), development set (10%), and test set (10%). Hyperparameters are tuned on the development set.\n\nWe employed several machine learning methods, which served as baseline text categorization models. The feature vectors were constructed using the TF-IDF and Count Vectorizer methods.\n\nWe employed a single decision tree for classification. The maximum depth of the tree was set to None, allowing nodes to expand until all leaves were pure or contained a minimum number of samples required for splitting, and the minimum samples required to split a node was set to 2. For the Random Forest Classifier, we employed 200 decision trees as estimators. The maximum depth of the trees was not restricted (None), and the minimum number of samples required to split a node was set to 2. The Gaussian Naive Bayes algorithm assumes that features follow a Gaussian distribution. It is particularly useful for continuous features. As it doesn\u2019t have any hyperparameters to tune, we used it with its default settings.\n\nIn our experimental analysis, we employed the following transformer-based models, including multilingual and monolingual models, to comprehensively explore their respective performances and capabilities.\n\nParsBERT is a monolingual language model based on the BERT architecture. It is pre-trained on more than 3.9 million Persian documents, allowing it to capture contextual representations of Persian text.\n\nALBERT is a \"Lite\" version of BERT designed to improve memory efficiency. It employs parameter-reduction techniques such as dividing the embedding matrix into smaller matrices and using shared parameters across different BERT hidden layers. ALBERT-fa is a monolingual language model based on the ALBERT architecture, specifically trained on Persian data.\n\nRoBERTa is a variant of the BERT model that employs a larger training dataset and longer training duration to capture more extensive language representations. It removes the next sentence prediction objective used in BERT and utilizes dynamic masking during pretraining. RoBERTa-fa is a variant of RoBERTa trained specifically on Persian data.\n\nXLM-RoBERTa (XLM-R) is a variant of RoBERTa that incorporates cross-lingual language modeling capabilities. It is designed for multilingual tasks, enabling effective understanding and processing of text in multiple languages. XLM-RoBERTa leverages pre-training on large-scale multilingual corpora to capture cross-lingual knowledge.\n\nFor the fine-tuning of these models, we added a fully-connected layer to the pre-trained model. We used weighted cross-entropy loss as the objective function, a batch size of 32, a maximum sequence length of 128, a learning rate of , a dropout rate of 0.5, and the number of epochs 10. The AdamW optimizer with a linear learning rate scheduler (without warm-up steps) was used.\n\nTo further enhance the model\u2019s performance, we conducted an extensive hyperparameter tuning process. We experimented with various hyperparameter configurations, exploring batch size , maximum sequence length , learning rate , and dropout rate in the range of . Through this thorough grid search approach, we aimed to find the most suitable hyperparameters for our specific task.\n\nAfter conducting the hyperparameter tuning, we found that the initial hyperparameter values provided the best performance on our evaluation metrics. Consequently, we decided to retain these hyperparameters for the final training of models.\n\nTo evaluate models on the dataset, we used a multilingual model and three monolingual models that are specifically pre-trained on Persian data. According to table 6, XLM-RoBERTa has performed best accuracy and F1-score on the test data in both NEG+ and 3-class.\n\nIn this section, we present an experiment conducted to investigate the effects of additional training of LLMs using OPSD (unlabeled) data. Specifically, the approach involved training the LLM with OPSD (unlabeled) through Masked Language Modeling (MLM). Subsequently, the LLM was trained on Sentiment Analysis task (SA) using OPSD (labeled) data to observe potential improvements.\n\nThe ALBERT-fa, ParsBERT, RoBERTa-fa, and XLM-RoBERTa models underwent additional training using the following parameters: epoch=50, learning rate=2e-6, max sequence length=128, and batch sizes of 8, 16, 32, and 64 on MLM task. Subsequently, the proposed models were fine-tuned in the same manner as in previous experiments on the SA task, and the corresponding results are presented in Table 7.\n\nThe comparison of results between Table 6 and Table"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Error Analysis of the Best Model (XLM-RoBERTa)",
            "text": "To gain deeper insights into the performance of the best models in our experiment, we conduct an error analysis using their predictions. This analysis allows us to examine cases where the model might have made mistakes and provides valuable information for understanding its strengths and weaknesses. XLM-RoBERTa was chosen for this analysis due to its outstanding performance on the validation and test sets during our experimental evaluations. Figure 5  ###reference_### shows the confusion matrices of the XLM-RoBERTa model on the test set for both NEG+ and 3-class cases.\n\nError analysis for the 3-class case shows that misclassification on the test data can be categorized in two groups. The first group is the data samples that have POS labels, but the model predicts T-NEG or NT-NEG labels (FN) based on the existence of some specific words. The observed issue arises from the discrepancy in the frequency of certain words within comments labeled as T-NEG and NT-NEG, as opposed to the significantly lower occurrence of these words in positive comments. This imbalance in word distribution could potentially lead to difficulties in effectively classifying comments with T-NEG and NT-NEG labels, as the model may not have encountered sufficient positive instances to learn from. Consequently, this disparity in the occurrence of specific words between different classes might contribute to the model\u2019s challenges in accurately distinguishing between the negative and positive comments.\n\nThe second group is the data samples that are not correctly labeled, even with high agreement between two annotators. This means that it seemed the model had predicted the labels correctly, but the data samples had been mislabeled. If we remove these data points from the test dataset, the results in Table 8  ###reference_### are obtained, showing an approximately 3% increase in accuracy for the XLM-RoBERTa model."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we introduced OPSD, a dataset consisting of Twitter and Instagram comments written in Persian. The dataset is specifically designed to facilitate the detection of offensive language in text. We applied a three-phase process for annotating the comments in the dataset manually. In the first phase, the dataset is divided into five parts, and each part is annotated by two annotators. If the agreement falls below 75%, the process moves to the second phase. In the second phase, a meeting is conducted to discuss the low Kappa scores, leading to updates in annotation guidelines. The annotators then re-label the previously disagreed data. Finally, in the third phase, any remaining disagreements are resolved by a third annotator, determining the final label based on the majority vote among the annotators. Our proposed dataset, OPSD, includes an unlabeled subdivision known as OPSD (Unlabeled). This particular subset was utilized for an additional training stage on the Masked Language Modeling task. The results demonstrated that incorporating this training step with OPSD (unlabeled) data prior to conducting the sentiment analysis task enhanced the performance of baseline text classification models. We applied baseline text classification models, by using machine learning models and also state-of-the-work monolingual and multilingual BERT-based models, to show the effectiveness of the dataset. In the future, there is a plan to increase the size of the OPSD dataset. This would require considering semi-supervised or unsupervised techniques to determine offensive scores and classify textual entities, as labeling all items would be time-consuming. Additionally, improving preprocessing modules, such as converting emojis to their Persian textual descriptors, can enhance performance. Considering language structures and exploring other approaches, such as deep learning methods, for determining probabilities (offensive scores) can further improve the proposed methodology. \n\nAcknowledgments This publication was supported by grant No. RD-53-0105-0012 from the R&D Center of Mobile Telecommunication Company of Iran (MCI) for advancing information and communications technologies."
        }
    ],
    "url": "http://arxiv.org/html/2404.05540v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5"
        ]
    },
    "research_context": {
        "paper_id": "2404.05540v1",
        "paper_title": "OPSD: an Offensive Persian Social media Dataset and its baseline evaluations",
        "research_background": "### Motivation\nThe motivation for this paper arises from the increasing prevalence of social media platforms and the corresponding rise in the use of abusive language within these platforms. Detecting offensive comments is crucial for maintaining a respectful online environment, but manual detection is both time-consuming and costly. The advent of Natural Language Processing (NLP) applications offers a feasible solution by enabling the automatic identification and classification of offensive comments.\n\n### Research Problem\nThe primary research problem addressed in this paper is the lack of comprehensive datasets specifically designed for detecting offensive comments in Persian social media contexts. The study aims to fill this gap by introducing a new dataset, the Offensive Persian Social Media Dataset (OPSD), and evaluating its effectiveness through various NLP approaches, including transformer-based language models and traditional machine learning techniques.\n\n### Relevant Prior Work\nThe paper acknowledges that while there have been efforts to create datasets for detecting offensive comments, these resources are limited, particularly for the Persian language. Existing studies in the field have predominantly focused on other languages and platforms, emphasizing the need for language-specific and culturally relevant datasets. This study builds on the principles and methodologies used in prior research to create a specialized dataset that can enhance NLP applications for Persian social media users, ensuring a safer online experience.",
        "methodology": "Methodology: In this section, we introduce OPSD and delve into how the dataset was collected and annotated. This section describes the data collection process for OPSD (Twitter), OPSD (Instagram), and also OPSD (Unlabeled), as demonstrated in Subsections 3.1 ###reference_###, 3.2 ###reference_###, and 3.3 ###reference_###, respectively.",
        "main_experiment_and_results": "### **Main Experiment Setup:**\n\n#### **Dataset:**\nThe dataset used in the classification experiments contains a total of 21,165 entries. It is divided into:\n- Training set: 80%\n- Development set: 10%\n- Test set: 10%\n\nHyperparameters are tuned using the development set.\n\n#### **Baselines and Models:**\n\nThe experiment involves both supervised and unsupervised approaches:\n\n**Supervised Approaches:**\n1. **Machine Learning Methods**:\n   - **Feature Vectors**: TF-IDF, Count Vectorizer\n   - **Algorithms**:\n     - Logistic Regression (regularization parameter (C))\n     - K-Nearest Neighbors (3 neighbors, Euclidean distance)\n     - SVM (RBF kernel, C = 0.025, gamma = 'scale')\n     - Decision Tree (maximum depth: None, minimum samples to split: 2)\n     - Random Forest Classifier (200 estimators, maximum depth: None, minimum samples to split: 2)\n     - Gaussian Naive Bayes (default settings)\n\n2. **Transformer-Based Models**:\n   - **ParsBERT**: Monolingual Persian BERT model.\n   - **ALBERT-fa**: Monolingual ALBERT model trained on Persian data.\n   - **RoBERTa-fa**: Persian variant of RoBERTa.\n   - **XLM-RoBERTa (XLM-R)**: Multilingual variant of RoBERTa.\n\n   **Fine-Tuning Settings**:\n   - Fully-connected layer added for classification.\n   - Weighted cross-entropy loss.\n   - Batch size: 32.\n   - Maximum sequence length: 128.\n   - Learning rate: Variable (tuned).\n   - Dropout rate: 0.5.\n   - Number of epochs: 10.\n   - Optimizer: AdamW with linear learning rate scheduler.\n\n**Unsupervised Approach:**\n- **Masked Language Model (MLM)**\n\n#### **Evaluation Metrics:**\n- Accuracy\n- F1-score\n\n### **Main Experimental Results:**\n\n- Among the transformer-based models, **XLM-RoBERTa** achieved the best performance in both accuracy and F1-score for the NEG+ and 3-class classification problems.\n- Results indicate that multilingual and monolingual models pre-trained on large corpora specific to Persian language perform better in text classification tasks.\n\nIn summary, the main findings highlight the effectiveness of XLM-RoBERTa for Persian text classification."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate the performance of various machine learning and Transformer-based models on a Persian social media dataset for offensive language detection.",
            "experiment_process": "We trained a classifier and compared multiple models using both supervised (machine learning and Transformer-based models) and unsupervised methods (masked language model - MLM). The dataset contains 21,165 entries divided into training (80%), development (10%), and test (10%) sets. Hyperparameters were tuned on the development set. The machine learning methods used include logistic regression, K-Nearest Neighbors (KNN), Support Vector Machine (SVM), decision tree, Random Forest Classifier, and Gaussian Naive Bayes. For Transformer-based models, multilingual and monolingual models, including ParsBERT, ALBERT-fa, RoBERTa-fa, and XLM-RoBERTa, were fine-tuned with a fully-connected layer added to the pre-trained model using weighted cross-entropy loss, a batch size of 32, maximum sequence length of 128, learning rate of 0.01, dropout rate of 0.5, and 10 epochs. The AdamW optimizer with a linear learning rate scheduler was used.",
            "result_discussion": "The comparison showed that XLM-RoBERTa achieved the best accuracy and F1-score on the test data in both NEG+ and 3-class classification tasks.",
            "ablation_id": "2404.05540v1.No1"
        },
        {
            "research_objective": "To evaluate the impact of additional masked language model (MLM) training on pre-trained models using the OPSD dataset for improving Sentiment Analysis (SA) task performance.",
            "experiment_process": "The ALBERT-fa, ParsBERT, RoBERTa-fa, and XLM-RoBERTa models underwent additional training using OPSD (unlabeled) with Masked Language Modeling (MLM) for 50 epochs, a learning rate of 2e-6, max sequence length of 128, and batch sizes of 8, 16, 32, and 64. Subsequently, the models were fine-tuned for the Sentiment Analysis task using the same method as previous experiments (objective function: weighted cross-entropy loss, batch size 32, maximum sequence length 128, learning rate 0.01, dropout rate 0.5, and 10 epochs using AdamW optimizer with linear learning rate scheduler).",
            "result_discussion": "Further MLM training improved model performance, particularly in terms of the F1-score. For example, the Roberta model fine-tuned on OPSD (3-class) showed a 4.06% increase in F1-score.",
            "ablation_id": "2404.05540v1.No2"
        }
    ]
}