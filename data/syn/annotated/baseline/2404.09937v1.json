{
    "title": "Compression Represents Intelligence Linearly",
    "abstract": "There is a belief that learning to compress well will lead to intelligence (Hutter, 2006). Recently, language modeling has been shown to be equivalent to compression, which offers a compelling rationale for the success of large language models (LLMs): the development of more advanced language models is essentially enhancing compression which facilitates intelligence. Despite such appealing discussions, little empirical evidence is present for the interplay between compression and intelligence. In this work, we examine their relationship in the context of LLMs, treating LLMs as data compressors. Given the abstract concept of \u201cintelligence\u201d, we adopt the average downstream benchmark scores as a surrogate, specifically targeting intelligence related to knowledge and commonsense, coding, and mathematical reasoning. Across 12 benchmarks, our study brings together 30 public LLMs that originate from diverse organizations. Remarkably, we find that LLMs\u2019 intelligence \u2013 reflected by average benchmark scores \u2013 correlates with their ability to compress external text corpora. These results provide concrete evidence supporting the belief that superior compression indicates greater intelligence. Furthermore, our findings suggest that compression efficiency, as an unsupervised metric derived from raw text corpora, serves as a reliable evaluation measure that is associated with the model capabilities. We open-source our compression datasets as well as our data collection pipelines to facilitate future researchers to assess compression properly.111https://github.com/hkust-nlp/llm-compression-intelligence.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The belief that compression is closely connected to intelligence has been held for a long time, with some researchers suggesting that they are fundamentally equivalent. This notion becomes even more compelling in light of the recent advancements in large language models (LLMs) and their implications for AI, leading to an exploration of language modeling from a compression standpoint.\n\nAccording to compression theory, any predictive model can be transformed into a lossless compressor and vice versa. Thus, language modeling can be considered a form of compression, with LLMs showing strong capabilities in data compression empirically. From this viewpoint, the argument that compression leads to intelligence becomes more relevant to the current paradigm of AI based on LLMs. Yet, despite these theoretical discussions, empirical evidence on the relationship between compression and intelligence remains limited. In this work, we seek to conduct such an empirical study and answer: If a language model can encode a text corpus with fewer bits in a lossless manner, does that indicate greater intelligence?\n\nWhile the precise definition of \u201cintelligence\u201d often ventures into philosophical territory and can vary significantly, our work approaches \u201cintelligence\u201d from a practical standpoint, focusing on the model\u2019s ability to complete various downstream tasks. This treatment aligns with the definition from Hutter, who notes that \u201cintelligence is an agent\u2019s ability to achieve goals in a wide range of environments.\u201d\n\nWe measure intelligence along three key abilities: knowledge and commonsense, coding, and mathematical reasoning. Specifically, we begin by collecting external raw corpora in the respective domain (e.g. GitHub code for the coding ability) and evaluate the efficiency of various LLMs in compressing the corpus. Subsequently, we assess these models on a range of downstream tasks, using the average benchmark scores to gauge their domain-specific intelligence.\n\nOn experiments across 30 public LLMs and 12 diverse benchmarks, we uncover a remarkable finding: LLMs\u2019 downstream ability is almost linearly correlated with their compression efficiency. Notably, this linear relationship even extends to most individual benchmarks as well. Recent and concurrent works have explored the relationship between benchmark scores and compression-equivalent metrics like validation loss within the same model series, where the model checkpoints share most configurations such as model designs, tokenizers, and data. Our study, however, is the first to document a linear correlation between compression and intelligence in LLMs across varying model sizes, tokenizers, context window lengths, and pretraining data distributions. Our findings establish the linear correlation between compression and intelligence as a universal principle, providing empirical support for the longstanding belief that superior compression is indicative of greater intelligence.\n\nFrom a practical perspective, compression efficiency serves as an unsupervised metric for LLMs where the text corpora can be easily updated to avoid overfitting or test contamination. Our findings advocate for adopting compression efficiency as a stable, flexible, and reliable metric to evaluate LLMs, which is linearly correlated with the models\u2019 abilities. We open-source our compression corpora, as well as the data collection and processing pipelines so that future researchers can easily collect and update their own corpora to assess compression."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background: Language Modeling is Compression",
            "text": "The discussion on the equivalence between language modeling and compression has a long history (Shannon, 1948; 1951). Recently, Deletang et al. (2024) have echoed this connection and demonstrated that language models are general-purpose compressors empirically. Below, we briefly introduce the background to acquaint readers with such a relationship.\n\nSuppose a distribution generates text sequence data. Given a text sequence, lossless compression seeks to encode it into a code that is smaller in size while preserving all original information. In practice, however, this distribution is often unknown, and we can use a language model to help compress data efficiently. Intuitively, we aim to encode rare sequences with more consideration and the frequent ones with less.\n\nThis implies that if a language model has a lower loss on the data to be compressed, it will be able to compress it effectively in a lossless manner. Strictly speaking, one needs to access the language model parameters to recover the original data from the code, thus the cost required to encode these parameters should be counted as well. However, such a cost can be amortized and becomes negligible when the model is used to compress a substantially large volume of data.\n\nThere are online or offline settings to compress a data stream. In the online setting, the model is randomly initialized and trained on the data to be compressed. In the offline setting, the model is trained on external data and adopted to compress a different dataset. We focus on the offline setting in this paper, using pretrained language models to compress external corpora."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Examining Compression and Intelligence \u2013 the Experimental Protocols",
            "text": "Our goal is to empirically examine the relationship between compression and intelligence in the context of large language models. Our method is to investigate a variety of pretrained LLMs of different sizes and from diverse organizations, assessing their compression efficiency and \u201cintelligence\u201d respectively. Then we aim to summarize the potential relationship between compression and intelligence based on the evaluation results. Our overall principle is to collect diverse LLMs that are created with different training data, tokenizers, computation, and even architectures (e.g., mixture of experts as in MistralAI (2023)), so that our conclusion on compression and intelligence is general and agnostic to specific model series.\n\nNext, we detail our experimental protocols to evaluate compression and intelligence. LLMs can have different context window sizes, and a longer context window gives advantages to compression efficiency. This is because a larger context window offers more information to predict the next token\u2019s distribution and allows for encoding with fewer bits. However, in downstream benchmarks where input lengths are short, the benefit of extended context windows diminishes. This applies to all our benchmarks in Table 1, where the inputs do not exceed 2048 tokens, even in few-shot in-context learning scenarios. Basically, the downstream tasks in our study only reflect the model\u2019s intelligence when operating within relatively short contexts. To study its correlation with compression, it is crucial to assess compression using comparable context length, ensuring consistency in the models\u2019 access to information during both corpus compression and execution of downstream tasks. Therefore, for all our compression and benchmark evaluations across different LLMs, we unify the context window size to be 1900 tokens that are sufficient for all benchmark evaluation. Ideally, a more holistic evaluation of intelligence should incorporate tasks involving both short and long contexts. However, in this paper, our focus is on the short- or medium-context regime, which encompasses the majority of benchmark tasks. We leave the exploration of long-context scenarios for future work. Apart from context window unification, we utilize a sliding window approach to evaluate compression more accurately, as detailed in Appendix A.1.\n\nIn the development of LLMs, there are typically two stages: the pre-training stage and the alignment stage. The models are referred to as base models after the pre-training stage and as fine-tuned models following the alignment stage. We note that fine-tuned models are no longer general-purpose compressors since it does not model the next token distribution for arbitrary text, but only for structured (query, response) data. Besides, it is commonly believed that the intelligence of LLMs is learned during the pretraining stage and remains relatively fixed during the alignment stage (Zhou et al., 2023). Therefore, in this work, we focus on the base models only, while in \u00a75 we will further discuss the case of compression v.s. intelligence for fine-tuned models.\n\nWhat kind of corpus shall we select to measure the models\u2019 compression? Firstly, it is important to recognize that different corpora can illuminate various aspects of models\u2019 abilities, and compressing one corpus well may not generalize to another (Magnusson et al., 2023). Therefore, we would like to select the corpora that align with the areas of our focus. Secondly, the chosen corpora should not intersect with the models\u2019 pretraining data to avoid data leakage. Given the opaque status of LLMs\u2019 pretraining datasets, we opt to use the newest corpora as a measure. Concretely, for assessing knowledge and commonsense, we have compiled texts from the latest Common Crawl dataset. To evaluate coding ability, we have sourced data from GitHub repositories mainly on the Python language since the downstream benchmarks focus on Python coding abilities. For mathematical reasoning, we collect academic papers from ArXiv, specifically selecting those designated with \u201cmath\u201d as their primary category. For each data source, we ensure the recency by utilizing the latest available data at the time of our experiments. The composition of our compression corpora is detailed in Table 1. More details about data collection and processing are in Appendix A.2."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Evaluating Intelligence of LLMs",
            "text": "The definition of \u201cintelligence\u201d has been a subject of debate for years. Alan Turing firstly proposed the definition based on the Turing Test, which evaluates a machine\u2019s ability to exhibit intelligent behavior indistinguishable from that of a human. Although the Turing test is probably the most commonly accepted definition for intelligence, the application of the Turing Test as a measure for AI is deemed impractical, primarily because the evaluation\u2019s outcome relies heavily on the subjective judgment of the interrogator.\n\nMore recently, Legg & Hutter proposed a definition termed universal intelligence, a broader interpretation than Turing\u2019s focus on human-like intelligence. They posited that an agent\u2019s capacity to achieve goals across a wide array of scenarios should define its intelligence. This practical approach to defining intelligence guides our evaluation of models, wherein we assess their intelligence based on average performance across multiple downstream tasks. Such practical evaluation is commonly adopted by most LLM developers to showcase the potential of their models, and Anthropic noted the \u201cintelligence\u201d of Claude 3 with the average benchmark scores.\n\nSpecifically, in this paper, we study intelligence along three key abilities: knowledge and commonsense, coding, and mathematical reasoning. These abilities represent the most focused LLM areas nowadays, and we collect the well-recognized benchmarks for each of these areas, utilizing their average score to indicate domain-specific intelligence. The benchmarks are listed in Table 1."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Evaluating Compression of LLMs",
            "text": "According to \u00a72, we evaluate the model\u2019s compression efficiency using the average coding length per message of compressing a given corpus.\n\nLLMs can have different context window sizes, and a longer context window gives advantages to compression efficiency. This is because a larger context window offers more information to predict the next token\u2019s distribution and allows for encoding with fewer bits.\n\nHowever, in downstream benchmarks where input lengths are short, the benefit of extended context windows diminishes. This applies to all our benchmarks in Table 1, where the inputs do not exceed 2048 tokens, even in few-shot in-context learning scenarios. Basically, the downstream tasks in our study only reflect the model\u2019s intelligence when operating within relatively short contexts. To study its correlation with compression, it is crucial to assess compression using comparable context length, ensuring consistency in the models\u2019 access to information during both corpus compression and execution of downstream tasks.\n\nTherefore, for all our compression and benchmark evaluations across different LLMs, we unify the context window size to be 1900 tokens that are sufficient for all benchmark evaluation. Ideally, a more holistic evaluation of intelligence should incorporate tasks involving both short and long contexts. However, in this paper, our focus is on the short- or medium-context regime, which encompasses the majority of benchmark tasks. We leave the exploration of long-context scenarios for future work. Apart from context window unification, we utilize a sliding window approach to evaluate compression more accurately, as detailed in Appendix A.1.\n\nIn the development of LLMs, there are typically two stages: the pre-training stage and the alignment stage. The models are referred to as base models after the pre-training stage and as fine-tuned models following the alignment stage. We note that fine-tuned models are no longer general-purpose compressors since it does not model the next token distribution for arbitrary text, but only for structured (query, response) data. Besides, it is commonly believed that the intelligence of LLMs is learned during the pretraining stage and remains relatively fixed during the alignment stage. Therefore, in this work, we focus on the base models only, while in \u00a75 we will further discuss the case of compression v.s. intelligence for fine-tuned models.\n\nWhat kind of corpus shall we select to measure the models\u2019 compression? Firstly, it is important to recognize that different corpora can illuminate various aspects of models\u2019 abilities, and compressing one corpus well may not generalize to another. Therefore, we would like to select the corpora that align with the areas of our focus. Secondly, the chosen corpora should not intersect with the models\u2019 pretraining data to avoid data leakage. Given the opaque status of LLMs\u2019 pretraining datasets, we opt to use the newest corpora as a measure.\n\nConcretely, for assessing knowledge and commonsense, we have compiled texts from the latest Common Crawl dataset. To evaluate coding ability, we have sourced data from GitHub repositories mainly on the Python language since the downstream benchmarks focus on Python coding abilities. For mathematical reasoning, we collect academic papers from ArXiv, specifically selecting those designated with \u201cmath\u201d as their primary category. For each data source, we ensure the recency by utilizing the latest available data at the time of our experiments. The composition of our compression corpora is detailed in Table 1. More details about data collection and processing are in Appendix A.2."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Overfitting Benchmarks",
            "text": "LLMs may be overoptimized towards certain benchmarks, in this case, the benchmark scores are not a good proxy for intelligence anymore. \u201cWhen a measure becomes a target, it ceases to be a good measure\u201d \u2013 Goodhart\u2019s law. \n\nFor example, Bi et al. (2024) show that when adding multi-choice QA training data, the model is able to achieve over 10 absolute points improvement on multi-choice QA benchmarks such as MMLU, while its performance on general QA benchmarks like TriviaQA remains unchanged. In the math domain, recent researches find that some models heavily optimize towards the GSM8K and MATH benchmarks, while performing poorly on held-out math tests (Paster, 2023).\n\nIn such scenarios, these benchmark scores are not reliable enough to represent the models\u2019 intelligence in the respective area. Although we focus on base models in this work, where such overfitting phenomenon may be less common, it is still possible that some LLMs are specially trained on the training data of the corresponding benchmarks in the pretraining stage, or even worse, suffer from test data contamination issues as evidenced in Wei et al. (2023).\n\nTo identify such cases, we adopt the MIN-K% PROB method (Shi et al., 2024) which is proposed to detect whether the given text is exposed to the model during pretraining. The MIN-K% PROB approach selects the tokens in a given example with minimum probabilities, if the average probability of these tokens is high, then the example is likely to be present in the pretraining data. We define the MIN-K% Score to be the average negative log-likelihood of these selected tokens.\n\nIn the experiments next, we compute the MIN-K% Score for all the test splits of the benchmarks, and for the training splits as well if available. We will spot and discuss the LLMs with extremely abnormal MIN-K% Scores. We choose the value to be 20 in our experiments as suggested by Shi et al. (2024)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "How is Intelligence Correlated with Compression?",
            "text": "To provide a comprehensive study of the correlation between compression efficiency and intelligence, we include 8 series of general-purpose language models, covering diverse organizations and varying in size, as shown in Table 2. Notably, in addition to dense transformer models, we further include Mixtral-8x7B, which utilizes a sparse mixture-of-experts (MoE) architecture.\n\nFurthermore, we incorporate state-of-the-art models specialized in coding and mathematical reasoning in addition to general-purpose models, as outlined in Table 2. We assess general-purpose LLMs across all benchmarks, while we include code LLMs in the code area only and mathematical LLMs in the mathematical reasoning area only. We note that our investigation focuses exclusively on well-trained LLMs, implying that the intelligence evaluated is likely already manifest, in contrast to models where such abilities have yet to emerge. This is because LLMs that are not optimized enough tend to perform poorly, where the results are overly noisy and less meaningful, complicating the analysis.\n\nModels are evaluated using few-shot in-context learning or in a zero-shot manner, adhering to the established norms for the respective benchmarks. All the models are evaluated exactly the same way with the same codebase for a fair comparison. More details on evaluation are included in Appendix A.3. \n\nWe utilize specific benchmarks scores to examine the relationship between model capabilities in different task areas. We extend this analysis to individual benchmarks as well in addition to the average.\n\nThe specific benchmark scores for every model are reported in Appendix B. Next, we discuss the results in the three areas with more details respectively, and extend the examination to individual benchmarks as well.\n\nWe present both the average and individual benchmark results for the knowledge and commonsense area. Notably, the scores from individual benchmarks exhibit strong correlation in this area. Meanwhile, we observe that in HellaSwag, the models hint at a saturation point in performance that blurs the distinctions between models. Conversely, MMLU presents a slight mismatch in distribution compared to the Common Crawl dataset. Further experiments on MMLU and textbook data are detailed in Appendix E. We note that for MMLU evaluation, we employ a cloze-style input format for this popular multiple-choice question (MCQ) benchmark, presenting only the question without the options, and subsequently selecting the choice with the highest likelihood. We intentionally opted for this less conventional evaluation style to minimize the risk of models overfitting to the MCQ format, thus over-optimizing the scores. Please see Appendix D for detailed discussion and ablation.\n\nWe notice strong correlation objects within the coding benchmarks as well. Furthermore, despite being of different architectures, the MoE model, Mixtral-8x7B, still fits well into this observed pattern. We notice the strongest correlation within the DS-1000 benchmark, surpassing those of HumanEval and MBPP. This is because we collect Python code from popular GitHub repositories related to data science and machine learning, aligned closer with DS-1000. We note that the Deepseek-coder model series are consistently well-above the fit, demonstrating strong abilities on the HumanEval and MBPP benchmarks.\n\nOverall the correlation results in the mathematical reasoning area are similar to the other two. As described, we compute the MIN-K% Score for every example on the benchmark test data as well as training data if available. Abnormally low MIN-K% Scores imply that the example is likely to be present in the model\u2019s pretraining data. We did not observe abnormal MIN-K% patterns in all the previous experiments, yet, there are models with far lower Min-K% Scores on the GSM8K and MATH datasets than all other models. We visualize the MIN-K% distribution of the training and test examples, which implies that the Qwen model series may be exposed to the training data in the pretraining stage. Therefore, we spot the Qwen model series in our visualization and exclude them when performing the regression analysis.\n\nAt last, we highlight that the strong correlations between single benchmark scores were originally unexpected, because individual benchmark scores are typically too noisy to represent models\u2019 general ability. However, current findings indicate that these individual scores may be predictable from baseline efficiency with a minor error in most cases.\n\nIn the previous experiments, we intentionally selected the appropriate data aligning with the area of focus. Herein, we further explore the effects of diverse corpora on the results. We investigate examples where the corpus does not align with the specific domain, to study whether an arbitrary corpus would suffice for a strong correlation: (1) mathematical reasoning benchmarks vs. Common Crawl data, and (2) knowledge and commonsense benchmarks vs. GitHub code data. Due to a substantial ability mismatch between the corpus and the benchmarks, the correlation is significantly weaker than that with in-domain data. These instances demonstrate that correlation relies on the alignment degree between the corpus and the specific area of study. However, we emphasize that our previous findings are"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Main Results \u2013 Compression Represents Intelligence Linearly",
            "text": "We have summarized our main results in Figure 1, where we examine the overall relationship by computing the average benchmark scores across all three areas. We consider such a linear correlation significant given the noisy nature of evaluating on downstream benchmarks \u2013 it is known that varying the prompts or decoding hyperparameters could easily cause several points of difference for the task scores. We also report the results for the three key areas respectively which demonstrate similar phenomena qualitatively and quantitatively.\n\nWe note that several previous and concurrent works have studied the correlation between benchmark scores and validation losses in limited settings \u2013 they focus on either the model checkpoints of the same model over the course of pretraining or the same model series. Their studied checkpoints share designs and tokenizers and are pretrained on the same data distributions. However, our work escapes the constraints on the same model (series) and compares across models that diverge dramatically on tokenizers, model designs, and pretraining data.\n\nNext, we discuss the results in the three areas with more details respectively, and extend the examination to individual benchmarks as well. We present both the average and individual benchmark results for the knowledge and commonsense area in Figure 2. Notably, the scores from individual benchmarks exhibit a strong linear correlation with compression efficiency. Meanwhile, we observe that the linear correlation in both HellaSwag and MMLU is not as pronounced as in TriviaQA, albeit for different reasons. For HellaSwag, the average accuracy among models has exceeded 0.8, hinting at a saturation point in performance that blurs the distinctions between models. Conversely, MMLU, which draws heavily from real-world examinations and textbooks, presents a slight mismatch in distribution compared to the Common Crawl dataset used to measure compression efficiency. Further experiments on MMLU and textbook compression are detailed in Appendix E.\n\nWe note that for MMLU evaluation, we employ a cloze-style input format for this popular multiple-choice question (MCQ) benchmark, presenting only the question without the options, and subsequently selecting the choice with the highest likelihood. We intentionally opted for this less conventional evaluation style to minimize the risk of models overfitting to the MCQ format, thus over-optimizing the scores. Please see Appendix D for detailed discussion and ablation.\n\nSimilar to the knowledge and commonsense area, there is a strong linear correlation between the models\u2019 compression efficiency and its coding ability. Furthermore, despite being of different architectures, the MoE model, Mixtral-8x7B, still fits well into this linear correlation. We notice the strongest linear correlation within the DS-1000 benchmark, surpassing those of HumanEval and MBPP. We think that this is because we collect Python code from the popular GitHub repositories in recent months, which are mostly related to data science and machine learning, aligned closer with DS-1000. We note that the Deepseek-coder model series are consistently well-above the linear fit, demonstrating strong abilities on the HumanEval and MBPP benchmarks. We hypothesize that this is due to the fact that Deepseek-coder models are exposed to private, task-specific data during pretraining that help HumanEval and MBPP significantly, yet we do not have reliable methods to detect the existence of such data and verify this hypothesis.\n\nOverall the correlation results in the mathematical reasoning area are similar to the other two. However, overfitting benchmarks in mathematical reasoning warrants particular attention. As described in \u00a73.3, we compute the MIN-K% Score for every example \u2013 which is the average negative log likelihood of the tokens with the lowest probabilities \u2013 on the benchmark test data as well as training data if available. Abnormally low MIN-K% Scores imply that the example is likely to be present in the model\u2019s pretraining data. We did not observe abnormal MIN-K% patterns in all the previous experiments, yet, there are models with far lower Min-K% Scores on the GSM8K and MATH datasets than all other models. We visualize the MIN-K% distribution of the training and test examples on both GSM8K and MATH, which implies that the Qwen model series may be exposed to the GSM8K training data, MATH training data, and even the MATH test data in the pretraining stage. Therefore, we spot the Qwen model series in our visualization and exclude them when performing the linear fit and compute the correlation metrics. As expected, the three Qwen models achieve generally higher accuracies than the predicted performance from the linear fit and appear as outliers.\n\nAt last, we highlight that the strongly linear correlation between the single benchmark scores and compression efficiency was originally unexpected, because individual benchmark scores are typically too noisy to represent models\u2019 general ability. However, current findings indicate that these individual scores may be predictable with a minor error in most cases."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Remaining Questions",
            "text": "In the previous experiments, we intentionally selected the appropriate compression corpora aligning with the area of focus. Herein, we further explore the effects of diverse compression corpora on the results. We investigate two representative examples where the compression corpus does not align with the specific domain: (1) mathematical reasoning benchmarks vs. compression on Common Crawl data, and (2) knowledge and commonsense benchmarks vs. compression on GitHub code data. \n\nDue to a substantial ability mismatch between the compression corpus and the benchmarks, the correlation is significantly weaker than that with in-domain data. Coupled with the MMLU case discussed in Appendix E, these instances demonstrate that correlation relies on the alignment degree between the compression corpus and the specific area of study.\n\nHowever, we emphasize that our previous findings are relatively robust since our chosen compression corpora as listed in Table 1 rely on simple heuristics at a coarse granularity without meticulous designing. How to automatically identify the most appropriate compression corpus that correlates with certain benchmarks is an important problem that we leave for future work.\n\nAs an example, we investigate the effect of the size of the compression corpus on the knowledge and commonsense ability. We vary the number of characters of the compression corpus from 50K to 100M. As the number of sampled characters increases, the correlation gradually strengthens and reaches saturation at around 30M. This observation suggests that the compression corpus is sufficiently large with only tens of millions of characters. We further provide an analysis of the impact of random sampling on compression evaluation in Appendix C.\n\nThe previously discussed experiments focus exclusively on tasks related to a single ability. Nevertheless, numerous tasks require abilities in multiple aspects. For example, incorporating programming as an intermediate step for reasoning has been shown to enhance performance in mathematical tasks, requiring abilities in both coding and mathematical reasoning (Gao et al., 2023b; Chen et al., 2023). Following Gao et al. (2023b), we apply the Program-Aided Math Reasoning (PAL) method to GSM8K. To align with the abilities involved, we combined the Python and ArXiv-Math corpora to create a mixed corpus. This mixed corpus exhibits a stronger correlation than using either the Python or ArXiv-Math corpora alone."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Our findings provide evidence for the belief that superior compression is indicative of greater intelligence.\nPractically, our experiments strengthen the rationale to adopt compression efficiency as an unsupervised, flexible, and reliable metric to assess LLMs\u2019 abilities.\nThe compression corpus can be easily updated and composed flexibly, which mitigates the data contamination and benchmark overfitting issues that most evaluations suffer from.\nWhile recent work establishes a large collection of diverse corpora to evaluate LLMs with losses (Magnusson et al., 2023  ###reference_b38###), our work offers strong empirical support for doing so by demonstrating the correlation with intelligence.\nOur study admits several limitations.\nFirst, we only focus on base models because fine-tuned models are not general-purpose compressors for arbitrary text. However, we posit that there are still interesting relationships to be explored about the base model compression efficiency and the benchmark scores of the corresponding fine-tuned models.\n Yuan et al. (2023  ###reference_b59###) initially provides evidence of this correlation in mathematical reasoning ability, albeit limited to LLaMA models.\nSecond,\nour focus lies on the short- to medium-context regimes, deferring the examination of long-context scenarios.\nThird, our conclusion and findings may only apply to well-trained models and not hold for LMs where the evaluated abilities have not emerged yet.\nWe leave study of these problems for future work."
        }
    ],
    "url": "http://arxiv.org/html/2404.09937v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "3.1",
            "3.2",
            "4",
            "4.1"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.3",
            "4",
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.09937v1",
        "paper_title": "Compression Represents Intelligence Linearly",
        "research_background": "### Paper's Motivation\nThe motivation for this paper stems from the longstanding hypothesis that there is a close connection between compression and intelligence. This idea is particularly relevant now due to significant advancements in large language models (LLMs) and their role in artificial intelligence. Theoretical discussions have suggested that predictive models (like LLMs) can be transformed into lossless compressors, leading to the notion that effective data compression may be indicative of greater intelligence. Despite these theoretical underpinnings, empirical evidence supporting this relationship has been sparse.\n\n### Research Problem\nThe core research problem addressed in this paper is to empirically investigate whether a language model's ability to compress a text corpus efficiently can serve as a reliable indicator of its intelligence. The research seeks to answer: If a language model can encode a text corpus with fewer bits in a lossless manner, does that indicate greater intelligence? Intelligence, in this context, is defined practically, focusing on the model\u2019s performance in completing a variety of downstream tasks.\n\n### Relevant Prior Work\nSeveral works form the foundation of this research:\n1. **Equating Compression and Intelligence**: Previous scholars like Hern\u00e1ndez-Orallo & Minaya-Collado (1998), Mahoney (1999), Legg et al. (2005), Hutter (2006), and Legg & Hutter (2007) have discussed the conceptual similarity between compression and intelligence.\n2. **Empirical Studies on Language Models**: Deletang et al. (2024) empirically demonstrated the strong data compression capabilities of LLMs, laying the groundwork for exploring their intelligence through the lens of compression.\n3. **Benchmark Scores and Compression Metrics**: Recent studies by Wei et al. (2023), Gadre et al. (2024), and Du et al. (2024) examined the relationship between benchmark scores and compression-equivalent metrics within specific model series, providing a partial basis for this paper's broader investigation.\n\n### Contribution of the Paper\nThe paper distinguishes itself by conducting a comprehensive study across 30 public LLMs and 12 diverse benchmarks, thus providing a generalized empirical basis for the hypothesis. This study uncovers a nearly linear correlation between a model's downstream ability and its compression efficiency, establishing this relationship as a universal principle. The findings advocate for using compression efficiency as a stable, flexible, and reliable metric to evaluate the intelligence of LLMs.",
        "methodology": "**Methodology:**\n\n**Objective:** \nThe primary goal of this study is to empirically examine the relationship between compression efficiency and the intelligence of large language models (LLMs). \n\n**Approach:** \n1. **Model Selection:** The study involves investigating various pretrained LLMs of different sizes from diverse organizations. These models differ in training data, tokenizers, computation, and architectures (including mixture of experts as in MistralAI, 2023), ensuring that the findings are generalizable and not specific to any particular model series.\n  \n2. **Metrics:**\n   - **Compression Efficiency:** The average number of bits per character (BPC) is used as the primary metric to measure compression efficiency. This is chosen over the average bits per token due to the variety of tokenizers different models may use. BPC is derived from the average coding length per message of compressing a given corpus.\n   - **Intelligence Evaluation:** Intelligence is judged based on performance in downstream benchmark tasks, which generally involve inputs not exceeding 2048 tokens. Therefore, while assessing compression, the context window size is unified to 1900 tokens to ensure consistency and fairness across models.\n\n3. **Context Window:** \n   - **Standardization:** Context window sizes are standardized to 1900 tokens for all evaluations.\n   - **Sliding Window Approach:** A sliding window approach is used for a more accurate evaluation of compression, which is detailed in Appendix A.1.\n\n4. **Stages of Model Development:** \n   - Only base models (those obtained after pre-training but before alignment) are considered. This is because fine-tuned models are specialized for (query, response) data rather than general-purpose text compression. Moreover, intelligence is believed to be mainly learned during the pre-training stage.\n\n5. **Corpus Selection for Compression Measurement:** \n   - **Diversity:** Different corpora highlight different aspects of model abilities, so a varied set of corpora aligned with specific focus areas is chosen.\n   - **Recent Data:** To avoid data leakage due to overlap with pre-training datasets whose contents are often opaque, the latest available data is used. \n   - **Specific Corpora:** Corpus selection includes:\n     - Common Crawl dataset for assessing knowledge and commonsense.\n     - GitHub repositories mainly focused on Python for evaluating coding ability.\n     - Academic papers from ArXiv labeled with \u201cmath\u201d for mathematical reasoning.\n\n**Data Collection and Processing:** Detailed protocols for data collection and processing are provided in Appendix A.2.\n\n**Conclusion:** \nThis methodology aims to provide robust insights into the relationship between compression and intelligence by using a diverse and recently-sourced dataset, standardizing evaluation conditions, and focusing on the pre-trained (base) models of LLMs.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### Main Experiment Setup:\n- **Objective**: The main objective is to study \"intelligence\" focusing on three key abilities: knowledge and commonsense, coding, and mathematical reasoning.\n- **Datasets**: The evaluation uses well-recognized benchmarks for each of these areas:\n  - **Knowledge and Commonsense**: Specific benchmarks for assessing general knowledge and commonsense intelligence.\n  - **Coding**: Benchmarks designed to evaluate coding skills.\n  - **Mathematical Reasoning**: Benchmarks focusing on mathematical problem-solving abilities.\n- **Evaluation Strategy**: The assessment is based on the average performance of the models across multiple downstream tasks, in line with the practical approach of defining intelligence as the capacity to achieve goals across a broad array of scenarios.\n  \n#### Baselines:\n- The baselines include other large language models and their respective evaluations, referencing prior works that showcase model potential through average benchmark performances (Touvron et al., 2023a; b; Jiang et al., 2023; Team et al., 2023; Anthropic, 2024).\n\n#### Evaluation Metrics:\n- **Average Benchmark Scores**: The primary evaluation metric is the average score across the domains of knowledge and commonsense, coding, and mathematical reasoning, reflecting domain-specific intelligence.\n  \n#### Main Experimental Results:\n- The paper uses the benchmark scores to indicate the specific domain intelligence of the models. The results are indicative of how well models perform across diverse tasks, synthesizing overall intelligence through average performance metrics.\n"
    },
    "reference_ablation_studies": [
        {
            "research_objective": "LLMs may be overoptimized towards certain benchmarks, making their scores unreliable indicators of intelligence. This study seeks to detect and address such overfitting.",
            "experiment_process": "The study adopts the MIN-K% PROB method to detect potential overfitting. This approach involves calculating the average negative log-likelihood of the least probable tokens in a test example to determine if it was likely present during the model's pretraining. The researchers compute this score for both test and training splits of various benchmarks to identify any abnormalities.",
            "result_discussion": "The results indicate that certain LLMs show unusually low MIN-K% Scores on specific benchmarks, suggesting potential overfitting or exposure to test data during pretraining. These findings underline the need for careful evaluation and suggest that overfitting significantly impacts perceived model intelligence.",
            "ablation_id": "2404.09937v1.No1"
        },
        {
            "research_objective": "To examine the correlation between compression efficiency and intelligence, using LLMs' ability to compress text as an indicator of their performance across various benchmarks.",
            "experiment_process": "Eight series of general-purpose language models, including dense transformers and a sparse mixture-of-experts (MoE) model, were evaluated on benchmarks related to knowledge and commonsense, coding, and mathematical reasoning. Models were assessed through few-shot or zero-shot learning, and compression efficiency was measured using bits per character (BPC). Pearson correlation and linear regression were employed to quantify and visualize the relationship between compression and benchmark scores.",
            "result_discussion": "The study found a highly linear correlation between compression efficiency and intelligence, with a Pearson coefficient of -0.94 and an RMSE of 2.8%. This suggests that LLMs' ability to compress data effectively is a reliable indicator of their intelligence, regardless of model architecture. The correlation held across different benchmarks and specific abilities, though slight variations were observed due to overfitting and domain alignment issues in mathematical reasoning.",
            "ablation_id": "2404.09937v1.No2"
        },
        {
            "research_objective": "To explore the impact of mismatched compression corpora on the correlation between compression efficiency and model performance on specific benchmarks.",
            "experiment_process": "The study investigated the effects of using unrelated compression corpora, such as Common Crawl data for mathematical reasoning benchmarks and GitHub code data for knowledge and commonsense benchmarks. Additionally, the impact of corpus size on correlation was examined by varying the number of characters from 50K to 100M and observing changes in Pearson correlation coefficient and RMSE.",
            "result_discussion": "When using mismatched compression corpora, the linear correlation between compression efficiency and benchmark scores weakened significantly, highlighting the importance of corpus alignment. The study also found that larger compression corpora enhanced correlation accuracy, suggesting that tens of millions of characters are sufficient for reliable evaluation.",
            "ablation_id": "2404.09937v1.No3"
        },
        {
            "research_objective": "To investigate the correlation between compression efficiency using mixed-domain corpora and models' abilities on tasks requiring multiple skills, such as programming and mathematical reasoning.",
            "experiment_process": "Following Gao et al. (2023b), the researchers applied the Program-Aided Math Reasoning (PAL) method to GSM8K. They created a mixed compression corpus combining Python and ArXiv-Math text. The study analyzed the correlation between this mixed corpus's compression efficiency and performance on mathematical reasoning benchmarks.",
            "result_discussion": "The mixed compression corpus exhibited a stronger linear correlation with mathematical reasoning benchmarks compared to single-domain corpora. This indicates that using comprehensive, mixed-domain corpora can better capture the multifaceted abilities required for complex tasks.",
            "ablation_id": "2404.09937v1.No4"
        }
    ]
}