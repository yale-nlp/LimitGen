{
    "title": "Adaptive Memory Replay for Continual Learning",
    "abstract": "Foundation Models (FMs) have become the hallmark of modern AI, however, these models are trained on massive data, leading to financially expensive training. Updating FMs as new data becomes available is important, however, can lead to \u2018catastrophic forgetting\u2019, where models underperform on tasks related to data sub-populations observed too long ago. \u2020\u2020*Work done during internship at MIT-IBM Watson AI Lab.\n\nThis continual learning (CL) phenomenon has been extensively studied, but primarily in a setting where only a small amount of past data can be stored. We advocate for the paradigm where memory is abundant, allowing us to keep all previous data, but computational resources are limited. In this setting, traditional replay-based CL approaches are outperformed by a simple baseline which replays past data selected uniformly at random, indicating that this setting necessitates a new approach. We address this by introducing a framework of adaptive memory replay for continual learning, where sampling of past data is phrased as a multi-armed bandit problem. We utilize Bolzmann sampling to derive a method which dynamically selects past data for training conditioned on the current task, assuming full data access and emphasizing training efficiency.\n\nThrough extensive evaluations on both vision and language pre-training tasks, we demonstrate the effectiveness of our approach, which maintains high performance while reducing forgetting by up to  at no training efficiency cost.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The concept of the Foundation Models (FMs) has recently gained popularity and became ubiquitous in many downstream applications, including language, vision, and other application domains - advocating towards the \u2018train-once-and-use-everywhere\u2019 paradigm shift in AI/ML. One of the most attractive features of FMs is their ability for Zero-Shot prompting, few-shot In-Context Learning (ICL), and great transferability to any task. This is due to their massive scale pre-training, often on billions or trillions of data points. However, such power comes with high training costs. The pre-training data size is so large that normally each sample is observed only a few times or, as in language training, once (single epoch). Moreover, a common requirement for FMs is to rather frequently undergo \u2018Extended Pre-Training\u2019 (EPT) \u2013 a process of updating the model with new (massive) additional data intended to improve the model\u2019s temporal currency. During EPT the original pre-training data cannot be naively replayed as, given the massive size of both pre-training and EPT data, it would effectively double the EPT cost (naturally assuming a single epoch and 50% replay mix for EPT as is customary in such cases). This would be prohibitive both in terms of the high cost (millions of dollars) as well as non-negligible negative environmental impact (extra heat emission).\n\nHowever, neglecting past data during EPT is prone to the issue of catastrophic forgetting, where models updated with new data tend to underperform on previously seen data. This leads to an important question: how can we adapt large-scale FM models to an ever-evolving world without compromising on performance or efficiency?\n\nThe realm of continual learning offers some insights, but also limitations. While current benchmarks effectively highlight the challenge of catastrophic forgetting by training on non-overlapping data tasks sequentially, they are less applicable to (massive scale) EPT, as they either restrict themselves to limited memory storage (while in practical EPT all data, past and current, is usually available) and do not take into account the training cost of replay. For practical EPT, we argue the cost impact needs to be minimal in the sense that the \u2018continual\u2019 EPT needs to have similar cost as \u2018naive\u2019 EPT (disregarding old data and catastrophic forgetting issue). This is intuitive, as even the tiny overhead fraction due to replay will be applied as a factor to the training cost.\n\nTaking inspiration from prior works that have tried to select memory data intelligently by selecting the most representative samples of each memory replay dataset, we push the boundaries by considering not which past samples are the most representative (which is typically pre-decided before training future tasks), but rather which samples most effectively prevent forgetting conditioned on the current task data (which is decided during the training of future tasks). This notion is based on the intuitive concept that the model retains full access to previously seen data and that the \u2018optimal\u2019 replay data may be contingent upon the new data a model encounters during EPT.\n\nWe specifically propose an approach that dynamically adjusts the proportion of replay samples from each past task based on its propensity to be forgotten given the new task data. In such an adaptive memory replay for continual learning, our algorithm efficiently decides on the optimal allocation of memory replay samples among past tasks to minimize overall forgetting, under the vital consideration of how to do this without the requirement for drastic computation. We evaluate our replay strategy for both vision and language large-scale pre-training tasks.\n\nIn particular, we propose and evaluate a zero-cost protocol that includes intelligent selection of both data to replay and reduction in the new EPT data to compensate for the (relatively small) extra cost of the selection algorithm itself.\n\nIn summary, we make the following contributions: We present an adaptive memory replay for continual learning, a novel scheme inspired by a bandit estimation formulation that assumes full memory access and dynamically adjusts replay samples based on the new data, ensuring reduced forgetting. Extensive evaluations demonstrate the efficacy of our method across both vision and language large-scale pre-training tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background and Related Work",
            "text": "Continual Learning:\nIn the past few years, there has been significant progress in continual learning to alleviate catastrophic forgetting. Regularization-based methods modify the model parameters with additional regularization constraints to prevent catastrophic forgetting. They store no data but explore extra regularization terms in the loss function to consolidate previous knowledge. Rehearsal approaches memorize or generate a small fraction of data points for previous tasks and utilize them to retain the task knowledge. Importantly, what data to retain is decided during the task itself, and subsequently used throughout future tasks. Expansion approaches expand a model\u2019s architecture as new tasks are encountered; these are highly effective for applications where a model growing with tasks is practical. Our work does not consider these methods because the model parameters grow with the number of tasks, but acknowledge that the contributions could be incorporated into these approaches.\n\nRecently, prompt-tuning methods such as those that outperformed rehearsal-based methods without using a replay buffer by learning a small number of insertable model instructions or prompts. Another line of research is the parameter isolation-based approaches, which focus on freezing the task-specific parameters and growing new branches for new tasks. Proposals like adapters add a small number of parameters to the model for training on downstream tasks. Low-Rank Adaptation (LoRA) extends on the above by using low-rank matrix counterparts of the original weights during fine-tuning, and keeps the actual weights frozen to further reduce inference costs.\n\nContinual Learning in Transformers:\nThe recent Vision Transformer (ViT) has made a pure Transformer architecture scalable for large-scale image classification and several works have successfully applied the Transformers architecture for continual learning. Some methods use the model as a teacher model in a distillation phase. Other approaches propose a unified model by building upon new architectures that dynamically expand the tokens processed by the last layer to mitigate forgetting, with some learning new task-specific tokens per head using task-attention-based decoder blocks. Recently, certain methods propose using pre-trained Transformers while maintaining strict control of the memory usage and reaching state-of-the-art predictive performance. However, these methods either train a new transformer or need to fine-tune large pre-trained transformer models which require significant compute, in contrast to our objective of achieving optimal performance with limited compute."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "In continual learning (CL)111In our setting, the model does not have access to the task id during inference., the objective during task , is to find parameters  which minimize the loss  over the current dataset  and all previously seen datasets:\nTypically, CL approaches assume that past data cannot all be stored. Instead, experience replay approaches store a subset of past data from all previous tasks in a memory buffer , where the size of  is much smaller compared to the combined number of data points from all past tasks. These approaches [40  ###reference_b40###] use  to approximate the true objective (Eq. 1  ###reference_###) and minimize:\nwhere  is a hyper-parameter. The memory buffer is updated after each task but the total number of stored items is constant.\nThe resulting method\u2019s computational requirements scale well with the number of learned tasks, but the limited size of  means that it becomes less effective at representing past data, as the number of learned tasks increases.\nThe (stochastic) K-armed bandit problem [5  ###reference_b5###] considers a setting in which there are  available actions, referred to as arms. Performing one of the actions returns a stochastic reward drawn from an unknown distribution. The problem is selecting a number of actions in a way which minimizes the expected regret, defined as the expected difference between the rewards obtained by always choosing the optimal action and the rewards obtained by following our strategy. At each step, a bandit strategy approximates the parameters of the reward distribution of each of the  actions. Thereafter, the strategy needs to select an action to perform. One such strategy is Boltzmann Exploration [23  ###reference_b23###] which computes the mean of the observed rewards for each action, and then uses all means to define a categorical distribution, from which the choice of action is drawn. Finally, if the action\u2019s distributions change between steps, the bandit problem is referred to as non-stationary [59  ###reference_b59###]."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Adaptive Memory Replay",
            "text": "In this section, we modify the typical CL setting by challenging the restrictive assumption that past data cannot be accessed. We also modify the objective which we minimize, in order to better reflect the new CL setting. Finally, we link the resulting problem to that of multi-armed bandit allocation and detail our approach."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Replay: A New Perspective",
            "text": "For FM extended pre-training, we challenge the common CL assumption that past data is unavailable and alter the common CL setting after making two observations. First, data storage is cheap, therefore it is possible to store and sample from any dataset we have seen before. Second, computation is expensive, meaning that we cannot re-train on all of the past data in memory. Following this insight, we modify the CL setting so that all of the past data can be stored, but a CL algorithm\u2019s computational demands to access and use the past data need to be constant and compensated during training to lead to zero extra cost (compared to naive training only on new data).\n\nWe begin developing our approach by modifying the objective function to better reflect our goal. First, as detailed in the Appendix, we express the main objective in terms of its forgetting on past data, compared to the performance of the optimal parameters for the previous task:\n\nThis change reflects the fact that we fine-tune the previously optimal model on the new task and that our focus is on minimizing forgetting, rather than improving our performance on past data.\n\nIn order to implement our approach, it is crucial to select the subset of past data with the highest forgetting. This subset changes as we update the parameters, and it is computationally infeasible to evaluate the forgetting of each of the past data points. Instead, we seek to divide all past data into clusters of items expected to have similar forgetting values.\n\nThis allows us to infer the forgetting values of the data points in a cluster, based on a small number of evaluations, and use this to select data points which exhibit a high amount of forgetting. Currently, we place all of the data from the same previous task into the same cluster and assume that it would exhibit similar forgetting values. We leave more elaborate clustering techniques for future work."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Adaptive Memory as a Bandit Optimization",
            "text": "Formally, we divide all past data into disjoint subsets, such that , where the forgetting of an input is distributed according to a subset-specific distribution: , for . For the parameters at training iteration , we would like to select a subset which exhibits close to the worst forgetting, minimizing the following quantity:\n\nWe frame this as a non-stationary K-armed bandit (KAB) problem, where pulling an arm and receiving a reward corresponds to sampling a data point from a cluster and evaluating its forgetting. Then, at each training iteration, we have to choose which of the K arms to pull to select data points with maximum forgetting. As we select different over all training steps, we would like to reduce the expected regret: , which is the expected difference in forgetting values between and over all training steps.\n\nIn this work, we implement the Boltzmann Exploration approach which, at training step , approximates the mean reward of each arm, denoted by , and then uses all arms\u2019 means as parameters for a categorical distribution over the choice of arms to pull. This distribution is then used to sample arms and in turn sample the memory buffer.\n\nTo approximate the mean forgetting values of cluster at training step , we first sample a small number of data points from the cluster and evaluate the average of their forgetting values \u2014 . We would like to compute based on the previously computed mean value and the currently computed forgetting average . However, we note that the forgetting values depend on our model\u2019s parameters, thus change between training iterations. We account for this by using a moving average, which is used for KAB when the underlying distributions are non-stationary:\n\n.\n\nOnce we have approximated the mean forgetting values for all clusters, we use them to create a categorical distribution over the choice of clusters, with the help of the tempered softmax function. We compute: , where is the temperature hyperparameter and is the normalization constant. Afterwards, we use this distribution to sample cluster indices. Finally, we sample one input from each selected cluster, uniformly at random, and combine the samples to create the memory buffer for the current training step.\n\nOur full method is summarized in Algorithm 1."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We evaluate the efficacy of our proposed method for continual learning of FM pre-training (i.e., extended pre-training) in both the vision and language domains. We utilize two distinct pre-trained models as our backbones for these experiments: a Vision Masked Autoencoder (MAE) pre-trained on ImageNet-1K for vision-related tasks, and LLaMA with 7 billion parameters for language experiments.\n\nThe evaluation metrics for our experiments are twofold: test data Final Loss and test data loss Forgetting. These metrics are normalized between 0% and 100%, where 0% represents an offline upper bound with all data trained independently and identically distributed (iid), and 100% corresponds to the performance of the pre-trained model without any fine-tuning.\n\nOur experiments are designed to demonstrate the advantages of our approach over traditional iid replay, especially in terms of computational efficiency and reduced forgetting. We consider gains of our approach to be orthogonal to the realms of non-replay regularization-based continual learning methods, and thus these comparisons are not the main focus of our results. From the perspective of computational efficiency, recent work has found such approaches to be impractical for computationally bounded continual learning. However, we discuss the interaction of our approach with different continual learning strategies like regularization methods and knowledge distillation in our Appendix.\n\nThe hyperparameters for our experiments were meticulously chosen based on a series of small task experiments. We update our model on new data examples per task. In the interest of computational resources for the larger Llama model, we approximate the training of all the model parameters with LoRA finetuning in the language modeling experiments. In our experience, conclusions attained for LoRA finetuning reflect the same in full model training. We use a learning rate for full model fine-tuning and for LoRA-based fine-tuning. For our method, we found that a temperature and forgetting mean update ratio performed best. We compose our replay batches for both iid replay and our method with a 1:1 ratio of replay data to new task training data. We conducted evaluations on a hold-out test dataset comprising 500 samples per dataset. Additional training details can be found in our Appendix."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Results for Vision SSL",
            "text": "In Tables 1, 2, 3, we benchmark our proposed approach on 3 different continual pre-training sequences composed of vision datasets. Our goal was to demonstrate the robustness of our findings with a variety of unique and practical dataset sequences. The first dataset is the DomainNet dataset (Table 1), containing 6 different domains of common objects. The next is the Medical MNIST dataset (Table 2), from which we sampled 5 standardized biomedical image datasets containing the highest number of samples. Finally, we use 4 attribute splits from the Synthetic Visual Concepts (SyViC) dataset (Table 3).\n\nOur results demonstrate the advantages of our adaptive memory replay method in the vision domain. Our approach consistently achieves lower final loss and forgetting rates. The slight increase in normalized training time is negligible compared to the performance gains. We show a 0-cost result where we reduce the number of training steps of our approach to align with the training time of naive fine-tuning, and show that this result achieves enhanced performance.\n\nWe note that our strongest performance gains come from the DomainNet results. The gains for the medical data sequence and synthetic data pre-training are much more modest, yet remain pronounced. The synthetic data sequence is interesting in that forward transfer (i.e., negative forgetting) appears in all results. In practical terms, our results imply that vision systems equipped with our continual learning strategy would exhibit improved robustness over time, adapting to new data without significant loss of prior knowledge or computational costs. We re-iterate that there is much more room for improvement from our full-memory continual learning perspective - advanced strategies can close the gap between our method and an ideal scenario with fixed time costs by exploring interesting questions such as how to better cluster the data and which new data is more or less favorable to discard."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Results for Causal Language Modeling",
            "text": "Our approach is further affirmed through our language experiments using the Llama model. In Table 4, we benchmark on a 5-dataset sequence using datasets from Huggingface. These datasets were chosen based on the significant variations in loss observed post fine-tuning, thus providing a rigorous test for our approach. The datasets encompass a broad range of language tasks, ensuring that our results are representative of diverse language modeling scenarios. Further specifics about these datasets are available in our Appendix.\n\nThe performance of our method in language experiments shows a significant reduction in both forgetting and final loss compared to the iid full-memory replay. The \u20180-cost\u2019 variant of our method is particularly noteworthy, as it manages to retain a high level of performance without additional computational expenditure compared to naive fine-tuning. This aspect is crucial for applications where computational resources are limited, especially fitting LLM extended pre-training where due to high data volumes and enormous model sizes even a tiny fraction of extra cost is intolerable."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Additional Analysis",
            "text": "In Figure 4, we present a comprehensive comparison of the final loss versus training time for our adaptive memory replay method against the Oracle, using the Synthetic Visual Concepts dataset sequence [9]. This plot demonstrates how our method converges towards the Oracle\u2019s performance as we increase the compute budget of our method (via using more replay samples and discarding fewer new task samples). With a limited budget, there is a notable difference in the final loss between our method and the Oracle. However, as training progresses, our method steadily approaches the Oracle\u2019s level of performance, matching and even outperforming Oracle (which has a fixed compute budget itself, pre-defined by the number of training steps we use) with a lower compute cost. The horizontal dotted line marks the point at which our approach reaches the Oracle\u2019s normalized loss, showcasing the efficiency of our method in terms of both loss minimization and computational time. This result is significant as it not only validates the effectiveness of our adaptive memory replay in reducing the final loss but also highlights its capability to achieve this with a substantially lower training time."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we underscored the importance of adapting machine learning methodologies to the ever-evolving demands of real-world large-scale continual learning. Our findings, rooted in extensive evaluations across both vision and language tasks, validate the advantages of our approach for continual learning. Ultimately, we hope to inspire methodologies that are both computationally efficient and effective in real-world continual learning scenarios, where data access and computational resources are bound by practical constraints. This research trajectory serves as a stepping stone towards the development of continually learning systems that efficiently and intelligently utilize all available data, enhancing their learning and adaptability across a series of tasks throughout their life-cycle.\n\nFuture work should focus on refining the mechanism, particularly exploring more sophisticated strategies to further enhance the balance between retaining old knowledge and accommodating new information. The decision of which data to discard during the learning phase also warrants deeper investigation to avoid potential loss of critical information. There is also a great need to develop advancing techniques to capture the subtleties of data evolution. In addition, bringing greater realism into continual learning models by incorporating real-world constraints and scenarios will be crucial, such as blurred task boundaries and online learning."
        }
    ],
    "url": "http://arxiv.org/html/2404.12526v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.12526v1",
        "paper_title": "Adaptive Memory Replay for Continual Learning",
        "research_background": "### Paper's Motivation ###\n\nThe paper is motivated by the growing prominence and utility of Foundation Models (FMs), which are capable of Zero-Shot prompting, few-shot In-Context Learning (ICL), and task transferability due to their extensive pre-training on massive datasets. Despite their strengths, these models face challenges of high training costs, potential catastrophic forgetting during Extended Pre-Training (EPT), and the environmental impacts associated with additional pre-training. The authors aim to address these challenges by developing a method to integrate and optimize continual learning during EPT without compromising efficiency or performance.\n\n### Research Problem ###\n\nThe primary research problem tackled by the paper is how to adapt large-scale Foundation Models (FMs) in a continually evolving environment without incurring prohibitive costs or suffering from performance degradation due to catastrophic forgetting. The goal is to establish an efficient method for memory replay during EPT, leveraging all past data while maintaining minimal additional computational costs.\n\n### Relevant Prior Work ###\n\n1. **Foundation Models (FMs):**\n   - These models have revolutionized AI/ML applications across different domains by adopting the \"train-once-and-use-everywhere\" paradigm, supported by substantial pre-training on massive datasets ([2  ###reference_b2###], [33  ###reference_b33###], [43  ###reference_b43###], [51  ###reference_b51###]).\n   - FMs exhibit capabilities like Zero-Shot prompting and ICL, which owe to their extensive training ([4  ###reference_b4###], [52  ###reference_b52###], [55  ###reference_b55###]).\n\n2. **Challenges with EPT:**\n   - EPT is a process that updates models with new data to keep them temporally current but is fraught with issues like catastrophic forgetting of earlier data ([21  ###reference_b21###]) and high costs if not managed efficiently.\n\n3. **Continual Learning and Catastrophic Forgetting:**\n   - Classical benchmarks in continual learning reveal shortcomings in addressing catastrophic forgetting but usually operate under limited memory storage constraints, which are not applicable at the scale of EPT ([21  ###reference_b21###], [37  ###reference_b37###]).\n\n4. **Adaptive Memory Selection:**\n   - Prior works ([3  ###reference_b3###], [10  ###reference_b10###], [17  ###reference_b17###]) have attempted to address forgetting by intelligently selecting representative samples for memory replay. However, they typically pre-select samples before the training of future tasks rather than dynamically adjusting during training.\n\nThe approach proposed in this paper builds upon these foundations by introducing an algorithm that dynamically selects and adjusts replay samples from past data during EPT, leveraging bandit estimation and Boltzmann sampling to keep computing costs inline while reducing forgetting and improving model performance.",
        "methodology": "In this section, we first modify the typical CL setting by challenging the restrictive assumption that past data cannot be accessed. Second, we modify the objective which we minimize, in order to better reflect the new CL setting. Third, we link the resulting problem to that of multi-armed bandit allocation and detail our approach.\n\n**Key Components and Innovations:**\n\n1. **Modified Continual Learning (CL) Setting:**\n   The traditional CL setting strictly prohibits access to past data. We relax this constraint by allowing access to previously seen data, which modifies the CL setting to be more realistic and practical for real-world applications. This alteration addresses limitations in the conventional approach and enables more effective continual learning by leveraging past experiences.\n\n2. **New Objective Function:**\n   We redefine the objective function to better align with the modified CL setting. The new objective accounts for the availability of past data and optimizes the learning algorithm to make more informed decisions based on previously encountered experiences. This shift in the objective function enhances the model's ability to learn continually without succumbing to the drawbacks of stringent data restrictions.\n\n3. **Connection to Multi-Armed Bandit Allocation:**\n   The newly formulated problem is connected to the well-known multi-armed bandit allocation problem. Drawing parallels with this problem allows us to employ established strategies and theoretical foundations from the multi-armed bandit domain to solve our CL task. The multi-armed bandit framework provides a robust mechanism for balancing exploration and exploitation, which is crucial for effective continual learning.\n\n4. **Detailed Approach:**\n   Our approach leverages techniques from the multi-armed bandit literature to guide the memory replay process. By intelligently selecting which past data to replay based on a bandit algorithm, we ensure that the learning model continually revisits important past experiences while still absorbing new information. This method allows for an adaptive and efficient memory replay system that enhances the overall learning process.\n\nThis methodology section outlines the innovative steps taken to improve the continual learning process by relaxing data access constraints, redefining the learning objective, and employing a bandit-based approach for adaptive memory replay.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### Experiment Setup:\n- **Objective**: Evaluate the efficacy of adaptive memory replay in continual learning for vision and language tasks.\n- **Backbone Models**: \n  - Vision: Vision Masked Autoencoder (MAE) pre-trained on ImageNet-1K.\n  - Language: LLaMA with 7 billion parameters.\n- **Evaluation Metrics**: \n  - Test data Final Loss.\n  - Test data loss Forgetting.\n  Both metrics are normalized between 0% (offline upper bound) and 100% (performance of pre-trained model without fine-tuning).\n- **Comparison Baseline**: \n  - Full memory-access iid data replay.\n  - This approach assumes that other replay methods (e.g., coreset selection) would be upper-bounded by full iid replay, given the goal of choosing representative data is less effective than identifying forgotten data for replay.\n\n#### Experimental Approach:\n- **Adaptive Memory Replay**:\n  - Designed to improve computational efficiency and reduce forgetting compared to traditional iid replay.\n  - Memory reuse for new data is prioritized based on forgetting metrics.\n- **Training Details**:\n  - Model updates are performed on new data examples per task.\n  - For the larger LLaMA model, LoRA finetuning was used to approximate full model parameter training.\n  - Learning rates: Specific rates were used for full model fine-tuning and LoRA finetuning.\n  - Hyperparameters for the adaptive replay scheme included temperature and forgetting mean update ratio.\n  - Replay batches for both iid and adaptive memory replay used a 1:1 ratio of replay data to new task training data.\n- **Dataset**: \n  - Evaluations were performed on a hold-out test dataset with 500 samples per dataset.\n- **Scope of Comparison**:\n  - The main comparisons focus on adaptive memory replay vs. iid replay due to computational resource constraints and the orthogonal nature of evaluations against non-replay continual learning methods (like regularization-based methods).\n\n#### Main Experimental Results:\n1. **Vision Tasks**:\n   - **Test data Final Loss**: Adaptive memory replay showed consistently lower (better) final loss compared to iid replay.\n   - **Test data loss Forgetting**: Adaptive memory replay significantly reduced forgetting, demonstrating its robustness in preserving learned knowledge.\n\n2. **Language Tasks**:\n   - **Test data Final Loss**: Similar trends were observed where adaptive memory replay improved final loss over iid replay.\n   - **Test data loss Forgetting**: Notably lower forgetting rates were observed, underscoring the effectiveness of the adaptive approach in the language domain.\n\nIn summary, the main experimental results illustrate that the adaptive memory replay approach outperforms traditional iid replay in both vision and language tasks, particularly highlighting advantages in reducing model forgetting and improving computational efficiency. Detailed quantitative results validating these findings are presented in the tables provided in the original document (###table_1### through ###table_4###). Additional training details and interaction with various continual learning methods can be found in the Appendix."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the efficacy of the proposed adaptive memory replay for continual learning of Foundation Models (FMs) in both vision and language tasks, and to demonstrate its advantages over traditional iid replay, especially in terms of computational efficiency and reduced forgetting.",
            "experiment_process": "We utilize Vision Masked Autoencoder (MAE) pre-trained on ImageNet-1K for vision tasks, and LLaMA with 7 billion parameters for language tasks. Evaluation metrics include test data Final Loss and test data loss Forgetting, normalized between 0% (ideal performance) and 100% (initial pre-trained model without fine-tuning). The adaptive memory replay approach is compared against full memory-access iid data replay. Hyperparameters were optimized through small task experiments, and the model was updated with new data examples per task. For computational efficiency in language modeling, LoRA finetuning was used. We evaluated on a hold-out test dataset of 500 samples per dataset.",
            "result_discussion": "The adaptive memory replay approach consistently outperforms full memory iid replay, achieving lower final loss and forgetting rates. Despite a slight increase in normalized training time, the performance gains justify the approach. The strongest gains were noted in the DomainNet results, with smaller yet significant improvements in medical and synthetic data sequences. This demonstrates improved robustness and adaptability in vision systems without significant computational costs.",
            "ablation_id": "2404.12526v1.No1"
        },
        {
            "research_objective": "To confirm the effectiveness of the adaptive memory replay method for continual learning in language tasks using the Llama model.",
            "experiment_process": "The experiment involves using a 5-dataset sequence from Huggingface, chosen for significant loss variations post fine-tuning, to rigorously test the approach across diverse language modeling scenarios. Evaluation metrics and setup are similar to those used in the vision experiments.",
            "result_discussion": "The adaptive memory replay method significantly reduces both forgetting and final loss compared to iid full-memory replay. The '0-cost' variant of this method maintains high performance without additional computational expense. This is particularly crucial for language model pre-training, where computational resources are highly constrained.",
            "ablation_id": "2404.12526v1.No2"
        },
        {
            "research_objective": "To analyze the performance and computational efficiency of the adaptive memory replay method in reducing final loss compared to an Oracle model using the Synthetic Visual Concepts dataset.",
            "experiment_process": "A comparison is presented in Figure 4, using the Synthetic Visual Concepts dataset sequence. The method's performance is tracked relative to its convergence towards the Oracle's performance by increasing the compute budget (more replay samples, fewer new task samples discarded). The evaluation focuses on final loss versus training time.",
            "result_discussion": "The adaptive memory replay method converges towards the Oracle's performance as more compute is allotted, eventually matching and even surpassing it with lower compute costs. The method shows significant efficiency, achieving performance levels comparable to Oracle with less training time, validating the approach in terms of loss minimization and computational efficiency.",
            "ablation_id": "2404.12526v1.No3"
        }
    ]
}