{
    "title": "Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding",
    "abstract": "Large foundation models have recently emerged as a prominent focus of interest, attaining superior performance in widespread scenarios. Due to the scarcity of 3D data, many efforts have been made to adapt pre-trained transformers from vision to 3D domains. However, such 2D-to-3D approaches are still limited, due to the potential loss of spatial geometries and high computation cost. More importantly, their frameworks are mainly designed for 2D models, lacking a general any-to-3D paradigm. In this paper, we introduce Any2Point, a parameter-efficient method to empower any-modality large models (vision, language, audio) for 3D understanding. Given a frozen transformer from any source modality, we propose a 3D-to-any (1D or 2D) virtual projection strategy that correlates the input 3D points to the original 1D or 2D positions within the source modality. This mechanism enables us to assign each 3D token with a positional encoding paired with the pre-trained model, which avoids 3D geometry loss caused by the true projection and better motivates the transformer for 3D learning with 1D/2D positional priors. The code is released at https://github.com/Ivan-Tang-3D/Any2Point.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Driven by the growing volume of model parameters and training data, large foundation models have gained unprecedented attention in a diverse array of domains and tasks. Numerous large models have been pre-trained for natural language processing, including BERT Devlin et al. (2018), T5 Raffel et al. (2020), and GPT series OpenAI (2023); Floridi and Chiriatti (2020), as well as visual understanding like DINOV2 Oquab et al. (2023), MAE He et al. (2022); Wei et al. (2022); Xie et al. (2022), and ViT-22B Dehghani et al. (2023). Existing works also explore efficient fine-tuning techniques to transfer pre-trained large models to a variety of downstream tasks, consistently achieving excellent performance. Meanwhile, 3D visual understanding is also a significant topic, with its rich geometric representation contributing to the development of many applications (e.g., robotics and autonomous driving). Unfortunately, due to a lack of large-scale 3D data, the efforts towards 3D foundational models are significantly lagging compared to language and 2D vision. Specifically, the acquisition and annotation of high-quality 3D data requires expensive resources and human labor, while synthetic 3D data training falls short of distribution diversity and real-world applications.\n\nTherefore, some previous works have transferred pre-trained models from other modalities (mainly 2D vision) to 3D modality, leveraging sufficient pre-trained knowledge from diverse sources. We categorize existing 2D-to-3D works into two groups. 1) Data modality transformation. This approach involves projecting 3D point clouds into 2D images, which are subsequently fed into 2D pre-trained models. Despite the promising performance on downstream tasks, the process of modality transformation inevitably causes the loss of spatial information in 3D data, hindering the full potential for 3D understanding. 2) Cross-modality knowledge distillation. These approaches involve the pre-training knowledge transfer from 2D or vision-language models to a newly trained 3D model. They are not only required to forward propagate both the 2D and 3D models during training, but also highly rely on the large-scale paired 2D-3D data. This leads to substantial computation costs and data engineering, limiting their ability for efficient implementation. Besides the aforementioned issues, more importantly, current methods mostly focus on the model adaptation from 2D vision to 3D point clouds, rather than a shared methodology for other modalities.\n\nTherefore, we pose a question: can we develop a general any-to-3D paradigm that empowers any-modality large models for efficient and effective point-cloud understanding? To address this issue, we propose Any2Point, a unified any-to-3D framework that transfers any 1D (language) or 2D (image/audio) large models to 3D domains with Parameter-Efficient Fine-Tuning (PEFT). Different from prior methods, our Any2Point avoids the point cloud projection, thereby mitigating the 3D information loss, and directly fine-tunes pre-trained models from source modalities. Specifically, given an any-modality pre-trained transformer, we introduce a 3D-to-any (1D or 2D) virtual projection mechanism. This mechanism establishes a positional mapping between the input 3D points and their virtually projected 1D lines or 2D planes. This enables us to encode 3D coordinates using the original positional embeddings of the source modality of pre-trained large models. In this way, we no longer need to conduct a true projection losing 3D geometries, while better promoting the pre-trained transformer to acquire 3D features with their original 1D/2D positional priors. Afterward, we perform an adaptive ensemble for the 3D features guided by different 1D/2D priors, which attains superior 3D representations.\n\nExtensive experiments across various tasks demonstrate that our Any2Point framework achieves superior performance compared to current 3D pre-trained models, while utilizing only 1.0% of the trainable parameters. Using the pre-trained CLIP Text Encoder, Any2Point fine-tunes only 0.8M parameters and attains 91.9% on ScanObjectNN, outperforming the previous state-of-the-art (SOTA) 3D pre-trained model by +1.3%, and 94.3% on ModelNet40. Furthermore, Any2Point also achieves comparable results and efficiency by utilizing other pre-trained models of different modalities, including 2D vision, language, and audio, validating the robustness of our approach. \n\nThe contributions of our paper are as follows:\nTo enable a general any-to-3D transferring framework, we propose Any2Point, which empowers any-modality pre-trained large models ("
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Large Models",
            "text": "Large-scale pre-trained models have achieved remarkable downstream performance in language, 2D vision, and audio processing. In the field of natural language field, BERT revolutionized natural language understanding by pre-training deep bidirectional representations from unlabeled text. Building on this, RoBERTa enhances BERT by optimizing training and data scale, significantly boosting performance across language understanding benchmarks. GPT pioneers in generating coherent, contextually relevant text using a transformer model pre-trained on a diverse corpus of text. CLIP further bridges visual and linguistic information by pre-training on a vast collection of image-text pairs.\n\nIn 2D vision, DeiT achieves efficient image classification with transformers, using data augmentation and knowledge distillation for minimal data reliance. DINO V2 also advances self-supervised learning with vision transformers through innovative self-distillation, requiring no labels. MAE proposes an asymmetric encoder and decoder to reconstruct images from masked data.\n\nFor audio processing, AST transforms audio recognition to spectrograms by applying a vision transformer. SSAST leverages self-supervised learning on unlabeled data with transformers for enhanced audio classification performance. ImageBind unifies multi-modal space by jointly training on multi-modal data including audio, improving multi-modal understanding and generation. Our method first utilizes the abundant knowledge from large models of any modalities and achieves 3D understanding capacity."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2D-to-3D Transfer Learning",
            "text": "The paradigm of 2D-to-3D transfer learning aims to leverage the rich contextual and textural knowledge in the 2D domain to boost 3D understanding. Some works propose specific designs for 3D learning guided by 2D pre-trained knowledge, and achieve promising 3D understanding performance. Image2Point Xu et al. (2022) proposes to transfer 2D semantics to 3D by convolutional layer inflating. ULIP Xue et al. (2023) enhances 3D understanding performance by unifying image, text, and 3D point cloud representations. ReCon Qi et al. (2023) leverages contrastive cross-modal learning and generative models for knowledge transfer. Meanwhile, PointCLIP V1 Zhang et al. (2022b) and V2 Zhu et al. (2023) first adopt CLIP\u2019s 2D pre-trained knowledge on different 3D downstream tasks via projecting 3D point clouds to 2D images as input to the pre-trained backbone. P2P Wang et al. (2022) also proposes 2D-to-3D projection through a learnable coloring module. Our approach skips the step of projecting point clouds, which reduces the loss of 3D geometric information, and fine-tunes pre-trained models instead of using computationally expensive knowledge distillation."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Parameter-Efficient Fine-tuning",
            "text": "The pre-training and fine-tuning paradigm has been proven highly effective in various tasks such as 2D visual recognition, language understanding, text-to-image generation, and audio recognition. However, fully fine-tuning the whole model becomes impractical as model volume increases exponentially. In contrast, the parameter-efficient fine-tuning (PEFT) methods aim at only updating a tiny part of the model\u2019s parameters while freezing the rest parts, which have been proven to be effective and efficient on multiple popular pre-trained models such as BERT, GPT, ViT, CLIP, and Stable Diffusion. \n\nRecently, Point-PEFT introduced PEFT techniques into 3D domains. In this paper, the Any2Point framework utilizes PEFT techniques to transfer Any-Modality pre-trained models to 3D understanding tasks at a low computational and storage cost."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Any2Point",
            "text": "In Section 3.1, we first provide a paradigm overview of Any2Point, including the problem definition and network architecture. We then elaborate on the methodologies for adapting any-modality large models for 3D domains.\n\nGiven a pre-trained transformer from any modality, e.g., vision, language, and audio, our objective is to empower it with 3D understanding capabilities in an effective and efficient manner. Instead of employing full fine-tuning on 3D data, we seek a parameter-efficient solution with the source transformers frozen, since their large-scale parameters might cause high computation cost and over-fitting issues on the limited 3D dataset.\n\nWe generally divide the source models into two categories according to their pre-training data dimension, denoted as 1D and 2D transformers. The 1D transformers process sequential data, exemplified by language models like RoBERTa, T5, and CLIP\u2019s text encoder. The 2D transformers handle 2D spatial data, including vision models like DINOv2 and DeiT, and audio models like ImageBind Audio Encoder and SSAST.\n\nThe overall paradigm of Any2Point is depicted in Figure 2. To encode the input point cloud, we discard the original embedding modules in source transformers, like tokenizers in 1D language models and convolutions in 2D vision/audio models. We employ a 3D mini-network for point cloud tokenization. The encoded 3D tokens are then fed into a 3D-to-any virtual projection module for positional encoding, followed by the frozen transformer. The virtual projection assigns 3D tokens with positional information compatible with the source modality, while adaptive representation learning is facilitated through efficient tokenization with only the initial tokenization network as learnable.\n\nTo avoid information degradation, we directly tokenize the input point cloud within the 3D space for the subsequent transformer. This is achieved through a 3D mini-network containing small-scale parameters. The network involves Farthest Point Sampling for point number downsampling, k-Nearest Neighbor algorithm for local aggregation, and learnable linear layers for feature encoding.\n\nPositional encodings (PEs) are crucial as the inner attention mechanism is permutation-invariant. Therefore, transformers need new 3D PEs integrated with 3D tokens for positional comprehension. However, source transformers are paired with their original PEs, leading to potential semantic discrepancy between frozen weights and new 3D PEs. We address this by virtually projecting 3D tokens into the source modality to acquire corresponding 1D/2D PEs, aligning the transformers more effectively.\n\nFor 2D transformers, we virtually project each 3D coordinate into 2D views, deriving 2D coordinates. The projection provides diverse positional relations without producing actual multi-view images, only obtaining virtual positions. Then, original 2D PEs within pre-trained transformers assign each 3D token with different PEs.\n\nSimilarly, for 1D transformers, we project 3D coordinates into 1D lines. Select lines pass through the point cloud center with uniform rotation angles, calculating the projected length through dot production for alignment with original 1D PEs. Thus, we assign each 3D token with different PEs and average them as an indicator to combine with the 3D token.\n\nThe self-attention mechanisms in source transformers focus on long-range interactions, lacking in local feature extraction, which is significant for fine-grained understanding. To complement this, we utilize adapter layers to capture 3D semantics within local neighborhoods. Simple FPS and -NN for local grouping might cause discrepancy, so we design a 1D/2D-guided aggregation strategy and an adaptive any-to-3D ensemble approach for robust encoding.\n\nWithin the adapter, we group 3D tokens into local neighborhoods guided by 1D/2D positional priors, aligning with adopted PEs. For different views/lines, concurrent local aggregation makes use of different perspectives. For 2D transformers, we divide projected images into patches, grouping tokens within each using 2D positions. For 1D transformers, projected lines divide into segments using 1D positions. A self-attention layer then performs local feature interaction guided by 1D/2D priors. Pooling and propagation propagate the aggregated feature.\n\nAfter parallel local aggregation, we obtain sets of 3D tokens representing different views or lines. Different perspectives show varying significance, so we propose an adaptive any-to-3D ensemble approach to aggregate features. This involves a 3D feature transformation branch independent of 2D-guided aggregation. It uses local grouping in space, average pooling, and propagation, providing a baseline for ensemble. Relative weights are calculated and features aggregated for the final output, allowing adaptive determination of critical perspectives, enhancing feature adaptation quality."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Method Overview",
            "text": "Given a pre-trained transformer from any modality, e.g., vision, language, and audio, our objective is to empower it with 3D understanding capabilities in an effective and efficient manner. Instead of employing full fine-tuning on 3D data, we seek a parameter-efficient solution with the source transformers frozen, since their large-scale parameters might cause high computation cost and over-fitting issues on the limited 3D dataset.\n\nWe generally divide the source models into two categories according to their pre-training data dimension, denoted as 1D and 2D transformers. The 1D transformers are specialized in processing sequential data, exemplified by language models like RoBERTa, T5, and CLIP\u2019s text encoder. The 2D transformers are expert at 2D spatial data, including vision models, e.g., DINOv2 and DeiT, and audio models, e.g., ImageBind Audio Encoder and SSAST.\n\nThe overall paradigm of Any2Point is depicted in Figure 2. To encode the input point cloud, we discard the original embedding modules in source transformers, e.g., tokenizers in 1D language models and convolutions in 2D vision/audio models, and employ a 3D mini-network for point cloud tokenization.\n\nOn top of this, the encoded 3D tokens are fed first into a 3D-to-any virtual projection module for positional encoding, and then into the frozen 1D/2D transformer with any-to-3D guided adapters. The former mechanism aims to assign each 3D token with positional information within the source modality, and the latter is designed for adaptive 1D/2D-guided 3D representation learning, which we will detail in the following sections. Note that, as the source transformers are kept frozen, only the initial tokenization network and the inserted adapters are learnable for parameter-efficient fine-tuning."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3D-to-any Virtual Projection",
            "text": "Many current 2D-to-3D methods Zhu et al. (2023); Zhang et al. (2022b); Wang et al. (2022) project 3D point clouds into multi-view images to meet the input modality of pre-trained 2D models. This dimension reduction process potentially leads to the information loss of 3D geometries and deep measurements, enabling insufficient 3D feature encoding. In addition, these approaches are merely validated on large models within 2D images, without considering other modalities like language and audio. Therefore, we propose a 3D-to-any virtual projection strategy that mitigates the geometric loss and is generalizable to any 1D/2D pre-trained models, as shown in Figure 3.\n\nTo avoid any information degradation, we directly tokenize the input point cloud within the 3D space for the subsequent 1D/2D transformer. Specifically, we employ a 3D mini-network containing small-scale parameters, which is a lighter-weight variant of Point-PN Zhang et al. (2023c); Zhu et al. (2024). The tokenization process involves Farthest Point Sampling (FPS) Qi et al. (2017a) for point number downsampling, -Nearest Neighbor (-NN) algorithm for local aggregation, and learnable linear layers for feature encoding. After this, we transform the raw point clouds into high-dimensional vectors, obtaining 3D tokens as , with denoting their 3D coordinates.\n\nPositional encodings (PEs) serve as the only indicator for positional information to the transformer model, since the inner attention mechanism is permutation-invariant, treating every token at different orders all the same. Therefore, a straightforward way for 1D/2D transformers to comprehend 3D positional information is to integrate new 3D PEs with 3D tokens. However, the source transformers are pre-trained paired with their original PEs in 1D/2D space, which leads to semantic discrepancy between the frozen 1D/2D weights and newly learned 3D PEs. To address this issue, we virtually project 3D tokens into the source modality, and obtain the corresponding 1D/2D PEs for better aligning with the transformers.\n\nFor 2D transformers in 2D vision and audio modalities, we virtually project each 3D coordinate, e.g., , into views, deriving the corresponding 2D coordinates as . The different perspectives are capable of providing diverse positional relations within 2D space. We adopt a simple projection in PointCLIP Zhang et al. (2022b) without learnable parameters. Importantly, we do not truly produce the projected multi-view images, but only aim to obtain the virtual 2D positions. Then, according to the original 2D PEs within pre-trained transformers, we assign each 3D token, e.g., , with different PEs, denoted as .\n\nSimilarly, for 1D transformers in language modality, we virtually project the 3D coordinates into different 1D lines. To align the number with 2D modality, we also select lines passing through the center of the point cloud with uniform rotation angles. For simplicity, we suppose the point cloud center as the origin, the unit direction vectors of lines as , and the point coordinate, , vectorized as . Then, the 1D coordinate of point in line is formulated by the dot product, denoting the projected length. In this way, we refer to the original 1D PEs, and assign each 3D token, e.g., , with different PEs as .\n\nAfter acquiring the corresponding 1D/2D PEs, we average them as an overall positional indicator, and incorporate it with the 3D token, e.g., , by This approach injects sufficient positional information of the source modality into 3D tokens to better collaborate with the frozen transformer, while mitigating the information loss of the true projection."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Any-to-3D Guided Adapter",
            "text": "Different from existing distillation-based methods Zhang et al. (2023b  ###reference_b53###); Guo et al. (2023a  ###reference_b11###) training a new 3D network, we directly feed the encoded 3D tokens to the pre-trained 1D/2D transformer. Although the PEs of 3D tokens have been aligned with the source model, the entirely frozen weights pre-trained by other modalities are still restricted to learning superior 3D representations.\n\nThe self-attention mechanisms within source transformers normally focus on long-range token interaction in global contexts, which lacks local feature extraction. However, the detailed spatial geometries are also significant for the fine-grained understanding of 3D shapes. To complement the gap, we utilize the proposed layers for specifically capturing 3D semantics within local neighborhoods. In addition, as the source transformers are powered by 1D/2D PEs as discussed above, the naive FPS and -NN for 3D local grouping might cause positional discrepancy. Therefore, we further design a 1D/2D-guided aggregation strategy and an adaptive any-to-3D ensemble approach for robust 3D fine-grained encoding.\n\nWe first group 3D tokens into different local neighborhoods guided by 1D/2D positional priors, which better align the adopted 1D/2D PEs. For different views/lines, we conduct a concurrent local aggregation process to make the best of different projection perspectives. Specifically, for 2D transformers, we divide each virtually projected image, e.g., the -th view, into uniform local 2D patches, and group the 3D tokens within the same patch into a neighborhood, according to their 2D positions. For 1D transformers, we similarly divide each virtually projected line, e.g., the -th direction, into uniform local 1D segments, and group the 3D tokens within different segments referring to their 1D positions. On top of this, we adopt a self-attention layer for 3D tokens within each 1D/2D neighborhoods, performing local feature interaction guided by 1D/2D priors. Then we employ the operations of pooling and propagation to propagate the local aggregated feature to every point within the same neighborhood.\n\nAfter the parallel local aggregation, we obtain sets of 3D tokens, each representing a 2D view or 1D line. As different projection perspectives normally showcase different significance for 3D representations, we propose an adaptive any-to-3D ensemble approach to aggregate the features for each token. We denote the -th 3D token with sets of features at this stage as. To properly indicate the relative importance of each view/line, we additionally employ a 3D feature transformation branch independent of the 2D-guided local aggregation. This non-parametric branch only contains the local grouping in 3D space, feature average pooling within local groups, and propagation operations, converting the 3D token before the layer into a feature baseline for adaptive ensemble, denoted as. Then, we calculate the relative weights for different views/lines by the cosine similarity, and finally aggregate their features to obtain the final output as\n\nWith the ensemble strategy, we integrate different features with dynamic weights, enabling the model to adaptively determine which view/line is more critical, contributing to high-quality adapted features."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we conduct extensive experiments on the ScanObjectNN Uy et al. (2019  ###reference_b38###) and ModelNet40 Wu et al. (2015  ###reference_b43###) datasets. We first introduce the fine-tuning settings and implementation details in Section 4.1  ###reference_###. Then, in Section 4.2  ###reference_###, we present the main experiment of transferring any-modality large models (language, 2D image and audio) to 3D classification tasks. Finally, in Section 4.3  ###reference_###, we conduct ablation studies to evaluate each component within our proposed Any2Point framework. The ScanObjectNN dataset Uy et al. (2019  ###reference_b38###) consists of real-world 3D object scans, categorized into 15 distinct classes. We select the most challenging PB-T50-RS split to test the performance of the Any2Point framework without the voting strategy. For all models, we employ the AdamW optimizer Loshchilov and Hutter (2017  ###reference_b22###) and the CosineAnnealing scheduler. The initial learning rate is set to 5e-4, with a weight decay factor of 0.05. We fine-tune the model for 300 epochs with a batch size of 32. For data augmentation, we use Random scaling, translation, and rotation. For language, 2D vision, and audio modalities, we respectively select the CLIP Text Encoder Radford et al. (2021  ###reference_b33###), DINO V2 Oquab et al. (2023  ###reference_b25###), and ImageBind Audio Encoder Girdhar et al. (2023  ###reference_b8###) as pre-trained models. For these three models, the transformer architecture is the same: a 12-block encoder with 768 feature channels and 1,024 input point number. The hyperparameter M in the 3D-to-any Virtual Projection is set to 6 with identical angles for the Any-Modality Transformers. To match the shape of the original PEs within pre-trained models, we virtually project 3D points into a 1D line segment of length 77 with a line size of 2 in the language modality; a 2D plane measuring 512x512 with a patch size of 26 in the 2D vision modality; and a 2D plane sized 192x304 with a patch size of 16 in the audio modality. The ModelNet40 dataset Wu et al. (2015  ###reference_b43###) consists of 40 categories of synthesized 3D CAD models, with 9,843 training samples and 2,468 test samples. In our experiments on ModelNet40, we adopt the same fine-tuning settings and the same pre-trained models as in ScanObjectNN. For data augmentation, we utilize default random scaling and translation. Notably, during the testing process, we do not employ the voting strategy. The experimental results demonstrate that our Any-to-3D Guided Adapter can efficiently mine and integrate pre-trained knowledge from other modalities to understand the semantics of 3D objects. Unlike other methods, our framework leverages 1D/2D spatial guidance to aggregate the local semantics of 3D tokens, capturing the local fine-grained information of 3D objects. As shown in Table 3  ###reference_###, to substantiate the efficacy of our proposed methods, we conducted ablation experiments by progressively incorporating each component into the baseline. The first row indicates the baseline configuration, which consists of the 3D tokenizer, the pre-trained transformer, and the task head, with updates applied only to the tokenizer and head. Introducing the 3D-to-any Virtual Projection resulted in performance improvements to 89.3% in the 1D modality and 86.6% in the 2D modality. This suggests that using virtual projection, rather than true projection, helps mitigate the loss of 3D spatial information caused by modality conversion. Following the inclusion of the Any-to-3D Guided Adapter, performance in the 1D modality surged to 90.9%, while in the 2D modality, it rose to 87.6%, with a focus on local structures leading to greater improvements. Introducing both aforementioned methods simultaneously led to a surge in performance to 91.9% in the 1D modality and a rise to 87.7% in the 2D modality, effectively showcasing the effectiveness of our comprehensive framework. ###table_2### ###table_3### ###table_4### In Table 4  ###reference_###, we investigated the effects of employing different positional encoding methods on the Any2Point framework. The first row indicates the absence of any positional encoding. Introducing"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Settings",
            "text": "The ScanObjectNN dataset consists of real-world 3D object scans, categorized into 15 distinct classes. We select the most challenging PB-T50-RS split to test the performance of the Any2Point framework without the voting strategy. For all models, we employ the AdamW optimizer and the CosineAnnealing scheduler. The initial learning rate is set to 5e-4, with a weight decay factor of 0.05. We fine-tune the model for 300 epochs with a batch size of 32. For data augmentation, we use Random scaling, translation, and rotation. For language, 2D vision, and audio modalities, we respectively select the CLIP Text Encoder, DINO V2, and ImageBind Audio Encoder as pre-trained models. For these three models, the transformer architecture is the same: a 12-block encoder with 768 feature channels and 1,024 input point number. The hyperparameter M in the 3D-to-any Virtual Projection is set to 6 with identical angles for the Any-Modality Transformers.\n\nTo match the shape of the original PEs within pre-trained models, we virtually project 3D points into a 1D line segment of length 77 with a line size of 2 in the language modality; a 2D plane measuring 512x512 with a patch size of 26 in the 2D vision modality; and a 2D plane sized 192x304 with a patch size of 16 in the audio modality.\n\nThe ModelNet40 dataset consists of 40 categories of synthesized 3D CAD models, with 9,843 training samples and 2,468 test samples. In our experiments on ModelNet40, we adopt the same fine-tuning settings and the same pre-trained models as in ScanObjectNN. For data augmentation, we utilize default random scaling and translation. Notably, during the testing process, we do not employ the voting strategy."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Quantitative Analysis",
            "text": "The results are shown in Table 1. It is observed that: (i) On the 3D real-world object dataset ScanObjectNN, the Any2Point framework achieves 91.9%, 87.7%, and 87.0% accuracy based on Language (CLIP-Text), 2D Vision (DINO V2-B), and Audio (ImageBind-Audio) modalities, respectively. Compared to the previous SOTA method (ReCon), 1D language pre-trained Any2Point achieves a 1.3% improvement with only 0.9M learnable parameters. For the 2D (Vision/Audio) modalities, Any2Point significantly outperforms Point-M2AE, which is the SOTA method pre-trained only on 3D datasets, by 0.6% and 1.3%, respectively. This reveals that our framework is capable of fully exploiting pre-trained knowledge from other modalities to solve 3D recognition tasks. \n\n(ii) On the 3D synthetic object dataset ModelNet40, across the Language, 2D Vision, and Audio modalities, our Any2Point framework attains 94.3%, 93.2%, and 92.7%. Our framework exclusively utilizes one pre-trained model in the 1D language modality, achieving a 0.2% improvement over the previous SOTA method (ReCon), and reducing 42.7M learnable parameters. For 2D modalities, Any2Point demonstrates performance on par with models pre-trained exclusively on 3D datasets. \n\n(iii) Surprisingly, whether on the ScanObjectNN or the ModelNet40 dataset, the Any2Point framework maintains a performance trend where 1D modality (language) outperforms 2D modalities (image and audio). Large language models provide abundant spatial and semantic information in low-dimensional spaces to assist in 3D learning. This trend is further validated in the upcoming Section 4.3."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Ablation Study",
            "text": "In this section, we conduct extensive ablation studies to explore the effectiveness of different components within our Any2Point framework. We adopt CLIP-Text (1D) and DINO V2 (2D) as the pre-trained transformer, and report the classification accuracy (%) on the \"PB-T50-RS\" split of the ScanObjectNN dataset.\n\nThe experimental results demonstrate that our Any-to-3D Guided Adapter can efficiently mine and integrate pre-trained knowledge from other modalities to understand the semantics of 3D objects. Unlike other methods, our framework leverages 1D/2D spatial guidance to aggregate the local semantics of 3D tokens, capturing the local fine-grained information of 3D objects.\n\nAs shown in Table 3, to substantiate the efficacy of our proposed methods, we conducted ablation experiments by progressively incorporating each component into the baseline. The first row indicates the baseline configuration, which consists of the 3D tokenizer, the pre-trained transformer, and the task head, with updates applied only to the tokenizer and head. Introducing the 3D-to-any Virtual Projection resulted in performance improvements to 89.3% in the 1D modality and 86.6% in the 2D modality. This suggests that using virtual projection, rather than true projection, helps mitigate the loss of 3D spatial information caused by modality conversion. Following the inclusion of the Any-to-3D Guided Adapter, performance in the 1D modality surged to 90.9%, while in the 2D modality, it rose to 87.6%, with a focus on local structures leading to greater improvements. Introducing both aforementioned methods simultaneously led to a surge in performance to 91.9% in the 1D modality and a rise to 87.7% in the 2D modality, effectively showcasing the effectiveness of our comprehensive framework.\n\nIn Table 4, we investigated the effects of employing different positional encoding methods on the Any2Point framework. The first row indicates the absence of any positional encoding. Introducing sinusoidal positional encoding or learnable positional encoding led to a certain degree of performance degradation. This is due to the conflict between the newly introduced positional information and the inherent semantics within the source modality transformer. On the other hand, employing 3D-to-any Virtual Projection resulted in respective improvements of 1.0% and 0.1% accuracy. The results demonstrate that using original 1D/2D positional priors can promote the pre-trained transformer to acquire 3D features.\n\nAs shown in Table 5, we conduct ablation experiments by incrementally adding components to the Any-to-3D Guided Adapter. The first row signifies the baseline adapter, consisting of only an MLP with bottleneck layers. By incorporating 1D/2D-guided Local Aggregation, composed of local aggregation in 1D/2D spaces, self-attention interactions, pooling, and propagation, our approach achieves performance gains of 0.9% and 0.2%. Leveraging the positional priors from the pre-trained model facilitates mining fine-grained 3D structural information from different perspectives. The Adaptive Any-to-3D Ensemble brings further improvements of 1.7% and 0.9% for 1D and 2D modalities, effectively integrating parallel features in accordance with 3D structural features. The experiments demonstrate the effectiveness of each component in our Any-to-3D Guided Adapter to gather 3D local geometric information, complementing the global attention in the pre-trained model.\n\nTo further validate our previous findings that the Any2Point framework, based on 1D Language pre-trained models, significantly outperforms those based on 2D modalities (Vision/Audio) in the 3D object recognition task, we conduct additional experiments in Table 6. On the \"PB-T50-RS\" split of ScanObjectNN dataset, we select RoBERTa (1D), DeiT (2D Vision), and SSAST (Audio) as the pre-trained models, with fine-tuning settings consistent with our previous experiments. These models achieve performance of 89.7%, 87.3%, and 87.1%, respectively. The performance trend across modalities is observed: 1D language > 2D Vision > 2D Audio. We suspect that due to the pre-training data, large language models possess stronger semantic information compared to other modalities, which is beneficial for the deep understanding of different 3D objects."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Visualization",
            "text": "In this section, we opt to validate the efficacy of the proposed 3D-to-any Virtual Projection and the Any-to-3D Guided Adapter by visualizing on the ScanObjectNN test set, utilizing the CLIP-Text Encoder (1D) Radford et al. (2021  ###reference_b33###) and DINO V2 (2D) Oquab et al. (2023  ###reference_b25###).\n###figure_7### ###figure_8###"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Different Positional Encoding Methods",
            "text": "Our 3D-to-any Virtual Projection fully exploits the positional encoding paired with the pre-trained model, injecting the source modality spatial knowledge into the 3D tokens during fine-tuning. In Figure 5  ###reference_###, when using sinusoidal positional encodings, learnable positional encodings, and our 3D-to-any Virtual Projection respectively, we visualize the attention scores of the [CLS] token to other point cloud tokens. As illustrated, for the 1D language modality, learnable positional encodings grasp useless information. After applying the commonly used sinusoidal positional encodings in Large Language Models, they fail to capture the critical 3D semantics. However, our method focuses more on the salient object parts, such as the armrests and wheels of chairs, and the legs of tables. For the 2D visual modality, learnable encodings are slightly better than sinusoidal positional encodings, as 2D pre-trained models mainly adopt the learnable encoding method. Meanwhile, our method directly recognizes the whole object and its key parts, for example, giving high weights to the chair\u2019s backrest."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Effects of Any-to-3D Guided Adapter",
            "text": "The Any-to-3D Guided Adapter captures the 3D fine-grained information through interactions within the local regions of the source modality. In Figure 6  ###reference_###, we visualize the clustering results of the similarities between the [CLS] token and other point token features, utilizing the complete Any-to-3D Guided Adapter, replacing the Adaptive Any-to-3D Ensemble with maxmean pooling, and further only using 3D positional information. As shown, for simple objects like chairs (1st row), our method effectively distinguishes between the chair\u2019s backrest, armrests, seat, and wheels, whereas removing components fails to capture the differences between key parts. For more challenging objects like shelves (2nd row), removing any components leads to semantic confusion of the object, while our approach clearly differentiates the shelf\u2019s base, middle layer, and backrest. These experiments indicate that each component within the Adapter effectively utilizes the positional information from different modalities to promote the 3D structure extraction."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Additional Ablation Study",
            "text": "In this section, we conduct ablation studies to explore the effectiveness of different components and hyper-parameters, which are not discussed in the main text. We select CLIP-Text Radford et al. (2021  ###reference_b33###) (1D) and DINO V2 Oquab et al. (2023  ###reference_b25###) (2D) as pre-trained models to compare the accuracy (%) on the \"PB-T50-RS\" split of ScanObjectNN Uy et al. (2019  ###reference_b38###).\n###table_11###"
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "The Adapter Positions and Depths.",
            "text": "In Table 7  ###reference_###, we further conducted ablation studies on the positions and depths of the proposed Any-to-3D Guided Adapter. As shown in Table 7  ###reference_###, the best performance is achieved when the adapter is placed after the Feed Forward Networks (FFNs) and at a depth of 12. This is because, when placed after the FFN layers, the globally interacted point cloud features undergo local aggregation within the adapter, extracting the fine-grained structures of the 3D point clouds. Moreover, deeper insertion allows the adapter to leverage both low-level and high-level pre-trained knowledge to process the point cloud information. It is important to note that for all pre-trained models, we inserted the Any-to-3D Guided Adapter after the FFN layers of all blocks.\n###table_12###"
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "The Components of 3D-to-any Virtual Projection.",
            "text": "In Table 8  ###reference_###, we validated the impact of different projection methods and the number of projection views on the pre-trained models for different modalities. As shown in Table 8  ###reference_###, for the 1D/2D modalities, the optimal performance is obtained when the number of views is set to 6. Meanwhile, for the 2D modality, the simple projection in PointCLIP Zhang et al. (2022b  ###reference_b51###) performs better than the more complex projection in PointCLIP V2 Zhu et al. (2023  ###reference_b55###). The findings suggest that employing an appropriate number of projection views sufficiently captures the diversity and complexity inherent in 3D data in low-dimensional spaces. Furthermore, a simple projection method proves adequate for representing the fine-grained structure and data characteristics of 3D point clouds. It is worth noting that we use simple projection Zhang et al. (2022b  ###reference_b51###) and 6 views for all pre-trained models of any modalities.\n###table_13### ###table_14###"
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "The Influences of Different Local Neighborhood Sizes.",
            "text": "In Table 9  ###reference_###, we investigated the performance impact of various combinations of 1D line sizes, 2D patch sizes, and 3D grid sizes Wu et al. (2022  ###reference_b42###) during the 1D/2D-guided local aggregation stage. As demonstrated in Table 9  ###reference_###, when the 1D line size and 2D patch size are set to moderate values of 2 and 26, respectively, remarkable performance is attained. These findings indicate that an appropriate local aggregation size enhances the model\u2019s comprehension of 3D local information, whereas excessively large or small sizes could lead to the loss of critical features. For diverse tasks, we have consistently employed similar local aggregation sizes for pre-trained transformers of 1D/2D modalities.\n###figure_9###"
        },
        {
            "section_id": "6.4",
            "parent_section_id": "6",
            "section_name": "The Importance of 1D/2D-guided Local Aggregation.",
            "text": "In Table 10  ###reference_###, we validated the advantages of utilizing positional priors from the source modalities that are compatible with the pre-trained model, by injecting the positional priors of different modalities (1D/2D/3D) into the local aggregation step within the Any-to-3D Guided Adapter. As shown in Table 10  ###reference_###, compared to using the original 3D spatial knowledge, our method exhibits a performance improvement of 1.5%-4.5% for 2D/1D modalities. This demonstrates that our proposed method effectively addresses the issue of positional discrepancy between the original 3D positions and the pre-trained model."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Additional Visualization",
            "text": "We conduct visualization experiments on the test split of the ModelNet40 dataset Wu et al. (2015  ###reference_b43###) utilizing the CLIP-Text Radford et al. (2021  ###reference_b33###) (1D) and DINO V2 Oquab et al. (2023  ###reference_b25###) (2D)."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "The Significance of Encoding 3D Positions in 1D/2D PEs",
            "text": "To demonstrate the effect of our method that uses 1D/2D Positional Embeddings (PEs) from source modalities to encode 3D positions, we randomly selected one point on a 3D object (inside the black circle in the first column) and computed the cosine similarity between the positional embedding of the point and those of other tokens. We compared 3D PEs assigned under different views (the six columns on the right) and the final 3D PEs obtained by averaging over M views (the first column), where M is set to 6. As shown in Figure 7  ###reference_###, the first column shows higher similarity in areas closer to our selected point, while in other columns, high similarity areas are scattered at farther locations. For example, in the first and second rows, we selected a point on the nose of a plane, and in the third and fourth rows, we selected a point on the base of a lamp. In our method (the first column), similarity values decreases with distance, showing a transition from strong to weak, whereas in the other columns, the distribution appears irregular. It indicates that our proposed 3D positional embeddings implicitly establish spatial relationships in 3D space."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In conclusion, our paper proposes Any2Point to enable a general any-to-3D transferring framework, empowering any-modality pre-trained large models (e.g., 2D vision, language, and audio) for efficient 3D understanding. Within Any2Point framework, we introduce two techniques, named 3D-to-any virtual projection and any-to-3D guided adapter, to extract 3D structure knowledge while efficiently fine-tuning pre-trained models. This enables us to overcome issues within current methods, such as 3D geometry loss and excessive resource cost. Our extensive experiments across various tasks demonstrate the superior performance and efficiency of Any2Point compared to previous SOTA 3D pre-trained models, achieving remarkable results with only a fraction of the trainable parameters."
        }
    ],
    "url": "http://arxiv.org/html/2404.07989v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.3",
            "6"
        ]
    },
    "research_context": {
        "paper_id": "2404.07989v2",
        "paper_title": "Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding",
        "research_background": "**Motivation:**\nThe motivation behind this paper arises from the rapid advancements and widespread applicability of large foundation models in various domains and tasks, such as natural language processing (e.g., BERT, T5, GPT) and visual understanding (e.g., DINOV2, MAE, ViT-22B). While these large models have seen substantial success, the field of 3D visual understanding lags significantly. This gap is due to the high cost and complexity of acquiring large-scale, annotated 3D data, as well as the insufficiencies of synthetic 3D data. Current attempts to bridge this gap primarily rely on transferring knowledge from 2D vision models to 3D tasks through data modality transformation or cross-modality knowledge distillation. However, these methods often lead to a loss of 3D spatial information and high computational costs. There is a pressing need for a more efficient and generalized approach to leverage the capabilities of large pre-trained models from various modalities for 3D understanding.\n\n**Research Problem:**\nThe primary research problem addressed by this paper is the development of a unified framework that enables the transfer of pre-trained large models from various modalities (such as 1D language, 2D image, and audio) to 3D domains efficiently and effectively. Specifically, the paper aims to overcome the limitations of existing approaches that involve information loss during 2D projection of 3D data and the high resource costs associated with cross-modality knowledge distillation. The goal is to create a method that preserves 3D geometric information while leveraging the pre-trained knowledge from various source modalities.\n\n**Relevant Prior Work:**\n1. **Large Models in NLP and Visual Understanding:**\n    - BERT (Devlin et al., 2018), T5 (Raffel et al., 2020), GPT series (OpenAI, 2023), and language models discussed by Floridi and Chiriatti (2020).\n    - Visual understanding models like DINOV2 (Oquab et al., 2023), MAE (He et al., 2022; Wei et al., 2022; Xie et al., 2022), and ViT-22B (Dehghani et al., 2023).\n    - Efficient fine-tuning techniques (Hu et al., 2021; Liu et al., 2021; Chen et al., 2022; Jia et al., 2022).\n\n2. **3D Visual Understanding:**\n    - General 3D visual understanding models (Zhang et al., 2022b; Dai et al., 2017; Qi et al., 2017a; Guo et al., 2023b).\n    - Applications in robotics and autonomous driving (Li et al., 2023; Guo et al., 2023c; Yang et al., 2023; Pan et al., 2023).\n\n3. **2D-to-3D Knowledge Transfer Approaches:**\n    - **Data Modality Transformation:**\n        - Projecting 3D point clouds into 2D images for processing by 2D pre-trained models (Wang et al., 2022; Zhang et al., 2022b; Zhu et al., 2023).\n    - **Cross-Modality Knowledge Distillation:**\n        - Transferring pre-trained knowledge from 2D or vision-language models to newly trained 3D models (Zhang et al., 2023b; Dong et al., 2022; Xue et al., 2023).\n\nThese prior works highlight both the potential and the limitations in current methods of transferring pre-trained knowledge from other modalities to 3D understanding tasks, thus anchoring the motivation and necessity for the innovative approach proposed in the paper.",
        "methodology": "The proposed method, Any2Point, aims to adapt large models pre-trained on any modality (vision, language, and audio) to empower them with 3D understanding capabilities. The methodology emphasizes efficiency by keeping the source transformers frozen to avoid high computation costs and over-fitting issues.\n\n### Paradigm Overview\n1. **Pre-trained Transformers**: The method uses pre-trained transformers from various modalities (1D transformers for sequential data and 2D transformers for spatial data).\n2. **3D Mini-network for Tokenization**: The original embedding modules of the source transformers are discarded. Instead, a 3D mini-network tokenizes input point clouds, transforming raw point clouds into high-dimensional vectors.\n\n### 3D-to-any Virtual Projection Module\n1. **Positional Encoding**: The module assigns each 3D token with positional information within the source modality. This helps the frozen 1D/2D transformers comprehend 3D positional information.\n2. **Virtual Projection**: For 2D transformers, 3D coordinates are virtually projected into multiple 2D views to derive corresponding 2D coordinates, which are then used to assign positional encodings. For 1D transformers, 3D coordinates are virtually projected into 1D lines, and similar positional encodings are assigned.\n\n### Any-to-3D Guided Adapters\n1. **1D/2D Guided Local Aggregation**: Adapters are used to capture 3D semantics within local neighborhoods guided by 1D/2D positional priors. This process helps align 1D/2D PEs with 3D tokens.\n2. **Parallel Local Aggregation**: For each 3D token, the local feature interaction is performed within 1D/2D neighborhoods, followed by pooling and propagation operations.\n3. **Adaptive Any-to-3D Ensemble Approach**: After obtaining several sets of 3D tokens representing different views or lines, their features are aggregated adaptively. The relative importance of each view/line is determined through cosine similarity, and the features are integrated to produce high-quality adapted features.\n\n### Key Components\n1. **3D Tokenization**: Utilizes Farthest Point Sampling (FPS), -Nearest Neighbor (-NN) algorithm, and learnable linear layers for feature encoding.\n2. **Positional Encoding Integration**: The method ensures that new 3D PEs align with the pre-trained 1D/2D PEs by virtually projecting 3D coordinates into 1D/2D space and averaging the positional encodings.\n3. **Local Aggregation within Adapters**: Self-attention layers and pooling operations are used to handle local interactions within neighborhoods, guided by positional priors.\n4. **Final Feature Aggregation**: Employs an adaptive ensemble strategy using a 3D feature transformation branch to weight different views/lines dynamically.\n\n### Innovations\n1. **Virtual Projection Mechanism**: Allows the integration of 3D positional information with frozen 1D/2D transformers.\n2. **1D/2D Guided Local Aggregation**: Ensures better alignment of positional encodings with the tokenized 3D data.\n3. **Adaptive Ensemble Strategy**: Dynamically aggregates features from multiple perspectives, enhancing the quality of adapted features.\n\nIn summary, Any2Point offers a parameter-efficient solution to adapting any-modality large models for 3D understanding by employing frozen transformers with strategically designed tokenization and adapter mechanisms.",
        "main_experiment_and_results": "**Main Experiment Setup:**\n\n1. **Datasets:**\n   - **ScanObjectNN:** Contains real-world 3D object scans categorized into 15 classes. The PB-T50-RS split is chosen to evaluate performance.\n   - **ModelNet40:** Comprises 40 categories of synthesized 3D CAD models, with 9,843 training and 2,468 test samples.\n\n2. **Pre-trained Models and Modalities:**\n   - **Language Modality:** CLIP Text Encoder.\n   - **2D Vision Modality:** DINO V2.\n   - **Audio Modality:** ImageBind Audio Encoder.\n   - These models utilize a 12-block transformer encoder with 768 feature channels and 1,024 input points.\n\n3. **Fine-tuning Settings:**\n   - **Optimizer:** AdamW.\n   - **Scheduler:** CosineAnnealing.\n   - **Learning Rate:** 5e-4.\n   - **Weight Decay:** 0.05.\n   - **Epochs:** 300.\n   - **Batch Size:** 32.\n   - **Data Augmentation:** Random scaling, translation, and rotation.\n\n4. **Virtual Projection:**\n   - **Language Modality:** Projects 3D points into a 1D line segment of length 77 with a line size of 2.\n   - **2D Vision Modality:** Projects into a 2D plane of 512x512 pixels with a patch size of 26.\n   - **Audio Modality:** Projects into a 2D plane of 192x304 pixels with a patch size of 16.\n   - Hyperparameter M for 3D-to-any Virtual Projection is set to 6.\n\n**Evaluation Metrics:**\n- Performance improvements are measured in terms of accuracy.\n\n**Baselines:**\n- **Prompt Tuning.**\n- **Adapter Tuning.**\n- **Low-Rank Adaptation (LoRA).**\n\n**Main Experimental Results:**\n\n- **Performance on ScanObjectNN (PB-T50-RS split):**\n  - The **Any-to-3D Guided Adapter** shows significant performance gains over traditional PEFT techniques.\n  - Improvements over Prompt Tuning: +2.8%, +1.3%.\n  - Improvements over Adapter Tuning: +2.3%, +1.8%.\n  - Improvements over LoRA: +5.6%, +2.6%.\n\n- **Performance on ModelNet40:**\n  - Consistent performance improvements aligning with those observed on ScanObjectNN.\n  - The Any2Point framework shows an ability to efficiently integrate pre-trained knowledge from different modalities (1D, 2D Vision, Audio) for 3D object understanding.\n  - The framework's superiority is attributed to leveraging 1D/2D spatial guidance to capture 3D object local semantics. \n\n- **Modality-Based Performance Trends:**\n  - **1D Language (RoBERTa):** 89.7%.\n  - **2D Vision (DeiT):** 87.3%.\n  - **Audio (SSAST):** 87.1%.\n  - Performance ranking: 1D Language > 2D Vision > 2D Audio, suggesting stronger semantic information integration in language-based pre-trained models, beneficial for 3D objects recognition.\n\nThese results emphasize the efficiency and effectiveness of the Any2Point framework in adapting pre-trained 1D and 2D models for 3D understanding tasks. The robustness across datasets and the substantial improvements over baseline PEFT methods illustrate the framework's ability to synergize multi-modal pre-trained knowledge for enhanced 3D classification accuracy."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To explore the effectiveness of different components within the Any2Point framework by utilizing CLIP-Text (1D) and DINO V2 (2D) as pre-trained transformers and measuring the classification accuracy on the ScanObjectNN dataset.",
            "experiment_process": "The experiments use the 'PB-T50-RS' split of the ScanObjectNN dataset, with pre-trained transformers CLIP-Text (1D) and DINO V2 (2D). Different techniques, such as Prompt Tuning, Adapter Tuning, and Low-Rank Adaptation (LoRA), are compared to the Any-to-3D Guided Adapter. The baseline configuration includes the 3D tokenizer, pre-trained transformer, and task head. Components such as 3D-to-any Virtual Projection and Any-to-3D Guided Adapter are progressively introduced, with performance measured at each step. Different positional encoding methods and components within the Any-to-3D Guided Adapter are also tested for their impact on performance.",
            "result_discussion": "The Any-to-3D Guided Adapter significantly outperforms traditional PEFT techniques, with improvements of up to 5.6% over LoRA. Progressive incorporation of components, such as 3D-to-any Virtual Projection and Any-to-3D Guided Adapter, shows substantial performance gains. Specifically, the combination of both methods results in 91.9% accuracy for 1D and 87.7% for 2D. Positional encoding methods reveal that 3D-to-any Virtual Projection is more effective than sinusoidal or learnable positional encodings. Additional components like 1D/2D-guided Local Aggregation and Adaptive Any-to-3D Ensemble further enhance the model's performance. Finally, models based on 1D language pre-trained transformers significantly outperformed 2D Vision and Audio models, indicating that 1D modalities like language offer stronger semantic information beneficial for 3D understanding.",
            "ablation_id": "2404.07989v2.No1"
        },
        {
            "research_objective": "To explore the effectiveness of different components and hyper-parameters within the Any2Point framework that were not discussed in the main text.",
            "experiment_process": "CLIP-Text (1D) and DINO V2 (2D) pre-trained models are selected to compare their accuracy on the 'PB-T50-RS' split of the ScanObjectNN dataset. The study extends the existing experimental setup to investigate additional components and hyper-parameters.",
            "result_discussion": "The additional experiments conducted in Table 6 validate previous findings that 1D Language pre-trained models (like RoBERTa) significantly outperform 2D modalities (like DeiT for Vision and SSAST for Audio) on the 3D object recognition task. Specifically, RoBERTa achieves 89.7% accuracy, while DeiT and SSAST achieve 87.3% and 87.1%, respectively. This trend is likely due to the stronger semantic information present in large language models, which aids in the deep understanding of 3D objects.",
            "ablation_id": "2404.07989v2.No2"
        }
    ]
}