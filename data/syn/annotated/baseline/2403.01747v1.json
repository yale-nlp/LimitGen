{
    "title": "Towards Self-Contained Answers: Entity-Based Answer Rewriting in Conversational Search",
    "abstract": "Conversational Information Seeking (CIS) is an emerging paradigm for knowledge acquisition and exploratory search. Traditional web search interfaces enable easy exploration of entities, but this is limited in conversational settings due to the limited-bandwidth interface. This paper explores ways to rewrite answers in CIS, so that users can understand them without having to resort to external services or sources. Specifically, we focus on salient entities\u2014entities that are central to understanding the answer.\n\nAs our first contribution, we create a dataset of conversations annotated with entities for saliency. Our analysis of the collected data reveals that the majority of answers contain salient entities. As our second contribution, we propose two answer rewriting strategies aimed at improving the overall user experience in CIS. One approach expands answers with inline definitions of salient entities, making the answer self-contained. The other approach complements answers with follow-up questions, offering users the possibility to learn more about specific entities.\n\nResults of a crowdsourcing-based study indicate that rewritten answers are clearly preferred over the original ones. We also find that inline definitions tend to be favored over follow-up questions, but this choice is highly subjective, thereby providing a promising future direction for personalization.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "Satisfying users\u2019 information needs is the primary goal of any information retrieval system. Such search systems are frequently being used for acquiring new knowledge and enabling effective interaction with them has been the focus of a significant body of research. With the advent of conversational agents, the landscape of search is changing, with rapid progress being made in question understanding and result retrieval. However, little attention has been paid to supporting users according to their knowledge level and ensuring that they can actually understand the answers returned by the system. While traditional web search offers users the possibility to follow hyperlinks or consult knowledge panels in search engine results pages (SERPs) to learn about certain concepts they might be unfamiliar with, such an opportunity is taken away in conversational information seeking (CIS) due to the limited bandwidth interface. For example, while the system\u2019s generated response might be concise and indeed answer the given question, it might mention concepts that the user is unfamiliar with. We argue that CIS systems offer a unique opportunity to proactively assist an individual\u2014with this work, we aim to make a step in this direction.\n\nEntities are natural units for organizing information and can improve the user experience throughout the search process. This paper investigates how to make answers more accessible to users in a text-based conversational setting. The main hypothesis underlying our work is that allowing users to learn more about certain entities mentioned in the answer would lead to an improved user experience. However, not all entities are equally important. Therefore, we utilize the notion of entity salience to capture how central a given entity is to understanding the answer returned by the system in response to a question. Entity salience has been studied in the context of web search, where it is defined as entities being central and prominent, capturing the aboutness of the Web page.\n\nIn this study, we regard entities as anything that could have a Wikipedia page, including named entities, events, and general concepts. While only about 5% of entities are salient in Web pages, answers in a conversational setting are short with only a few entities present, therefore yielding a higher ratio of salient entities. However, not knowing those entities might seriously impair the user\u2019s understanding of the answer. Once the top salient entities are identified, we propose two answer rewriting strategies aimed at helping users to understand the system\u2019s response. One approach rewrites the answer to expand it with inline definitions of salient entities, making the answer self-contained. The other approach complements the answer with a follow-up question, offering users the possibility to learn more about specific entities. See Fig. 1 for an illustration.\n\nThe first research question we ask is (RQ1) What are the characteristics of salient entities in CIS? To address this question, we conduct an analysis of answers using crowdsourcing. Specifically, we extract a number of entities from the answers and ask crowd workers to assess their saliency based on how essential they are to properly understand the answer to the given question. We find that the majority of the answers contain a number of highly salient entities, providing strong motivation for answer rewriting. At the same time, our results also suggest that saliency is highly subjective and is likely influenced by the user\u2019s background knowledge. Additionally, we identify categories of salient entities that do not require further definitions as they belong to common sense knowledge or are already explained in the answer.\n\nThe second research question we address is (RQ2) How to utilize salient entities in answer rewriting for an improved user experience? We consider two variants of answer expansion by (1) adding definitions from a knowledge base after the entity mention in parentheses, and (2) inserting human-written descriptions in the text in a natural manner. Similarly, we study two options for follow-up generation: (1) asking the user directly whether they want definitions of salient entities, and (2) offering an optional follow-up to learn more about specific entities. An experimental comparison of these four alternatives using crowdsourcing reveals that users generally prefer some type of answer rewrite over the original answer, with inline definitions being generally favored over answers with follow-up questions.\n\nAs part of our experimental protocol, we also ask crowd workers to provide a free-text justification for their choice of answer rewrite preference. We observe high subjectivity in these responses, with some annotators favoring the original answer for its conciseness, some preferring the one with inline definitions for its comprehensiveness, and others appreciating the conversational nature of answers with follow-up questions. Overall, our results provide a strong motivation for future research on personalizing answer rewriting, considering both the background knowledge and interaction preferences of users.\n\nAdditionally, we explore the potential of using large language models (LLMs) for the entity-based answer rewriting task, given the recent success of LLMs in a wide array of natural language processing and information retrieval tasks. Specifically, we experiment with various ways of prompting ChatGPT for end-to-end answer rewriting. Our initial analysis revealed significant shortcomings in terms of knowledge distortion, failure to explain entities, or significantly increasing the answer length, making it un"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Related work",
            "text": "We highlight relevant research in the areas of CIS and entity-centric search. As the distinction between conversational search and conversational Q&A is blurred (Zamani et al., 2023  ###reference_b43###), we use CIS as an umbrella term."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. Conversational Information Seeking",
            "text": "CIS has emerged as an increasingly popular method of retrieving information, including information from the Web (Anand et al., 2020 ###reference_b4###). Several research directions have span from CIS, including mixed-initiative interactions (Zamani et al., 2023 ###reference_b43###). Under the mixed-initiative paradigm, the system can at any point proactively take initiative and ask the user clarifying questions or offer suggestions.\n\nWhile mixed-initiative is a relatively well-established concept in the IR community (Allen et al., 1999 ###reference_b3###), recent advancements in CIS systems have demonstrated the effectiveness of asking clarifying questions with a goal of elucidating the underlying user\u2019s information need (Aliannejadi et al., 2019 ###reference_b2###). We take advantage of such opportunity and propose to rewrite the answer, offering follow-up to users, as discussed in Section 4 ###reference_###.\n\nSzpektor et al. (2020 ###reference_b36###) proposed a dynamic composition-based model for conversational domain exploration (CODEX), which enables users to enrich their knowledge through interactions with the system. They highlight several challenges, including maintaining an engaging experience, avoiding repetitions, and choosing the appropriate response length. While in this work we focus on ensuring user\u2019s understanding of answers, some of the points we touch upon are related to the goal of user engagement and not burdening the user with long or repetitive definitions of salient entities.\n\nTo the best of our knowledge, response rewriting with the purpose of making sure a user understands the response to their question in CIS has not been explored. However, researchers have studied text rewriting in IR for personalization and text simplification. While text simplification has been shown to improve readability and understanding in medical (Leroy et al., 2013 ###reference_b25###) and scientific texts (Ermakova et al., 2022 ###reference_b13###), it is usually done by swapping relatively unfamiliar words with more common alternative words (Leroy et al., 2013 ###reference_b25###) or leveraging large-scale language models for complete rewriting of the text (Sheang and Saggion, 2021 ###reference_b35###). In this setting, a certain degree of information distortion is acceptable, as the text rewritten with such methods might differ from the original due to word substitutions. On the other hand, we aim to allow the user to learn about a topic of interest, thus retaining the original terminology."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. Entities",
            "text": "Marchionini (2006) categorizes search activities in two broad categories: look-up questions and exploratory search, with the latter requiring carefully curated user interaction (C mara et al., 2021).\nDuring their exploratory web search, users often have the possibility to learn about entities of their interest by following hyperlinks or reformulating their query based on newly seen entities (Eickhoff et al., 2014).\nEntity linking and entity-based search are core components in that process (Balog, 2018).\nThus, significant research efforts were put into developing entity linking methods, including entity linking in the Web (Han et al., 2011), in free texts (Piccinno and Ferragina, 2014), and in CIS (Joko and Hasibi, 2022; Joko et al., 2021).\nWhile documents may contain a large number of entities, some of them are salient, thus central to modeling the aboutness of a document (Paranjpe, 2009), and others are not.\nMoreover, Gamon et al. (2013) find that only about 5% of the entities in Web pages are salient, while others are often mentioned somewhat sporadically. These salient entities are crucial for the user to be familiar with, in order to satisfy their information need.\nHowever, in shorter texts that contain fewer entities, this percentage is anticipated to be higher (Wu et al., 2020). Answers in CIS are a prime example of such shorter texts.\nYet, research on entity salience in CIS is lacking, providing a strong motivation for this work. Another aspect of entity salience we aim to explore is how important they are for the user\u2019s understanding of the texts and readability (Collins-Thompson et al., 2011).\nThere is an important distinction to be made between entity salience and entity relevance or entity importance (Gamon et al., 2013).\nFor example, Joe Biden is objectively an important entity, however, it can be marginal to the document\u2019s topic.\nAs such, entity relevance is dependent on the user\u2019s intent and their underlying information need.\nOn the other hand, an entity is salient to a document if it is central and important for the overall topical and informational coherency of the document.\nThus, we argue that salient entities are essential to know about for a complete understanding of the provided answers in CIS.\nIn this work, we explore their prevalence, characteristics, and ways of improving user experience via answer rewriting around identified salient entities."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Understanding Salient Entities in Conversational Information Seeking",
            "text": "In this section, we define salient entities in CIS and present several research questions. Then, we describe the dataset acquisition process with crowdsourcing. Finally, we showcase relevant aspects of the created dataset and analyze special cases of salience."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Problem statement",
            "text": "A salient entity captures the aboutness of the text and is thus central to the given document (Paranjpe, 2009).\nIn CIS, answers to user\u2019s questions are usually short, containing from a single to a few sentences with only a few entities present.\nIdentifying salient entities in such answers is thus imperative, as they are essential for the user\u2019s understanding of the given answer.\nIn this work, we inspect the prevalence of entity salience in CIS.\nWe define entity saliency on a graded scale of 0 to 2, with a score of 0 corresponding to the entity not being salient at all and 2 to the entity being highly salient.\nIn this section, we aim to shed light on RQ1: What are characteristics of salient entities in CIS? We break this generic question into a series of more specific subquestions:\nHow prevalent are salient entities in answers in CIS?\nHow well do users agree on which entities are salient?\nIs there empirical evidence that the notion of entity salience is different in conversational answers than in documents?\nAre there entities that are salient, but do not require explicit definitions?"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Dataset Acquisition",
            "text": "In order to model entity salience in CIS, we extend QReCC (Anantha et al., 2021)\u2014an open-domain conversational question answering dataset containing 14k conversations. Excerpts from QReCC with their original sources and saliency annotations are shown in Table 1. All of the conversations have been normalized to contain multi-turn interactions with manually resolved utterances and manually checked responses. This, together with its diversity, makes QReCC appropriate for our work on entity salience in CIS.\n\nIn this work, we provide a deep analysis of the dataset in terms of entity salience modeling and thus subsample the original QReCC dataset. We restrict ourselves to the test portion of QReCC, as it contains utterances from the curated dataset. Additionally, in order to annotate as many conversations as possible within reasonable cost, we restrict ourselves to conversations up to a depth of 3, thereby trading off conversation depth for higher breadth coverage.\n\nWe employ an established entity linker, WAT (Piccinno and Ferragina, 2014), to extract entities from the system\u2019s responses. As suggested by the authors, we use a reasonable, slightly precision-oriented confidence threshold for extracting entities from texts. We filter out the entities that appear in the question as well, assuming the user asking the question already knows about them. This procedure results in an entity set, containing several entities extracted from the given answer, that do not appear in the question.\n\nNow that we have (question, answer, entity_set) triplets, we employ crowdsourcing to annotate which entities from the entity set can be considered salient. Given the question and the answer, the task is to annotate the degree to which a given entity is considered essential for understanding the answer. After an initial analysis of the entities and their importance in understanding the answer, we opted for a graded relevance scale. We adopt an annotation scenario where an entity can be either essential, important, or not important. We draw the similarities of our annotation scheme with well-established graded relevance schemes in IR (Sakai, 2021), where a document can fully satisfy a user\u2019s information need, partially, or be irrelevant.\n\nWe define the following labels for an entity:\n- Knowing about the entity is essential for understanding the answer to the question. It is not possible to comprehend the answer without knowing about (being familiar with) the entity. This label corresponds to a salience score of 2.\n- Knowing about the entity is important for a deeper and more complete understanding of the answer. However, it is not essential and the user can partially comprehend the answer without knowing about the entity. This label corresponds to a salience score of 1.\n- The entity is not important for understanding the answer to the question, nor does its knowledge benefit the user\u2019s knowledge on the topic. This label corresponds to a salience score of 0.\n\nWe use Amazon Mechanical Turk as our annotation platform. All workers are required to have at least 1,000 approved annotations with a minimum 95% overall approval rate and be based in the United States, in order to mitigate the potential language barrier for understanding the task. Each (question, answer, entity) triplet is annotated by five different workers. To ensure high quality annotations, we manually curate a test set of (question, answer, entity) triplets that the workers need to annotate correctly in order for their annotations to count towards the final dataset. The size of the test set is 25% of the final dataset size. Additionally, we track workers\u2019 mouse clicks and discard annotations that are done recklessly and quickly. Workers take on average seconds per (question, answer, entity) triplet. To ensure ethical use of crowd workforce, we provide an appropriate compensation of $0.20 for 5 annotated entities, resulting in an average of $18/h, which is over 250% of the minimum wage in the USA."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3. Analysis",
            "text": "In this section, we answer our research questions through an extensive analysis of the acquired dataset on entity salience in CIS."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1. Presence of Salient Entities.",
            "text": "In order to focus on answers with a certain level of complexity, we selected answers with at least 2 entities present (as extracted by WAT). This resulted in annotation of 120 QA pairs, containing more than 400 entities. Each (question, answer, entity) pair was assessed by five different workers, resulting in a total of over 2,000 annotations. In the annotated dataset, there are on average entities present in the answers. The average salience of those entities, as assessed by crowd workers, is (40% annotated with salience of 2, 53% with 1, and 7% with 0). In response to RQ1.a, this means that there are more salient entities than non-salient ones in CIS answers. This finding is further confirmed by averaging the saliency scores for each entity and computing the portion of salient ones (e.g., average saliency score) over the total number of entities in the answer. This ratio is, meaning that on average 63% of all entities in CIS can be considered salient entities. Moreover, we analyze salience throughout the conversation. Figure 2 shows examples of the development of an entity salience through three turns of the conversation. The entity epilepsy is mentioned sporadically in the answer at turn 1, but becomes considerably more salient in the subsequent turn. Overall, we observe an average change of saliency score between two consecutive turns of, suggesting that an entity might become more or less essential as the focus of the conversation changes. Entities might be sporadically mentioned in earlier turns of the conversation, but with users\u2019 further queries they can become central to the topic of the conversation."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2. Subjectivity in Assessing Entity Salience",
            "text": "To answer RQ1.b, we compute Fleiss\u2019 kappa (Fleiss, 1971) to measure the subjectivity of the annotators assessing the degree of saliency of an entity, i.e., how essential is the entity for a complete understanding of the answer. The computed kappa is low, suggesting weak inter-annotator agreement and high subjectivity for the task (Viera et al., 2005).\n\nWe additionally compute Spearman\u2019s rank correlation coefficient between all pairs of workers that annotated a specific QA pair. With this step, we try to assess potential subjectivity levels that are due to different perceptions of the scale of essential/important/unimportant entities. For example, two workers might agree on which of the entities is more salient, while their perception of the saliency scale differs slightly. The average Spearman\u2019s rho is 0.45, which suggests a fair agreement and thus a certain level of skewed score subjectivity, which is different from weak agreement measured by kappa.\n\nOverall, we conclude that the task of assessing which entities are essential for answer understanding is highly subjective. The subjectiveness may come from different user background knowledge, their perception of salience, but also from personalities. However, having labels collected from five different annotators allows for a robust assessment of entity salience. The data suggests that there is a lot of potential for dealing with personal preferences and subjectivity when estimating entity salience."
        },
        {
            "section_id": "3.3.3",
            "parent_section_id": "3.3",
            "section_name": "3.3.3. Entity Salience in Documents vs. in CIS.",
            "text": "We hypothesized that the notion of entity salience is different in CIS than in Web documents. To assess this hypothesis, we compute the entity salience score using a state-of-the-art model for salience prediction in documents, SWAT (Ponza et al., 2019). For each QA pair, we compute Spearman\u2019s over the entities ranked by salience score from the dataset and the entities ranked by salience score as predicted by SWAT. The computed average is 0.25, indicating low to moderate correlation. This suggests that document-level salience prediction methods are not entirely fit for the task of entity salience identification in CIS. Furthermore, the prevalence of salient entities is significantly higher in CIS answers (63%), as opposed to Web documents (5%), as reported by Gamon et al. (2013)."
        },
        {
            "section_id": "3.3.4",
            "parent_section_id": "3.3",
            "section_name": "3.3.4. Special Cases of Salient Entities",
            "text": "Another important finding of the analysis is the case that although most of the answers contain salient entities, which require user\u2019s familiarity to comprehend the answer, not all such entities necessarily require definitions. To answer RQ1.d, we take a random subsample of more than a hundred entities from the crowd-annotated answers for analysis with a goal of finding potential patterns. We then perform expert annotation (done by one of the authors of the paper) by carefully inspecting entities in the context of a conversation and note whether they would potentially require explicit definitions or not. In our analysis, several special cases of entities arose, which might not require further steps to be taken by the CIS system, even if deemed salient. Table 2 presents the described cases, with an example and their prevalence, as indicated by the percentage of such entities in the subsampled set. We estimate that around of the entities belong to one of the special cases and potentially do not require definitions, with the biggest category being common-sense knowledge entities."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Answer Rewriting",
            "text": "We have established that salient entities occur frequently in answers to CIS questions. In this section, we aim at rewriting the answers containing salient entities with the goal to aid users\u2019 understanding. To this end, we propose two answer rewriting strategies. The first strategy aims to rewrite the original answer by inserting inline definitions of the identified salient entities, thus making the answer self-contained. The second strategy makes use of the mixed-initiative CIS paradigm and offers the user to learn more about any of the identified salient entities. These rewrite types are further explained in the following sections."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. Inline Entity Descriptions",
            "text": "Our first strategy towards ensuring the complete understanding of the answer is based on including the explanations of the identified salient entities in the answer itself. Formally, we rewrite the original answer by providing inline definitions for each of the salient entities, resulting in the answer rewrite A-inlinedef. The answer A-inlinedef is thus self-contained, as all of the salient entities are explicitly described. One of the challenges here is to keep the explanations reasonably short and adequate for a conversational setting, as explaining the answer with long definitions would result in a significantly longer answer than the original, thereby overwhelming the user. Thus, we experiment with two alternatives for providing inline definitions."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1. Wikibase Entity Descriptions.",
            "text": "We utilize a knowledge base to extract definitions of salient entities. Specifically, we consult Wikibase to retrieve the entry of a given entity and get its definition. To construct the final rewritten answer, we insert the definition in parentheses immediately after the first mention of the entity."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2. Manually Curated Entity Description.",
            "text": "We hypothesize that entity descriptions inserted into parentheses might appear ineloquent and unnatural for a conversational setting. Therefore, we manually go through the entity descriptions in A-inlinedef/wiki answers and rewrite them to sound more natural. The process of manual rewriting involves, among others, avoiding highly technical or too verbose definitions. Moreover, we insert after such that the definition is blended in the text more naturally, i.e., using commas ( ). This text simplification task could potentially be carried out by a pretrained large-scale language model. However, in our initial experiments with T5 (Raffel et al., 2020), we observed several inaccuracies. As our objective is to measure the usefulness of answer rewrites to users, we opted for human curation to ensure that the findings of this study are not impacted by the imperfections of automatic rewrites. The answer rewritten with this method is referred to as A-inlinedef/human."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Mixed-initiative Follow-up Prompt",
            "text": "Under the mixed-initiative paradigm in CIS, the system can at any point take initiative and prompt the user with various elicitation, clarification, or other questions (Allen et al., 1999; Zamani et al., 2023). As one of the potential limitations of the previously described approach is overwhelming the user with potentially unnecessary entity definitions, we instead ask the user whether they require the explanations of salient entities or not. To this end, we experiment with two different follow-up prompts, described below."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1. Follow-up Question.",
            "text": "The first type of follow-up we propose is a direct question, aimed at asking whether the user is familiar with the salient entities identified in the answer. To construct a direct clarifying question, we create a new answer by expanding the original answer with a question formatted as \u201cDo you want to learn more about , , or ?\u201d, where is among the top N most salient entities identified."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2. Follow-up Offer.",
            "text": "An offered follow-up prompt (A-followup/offer) is designed by expanding the original answer with \u201cIf you wish to learn more about , , or , feel free to ask.\u201d We hypothesize that this strategy offers several benefits over the inline explanation rewrites. First, the user can choose whether they want to learn about the identified salient entities or they are comfortable with moving on with the conversation (they either know enough about the entities or do not care). We note that phrasing the follow-up prompt as a direct question, i.e., \u201cDo you want to learn more about entity?\u201d would require the direct answer from the user, potentially disrupting the conversation flow. Instead, our proposed construction of the prompt simply offers the user a possibility for expansion, enabling them to ignore it if they are not interested in learning about the proposed entities. Second, we can learn about the user\u2019s background knowledge by them choosing or not choosing to learn about the salient entities, leading to a potential for personalization of subsequent answers. Third, we encourage engagement with the user by providing potential topics to converse about. While these assumptions intuitively make sense, we formulate specific research questions to assess them empirically."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3. Evaluation of Answer Rewrite Strategies",
            "text": "In this section, we describe the human-based evaluation procedure for comparing the original answer with the rewritten answers."
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1. Research Questions.",
            "text": "The main research question we aim to answer is RQ2: How to utilize salient entities for answer rewriting for an improved user experience? We also aim to explore what type of rewritten answers users prefer and what methods work the best for generating such rewrites. Thus, we extend our main research question to four more specific questions: Do users prefer the rewritten questions over the original ones? Which of the two answer rewrite strategies (A-inlinedef or A-followup) is preferred? Is there a preferred way of explaining the salient entities inline (A-inlinedef/wiki or A-inlinedef/human)? Is there a preferred way of offering follow up to the user (A-followup/question or A-followup/offer)? How does the number of salient entities considered in the rewrite (top 1, 2, or 3) affect user preferences?"
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2. Experiment Design.",
            "text": "We design the evaluation study as a multiple choice inquiry and ask crowd workers to provide their assessments. Given an initial question, the workers need to assume the role of a user and select the answer that they would prefer in an interaction with a conversational assistant. The given options are threefold: an original answer, a rewritten answer with inline explanations, and a rewritten answer with a follow-up prompt in the end. Furthermore, to answer research questions RQ2.c and RQ2.d, we vary the methods for inline explanations, as well as the types of questions for the prompt-based rewrite. Note that crowd workers are not aware of those changes and they always have the three mentioned options, without knowing how the rewrites are generated. To ensure consistency, we generate rewrites on the same pool of QA pairs, thus controlling the potential impact of different topics on the rewrite preference. Each question and three answer options, corresponding to original answer, an answer with inline definitions, and an answer with follow-up, is annotated by three different crowd workers. We ensure the quality and consistency of the annotations by selecting high-quality workers, as described in Section 3.2. Moreover, we randomize the order of A-original, A-inlinedef, and A-followup to reduce any potential position bias. In order to gain further insights into the underlying rationales, we ask annotators to provide a brief explanation of why they chose the answer they did. We analyze the provided reasons in depth in the next section. To additionally ensure high quality annotations, we manually inspect all of them, rejecting crowd workers who carelessly provided nonsensical reasons (e.g., \u201cfirst one,\u201d \u201cbest text,\u201d or simply copy-pasted parts of the answers), and blocking them from further participation in the study. In total, we acquire more than 600 assessments on rewrite type preference with justifications for the choice."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "4.4. Results",
            "text": "In this section, we present the results of the crowdsourcing study on answer rewrite type preference and analyze them in the light of the aforementioned research questions."
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1. Original or Rewritten Answer Preference",
            "text": "To assess whether differences in answer preference are statistically significant, we perform a test under the null hypothesis of data being drawn from a uniform probability distribution across the three rewrites (i.e., each row of the table). \n\nIn response to RQ2.a, we observe a preference for one of the answer rewrites, over the original answer (222 for original vs 399 for rewrites, p-value). These results suggest that there is a large potential for improving the user experience through answer rewriting. Moreover, the findings suggest a promising direction for further research on answer rewriting in CIS systems, both by providing further inline explanations of certain entities and by offering follow-up clarifications."
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2. Rewrite Type Preference",
            "text": "Regarding RQ2.b, we observe a preference for answers with inline explanations (A-inlinedef) over the answers with a follow up (A-followup). Moreover, as indicated in Table 3, this preference is prevalent across all combinations of rewrite subtypes. Although not all combinations in Table 3 yield statistically significant differences, the overall trend is prevalent across all of the experiments. This suggests that making the answer self-contained by providing inline entity explanations is more desirable than offering the user to clarify these entities. Contrary to our hypothesis, longer answers obtained by inserting entity descriptions do not seem to overwhelm the majority of the users. However, subjectivity is still important in this scenario, as some users indeed find A-inlinedef to be too cluttered, as discussed in the next section."
        },
        {
            "section_id": "4.4.3",
            "parent_section_id": "4.4",
            "section_name": "4.4.3. Rewrite Subtype Preference",
            "text": "To address research questions RQ2.c and RQ2.d, we aggregate the results of different subtypes of answer rewriting. Experiments indicate humanly-curated answers to be slightly more preferred over the Wikbase definitions in parentheses (131 vs. 119), suggesting that more natural rewrites could better help the user understand the answers. This finding is a motivation for the development of answer rewriting methods aimed at defining entities in a more natural manner, compared to entity definitions being inserted into parentheses. Similarly, A-followup/offer is slightly more preferred than A-followup/question (81 vs. 68). We hypothesize that a prompt that could be ignored, as opposed to a direct question, would benefit the overall user experience. While both strategies are equally effective in providing the user with desired information, A-followup/offer might not impair the flow of the conversation, as it can be ignored if the user does not desire to learn more about proposed entities."
        },
        {
            "section_id": "4.4.4",
            "parent_section_id": "4.4",
            "section_name": "4.4.4. Result Preference by Top N Entities",
            "text": "Regarding RQ2.e, we report the results on answer preference across top most salient entities rewritten in Table 4. Specifically, we construct the experiment such that the same original answer is rewritten three times, each time with salient entities taken into account. Results suggest that the higher the number of salient entities, i.e., the more entities are defined in the answer, the stronger the user\u2019s preference for A-inlinedef. We hypothesize that such answers provide a more complete response to the given question, thus not requiring further explorations of the topic through clarifying prompts."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "4.5. Analysis",
            "text": "In order to gain further insight into answer rewrite preferences, we manually analyze responses from crowd workers. Recall that workers were asked to justify why they would prefer the answer rewrite they have chosen. We perform a qualitative analysis of the provided reasons by identifying re-occurring reasons for workers' choices. We find five distinct patterns of the provided reasons. To estimate the prevalence of each of these, we randomly select 100 responses from the crowd workers and label them using these patterns. In our analysis, we allow for multiple patterns to be associated with a single reason of choice, as, for example, a worker can pick an answer because it is both concise and natural. \n\nOverall, we find that:\n- Users prefer the original answer mostly because it is short, concise, and does not introduce unnecessary clutter.\n- Users prefer the answer with inline explanations when they appreciate additional information and think it provides a well-defined answer.\n- Users prefer the answer with a follow-up prompt when they think it is the most welcoming of the different answer alternatives, inviting for further conversation, but is not unnecessarily overwhelming with long explanations of entities they perhaps do not require explanation of.\n\nAlthough all of the workers provided reasonable justifications for their selection, the inter-annotator agreement, as measured by Fleiss\u2019 kappa, indicates high subjectivity.\n\nFrom the conducted experiments and observed patterns in user preferences, we find solid evidence that entity-based answer rewriting can lead to an improved conversational user experience. At the same time, we also find that the choice of preferred answer format is highly subjective, which calls for further research on the personalization of such approaches."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "4.6. LLM-based Answer Rewriting",
            "text": "While entity definitions are currently either taken from a knowledge base or curated by human intervention, the question naturally arises: Could this task not be tackled in an end-to-end manner by a large language model (LLM)? Given promising advancements in text simplification, e.g., with T5 (Sheang and Saggion, 2021), and the broad variety of knowledge and language capabilities demonstrated by ChatGPT, there are reasons to believe that the answer rewriting task could be performed by simply engineering the \u201cright\u201d prompt. Below, we present some anecdotal evidence to the contrary, by presenting results obtained with a state-of-the-art LLM, gpt-3.5-turbo (Brown et al., 2020).\n\nSpecifically, we consider two types of prompts: (1) instructing the LLM to rewrite the answer for easier understandability and (2) additionally, including the specific entities that need to be explained. For both settings, we experimented with both zero- and few-shot prompts. Due to space constraints, we only include a few examples in Table 6 to illustrate the main limitations we identified:\n\nFailure to identify salient entities: When entities that require explanations aren\u2019t explicitly stated, the LLM can simply reword the answer, without providing any additional information.\n\nKnowledge distortion: Certain salient entities are removed from the original answer, causing the loss of information by oversimplifying the text.\n\nAnswer length: when explicitly stated which entities require inline explanations, the LLM tends to significantly lengthen the original answer (from 1-2 to 5-6 sentences).\n\nInconsistency: Although hallucination is a known issue in LLMs (Ji et al., 2023), we also observe inconsistency, i.e., a high degree of variation in answer quality, when generating answers to the same prompt multiple times (controllable with parameters to some degree) and across different examples (not controllable).\n\nThat said, LLMs can also generate appropriate rewrites, as illustrated by the last example in Table 6, which is both concise and natural, while still covering all of the salient entities. However, it is evident that the salient entities had to be explicitly stated and that prompts need to be carefully engineered for the desired outcome.\n\nThe main takeaway of our study is that entity-based answer rewriting can improve the user experience, but to unlock its full potential, the identification of salient entities as well as the preferred form of answer rewrite need to be addressed in a personalized manner. These parts require future research. When it comes to the actual generation of the rewritten answer, there is a large potential for utilizing LLMs, provided that they are prompted with the specific entities and the desired format of rewrite."
        },
        {
            "section_id": "4.7",
            "parent_section_id": "4",
            "section_name": "4.7. Discussion",
            "text": "Ours is a novel task in a conversational setting, which makes evaluation inherently challenging. In this section, we reflect on some of the design decisions, acknowledge limitations, and highlight possible future research directions, including potentially revisiting some of the design choices."
        },
        {
            "section_id": "4.7.x",
            "parent_section_id": "4.7",
            "section_name": "Impact of Rewriting on Answer Length",
            "text": "Rewriting answers in CIS by inserting inline definitions of salient entities lengthens the original answer. As observed in our experiments, up to three entity definitions do not seem to hurt the answer rewrite, as such rewrite was often chosen by the crowd workers. However, in case the answer becomes too long due to a large number of salient entities, the amount of them we provide definitions for can be reduced by taking only the top N entities, as ordered by the salience scores."
        },
        {
            "section_id": "4.7.x",
            "parent_section_id": "4.7",
            "section_name": "Text- vs. Voice-based CIS",
            "text": "We hypothesize that results on answer rewrite preference might differ in a voice-only setting, as the user is not able to skim through potentially unnecessary parts of the answer. As such, preference for inline definitions might not be so prevalent, as users could not simply skim through the text and would in fact need to listen to the extended answers. We aim to explore the aforementioned questions in further research."
        },
        {
            "section_id": "4.7.x",
            "parent_section_id": "4.7",
            "section_name": "More Realistic Conversational Setting",
            "text": "Design-wise, we compare answer rewrites turn by turn, rather than evaluating the whole conversations. This is often the case in crowdsourcing-based studies due to the limited availability of users, although recent research points out the benefits of multi-turn dialogue evaluation (Li et al., 2019 ###reference_b26###). At the same time, utterances in our study are self-contained and do not necessarily require full conversation history for correct assessments. Also, we provide an analysis of the salient entity evolution throughout the conversation. Nevertheless, as part of our future work, we aim to build multiple CIS systems based on answer rewrite type (e.g., a system that generates answers with inline explanations of salient entities and a system that offers follow-up prompts) and perform a thorough user study to validate the findings of this work."
        },
        {
            "section_id": "4.7.x",
            "parent_section_id": "4.7",
            "section_name": "Salient Entity Annotation",
            "text": "While other automated options for extracting salient entities exist, we opted for annotating salience through crowdsourcing with a goal of acquiring high-quality data. Nevertheless, despite having multiple controls in place for ensuring quality (from the selection of crowd workers to using test questions), the inter-annotator agreement turned out to be relatively low. We attribute this to the high subjectivity of the task, as workers\u2019 perception of what is \u201cessential to understand\u201d might differ, in relation to their personal knowledge and their understanding of what \u201cessential\u201d means. We acknowledge the possibility of the annotation task being set up this way to be too open for interpretations, or simply too hard, thus leading to low inter-annotator agreement. In the future we plan to repeat the annotation process as part of a dedicated study, aiming to untangle what role prerequisite knowledge and subjectivity might play here. Nonetheless, we believe the findings of this study on question rewrite strategies and preferences to be sound and useful for the research community."
        },
        {
            "section_id": "4.7.x",
            "parent_section_id": "4.7",
            "section_name": "Role of Background Knowledge",
            "text": "High subjectivity of rewrite preferences potentially comes from different backgrounds and personal preferences of different users. Thus, having information about the background of the user would help the system tailor the rewrite to a specific user, both by selecting only entities which the user needs explanation for and by adjusting the style of the rewrite. Future work therefore includes personalized answer rewrites."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Conclusions",
            "text": "In this study, we analyzed the presence of salient entities in conversational information seeking interactions. We found that most of the answers generated by the search system contain some amount of salient entities, required for the complete comprehension of the answer. Moreover, with a goal of ensuring that the user understands these answers, we proposed two strategies for answer rewriting. The first one is based on providing inline definitions of salient entities, while the second one explicitly offers the user to learn more about the entities they might be unfamiliar with. The suggested methods were extensively assessed through human-based evaluation, indicating user preference for answers with inline definitions, over the follow-up prompt-based rewrites. We hope that these findings provide a strong motivation for further research on entity-based answer rewriting."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 1. </span>Excerpts from QReCC with our crowdsourcing-based annotations of entity salience scores.</figcaption>\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S3.T1.1\" style=\"width:433.6pt;height:281pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(23.8pt,-15.4pt) scale(1.12320406820633,1.12320406820633) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.1.1\">Source</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.1.2\">Question</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.1.3\">Answer</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.1.4\">Entity salience score</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.1.2.1.1\">CAsT</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.1.2.1.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.1.2.1.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.2.1.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.2.1.2.1.1.1\">What does it cost to</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.2.1.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.2.1.2.1.2.1\">become a physician\u2019s</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.2.1.2.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.2.1.2.1.3.1\">assistant?</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.1.2.1.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.1.2.1.3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.2.1.3.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.2.1.3.1.1.1\">Average cost of resident tuition</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.2.1.3.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.2.1.3.1.2.1\">for a 27-month physician</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.2.1.3.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.2.1.3.1.3.1\">assistant program is\u2026</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.1.2.1.4\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.1.2.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.2.1.4.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.2.1.4.1.1.1\">Residency: 1.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.2.1.4.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.2.1.4.1.2.1\">Tuition payments: 1.4</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.3.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.1.3.2.1\">NQ</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.1.3.2.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.1.3.2.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.3.2.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.3.2.2.1.1.1\">Why is snow used</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.3.2.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.3.2.2.1.2.1\">for igloos?</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.1.3.2.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.1.3.2.3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.3.2.3.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.3.2.3.1.1.1\">Snow is used for igloos because</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.3.2.3.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.3.2.3.1.2.1\">the air pockets trapped in it</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.3.2.3.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.3.2.3.1.3.1\">make it an insulator.</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.1.3.2.4\">Thermal insulation: 1.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.4.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T1.1.1.4.3.1\">QuAC</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T1.1.1.4.3.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.1.4.3.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.4.3.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.4.3.2.1.1.1\">What was Sigmund Freud</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.4.3.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.4.3.2.1.2.1\">and Wilhelm Fliess\u2019</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.4.3.2.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.4.3.2.1.3.1\">relationship?</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T1.1.1.4.3.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.1.4.3.3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.4.3.3.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.4.3.3.1.1.1\">(they)\u2026saw themselves as</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.4.3.3.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.4.3.3.1.2.1\">isolated from the prevailing</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.4.3.3.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.4.3.3.1.3.1\">clinical and theoretical</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.4.3.3.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.4.3.3.1.4.1\">mainstream because of their</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.4.3.3.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.4.3.3.1.5.1\">ambitions to develop radical</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.4.3.3.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.4.3.3.1.6.1\">new theories of sexuality.</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T1.1.1.4.3.4\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.1.4.3.4.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.4.3.4.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.4.3.4.1.1.1\">Human sexuality: 1.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.4.3.4.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.4.3.4.1.2.1\">Theory: 0.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.4.3.4.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.4.3.4.1.3.1\">Mainstream: 0.4</td>\n</tr>\n</table>\n</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 1. Excerpts from QReCC with our crowdsourcing-based annotations of entity salience scores."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2. </span>Examples of special cases of essential entities that do not necessarily require further definitions. Last column indicates the prevalence of such entity types in the expertly-annotated subsampled set of 122 entities across 37 QA pairs.</figcaption>\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S3.T2.4\" style=\"width:433.6pt;height:148pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-78.5pt,26.8pt) scale(0.734187372157634,0.734187372157634) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.4.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T2.4.4.5.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.4.4.5.1.1\">Special case</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.4.4.5.1.2\">Question</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.4.4.5.1.3\">Answer</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.4.4.5.1.4\">Entity</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.4.4.5.1.5\">% in subset</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.1.1.2\">Common sense</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.1.1.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.1.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1.3.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.1.1.3.1.1.1\">Who is Sigmund Freud\u2019s</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1.3.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.1.1.3.1.2.1\">friend Wilhelm Fliess?</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.1.1.4\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.1.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1.4.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.1.1.4.1.1.1\">During this formative period of his work\u2026his friend</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1.4.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.1.1.4.1.2.1\">Wilhelm Fliess, a Berlin-based ear, nose, and throat specialist.</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.1.1.5\">Human nose</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T2.1.1.1.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.2.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.2.2.2\">Location/NE</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.2.2.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.2.2.2.3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T2.2.2.2.3.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.2.2.3.1.1.1\">Did Hansie Cronje</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.2.2.3.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.2.2.3.1.2.1\">make any debuts?</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.2.2.4\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.2.2.2.4.1\">\n<tr class=\"ltx_tr\" id=\"S3.T2.2.2.2.4.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.2.2.4.1.1.1\">Hansie Cronje made his first-class debut for Orange</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.2.2.4.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.2.2.4.1.2.1\">Free State\u2026at Johannesburg.</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.2.2.5\">Johannesburg</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.2.2.2.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.3.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.3.3.3.2\">Already defined</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.3.3.3.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.3.3.3.3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T2.3.3.3.3.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.3.3.3.3.1.1.1\">What are some advantages</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.3.3.3.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.3.3.3.3.1.2.1\">of using Linux?</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.3.3.3.4\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.3.3.3.4.1\">\n<tr class=\"ltx_tr\" id=\"S3.T2.3.3.3.4.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.3.3.3.4.1.1.1\">One of the main advantages of Linux is that it is an</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.3.3.4.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.3.3.3.4.1.2.1\">open source operating system, i.e., its source code</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.3.3.4.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.3.3.3.4.1.3.1\">is easily available for everyone\u2026</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.3.3.3.5\">open source</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.3.3.3.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.4.4.4.2\">Entity is the answer</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.4.4.4.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.4.4.4.3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T2.4.4.4.3.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.4.4.4.3.1.1.1\">Who has to push</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.4.4.3.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.4.4.4.3.1.2.1\">the rock up the hill?</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.4.4.4.4\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.4.4.4.4.1\">\n<tr class=\"ltx_tr\" id=\"S3.T2.4.4.4.4.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.4.4.4.4.1.1.1\">Sisyphus, king of Ephyra, was punished</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.4.4.4.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.4.4.4.4.1.2.1\">to roll an immense boulder up a hill\u2026</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.4.4.4.5\">Sisyphus</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T2.4.4.4.1\"></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 2. Examples of special cases of essential entities that do not necessarily require further definitions. Last column indicates the prevalence of such entity types in the expertly-annotated subsampled set of 122 entities across 37 QA pairs."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 3. </span>Answer rewrite preference assessed by crowd workers. Workers are shown three rewritten answers (<em class=\"ltx_emph ltx_font_italic\" id=\"S4.T3.8.1\">A-original</em>, <em class=\"ltx_emph ltx_font_italic\" id=\"S4.T3.9.2\">A-inlindef</em>, and <em class=\"ltx_emph ltx_font_italic\" id=\"S4.T3.10.3\">A-followup</em>) in a randomized order. Results are broken down by varying the proposed answer rewrite strategies. p-values are reported using a  test.</figcaption>\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T3.4\" style=\"width:433.6pt;height:255.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(109.8pt,-64.7pt) scale(2.02625732509437,2.02625732509437) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.4.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.4.2.3.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.4.2.3.1.1\">Original</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T3.4.2.3.1.2\">\n<em class=\"ltx_emph ltx_font_italic\" id=\"S4.T3.4.2.3.1.2.1\">A-inlinedef</em></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T3.4.2.3.1.3\">\n<em class=\"ltx_emph ltx_font_italic\" id=\"S4.T3.4.2.3.1.3.1\">A-followup</em></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.4.2.3.1.4\">p-value</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.4.2.4.2\">\n<td class=\"ltx_td\" id=\"S4.T3.4.2.4.2.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.2.4.2.2\">/wiki</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.2.4.2.3\">/human</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.2.4.2.4\">/offer</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.2.4.2.5\">/question</td>\n<td class=\"ltx_td\" id=\"S4.T3.4.2.4.2.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.4.2.5.3\">\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T3.4.2.5.3.1\">60</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.4.2.5.3.2\">66</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T3.4.2.5.3.3\">-</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.4.2.5.3.4\">45</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T3.4.2.5.3.5\">-</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.4.2.5.3.6\">0.13</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.4.2.6.4\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.4.2.6.4.1\">56</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.4.2.6.4.2\">53</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.4.2.6.4.3\">-</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.4.2.6.4.4\">-</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.4.2.6.4.5\">41</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.4.2.6.4.6\">0.28</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.4.2.7.5\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.4.2.7.5.1\">54</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.4.2.7.5.2\">-</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.4.2.7.5.3\">60</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.4.2.7.5.4\">36</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.4.2.7.5.5\">-</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.4.2.7.5.6\">0.04</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.1.1\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.1.1.2\">52</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.1.1.3\">-</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.1.1.4\">71</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.1.1.5\">-</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.1.1.6\">27</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.1.1.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.4.2.2\">\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T3.4.2.2.2\">222</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" colspan=\"2\" id=\"S4.T3.4.2.2.3\">250</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" colspan=\"2\" id=\"S4.T3.4.2.2.4\">149</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T3.4.2.2.1\"></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 3. Answer rewrite preference assessed by crowd workers. Workers are shown three rewritten answers (A-original, A-inlindef, and A-followup) in a randomized order. Results are broken down by varying the proposed answer rewrite strategies. p-values are reported using a  test."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 4. </span>Results on answer rewrite preference by top N salient entities rewritten. The p-value is computed by  test.</figcaption>\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.7\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.3.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T4.3.1.2\"></td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.3.1.1\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.3.1.3\"><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T4.3.1.3.1\">A-inlinedef</em></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.3.1.4\"><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T4.3.1.4.1\">A-followup</em></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.3.1.5\">p-value</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.4.2.2\">Top 1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.4.2.3\">80</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.4.2.4\">84</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.4.2.5\">65</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.4.2.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.5.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.5.3.2\">Top 2</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.5.3.3\">87</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.5.3.4\">83</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.5.3.5\">42</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.5.3.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.6.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.6.4.2\">Top 3</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.6.4.3\">55</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.6.4.4\">83</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.6.4.5\">42</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.6.4.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.7.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T4.7.5.2\">Total</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T4.7.5.3\">222</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T4.7.5.4\">250</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T4.7.5.5\">149</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T4.7.5.1\"></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 4. Results on answer rewrite preference by top N salient entities rewritten. The p-value is computed by  test."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T5\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 5. </span>Reasons for choosing one answer over another, grouped together by observed patterns. The last column presents the prevalence of the pattern in the manually analyzed portion of the dataset.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T5.12\" style=\"width:433.6pt;height:297.6pt;vertical-align:-0.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-55.9pt,38.2pt) scale(0.795121112509011,0.795121112509011) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T5.12.12\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T5.12.12.13.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.12.12.13.1.1\"><span class=\"ltx_text\" id=\"S4.T5.12.12.13.1.1.1\" style=\"font-size:90%;\">Reason for choice</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.12.12.13.1.2\"><span class=\"ltx_text\" id=\"S4.T5.12.12.13.1.2.1\" style=\"font-size:90%;\">Preference</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.12.12.13.1.3\"><span class=\"ltx_text\" id=\"S4.T5.12.12.13.1.3.1\" style=\"font-size:90%;\">Pattern</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.12.12.13.1.4\"><span class=\"ltx_text\" id=\"S4.T5.12.12.13.1.4.1\" style=\"font-size:90%;\">Prev.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.1.1.1.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.1.1.1.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.1.1.1.1.1.1.1\">\n<span class=\"ltx_text\" id=\"S4.T5.1.1.1.1.1.1.1.1\" style=\"font-size:90%;\"> \u201cThe terms seem to be very specialized in terms of the question and need at least some elucidation to understand\u201d</span>\n</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.1.1.1.2\"><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T5.1.1.1.2.1\" style=\"font-size:90%;\">A-inlinedef</em></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.1.1.1.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T5.1.1.1.3.1\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.1.1.1.3.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T5.1.1.1.3.1.1.1\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T5.1.1.1.3.1.1.1.1\">Better</span></span>\n<span class=\"ltx_tr\" id=\"S4.T5.1.1.1.3.1.1.2\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T5.1.1.1.3.1.1.2.1\">explanation</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.1.1.1.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T5.1.1.1.4.1\" style=\"font-size:90%;\">28%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.2.2.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.2.2.2.1\">\n<span class=\"ltx_text\" id=\"S4.T5.2.2.2.1.1\" style=\"font-size:90%;\"> \u201cnot everyone knows how the body functions.\u201d</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.2.2.2.2\"><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T5.2.2.2.2.1\" style=\"font-size:90%;\">A-inlinedef</em></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.3.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.3.3.3.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.3.3.3.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T5.3.3.3.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.3.3.1.1.1.1\">\n<span class=\"ltx_text\" id=\"S4.T5.3.3.3.1.1.1.1.1\" style=\"font-size:90%;\"> \u201cThe most concise answer. I don\u2019t think the recipient would</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.3.3.3.1.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.3.3.1.1.2.1\"><span class=\"ltx_text\" id=\"S4.T5.3.3.3.1.1.2.1.1\" style=\"font-size:90%;\">want any more particular information especially about Africa\u201d</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.3.3.3.2\"><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T5.3.3.3.2.1\" style=\"font-size:90%;\">A-original</em></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.3.3.3.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T5.3.3.3.3.1\" style=\"font-size:90%;\">Concise</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.3.3.3.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T5.3.3.3.4.1\" style=\"font-size:90%;\">28%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.4.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.4.4.4.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.4.4.4.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T5.4.4.4.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.4.4.4.1.1.1.1\">\n<span class=\"ltx_text\" id=\"S4.T5.4.4.4.1.1.1.1.1\" style=\"font-size:90%;\"> \u201cit\u2019s the most complete answer that doesn\u2019t add unnecessary stuff inside brackets of brackets.\u201d</span>\n</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.4.4.4.2\"><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T5.4.4.4.2.1\" style=\"font-size:90%;\">A-original</em></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.5.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.5.5.5.1\">\n<span class=\"ltx_text\" id=\"S4.T5.5.5.5.1.1\" style=\"font-size:90%;\"> \u201cthe other answers have text that shouldn\u2019t be there or is too wordy.\u201d</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.5.5.5.2\"><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T5.5.5.5.2.1\" style=\"font-size:90%;\">A-original</em></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.5.5.5.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T5.5.5.5.3.1\" style=\"font-size:90%;\">No clutter</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.5.5.5.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T5.5.5.5.4.1\" style=\"font-size:90%;\">15%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.6.6.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.6.6.6.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.6.6.6.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T5.6.6.6.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.6.6.6.1.1.1.1\">\n<span class=\"ltx_text\" id=\"S4.T5.6.6.6.1.1.1.1.1\" style=\"font-size:90%;\"> \u201cI prefer this one because it doesn\u2019t have a question on the end, and</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.6.6.6.1.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.6.6.6.1.1.2.1\"><span class=\"ltx_text\" id=\"S4.T5.6.6.6.1.1.2.1.1\" style=\"font-size:90%;\">because it is the most clear and direct, w/o any parentheticals.\u201d</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.6.6.6.2\"><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T5.6.6.6.2.1\" style=\"font-size:90%;\">A-original</em></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.7.7.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.7.7.7.1\">\n<span class=\"ltx_text\" id=\"S4.T5.7.7.7.1.1\" style=\"font-size:90%;\"> \u201cinvites you to ask for more information about it\u201d</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.7.7.7.2\"><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T5.7.7.7.2.1\" style=\"font-size:90%;\">A-followup</em></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.7.7.7.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T5.7.7.7.3.1\" style=\"font-size:90%;\">Learn more</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.7.7.7.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T5.7.7.7.4.1\" style=\"font-size:90%;\">14%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.8.8.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.8.8.8.1\">\n<span class=\"ltx_text\" id=\"S4.T5.8.8.8.1.1\" style=\"font-size:90%;\"> \u201cI choose 1, because I want to learn more about solar energy.\u201d</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.8.8.8.2\"><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T5.8.8.8.2.1\" style=\"font-size:90%;\">A-followup</em></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.9.9.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.9.9.9.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.9.9.9.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T5.9.9.9.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.9.9.9.1.1.1.1\">\n<span class=\"ltx_text\" id=\"S4.T5.9.9.9.1.1.1.1.1\" style=\"font-size:90%;\"> \u201cI prefer the first one because it is the most concise answer. One doesn\u2019t need to</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.9.9.9.1.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.9.9.9.1.1.2.1\"><span class=\"ltx_text\" id=\"S4.T5.9.9.9.1.1.2.1.1\" style=\"font-size:90%;\">be told they ask more questions as in answer 2, and answer 3 rambles a bit.\u201d</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.9.9.9.2\"><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T5.9.9.9.2.1\" style=\"font-size:90%;\">A-original</em></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.9.9.9.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T5.9.9.9.3.1\" style=\"font-size:90%;\">Other</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.9.9.9.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T5.9.9.9.4.1\" style=\"font-size:90%;\">10%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.10.10.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.10.10.10.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.10.10.10.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T5.10.10.10.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.10.10.10.1.1.1.1\">\n<span class=\"ltx_text\" id=\"S4.T5.10.10.10.1.1.1.1.1\" style=\"font-size:90%;\"> \u201cthe topic is very familiar for me and i have some knowledge about chemical energy so i choose this option\u201d</span>\n</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.10.10.10.2\"><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T5.10.10.10.2.1\" style=\"font-size:90%;\">A-inlinedef</em></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.11.11.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.11.11.11.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.11.11.11.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T5.11.11.11.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.11.11.11.1.1.1.1\">\n<span class=\"ltx_text\" id=\"S4.T5.11.11.11.1.1.1.1.1\" style=\"font-size:90%;\"> \u201cThis doesn\u2019t overexplain the compatibility layer aspect, making me feel overwhelmed,</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.11.11.11.1.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.11.11.11.1.1.2.1\"><span class=\"ltx_text\" id=\"S4.T5.11.11.11.1.1.2.1.1\" style=\"font-size:90%;\">and allows me to ask about it if I want instead\u201d</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.11.11.11.2\"><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T5.11.11.11.2.1\" style=\"font-size:90%;\">A-followup</em></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T5.11.11.11.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T5.11.11.11.3.1\" style=\"font-size:90%;\">Natural</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T5.11.11.11.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T5.11.11.11.4.1\" style=\"font-size:90%;\">4%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.12.12.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T5.12.12.12.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.12.12.12.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T5.12.12.12.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.12.12.12.1.1.1.1\">\n<span class=\"ltx_text\" id=\"S4.T5.12.12.12.1.1.1.1.1\" style=\"font-size:90%;\"> \u201cApart from answering the question, the assistant is more interactive and</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.12.12.12.1.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.12.12.12.1.1.2.1\"><span class=\"ltx_text\" id=\"S4.T5.12.12.12.1.1.2.1.1\" style=\"font-size:90%;\">continues to ask whether I would like to learn more about the bank of England.\u201d</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T5.12.12.12.2\"><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T5.12.12.12.2.1\" style=\"font-size:90%;\">A-followup</em></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 5. Reasons for choosing one answer over another, grouped together by observed patterns. The last column presents the prevalence of the pattern in the manually analyzed portion of the dataset."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T6\">\n<figcaption class=\"ltx_caption\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 6. </span>Examples of LLM rewrites for answer rewriting with the original answer being: <span class=\"ltx_text ltx_font_italic\" id=\"S4.T6.4.1\">\u201cThe lipid bilayer is a thin polar membrane made of two layers of lipid molecules. These membranes are flat sheets that form a continuous barrier around all cells.\u201d</span></figcaption>\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T6.5\" style=\"width:433.6pt;height:242.8pt;vertical-align:-0.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-60.6pt,33.8pt) scale(0.78154239287713,0.78154239287713) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T6.5.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T6.5.1.1.1.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.1.1.1.1\" style=\"font-size:90%;\">Prompt</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T6.5.1.1.1.2\"><span class=\"ltx_text\" id=\"S4.T6.5.1.1.1.2.1\" style=\"font-size:90%;\">Answer rewrite</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T6.5.1.1.1.3\"><span class=\"ltx_text\" id=\"S4.T6.5.1.1.1.3.1\" style=\"font-size:90%;\">Limitation</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T6.5.1.2.1.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T6.5.1.2.1.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.2.1.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.2.1.1.1.1.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.2.1.1.1.1.1.1\" style=\"font-size:90%;\">Rewrite the passage below in a</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.2.1.1.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.2.1.1.1.2.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.2.1.1.1.2.1.1\" style=\"font-size:90%;\">simplified language, so it is</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.2.1.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.2.1.1.1.3.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.2.1.1.1.3.1.1\" style=\"font-size:90%;\">understandable to everyone.</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T6.5.1.2.1.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T6.5.1.2.1.2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.2.1.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.2.1.2.1.1.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.2.1.2.1.1.1.1\" style=\"font-size:90%;\">The lipid bilayer is a very thin and special kind of membrane made up of two</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.2.1.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.2.1.2.1.2.1\">\n<span class=\"ltx_text\" id=\"S4.T6.5.1.2.1.2.1.2.1.1\" style=\"font-size:90%;\">layers of lipid molecules. These membranes are </span><span class=\"ltx_text ltx_font_italic\" id=\"S4.T6.5.1.2.1.2.1.2.1.2\" style=\"font-size:90%;\">like flat sheets</span><span class=\"ltx_text\" id=\"S4.T6.5.1.2.1.2.1.2.1.3\" style=\"font-size:90%;\"> that wrap</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.2.1.2.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.2.1.2.1.3.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.2.1.2.1.3.1.1\" style=\"font-size:90%;\">around all cells and create a protective barrier.</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T6.5.1.2.1.3\"><span class=\"ltx_text\" id=\"S4.T6.5.1.2.1.3.1\" style=\"font-size:90%;\">Knowledge distortion</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.3.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T6.5.1.3.2.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T6.5.1.3.2.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.3.2.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.3.2.1.1.1.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.3.2.1.1.1.1.1\" style=\"font-size:90%;\">Rewrite the passage below so it contains</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.3.2.1.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.3.2.1.1.2.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.3.2.1.1.2.1.1\" style=\"font-size:90%;\">explanations of the main concepts</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.3.2.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.3.2.1.1.3.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.3.2.1.1.3.1.1\" style=\"font-size:90%;\">so that it is easier to understand.</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T6.5.1.3.2.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T6.5.1.3.2.2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.3.2.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.3.2.2.1.1.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.3.2.2.1.1.1.1\" style=\"font-size:90%;\">The lipid bilayer is a structure consisting of two layers of lipid</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.3.2.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.3.2.2.1.2.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.3.2.2.1.2.1.1\" style=\"font-size:90%;\">molecules that form a thin, polar membrane. This membrane serves as a</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.3.2.2.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.3.2.2.1.3.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.3.2.2.1.3.1.1\" style=\"font-size:90%;\">protective barrier encircling every cell in the body. It is composed</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.3.2.2.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.3.2.2.1.4.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.3.2.2.1.4.1.1\" style=\"font-size:90%;\">of flat sheets, ensuring a continuous and consistent covering.</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T6.5.1.3.2.3\"><span class=\"ltx_text\" id=\"S4.T6.5.1.3.2.3.1\" style=\"font-size:90%;\">No entity explanations</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.4.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T6.5.1.4.3.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T6.5.1.4.3.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.4.3.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.4.3.1.1.1.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.4.3.1.1.1.1.1\" style=\"font-size:90%;\">Rewrite the passage below so it is</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.4.3.1.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.4.3.1.1.2.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.4.3.1.1.2.1.1\" style=\"font-size:90%;\">understandable to wide audiences, by</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.4.3.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.4.3.1.1.3.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.4.3.1.1.3.1.1\" style=\"font-size:90%;\">explaining the following entities:</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.4.3.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.4.3.1.1.4.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.4.3.1.1.4.1.1\" style=\"font-size:90%;\">\u201dlipid molecules\u201d, \u201dpolar membrane\u201d.</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T6.5.1.4.3.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T6.5.1.4.3.2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.4.3.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.4.3.2.1.1.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.4.3.2.1.1.1.1\" style=\"font-size:90%;\">The cells in our body are surrounded by a very thin\u2026</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.4.3.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.4.3.2.1.2.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.4.3.2.1.2.1.1\" style=\"font-size:90%;\">head is attracted to water (hydrophilic) while the</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.4.3.2.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.4.3.2.1.3.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.4.3.2.1.3.1.1\" style=\"font-size:90%;\">tail is repelled by it (hydrophobic)\u2026</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.4.3.2.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.4.3.2.1.4.1\">\n<span class=\"ltx_text ltx_font_italic\" id=\"S4.T6.5.1.4.3.2.1.4.1.1\" style=\"font-size:90%;\">(5 sentences)</span><span class=\"ltx_text\" id=\"S4.T6.5.1.4.3.2.1.4.1.2\" style=\"font-size:90%;\">.</span>\n</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T6.5.1.4.3.3\"><span class=\"ltx_text\" id=\"S4.T6.5.1.4.3.3.1\" style=\"font-size:90%;\">Answer lengthening</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.5.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T6.5.1.5.4.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T6.5.1.5.4.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.5.4.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.5.4.1.1.1.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.5.4.1.1.1.1.1\" style=\"font-size:90%;\">Rewrite the passage below so it contains</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.5.4.1.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.5.4.1.1.2.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.5.4.1.1.2.1.1\" style=\"font-size:90%;\">brief explanations of \u201dlipid molecules\u201d</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.5.4.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.5.4.1.1.3.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.5.4.1.1.3.1.1\" style=\"font-size:90%;\">and \u201dpolar membrane\u201d.</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T6.5.1.5.4.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T6.5.1.5.4.2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.5.4.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.5.4.2.1.1.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.5.4.2.1.1.1.1\" style=\"font-size:90%;\">The lipid bilayer, which is made up of lipid molecules, is a thin and polar</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.5.4.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.5.4.2.1.2.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.5.4.2.1.2.1.1\" style=\"font-size:90%;\">membrane. Lipid molecules are special types of fats. This polar membrane</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.5.4.2.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.5.4.2.1.3.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.5.4.2.1.3.1.1\" style=\"font-size:90%;\">consists of two layers and forms flat sheets. It acts as a continuous</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.1.5.4.2.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.5.1.5.4.2.1.4.1\"><span class=\"ltx_text\" id=\"S4.T6.5.1.5.4.2.1.4.1.1\" style=\"font-size:90%;\">barrier that encloses and protects all cells.</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T6.5.1.5.4.3\"><span class=\"ltx_text\" id=\"S4.T6.5.1.5.4.3.1\" style=\"font-size:90%;\">\u2013</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 6. Examples of LLM rewrites for answer rewriting with the original answer being: \u201cThe lipid bilayer is a thin polar membrane made of two layers of lipid molecules. These membranes are flat sheets that form a continuous barrier around all cells.\u201d"
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.01747v1_figure_1.png",
            "caption": "Figure 1. Possible strategies to making sure the user understands the answer in conversational information-seeking."
        },
        "2": {
            "figure_path": "2403.01747v1_figure_2.png",
            "caption": "Figure 2. Four examples of changes in salience score through conversational turns."
        },
        "3": {
            "figure_path": "2403.01747v1_figure_3.png",
            "caption": "Figure 3. Answer rewriting strategies."
        }
    },
    "references": [
        {
            "1": {
                "title": "Asking clarifying questions in open-domain\ninformation-seeking conversations. In Proceedings\nof the 42nd International ACM SIGIR conference on research and development in\ninformation retrieval (SIGIR \u201919).\n475\u2013484.",
                "author": "Mohammad Aliannejadi,\nHamed Zamani, Fabio Crestani, and\nW Bruce Croft. 2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "2": {
                "title": "Mixed-initiative interaction.",
                "author": "James E Allen, Curry I\nGuinn, and Eric Horvtz.\n1999.",
                "venue": "IEEE Intelligent Systems and their\nApplications 14, 5\n(1999), 14\u201323.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Conversational search (Dagstuhl seminar 19461). In\nDagstuhl Reports, Vol. 9.\nSchloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik.",
                "author": "Avishek Anand, Lawrence\nCavedon, Hideo Joho, Mark Sanderson,\nand Benno Stein. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "4": {
                "title": "Open-Domain Question Answering Goes Conversational\nvia Question Rewriting. In Proceedings of the 2021\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies (NAACL\n\u201921). 520\u2013534.",
                "author": "Raviteja Anantha, Svitlana\nVakulenko, Zhucheng Tu, Shayne Longpre,\nStephen Pulman, and Srinivas Chappidi.\n2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "5": {
                "title": "Entity-oriented search.",
                "author": "Krisztian Balog.\n2018.",
                "venue": "Springer.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems\n(NeurIPS \u201920). 1877\u20131901.",
                "author": "Tom Brown, Benjamin Mann,\nNick Ryder, Melanie Subbiah,\nJared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam,\nGirish Sastry, Amanda Askell,\net al. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "7": {
                "title": "Searching to Learn with Instructional Scaffolding.\nIn Proceedings of the 2021 Conference on Human\nInformation Interaction and Retrieval (CHIIR \u201921).\n209\u2013218.",
                "author": "Arthur C mara, Nirmal\nRoy, David Maxwell, and Claudia\nHauff. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "8": {
                "title": "QuAC: Question answering in context.",
                "author": "Eunsol Choi, He He,\nMohit Iyyer, Mark Yatskar,\nWen-tau Yih, Yejin Choi,\nPercy Liang, and Luke Zettlemoyer.\n2018.",
                "venue": "arXiv preprint arXiv:1808.07036\n(2018).",
                "url": null
            }
        },
        {
            "9": {
                "title": "Personalizing Web Search Results by Reading Level.\nIn Proceedings of the 20th ACM International\nConference on Information and Knowledge Management\n(CIKM \u201911). 403\u2013412.",
                "author": "Kevyn Collins-Thompson,\nPaul N. Bennett, Ryen W. White,\nSebastian de la Chica, and David\nSontag. 2011.",
                "venue": "",
                "url": null
            }
        },
        {
            "10": {
                "title": "TREC CAsT 2019: The conversational assistance\ntrack overview. In The Twenty-Eighth Text\nREtrieval Conference Proceedings (TREC\u201919).",
                "author": "Jeffrey Dalton, Chenyan\nXiong, and Jamie Callan.\n2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "11": {
                "title": "Lessons from the Journey: A Query Log Analysis of\nwithin-Session Learning. In Proceedings of the 7th\nACM International Conference on Web Search and Data Mining\n(WSDM \u201914). 223\u2013232.",
                "author": "Carsten Eickhoff, Jaime\nTeevan, Ryen White, and Susan Dumais.\n2014.",
                "venue": "",
                "url": null
            }
        },
        {
            "12": {
                "title": "Automatic Simplification of Scientific Texts:\nSimpleText Lab at CLEF-2022. In European\nConference on Information Retrieval (ECIR \u201922).\n364\u2013373.",
                "author": "Liana Ermakova, Patrice\nBellot, Jaap Kamps, Diana Nurbakova,\nIrina Ovchinnikova, Eric SanJuan,\nElise Mathurin, S\u00edlvia Ara\u00fajo,\nRadia Hannachi, St\u00e9phane Huet,\net al. 2022.",
                "venue": "",
                "url": null
            }
        },
        {
            "13": {
                "title": "Measuring nominal scale agreement among many\nraters.",
                "author": "Joseph L Fleiss.\n1971.",
                "venue": "Psychological bulletin\n76, 5 (1971),\n378.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Analyzing knowledge gain of users in informational\nsearch sessions on the web. In Proceedings of the\n2018 Conference on Human Information Interaction & Retrieval\n(CHIIR \u201918). 2\u201311.",
                "author": "Ujwal Gadiraju, Ran Yu,\nStefan Dietze, and Peter Holtz.\n2018.",
                "venue": "",
                "url": null
            }
        },
        {
            "15": {
                "title": "Identifying Salient Entities in Web Pages. In\nProceedings of the 22nd ACM International\nConference on Information and Knowledge Management\n(CIKM \u201913). 2375\u20132380.",
                "author": "Michael Gamon, Tae Yano,\nXinying Song, Johnson Apacible, and\nPatrick Pantel. 2013.",
                "venue": "",
                "url": null
            }
        },
        {
            "16": {
                "title": "Precise Zero-Shot Dense Retrieval without Relevance\nLabels.",
                "author": "Luyu Gao, Xueguang Ma,\nJimmy Lin, and Jamie Callan.\n2023.",
                "venue": "(2023), 1762\u20131777.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Relevance Models Based on the Knowledge Gap. In\nEuropean Conference on Information Retrieval\n(ECIR \u201922). Springer, 488\u2013495.",
                "author": "Yasin Ghafourian.\n2022.",
                "venue": "",
                "url": null
            }
        },
        {
            "18": {
                "title": "Collective entity linking in web text: a\ngraph-based method. In Proceedings of the 34th\ninternational ACM SIGIR conference on Research and development in Information\nRetrieval (SIGIR \u201911). 765\u2013774.",
                "author": "Xianpei Han, Le Sun,\nand Jun Zhao. 2011.",
                "venue": "",
                "url": null
            }
        },
        {
            "19": {
                "title": "Search user interfaces.",
                "author": "Marti Hearst.\n2009.",
                "venue": "Cambridge University Press.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Survey of hallucination in natural language\ngeneration.",
                "author": "Ziwei Ji, Nayeon Lee,\nRita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko\nIshii, Ye Jin Bang, Andrea Madotto,\nand Pascale Fung. 2023.",
                "venue": "Comput. Surveys 55,\n12 (2023), 1\u201338.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Personal Entity, Concept, and Named Entity Linking\nin Conversations. In Proceedings of the 31st ACM\nInternational Conference on Information & Knowledge Management\n(CIKM \u201922\u2019). 4099\u20134103.",
                "author": "Hideaki Joko and Faegheh\nHasibi. 2022.",
                "venue": "",
                "url": null
            }
        },
        {
            "22": {
                "title": "Conversational Entity Linking: Problem Definition\nand Datasets. In Proceedings of the 44th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR \u201921).\n2390\u20132397.",
                "author": "Hideaki Joko, Faegheh\nHasibi, Krisztian Balog, and Arjen P de\nVries. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "23": {
                "title": "Natural Questions: A Benchmark for Question\nAnswering Research.",
                "author": "Tom Kwiatkowski,\nJennimaria Palomaki, Olivia Redfield,\nMichael Collins, Ankur Parikh,\nChris Alberti, Danielle Epstein,\nIllia Polosukhin, Jacob Devlin,\nKenton Lee, et al.\n2019.",
                "venue": "Transactions of the Association for\nComputational Linguistics 7 (2019),\n452\u2013466.",
                "url": null
            }
        },
        {
            "24": {
                "title": "User evaluation of the effects of a text\nsimplification algorithm using term familiarity on perception, understanding,\nlearning, and information retention.",
                "author": "Gondy Leroy, James E\nEndicott, David Kauchak, Obay Mouradi,\nMelissa Just, et al.\n2013.",
                "venue": "Journal of medical Internet research\n15, 7 (2013),\ne2569.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Acute-eval: Improved dialogue evaluation with\noptimized questions and multi-turn comparisons.",
                "author": "Margaret Li, Jason\nWeston, and Stephen Roller.\n2019.",
                "venue": "arXiv preprint arXiv:1909.03087\n(2019).",
                "url": null
            }
        },
        {
            "26": {
                "title": "Exploratory search: from finding to understanding.",
                "author": "Gary Marchionini.\n2006.",
                "venue": "Commun. ACM 49,\n4 (2006), 41\u201346.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Training language models to follow instructions\nwith human feedback.",
                "author": "Long Ouyang, Jeffrey Wu,\nXu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal,\nKatarina Slama, Alex Ray,\net al. 2022.",
                "venue": "Advances in Neural Information Processing\nSystems, 27730\u201327744.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Learning document aboutness from implicit user\nfeedback and document structure. In Proceedings of\nthe 18th ACM conference on Information and knowledge management\n(CIKM \u201909). 365\u2013374.",
                "author": "Deepa Paranjpe.\n2009.",
                "venue": "",
                "url": null
            }
        },
        {
            "29": {
                "title": "Visconde: Multi-document QA with GPT-3 and Neural\nReranking. In Advances in Information Retrieval:\n45th European Conference on Information Retrieval, ECIR 2023, Dublin,\nIreland, April 2\u20136, 2023, Proceedings, Part II\n(ECIR \u201923\u2019). 534\u2013543.",
                "author": "Jayr Pereira, Robson\nFidalgo, Roberto Lotufo, and Rodrigo\nNogueira. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "30": {
                "title": "From TagME to WAT: A new entity annotator. In\nProceedings of the first international workshop on\nEntity recognition & disambiguation (ERD \u201914).\n55\u201362.",
                "author": "Francesco Piccinno and\nPaolo Ferragina. 2014.",
                "venue": "",
                "url": null
            }
        },
        {
            "31": {
                "title": "Swat: A system for detecting salient Wikipedia\nentities in texts.",
                "author": "Marco Ponza, Paolo\nFerragina, and Francesco Piccinno.\n2019.",
                "venue": "Computational Intelligence\n35, 4 (2019),\n858\u2013890.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Exploring the limits of transfer learning with a\nunified text-to-text transformer.",
                "author": "Colin Raffel, Noam\nShazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena,\nYanqi Zhou, Wei Li,\nPeter J Liu, et al.\n2020.",
                "venue": "J. Mach. Learn. Res. 21,\n140 (2020), 1\u201367.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Graded relevance.",
                "author": "Tetsuya Sakai.\n2021.",
                "venue": "In Evaluating Information Retrieval and\nAccess Tasks. 1\u201320.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Controllable Sentence Simplification with a Unified\nText-to-Text Transfer Transformer. In Proceedings\nof the 14th International Conference on Natural Language Generation\n(INLG \u201921). 341\u2013352.",
                "author": "Kim Cheng Sheang and\nHoracio Saggion. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "35": {
                "title": "Dynamic composition for conversational domain\nexploration. In Proceedings of The Web Conference\n2020 (TheWebConf \u201920). 872\u2013883.",
                "author": "Idan Szpektor, Deborah\nCohen, Gal Elidan, Michael Fink,\nAvinatan Hassidim, Orgad Keller,\nSayali Kulkarni, Eran Ofek,\nSagie Pudinsky, Asaf Revach,\net al. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "36": {
                "title": "A comparison of question rewriting methods for\nconversational passage retrieval. In European\nConference on Information Retrieval (ECIR \u201921).\n418\u2013424.",
                "author": "Svitlana Vakulenko, Nikos\nVoskarides, Zhucheng Tu, and Shayne\nLongpre. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "37": {
                "title": "Understanding interobserver agreement: the kappa\nstatistic.",
                "author": "Anthony J Viera, Joanne M\nGarrett, et al. 2005.",
                "venue": "Family Medicine 37,\n5 (2005), 360\u2013363.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Interactions with search systems.",
                "author": "Ryen W White.\n2016.",
                "venue": "Cambridge University Press.",
                "url": null
            }
        },
        {
            "39": {
                "title": "It all starts with entities: A Salient entity topic\nmodel.",
                "author": "Chuan Wu, Evangelos\nKanoulas, and Maarten de Rijke.\n2020.",
                "venue": "Natural Language Engineering\n26, 5 (2020),\n531\u2013549.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Few-shot generative conversational query\nrewriting. In Proceedings of the 43rd\nInternational ACM SIGIR conference on research and development in Information\nRetrieval (SIGIR \u201920).\n1933\u20131936.",
                "author": "Shi Yu, Jiahua Liu,\nJingqin Yang, Chenyan Xiong,\nPaul Bennett, Jianfeng Gao, and\nZhiyuan Liu. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "41": {
                "title": "Few-shot conversational dense retrieval. In\nProceedings of the 44th International ACM SIGIR\nConference on Research and Development in Information Retrieval\n(SIGIR \u201921). 829\u2013838.",
                "author": "Shi Yu, Zhenghao Liu,\nChenyan Xiong, Tao Feng, and\nZhiyuan Liu. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "42": {
                "title": "Conversational information seeking.",
                "author": "Hamed Zamani, Johanne R\nTrippas, Jeff Dalton, Filip Radlinski,\net al. 2023.",
                "venue": "Foundations and Trends\u00ae in\nInformation Retrieval 17, 3-4\n(2023), 244\u2013456.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.01747v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.2",
            "4.2.1",
            "4.2.2"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "4.3",
            "4.3.1",
            "4.3.2",
            "4.4",
            "4.4.1",
            "4.4.2",
            "4.4.3",
            "4.4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.3",
            "4.3.1",
            "4.3.2",
            "4.4",
            "4.4.1",
            "4.4.2",
            "4.4.3",
            "4.4.4",
            "4.5",
            "4.6"
        ]
    },
    "research_context": {
        "paper_id": "2403.01747v1",
        "paper_title": "Towards Self-Contained Answers: Entity-Based Answer Rewriting in Conversational Search",
        "research_background": "### Paper's Motivation and Research Problem:\n\nThe primary motivation behind the paper \"Towards Self-Contained Answers: Entity-Based Answer Rewriting in Conversational Search\" stems from the key goal of any information retrieval system: to satisfy the users' information needs efficiently. The advent of conversational agents has reshaped the search landscape, leading to advancements in question understanding and result retrieval. However, a gap exists in adapting these systems to users' varying knowledge levels and ensuring comprehensible answers. Unlike traditional web searches that allow users to follow hyperlinks for more information, conversational information seeking (CIS) systems present a limited interface, which can result in user confusion when concise answers include unfamiliar concepts.\n\nThe central research problem addressed by the paper is improving the accessibility and comprehensibility of answers in text-based conversational settings. This involves the hypothesis that users\u2019 experience can be enhanced by improving their understanding of entities mentioned in the responses.\n\n### Relevant Prior Work:\n\n1. **Role of Information Retrieval Systems:**\n   - Marchionini (2006) and Gadiraju et al. (2018) emphasized that search systems are pivotal for acquiring new knowledge.\n   - Researchers like Hearst (2009) and White (2016) have extensively focused on enhancing user interactions with these systems.\n\n2. **Evolution with Conversational Agents:**\n   - The emergence of conversational agents has shifted search paradigms, as highlighted by Zamani et al. (2023), with substantial progress in question understanding (Yu et al., 2020; Vakulenko et al., 2021) and result retrieval (Dalton et al., 2019; Yu et al., 2021).\n\n3. **Entity Salience in Web Search:**\n   - Entity salience has been explored in web search contexts, where salient entities are defined as central and prominent ones capturing the \"aboutness\" of a web page (Gamon et al., 2013).\n\n4. **Limitations and Current Frameworks:**\n   - Traditional search and retrieval methods rely on hyperlinks and result page elements for user education about unfamiliar terms (Eickhoff et al., 2014).\n   - Studies have pointed out the need for tailored support according to user knowledge levels (Ghafourian, 2022).\n\n5. **Large Language Models (LLMs):**\n   - Recent advancements in LLMs have shown potential in various natural language processing tasks (Brown et al., 2020; Ouyang et al., 2022; Pereira et al., 2023; Gao et al., 2023).\n\nBy building on these existing studies, the paper proposes novel methods to enhance user interaction in conversational search contexts by focusing on the salience of entities and employing strategies to rewrite answers. This research aims to fill the identified gap by offering more controlled and faithful rewriting methods that consider user background knowledge and preferences, potentially paving the way for personalized answer rewriting techniques.",
        "methodology": "The methodology involves rewriting answers to Conversational Information Seeking (CIS) questions to enhance user comprehension. The proposed method consists of two main strategies, as outlined below:\n\n1. **Inline Definitions Strategy**: \n   - **Goal**: To make answers self-contained by inserting definitions directly into the original response.\n   - **Process**: Identified salient entities within the answers are expanded upon by embedding concise definitions or explanatory phrases. This approach ensures that the answer becomes fully informative without requiring additional context or external sources.\n\n2. **Mixed-Initiative CIS Paradigm Strategy**: \n   - **Goal**: To leverage an interactive dialogue system that encourages user engagement and learning.\n   - **Process**: This strategy involves offering users the option to learn more about specific salient entities mentioned in the answers. It is designed to integrate into a conversational interface, where users can express interest in knowing more details, and the system responds with extended information about the entities. Each rewrite type is further elaborated in subsequent sections, which are assumed to follow in the original document.",
        "main_experiment_and_results": "Main Experiment Setup:\n1. **Datasets:** The study involves the acquisition of a dataset through crowdsourcing. Details on this dataset are critical for understanding the type of conversations and the context in which the answer rewriting happens. \n\n2. **Baselines:** Several research questions guide the experiments, focusing on defining salient entities in Conversational Information Seeking (CIS). Specific baselines, however, are not detailed in the given excerpt.\n\n3. **Evaluation Metrics:** The evaluation metrics were not explicitly mentioned in the given excerpt. Typically, for experiments involving conversational search and entity-based answer rewriting, metrics might include relevance, correctness, fluency, and user satisfaction, though these are assumed here as the document did not specify.\n\nMain Experimental Results:\nThe main results section would showcase relevant aspects of the created dataset and analyze special cases of salience. This indicates an analysis of how salient entities were annotated or identified within the dataset, and how these salient entities contribute to the effectiveness of answer rewriting in conversational search. However, specific numerical results or comparative analysis outcomes are not detailed in the provided section."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the effectiveness of different inline entity description strategies in making answers self-contained in conversational information seeking systems.",
            "experiment_process": "Two strategies were developed: (1) A-inlinedef/wiki, where definitions of salient entities were extracted from Wikibase and inserted in parentheses immediately after the first mention, and (2) A-inlinedef/human, where entity definitions were manually curated to sound more natural and integrated using commas. Both strategies aimed at making answers self-contained while keeping explanations short.",
            "result_discussion": "Manual curation (A-inlinedef/human) was slightly preferred over Wikibase definitions (A-inlinedef/wiki) with a preference score of 131 to 119, suggesting that natural-sounding rewrites help users understand answers better.",
            "ablation_id": "2403.01747v1.No1"
        },
        {
            "research_objective": "To study the impact of mixed-initiative follow-up prompts in assisting users to understand salient entities in conversational answers.",
            "experiment_process": "Developed two types of follow-up prompts: (1) A-followup/question, which expanded the original answer with a question asking if the user wanted to learn more about the entities, and (2) A-followup/offer, which offered additional information only if the user expressed the desire to know more. The goal was to provide a choice to the user while maintaining the conversation flow.",
            "result_discussion": "A-followup/offer was slightly more preferred over A-followup/question (81 vs. 68). This suggests that prompts which can be ignored do not impair the conversation flow and offer a better user experience.",
            "ablation_id": "2403.01747v1.No2"
        },
        {
            "research_objective": "To determine user preference between the original answers and the rewritten answers to identify if rewritten answers improve user experience in CIS systems.",
            "experiment_process": "Crowdsourced evaluation was conducted where workers were given a choice between the original answer, an answer with inline entity descriptions, and an answer with a follow-up prompt. Researchers randomized the order to avoid position bias and asked annotators to justify their preferences. Over 600 assessments were collected and analyzed.",
            "result_discussion": "Rewritten answers were significantly preferred over the original ones (222 for original vs 399 for rewrites), indicating that answer rewriting can substantially enhance user experience. Inline explanations (A-inlinedef) were generally more desirable than follow-up prompts (A-followup), suggesting that making answers self-contained is effective.",
            "ablation_id": "2403.01747v1.No3"
        },
        {
            "research_objective": "To explore the impact of the number of salient entities considered in answer rewrites on user preferences.",
            "experiment_process": "Original answers were rewritten considering different numbers (top 1, 2, or 3) of salient entities. These rewritten answers were then presented to crowd workers to assess their preferences.",
            "result_discussion": "Results indicated that users preferred answers where more entities were defined (higher N). This implies that more comprehensive answers, which provide detailed explanations of multiple entities, are more favored.",
            "ablation_id": "2403.01747v1.No4"
        }
    ]
}