{
    "title": "\u201cConfidently Nonsensical?\u201d: A Critical Survey on the Perspectives and Challenges of \u2018Hallucinations\u2019 in NLP",
    "abstract": "We investigate how hallucination in large language models (LLM) is characterized in peer-reviewed literature, using a critical examination of 103 publications across NLP research. Through a comprehensive review of sociological and technological literature, we identify a lack of agreement with the term \u2018hallucination\u2019. Additionally, we conduct a survey with 171 practitioners from the field of NLP and AI to capture varying perspectives on hallucination. Our analysis underscores the necessity for explicit definitions and frameworks outlining hallucination within NLP, highlighting potential challenges, and our survey inputs provide a thematic understanding of the influence and ramifications of hallucination in society.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The utilization of Natural Language Processing (NLP), particularly in language generation, has witnessed significant advancement in recent months, owing to the emergence of highly proficient large language multi-modal models like GPT-4 and Gemini Gautam et al. (2024). These models have evolved beyond NLP tools used in technical domains to become sociotechnical systems, amalgamations of social and technical elements that collectively contribute to goal-driven behavior, exerting influence on both social and technical stakeholders involved in the system Cooper and Foster (1971); Narayanan Venkit (2023). NLP applications are now prominently employed in diverse domains ranging from health and medicine Lee (2018) to policy-making Jin and Mihalcea (2022) and entertainment Werning (2024), thereby solidifying their pivotal role in everyday life.\n\nThe proliferation of language models has also drawn substantial attention to its limitations and potential risks like the propagation of misinformation and the exacerbation of biases Bender et al. (2021); Gupta et al. (2024). Concurrently, there has been a surge in research focusing on examining the phenomenon of \u2018hallucinations\u2019 in language models Ji et al. (2023a). This trend is evident in the recent rise in peer-reviewed publications investigating hallucinations in NLP and language models.\n\nIn the domain of NLP, numerous frameworks have been proposed to conceptualize hallucination, with a predominant focus on its negative connotation. The negative notion commonly revolves around a model\u2019s tendency to exhibit specific errors, particularly evident in tasks such as image captioning and text generation. Hallucination, in this context, denotes instances where the model generates references to non-existent objects or statements, despite lacking corresponding examples in the training dataset Ji et al. (2023a). Despite the increase in studies on hallucination, few works have underscored the need for a cohesive framework and precise definitions within NLP and language model research Filippova (2020a).\n\nAcknowledging \u2018hallucination\u2019 as a social construct is imperative. Given the evolution of these models into sociotechnical systems, a pressing need arises to grasp the interdisciplinary nuances surrounding this phenomenon to understand how the field addresses it. This necessity is further underscored by research illustrating the societal repercussions of hallucinations on society Dahl et al. (2024). Therefore we see an increase in the need to understand how NLP defines and conceptualises hallucination. Based on this requirement, the following questions guide this study:\n\nRQ1: What are the definitions and common frameworks used to explain hallucinations in NLG-published articles?  \nRQ2: What is the current understanding of researchers about hallucinations, and how do they encounter them in their work?\n\nTo answer RQ1, we first conduct an audit of the field of hallucinations in NLP by surveying 103 peer-reviewed articles. Subsequently, we conduct a practitioner survey involving 171 researchers and academics within the field to explore their viewpoints on this phenomenon and answer RQ2.\n\nThrough this work, we aim to understand the challenges posed by hallucinations in NLP. Finally, based on our findings, we construct an ethics framework to guide future endeavors to understand and address hallucinations in language models."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Evolution of Hallucination in NLP",
            "text": "The term \u2018hallucination\u2019 has a long history in machine learning and has been used in various contexts prior to the LM era. Its earliest documented usage can be traced to the 2000s when Baker and Kanade (2000  ###reference_b3###) applied it in the context of image resolution enhancement, referring to the generation of new pixel values. Subsequently, \"hallucination\" has been frequently employed in computer vision research, including notable works such as Hsu et al. (2010  ###reference_b39###) on face hallucination.\nIn the modern deep learning era, hallucination was used first by Andrej Karpathy in his blog focusing on Recurrent Neural Networks Karpathy (2015  ###reference_b51###). He used the term within the context of language models by illustrating how an LSTM could generate non-existent URLs, effectively \u2018hallucinating\u2019 data. The term then gained major traction with the launch of ChatGPT Wu et al. (2023  ###reference_b119###), where it referred to inaccuracies and factual mistakes produced by models Ji et al. (2023a  ###reference_b44###). However, the field lacked a unified definition, leading to a spectrum of interpretations Filippova (2020b  ###reference_b27###). In one of the earlier works, Maynez et al. (2020  ###reference_b73###) divides term usage into intrinsic and extrinsic hallucination. Intrinsic hallucinations are consequences of synthesizing content using the information present in the input. Extrinsic hallucinations are model generations that ignore the source material altogether.\nInitial attempts at detecting hallucination in LMs try to calibrate model probabilities to tell itself if it is hallucinating Tian et al. (2023  ###reference_b109###); Lin et al. (2022  ###reference_b60###); Kadavath et al. (2022  ###reference_b50###). Some attempts have been made to use transferable adversarial attacks to detect hallucination Yu et al. (2023  ###reference_b125###).\nInitial works on reducing hallucination aimed to retrain LMs with extra knowledge Zhang et al. (2019  ###reference_b129###); Sun et al. (2021  ###reference_b105###).\nRecently, self-checking, wherein models check their outputs for factual correctness, has shown progress as well Manakul et al. (2023  ###reference_b69###); Luo et al. (2023  ###reference_b64###). Another direction to reduce hallucination in LMs is by using external knowledge for producing outputs, commonly known as Retrieval Augmented Generation Systems (RAGs) Lewis et al. (2020  ###reference_b58###); Guu et al. (2020  ###reference_b38###); Zheng et al. (2023  ###reference_b131###) whereby the model reduces hallucination using a controlled setting, such as querying on internal data.\nHowever, there is a rise in discussion around terminology that reflects a deeper inquiry into the phenomena, with recent discourse advocating for \u2018confabulation\u2019 Millidge (2023  ###reference_b77###) or \u2018fabrications\u2019 McGowan et al. (2023  ###reference_b74###) as a more precise descriptor. This reflects the lack of consensus on the term and highlights the importance of looking at the use of hallucination with a more critical lens."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Social Perspectives on Hallucinations",
            "text": "Although the exploration of hallucination in NLP is solely technocentric, its conceptual roots and applications are deeply intertwined with societal interpretations. To gain a comprehensive understanding of the term \u2018hallucination,\u2019 it is crucial to consider its broader usage and implications beyond NLP. Hallucination has been studied across disciplines like psychology and neurology. Essentially, hallucinations involve \u201cperceptions arising in the absence of any external reality \u2013 seeing or hearing things that are not there.\u201d These experiences can be termed \u2018veridical hallucinations,\u2019 where \u201cone hallucinates, but one\u2019s experience is accurate.\u201d Although this definition is commonly used in NLP, often with negative connotations, hallucinations have a wide scope, originating from various fields such as psychology, neurology, and philosophy.\n\nHallucination and Medicine: Hallucination is believed to have neurological origins, often emerging from induced states such as drug usage, psychosis, sensory deprivation, or migraines. These experiences can encompass various sensory modalities like auditory, visual, olfactory, tactile, gustatory, or somatic sensations. Modern neurological research indicates a correlation between the modality of hallucinations and specific brain regions responsible for processing associated sensory information. This suggests that while hallucinations may not align with external reality, they are linked to brain regions responsible for processing perceptions from the external world.\n\nHallucination and Creativity: Hallucinations are distinct from imagination, rumination, or dreaming. Some studies suggest that individuals prone to non-severe forms of hallucinations may exhibit heightened generative creativity. However, it is essential to advocate for increased empirical rigor to establish strong connections between specific mental states that may lead to hallucinations and the creative thinking process. Another commonly held belief revolves around using hallucinations as a means to access intuition, creativity, and innovative ways of thinking. This belief often extends to the use of external substances like psychedelics, which are believed to facilitate altered states of consciousness conducive to creative exploration.\n\nThe exploration of various perceptions surrounding hallucination serves to explain its multifaceted interpretations. Contrary to prevailing negative assumptions in NLP, hallucination encompasses a spectrum of connotations. Utilizing the term \u2018hallucination\u2019 in the realm of NLP can lead to misconceptions for several reasons. Primarily, AI lacks sensory understanding, and the \u2018errors\u2019 in AI systems result from discrepancies in input data and prompts rather than a deficit in external stimuli. Secondly, this metaphor bears the potential to engender stigma by associating negative AI phenomena with specific aspects of mental illness. Such linkage risks undermining endeavors aimed at destigmatizing psychiatric and mental health domains. Lastly, with machine learning models widely used in fields like medicine, a lack of contextual understanding of \u2018hallucination\u2019 may lead to misinterpretations of common terminology."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Critical Analysis of Hallucination in NLP",
            "text": "Given the discourse outlined in the preceding sections, it becomes evident that the framework employed for hallucination in AI substantially deviates from established social frameworks. This discrepancy presents challenges, as mentioned previously, when examining hallucination within sociotechnical systems Venkit et al. (2023), which strive to devise interdisciplinary solutions for a nuanced understanding of and for society. Therefore, conducting a critical analysis of this phenomenon becomes necessary to understand how the field of NLP conceptualizes and measures hallucination. To accomplish this, we conducted an audit of works from the ACL anthology using specific keywords such as \u2018hallucination\u2019, \u2018NLP (OR) AI\u2019 AND \u2018hallucinations\u2019, \u2018fabrication\u2019, and \u2018confabulations\u2019. We surveyed papers released on and before March 19th, 2024. From this search, a total of 164 papers were retrieved. After filtering out papers that were not directly related to hallucination research or those that merely mentioned the term without substantial focus on the topic, we arrived at a corpus of 103 papers. This corpus forms the basis for our audit and analysis of hallucination research, specifically within the NLP domain."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Conceptualization of Hallucination",
            "text": "We performed an iterative thematic analysis Vaismoradi et al. (2013 ###reference_b111###) to uncover the various applications of hallucination research in NLP. To ensure accuracy and prevent misclassification, this recursive process was employed. This resulted in the identification of seven distinct fields that address research on hallucination (as shown in Table 1 ###reference_###).\n\nThis taxonomy affords insights into the pervasive nature of hallucination within this domain of literature. Notably, it reveals that hallucination in NLP transcends beyond text generation, extending its conceptualization to encompass broader domains such as Image-Video Captioning, Data Augmentation, and Data-to-Text Generation tasks. This underscores the significance of hallucination both within and beyond the realm of NLG. Moreover, our classification framework provides us with a faceted analysis of how each of these tasks defines the concept of hallucination.\n\nUsing thematic categorization, we come across definite attributes across the definitions of hallucination. One set of attributes elucidated how hallucinations are associated with the style/language generated by the model: Fluency, Plausibility, and Confidence.\n\nThe next set of attributes falls under the effects of hallucinations: Intrinsic, Extrinsic, Unfaithfulness and Nonsensical. The definition of each of these attributes is elaborated in Table 2 ###reference_###.\n\nIn each paper analyzed within the survey scope, hallucination is defined based on a combination of the set of attributes identified. Our survey revealed 31 unique frameworks for conceptualizing hallucination, illustrating the diverse approaches and perspectives used. This diversity underscores the ambiguity in the term\u2019s usage.\n\nTo illustrate this phenomenon, we present some examples showcasing the diverse approaches commonly observed in the literature:\nHallucination refers to the phenomenon where the model generates false information not supported by the input. - Xiao and Wang (2021a ###reference_b120###)\nLarge Language Models often exhibit a tendency to produce exceedingly confident, yet erroneous, assertions commonly referred to as hallucinations. - Zhang et al. (2023a ###reference_b126###)\nModels generate plausible-sounding but unfaithful or nonsensical information called hallucinations - Ji et al. (2023c ###reference_b46###)\n\nHence, within the domain of NLP, a notable deficiency persists in grasping coherent characteristics of hallucination. This shortfall underscores the risk of potential misappropriation of the term when employed in divergent contexts.\n\nWe now analyze what aspects of the definitions of hallucination most commonly occur within each of our identified sub-fields of NLP. The breakdown of all the works associated with each of the subfields is in our Appendix (Table 1 ###reference_###).\n\nConversational AI: In this sub-field, hallucination encompasses fluency, non-factuality, and both intrinsic and extrinsic hallucinations. The definitions\u2019 facets highlight that dialogue systems must balance conversational fluency with factual consistency, aligning both with prior conversation and real-world truths.\n\nAbstractive Summarization: Works in this sub-field mainly focuses on extrinsic and intrinsic hallucinations in defining it. Some definitions also mention the faithfulness of the generation. Despite the challenges of aligning with real-world facts and source consistency, prioritizing alignment and adherence to the original material has been shown to be essential in these works.\n\nData2Text Generation: Hallucinations are classified into extrinsic and intrinsic types, similar to abstractive summarization. Here, alignment with the underlying data is emphasized as the more critical factor when compared to the language used in generating the text.\n\nMachine Translation: Definitions of hallucination predominantly concentrate on extrinsic hallucination, with rare mentions of intrinsic hallucinations. This observation suggests a lesser concern for stylistic nuances in text generation within this field, with a greater emphasis on comprehending and conveying translated content accurately.\n\nImage and Video Captioning: Models are expected to maintain consistency with the source while also incorporating real-world knowledge to address gaps and apply common sense. Consequently, the definition of hallucination in this context encompasses intrinsic, extrinsic, and non-factual elements, highlighting these requirements.\n\nData Augmentation: Works from this domain often omit explicit definitions of hallucination, indicating a divergence in emphasis or a nascent exploration of this construct within this sub-field.\n\nMiscellaneous: Encompassing tasks such as language inference and factuality detection, this category\u2019s definitions of hallucination encompass aspects like factuality, intrinsic and extrinsic hallucination, fidelity, and nonsensicality. It\u2019s evident that within these subfields, hallucination addresses both the stylistic aspects of model output and the fidelity and accuracy of generated content.\n\nFrom the analysis of different subfields, it is evident that each perceives hallucination differently, emphasizing specific attributes such as factuality, fidelity, or linguistic styles like confidence, while potentially overlooking others. This diversity indicates that hallucination as a concept is still in its early stages in the field, with various frameworks emerging and a general lack of consensus regarding its"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Audit of Frameworks",
            "text": "The preceding section delved into examining hallucination within the various domains of NLP, exposing a need for coherence in the prevailing approaches within the field. Building upon this revelation, our analysis now pivots to underscore the pressing need for a unified understanding of hallucination, particularly in light of the pervasive use of language models in both public discourse and interdisciplinary research endeavors.\n\nWe now scrutinize existing literature through a critical lens to unravel the dominant frameworks employed in defining hallucination while also assessing the extent to which these models accurately capture the essence of this phenomenon. This examination facilitates the outlook on transparency in the approach to hallucination in NLP, as well as the potential challenges it poses.\n\nWe start by looking at how many of the selected works explicitly define hallucination. Out of the 103 papers under consideration, just 44 (42.7%) provide a definition of the term, leaving the majority\u201459 papers or 57.3%\u2014either altogether omitting their understanding of hallucination in the context of their research or providing no definition of a framework for the same. This lack of transparency is not only concerning but also underscores the need for clarity, especially given the varied interpretations of hallucination across different research domains.\n\nTaking our scrutiny a step further, we investigate whether the works defining hallucination reference and acknowledge preexisting frameworks. It emerges that only 29 papers or 27% of the selected works explicitly acknowledge and adhere to established frameworks of hallucination, while the remainder 73% either loosely define the term or devise new definitions tailored to their specific research scope. This trend within the field underscores a lack of consensus on the conceptualization of hallucination, leading to disparate interpretations and a shortage of discourse on the subject.\n\nWe also audit the sociotechnical nature of the definitions of hallucination in NLP. Hallucination, as elucidated in Section 3, is inherently imbued with social dimensions, manifesting varied perspectives across different social contexts. Recognizing this complexity, it becomes important to leverage and integrate these social nuances into the shared lexicon of hallucination within machine learning and NLP. Moreover, given the evolution of language models into sociotechnical systems, adopting a sociotechnical approach becomes necessary, given that the term \u2018hallucination\u2019 is inherently a shared vocabulary within these domains.\n\nUnfortunately, out of the 103 works examined, only 3 acknowledge the sociotechnical nature of hallucination, with none utilizing this framework to inform their approach. This oversight underscores a need for research to explore the sociotechnical dimensions inherent in hallucination, showcasing the current limited depth of understanding within the ML and AI communities."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Audit of Metrics",
            "text": "In the analysis of the 103 papers, we observed that 87 of these works dedicate efforts to measuring \u2018hallucination\u2019. This observation depicts the prevailing trend within NLP, emphasizing the significance of quantifying the concept of hallucination across diverse research endeavors. Building upon prior studies such as Ji et al. (2023a  ###reference_b44###), our analysis categorizes the common approaches in NLP for quantifying hallucination into two major themes: Statistical Metrics and Mixed Methodologies.\n\nStatistical Metrics approach entails the analysis of lexical features to assess information overlap and contradictions between generated and reference texts Ji et al. (2023a  ###reference_b44###). This method calculates a hallucination score based on the degree of mismatch, with higher discrepancies indicating lower accuracy, factuality or faithfulness and hence, higher hallucination. Statistical scores such as BLUE, ROUGE, and Error Rate metrics are commonly used in this approach. Our findings reveal that 35.2% of the works that quantify hallucination opt for statistical metrics, employing 25 distinct metrics (e.g., BLEU, BERTScore, ROUGE1/2, F1, Perplexity, Cosine Similarity) developed for this purpose. This variability in measurement underscores the lack of a standardized approach within the research community for quantifying hallucination using statistical methods.\n\nMixed Method approach is deployed by 28.4% of the works, combining human evaluation with statistical metrics to offer a holistic perspective on hallucination quantification. This trend reflects a concerted effort within the research community to address the limitations of individual methodologies and provide insights into the presence and nature of hallucination in generated texts.\n\nThis metrics audit reveals significant knowledge gaps and challenges across various approaches. Notably, established research highlights areas for improvement in standard methods for measuring hallucination. For instance, methodologies like CHAIR and metrics such as ROUGE scores exhibit instability in measuring hallucination due to the need for complex human-crafted parsing rules for exact matching, rendering them susceptible to errors Li et al. (2023  ###reference_b59###).\n\nBeyond methodological criticisms, our survey uncovers a trend of employing numerous distinct metrics and approaches within these frameworks to categorize hallucinations. Over time, this has led to a diverse set of parameters for measuring hallucination, with a general lack of consensus on a standardized measurement approach. This issue is exacerbated by the multitude of diverse approaches to perceiving hallucination in NLP, further highlighting the absence of a unified method to address this challenge, especially as these models have now shifted to become a sociotechnical solution."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Practitioner Survey of Hallucination",
            "text": "In this section, adopting a community-centric approach Narayanan Venkit (2023  ###reference_b80###), we conduct a survey to gain insights into researchers\u2019 perceptions on hallucinations in NLP. We aim to understand how frequently researchers utilize these models, how often they encounter hallucinations, and their firsthand experiences with model hallucinations.\nThe primary goal is to demonstrate how researchers and practitioners within the field perceive the concept of \u2018hallucination\u2019 and to expand our findings beyond the limitations of existing literature. This survey-based approach allows us to delve into the practical aspects of hallucination research and gather real-world perspectives from individuals actively engaged in NLP and AI research."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Survey Recruitment and Data Collection",
            "text": "For our survey, we employed a multi-faceted approach to reach a diverse population of respondents. We utilized direct emails, direct messages, and social media platforms such as LinkedIn and Twitter to distribute the survey. Our target audience included graduate students and professors from academic backgrounds as well as individuals from the industry who work in NLP, aiming to capture a wide range of perspectives on hallucinations.\nTo ensure a comprehensive view, we specifically targeted researchers familiar with AI and ML, primarily from disciplines such as computer science and information science. However, we also welcomed participants from other domains to explore their perceptions of whether they had the literature understanding of the concept of hallucination. The survey was examined and approved by the Institutional Review Board (IRB) for ethical practices.\nFor the survey, we employed a systematic approach by randomly selecting 15 universities from the top 100 in the USA as per the 2023 US News and World Report rankings News (2023  ###reference_b82###), to then reach out to potential participants from the field of NLP and AI, including faculty members and graduate students, to provide their insights through the survey.\nAdditionally, we expanded our outreach efforts by leveraging social media platforms to reach out to practitioners within the networks of all the authors involved in the survey.\nWe received a total of 221 responses, out of which 171 were complete and usable for analysis."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Survey Structure",
            "text": "The survey employed a combination of 12 open-ended and close-ended questions. For the analysis of open-ended questions, we utilized thematic analysis, drawing from established methodologies outlined in Blandford et al. (2016  ###reference_b6###); Terry et al. (2017  ###reference_b107###). The analysis involves identifying patterns, themes, and categories within the free-text responses provided by participants allowing us to uncover nuanced perspectives and key themes related to the research questions. The close-ended questions were analyzed using descriptive statistics to summarize and analyze the numerical data obtained from respondents. Throughout the analysis process, the research team made collective decisions regarding the retention, removal, or reorganization of themes derived from open-ended responses."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Survey Findings",
            "text": "Here, we summarize insights from our 171 survey responses to explore various perspectives on hallucinations in LLMs, including perceptions, weaknesses, and preferences. The breakdown of responses indicates that 76.54% of participants were from academia, 20.98% from the industry, and 2.47% selected both.\nParticipants were also asked about their research area\u2019s direct relation to AI and NLP. The analysis revealed that more than 68.52% of researchers indicated that their work is directly related to AI, while the remaining respondents either exhibited familiarity with or indirectly incorporated NLP and AI methodologies in their work. This highlights the substantial involvement of AI experts and practitioners in addressing issues related to LLM model hallucinations."
        },
        {
            "section_id": "5.3.1",
            "parent_section_id": "5.3",
            "section_name": "5.3.1 Weaknesses of LLM",
            "text": "Before delving into inquiries about hallucinations in LLMs, it is crucial to gain insights into the perceived weaknesses of these models from the participants\u2019 perspective, as well as understand how frequently they utilize these models in their work.\nThe survey results indicate that a significant portion of researchers heavily utilize LLMs in their daily life. Specifically, 67.28% of respondents reported using these models atleast once a day, while 20.37% mentioned using them all the time, highlighting the ubiquity of these models.\nUpon analyzing the themes derived from participants\u2019 responses on the weaknesses of generative AI tools, it was observed that a substantial majority (55%) of researchers perceive the main weaknesses to be the generation of misinformation and hallucinations, despite both phenomena being essentially similar in nature. For instance,\nI have been exploring these models to see what they get right and wrong. They get a lot of things wrong \u2013 what some people call \u201challucinations\u201d.\u2014Emeritus Professor, NLP\nSome of the other important weaknesses mentioned by the respondents are: biases, not following the prompts correctly, complex language, and not having a long memory. For example,\nThey produce a lot of inaccurate replies with great confidence. These models also tend to be very biased toward many socio-demographic groups.\u2014Graduate student, GenerativeAI\nIt is hard to distinguish whether the information provided by them is accurate or not. Sometimes, the models generate text with reasoning making it sound convincing enough to be true - but ends up being incorrect ultimately.\u2014Industry, GenerativeAI\nThe responses highlight a critical concern within the research community regarding the reliability and accuracy of outputs generated by LLMs, with potential implications for various applications and domains, providing us with a strong motivation behind this study.\nThe widespread use of LLMs, particularly prominent models such as GPT 3, 3.5, and 4, highlights their importance and impact on research and industry practices. However, it\u2019s noteworthy that respondents also mentioned other LLM models that they use or are familiar with. These include Mistral, BERT, LLaMA2, Midjourney, ClaudeAI, Gemini, Vicuna, t5, Falcon, PaLM, Imagen, Dolly, Perplexity, among others."
        },
        {
            "section_id": "5.3.2",
            "parent_section_id": "5.3",
            "section_name": "5.3.2 Familiarity with Hallucination",
            "text": "The survey included the question on participants\u2019 familiarity with the concept of \u2018hallucinations\u2019 in AI-generated text, measured on a 5-point Likert scale. The analysis revealed that 24.07% of researchers reported being extremely familiar with the concept, while 33.33% indicated being very familiar with it (Figure 3  ###reference_###).\nParticipants who indicated not being familiar with the term \u2018hallucination\u2019 (7.96%) also demonstrated implicit concerns with this phenomenon by highlighting issues such as generating incorrect responses, making mistakes, and crafting stories autonomously.\n###figure_3###"
        },
        {
            "section_id": "5.3.3",
            "parent_section_id": "5.3",
            "section_name": "5.3.3 Frequency of Encountering Hallucination",
            "text": "The survey included a question regarding the frequency of encountering \u2018hallucinated\u2019 content, defined as content that is factually incorrect or unrelated to the input, assessed on a 5-point Likert scale ranging from \u2018Never\u2019 to \u2018Very frequently\u2019 (Figure 4  ###reference_###). The analysis revealed that 46.91% of respondents reported encountering hallucinated content occasionally, while 29.01% indicated experiencing it frequently. The results suggest that a substantial portion of researchers and practitioners encounter instances of hallucinated content in AI-generated outputs, indicating a prevalent issue in generative NLP models.\n###figure_4###"
        },
        {
            "section_id": "5.3.4",
            "parent_section_id": "5.3",
            "section_name": "5.3.4 Perceptions of Hallucination",
            "text": "The survey findings revealed that more than 92% of respondents perceive hallucination as a weakness of LLMs. Subsequently, participants were asked to provide their own definitions of \u2018hallucination\u2019 in generative AI models through an open-ended question. To analyze these responses, we applied thematic categorization based on attributes generated from the literature audit (Table 2  ###reference_###).\nThe thematic categorization revealed that the majority of respondents categorized hallucination as pertaining to the factuality and faithfulness of input, with relatively lesser emphasis on the extrinsic and intrinsic nature of hallucination concerning the input. This trend reflects a common perception of how hallucination is understood within the context of larger-scope generative AI models.\nMoreover, the analysis identified 12 distinct frameworks regarding how hallucination is defined by respondents. For example:\nResponse that appears syntactically and semantically believable, but is not based on actual fact\u2014Academic Researcher, NLP\nWhen the model confidently states something that is not true\u2014Academic Researcher, AI\nThe diversity of viewpoints underscores the inconsistency within the field regarding the conceptualization and understanding of hallucination in the context of generative AI models."
        },
        {
            "section_id": "5.3.5",
            "parent_section_id": "5.3",
            "section_name": "5.3.5 Alternative Terms for Hallucination",
            "text": "The survey included a question asking participants if they prefer an alternate term to describe the phenomenon of \u2018hallucination\u2019 in AI-generated content and to provide an explanation if they do. The analysis revealed that 54.32% of respondents preferred the term hallucination or had no other term to provide. However, among the remaining responses, 40.46% of participants mentioned \u2018Fabrication\u2019 as a better term to describe the phenomenon.\nThis indicates that while the majority of respondents did not propose an alternative term, a notable proportion sees fabrications as a more suitable descriptor for the phenomenon of hallucination in AI-generated content. For example\nFabrication makes more sense. Hallucination makes it feel like AI is human and has the same sensory perceptions that could lead to hallucinations.\u2014Academic Researcher, AI & Education\nIt\u2019s interesting to note that a few researchers also prefer to use the term \u2018Confabulations\u2019 instead of \u2018hallucinations\u2019 when referring to AI-generated content. Their rationale likely stems from the nuanced difference in meaning between the two terms. While hallucinations generally convey the idea of perceiving something that is not based on reality or fact, confabulations specifically refer to the creation of false memories or information without the intention to deceive.\nBy opting for the term \u2018Confabulations,\u2019 researchers may be emphasizing the unintentional nature of the inaccuracies or false information generated by AI models, as opposed to implying deliberate deceit. For example,\nI think confabulation works better because it means creating a false memory without deceit. Fabrication gives the idea that it is intentional, which in the case of generative AI models, it is not.\u2014Academic Researcher, AI & HCI\nIt\u2019s also insightful to see that respondents proposed various alternative terms to describe the phenomenon of hallucination in AI-generated content such as incorrect information/misinformation, Non-factual information, Cognitive gap, hyper-generalization, Overconfidence, and Randomness. These alternatives highlight different aspects and nuances of the inaccuracies or distortions present in the generated content. Participants also mentioned how they prefer multiple terms based on the application in which they are used.\nAs I mentioned there are different types of hallucinations. For instruction and context hallucinations, I would refer to them as inconsistency instead. For factually incorrect hallucinations, the word hallucination is fine.\u2014Academia, NLP"
        },
        {
            "section_id": "5.3.6",
            "parent_section_id": "5.3",
            "section_name": "5.3.6 Creativity and Positive Applications",
            "text": "Not all researchers view hallucinations in AI-generated content through a negative lens. While the majority may associate hallucinations with inaccuracies or distortions, a notable minority (10% in our survey) provided insights into how they believe hallucinations in these models can be correlated with creativity rather than negatively impacted behaviors. In fields such as story narration and image generation, researchers often value the creative behaviors exhibited by AI models. Hallucinations, when viewed in this context, may be seen as manifestations of the model\u2019s ability to think outside the box, generate novel ideas, and explore unconventional possibilities. These creative outputs can inspire new approaches to storytelling, art, and problem-solving, contributing to innovation and artistic expression. For example:\nHallucinations are just what is needed for models to be creative. In truth, unless AI text-generators are factually grounded with external knowledge for a specific field, they are just story generators which aim to be creative, hence\u201challucinate.\"\u2014National Lab Researcher, NLP"
        },
        {
            "section_id": "5.3.7",
            "parent_section_id": "5.3",
            "section_name": "5.3.7 Social Ramifications of Hallucination",
            "text": "Participants were prompted to explain the effects of hallucination on their work/daily life.\nThe resulting themes, from our qualitative analysis of their inputs, are outlined below:\nNot Good for Education:\nRespondents raised concerns about the extensive use of these models by students for homework, indicating potential negative impacts on their performance and learning abilities. The respondents believe that such reliance on these models can lead to a degradation in students\u2019 learning. Additionally, respondents express skepticism about the suitability of these models for checking homework assignments.\nI don\u2019t actually use AI for my work; I just want to be aware of what it can do because my students are probably using it for their homework. It could have an impact on students\u2019 mastery of the material.\u2014Associate Prof, Biotechnology\nNot Good for Scholarly Work:\nSeveral respondents noted that these models are not effective for scholarly purposes, citing instances where the models generated information that was not present in the original paper. They express concerns that if researchers rely on these models for tasks like literature summarizing, it could lead to a deterioration in scholarly processes. For example:\nThey tend to generate a lot of misinformed facts about certain groups or cultures that I have seen happen often. They also generate \u2019facts\u2019 from scholarly works where the papers would not have mentioned the same.\u2014Graduate student, NLP\nStruggle with Code Generation:\nThe models were deemed inefficient for code generation by multiple respondents, often producing code that lacks utility due to hallucinations. Respondents highlighted mismatches between the generated code and its intended purpose, emphasizing the need for thorough review before utilization. Various concerns were raised, including the loss of context during prolonged interactions, inaccuracies in complex coding tasks leading to erroneous outputs, fabrication of functions or attributes, inaccuracies in both code and associated theoretical concepts, necessitating extensive debugging and corrections, and a tendency to cycle back to previously incorrect suggestions despite error notifications.\nI was asking an AI to generate me a piece of code. It ended up picking some code from one website and some from another and combining it. However those two websites (they were linked by chatgpt) we\u2019re using different versions of the library so the resulting code couldn\u2019t be executed.\u2014Industry, Network and Security\nIncrease in Time for Task:\nA common sentiment among respondents is that these models frequently produce errors or false information, resulting in potential time wastage. While they acknowledge occasional helpfulness, there\u2019s a consensus that reliance on these models can often lead to unfavorable outcomes, particularly when verifying outputs. This dependency on verification contributes to increased task duration, adding extra work and time toward the project\u2019s conclusion, as noted by several respondents.\nI use GPT API to conduct analysis for some of my work and accuracy and consistency would be good in my context, and I have to find ways to finetune it before I can trust the results of the analysis, which added more work on my end.\u2014Graduate Student, HCI\nMisleading and Distrust:\nGenerating incorrect outputs with confidence can lead to the dissemination of non-existent knowledge, such as misleading information in the literature that may confuse individuals with incorrect concepts. Most of our respondents mentioned this concern. Moreover, it poses challenges in differentiating between accurate AI responses and hallucinations, particularly for users lacking expertise in the relevant subject matter.\nIt leads to problems if even I do not have any idea about the work. It is hard to differentiate if it is a genuine output or hallucination.\u2014Graduate Student, Data Science"
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "An External Viewpoint",
            "text": "Additionally, our survey of 51 researchers who do not specialize in AI revealed that all except 3 have used text-generation models like various versions of ChatGPT. Despite their fields not being directly related to AI, a significant number integrate these tools into their workflow, with 19.6% using them multiple times daily and 11.76% using them several times per hour. Their extensive usage has allowed them to identify several limitations in the models; they are: mathematical inaccuracy, outdated information, misinformation, poor performance with complex tasks and creative thinking, lack of specificity in-depth, overconfidence, lack of transparency, bias, and irrelevant responses.\nBased on the definitions provided, it is observed that there is a lack of clarity among the respondents regarding what constitutes a \u2018hallucination\u2019 in generative AI models, with perspectives varying widely. Thematic analysis of their responses indicates that the predominant view associates \u2019hallucination\u2019 with the generation of nonfactual content and misinformation by AI systems. That means these models are generating facts that are not real and misleading. The remaining themes are factually incorrect, biased outputs, incompleteness, misinformation with confidence, and nonsensical but good-looking texts.\nThe results demonstrate the unclear comprehension and significance attributed to hallucination in language models beyond the field of NLP and AI. There is a pressing need to enhance public understanding of the concept of hallucination, emphasizing its meaning and strategies for mitigation. Given the increasing prominence of language models as sociotechnical systems Narayanan Venkit (2023  ###reference_b80###), it is crucial to grasp their social interactions and potential societal ramifications."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Additional Impacts and Concerns",
            "text": "We analyzed perceptions when participants were asked about any additional concerns during the survey. Participants emphasized the necessity for greater control and more nuanced mechanisms to address and manage AI hallucinations effectively. Presently, the detection and rectification of hallucinations rely heavily on meticulous human review, highlighting the need for tools designed specifically to identify and mitigate such occurrences. The presence of hallucinations can significantly impact the credibility and acceptance of generative models among the general public. These issues arise due to the inherent limitations of generative algorithms and the absence of access to real-time external knowledge.\nTransparency regarding the limitations of generative AI is deemed essential through our findings, and user education is seen as a key factor in mitigating risks associated with the unchecked use of AI-generated content, as the responsibility for identifying hallucinations often falls on the user. While inaccuracies in non-critical applications, like movie suggestions, may be tolerable, according to our survey, they are deemed crucially problematic in contexts such as business decision-making, law, or health Dahl et al. (2024  ###reference_b17###)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Research Challenges and Recommendations",
            "text": "Finally, based on our audit and survey analysis, we outline the key weaknesses encountered in hallucination within NLP and potential recommendations motivated by the weaknesses. Our focus is to utilize a community-centric and literature-based approach to define the primary weaknesses of the field currently and a path forward."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Challenges",
            "text": "Wide range of vague and absent definitions: The literature and the practitioner\u2019s survey show diverse and conflicting definitions and frameworks, often lacking clarity or omitting explicit definitions for hallucination and how it is perceived in various fields of NLP.\nLack of standardization in measurement: The absence of standardized metrics to quantify hallucination results in the use of multiple scales and categorizations. This lack of standardization makes it challenging to compare and interpret results across different models and studies, leading to a proliferation of diverse approaches for evaluating hallucinations.\nLimited awareness of hallucination in a sociotechnical context: Hallucination research often lacks the understanding of how the concept of hallucination is conceptualized beyond its technical purview (discussed in Section 4). When such analysis is employed in sociotechnical systems like healthcare and policy making Dahl et al. (2024  ###reference_b17###); Pal et al. (2023  ###reference_b84###), it is important to define the socially relevant framework of hallucination as well. There is no motivation shown to understand the social, political, and psychological considerations of hallucination in these works.\nLack of consensus between various subfields There are multiple frameworks of hallucination created across various subfields in NLP. For instance, hallucination in image captioning exhibits distinct characteristics compared to abstract summarization and conversational AI. These frameworks have been adopted based on individual usage without reaching an accord among other existing works.\nMultiple sentiment towards Hallucination\nThe perception of hallucination in generative AI varies depending on the context. For instance, it is often positively regarded as creativity in image generation, whereas in text generation, it is viewed negatively as errors or mistakes. Consequently, future research efforts should aim to better address this disparity to develop a more nuanced framework for understanding hallucination.\nLack of Standardized Nomenclature: Both our literature audit and practitioner survey revealed that the term \u2018hallucination\u2019 is inadequate to fully capture the behavior exhibited by NLG models. There is a need for further investigation into which terms are more appropriate and why they are necessary. For instance, terms like \u2018confabulation,\u2019 \u2018fabrications,\u2019 and \u2018misinformations\u2019 are increasingly being used to describe the same phenomenon. A more precise understanding is required to distinguish between these terms and how they are utilized in various fields within NLP.\nEthical and Social Challenges: Given that researchers and users often rely on these models for critical decision-making processes, there is a need for well-defined policies regarding the extent of their utilization and the transparency surrounding their use and the impact of hallucinations within such models. Currently, such policies are lacking in industries and academic settings.\nAddressing these issues requires careful consideration of the categorization approach, integration of contextual information, and, efforts towards robust evaluation methodologies in hallucinations."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Recommendations",
            "text": "Prior works like Blodgett et al. (2020  ###reference_b7###) & Venkit et al. (2023  ###reference_b112###) have formulated thematic recommendations to address AI issues. Expanding on this, we examine strategies for NLP practitioners studying \u2018hallucination\u2019 to overcome these challenges. We propose two overarching themes with four associated recommendations."
        },
        {
            "section_id": "6.2.1",
            "parent_section_id": "6.2",
            "section_name": "6.2.1 Author-Centric Recommendation",
            "text": "These recommendations prioritize actionable steps for both the author and developers, emphasizing transparent and accountable development in conceptualizing hallucinations.\n[R1] Ensure explicit documentation of the hallucination framework and analysis methodology employed during the development of NLP models. Provide guidelines that outline the expected measurements and quantifications for the model to enhance interpretability and applicability.\n[R2] Explicitly state the use cases and user profiles intended to interact with the NLP system. By considering the specific applications and targeted users, it is easier to construct the required framework of hallucination that is sensitive to the community in consideration. Raise awareness about potential ramifications introduced by NLP models, emphasizing the importance of fairness and equity."
        },
        {
            "section_id": "6.2.2",
            "parent_section_id": "6.2",
            "section_name": "6.2.2 Community-Centric Recommendation",
            "text": "These recommendations prioritize actionable steps for the research community to enhance frameworks and understanding related to hallucinations.\n[R3] Develop clear and standardized definitions for terms such as \u2018confabulations,\u2019 \u2018fabrications,\u2019 \u2018misinformation,\u2019 and \u2018hallucinations\u2019 within the context of NLP. Establish frameworks that provide clarity and consistency in understanding these concepts, particularly regarding hallucinations. This requirement is crucial due to the widespread misunderstanding of hallucination and the misnomers that have arisen as research progresses.\n[R4] Promote the creation of methods that offer visibility into the model\u2019s decision-making process, enabling users to comprehend how hallucinations or fabrications can occur within the system, thus fostering trust in its use. Facilitating research discussions for transparency through workshops and conferences is one approach to achieving this goal."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Our work delves into the conceptualization of hallucination within the scope of NLP. Our approach involved two key methodologies: first, an exhaustive audit of 103 peer-reviewed papers in the NLP domain, and second, a practitioner survey of 171 researchers and academics to gauge the field\u2019s perception and understanding of hallucination. Through this analysis, we have gained insights into how the NLP community conceptualizes and defines hallucination. Our thematic and community-based approach highlights potential weaknesses within the field, particularly in addressing misrepresentations and inaccurate characterizations associated with hallucination. Our work finally contributes to a deeper understanding of the challenges and gaps in research related to hallucination within NLP, paving the way for future advancements in this area."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Our study encompasses a selection of 103 papers, incorporating works from primarily the ACL Anthology. While our intention was not to provide an exhaustive collection of all published works on hallucination, we aimed to include diverse sources within NLP that cover various aspects of the field. Our intent was to curate peer-reviewed literature commonly found in the NLP domain, encompassing models, applications, survey papers, and frameworks.\nWe, therefore, did not scope the utility of hallucination and its impact beyond NLP to other fields of research, such as Computer Vision. Regarding the creation of the challenges and recommendations, it is important to note that the themes presented are not meant to be exhaustive but rather serve as a foundational framework to spark additional inquiries and foster further engagement.\nOur survey was designed to capture the viewpoints of researchers and practitioners in the AI and ML field, potentially limiting various experiences. As such, our analysis is centered on this perspective. While we did gather additional insights from participants outside this field, our focus was not comprehensive in that regard. Our future work intends to explore the public\u2019s perspective on hallucination."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Ethics Statement",
            "text": "We are aware of the ethical considerations involved in our research and have taken measures to ensure responsible practices throughout the study.\nData Publication: All the papers used in our research are listed in the Appendix. However, we recognize the importance of transparency and accountability. Therefore, we publish the complete collection of papers along with our qualitative classification and annotation, allowing for public scrutiny and examination 333github.com/PranavNV/The-Thing-Called-Hallucination.\nMitigating Qualitative Bias: We acknowledge the potential for bias when performing qualitative coding of the papers regarding their applications. To address this concern, we ensured that at least three different individuals independently reviewed and verified the coding to minimize the possibility of misclassification. Additionally, we followed the same approach to verify the presence of various definitions in each paper, enhancing the reliability and validity of our analysis.\nBy disclosing these ethical considerations, we emphasize our commitment to conducting research in an ethical and accountable manner."
        }
    ],
    "url": "http://arxiv.org/html/2404.07461v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "5.1",
            "5.2",
            "5.3",
            "5.3.1",
            "5.3.2",
            "5.3.3",
            "5.3.4",
            "5.3.5",
            "5.3.6",
            "5.3.7",
            "5.4",
            "5.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.07461v1",
        "paper_title": "\u201cConfidently Nonsensical?\u201d: A Critical Survey on the Perspectives and Challenges of \u2018Hallucinations\u2019 in NLP",
        "research_background": "### Paper\u2019s Motivation:\nThe paper is motivated by the rapid advancements in Natural Language Processing (NLP), especially in the field of language generation, driven by highly sophisticated models like GPT-4. These models are no longer just technical tools; they are becoming integral sociotechnical systems with significant influence on various domains, including health, policy-making, and entertainment. This evolution has surfaced limitations and risks, particularly the phenomenon of \u2018hallucinations\u2019 where language models generate incorrect or nonsensical outputs. The paper identifies a pressing need for a better understanding and cohesive framework for dealing with hallucinations in NLP, treating them as a sociotechnical issue with broad societal implications.\n\n### Research Problem:\nThe primary research problem revolves around understanding, defining, and conceptualizing \u2018hallucinations\u2019 in language models. Despite the growing body of research focused on this issue, there is a lack of cohesive frameworks and precise definitions in the field of NLP. The study aims to answer two key research questions:\n1. **RQ1**: What are the definitions and common frameworks used to explain hallucinations in NLG-published articles?\n2. **RQ2**: What is the current understanding of researchers about hallucinations, and how do they encounter them in their work?\n\n### Relevant Prior Work:\n1. **Advancements in Language Models**: The background of the emergence of multimodal models like GPT-4 and Gemini is acknowledged, which have advanced beyond mere NLP tools to become sociotechnical systems influencing various social and technical stakeholders (Cooper and Foster, 1971; Narayanan Venkit, 2023).\n\n2. **Applications of NLP**: The paper notes that NLP applications have extended into various essential domains such as healthcare, policy-making, and entertainment (Lee, 2018; Jin and Mihalcea, 2022; Werning, 2024).\n\n3. **Limitations and Risks**: The study highlights the substantial attention on the limitations and potential risks of language models, such as misinformation and biases (Bender et al., 2021; Gupta et al., 2024).\n\n4. **Hallucination Studies**: Prior works have primarily conceptualized hallucinations as negative phenomena where models generate inaccurate references or non-existent objects (Ji et al., 2023a). However, the necessity of a cohesive framework and precise definitions remains largely unexplored (Filippova, 2020a).\n\n5. **Societal Repercussions**: The societal impact of hallucinations in NLP models has also been discussed, emphasizing the need to understand and mitigate these effects (Dahl et al., 2024).\n\nThe paper ultimately aims to survey the field, gather practitioners\u2019 views, and develop an ethics framework to guide future research toward more robustly addressing the challenges posed by hallucinations in language models.",
        "methodology": "In the paper \u201cConfidently Nonsensical?\u201d: A Critical Survey on the Perspectives and Challenges of \u2018Hallucinations\u2019 in NLP, the methodology section outlines the approach taken to critically analyze how the field of NLP conceptualizes and measures hallucinations. Here is a summary of their proposed method or model, highlighting the key components and innovations:\n\n**Framework Analysis:**\n- The authors recognize a significant deviation between the framework for hallucination in AI and established social frameworks. This gap presents challenges in understanding hallucinations within sociotechnical systems.\n\n**Critical Analysis:**\n- The necessity for a nuanced interdisciplinary understanding is emphasized, prompting a comprehensive critical examination of NLP's approach to hallucinations.\n\n**Keyword Audit:**\n- An audit was conducted using the ACL anthology, employing specific keywords such as \u2018hallucination\u2019, \u2018NLP (OR) AI\u2019 AND \u2018hallucinations\u2019, \u2018fabrication\u2019, and \u2018confabulations\u2019.\n\n**Paper Selection:**\n- The audit included papers released on or before March 19th, 2024.\n- Initially, 164 papers were retrieved based on the keyword search.\n- After filtering out papers not directly related to hallucination research or those mentioning the term without substantial focus, a refined corpus of 103 papers was established.\n\n**Corpus for Analysis:**\n- The remaining 103 papers constitute the primary data set for further audit and analysis. \n\nThis methodology represents a comprehensive, systematic, and targeted approach to critique the phenomenon of hallucination in NLP research, with an emphasis on interdisciplinary understanding and accurate conceptualization within the field.",
        "main_experiment_and_results": "**Main Experiment Setup and Results:**\n\nThe main experiment involved conducting a comprehensive survey to gather perspectives on the phenomenon of 'hallucinations' in Natural Language Processing (NLP) from a diverse group of respondents.\n\n**Survey Distribution:**\n- **Channels Used:** Direct emails, direct messages, and social media platforms such as LinkedIn and Twitter.\n- **Target Audience:** Graduate students and professors from academic backgrounds, and individuals from the industry working in NLP.\n- **Scope:** Targeted researchers with backgrounds in AI and ML, particularly from computer science and information science disciplines. Participants from other domains were also included to explore their understanding of hallucinations in the literature.\n\n**Selection Process:**\n- **Universities:** Randomly selected 15 universities from the top 100 in the USA as per the 2023 US News and World Report rankings.\n- **Participants:** Specific outreach to faculty members and graduate students in NLP and AI fields, expanding through social media networks to include a broader practitioner community.\n\n**Respondent Data:**\n- **Total Responses:** 221 responses received.\n- **Usable Responses:** 171 complete and usable responses for analysis.\n\n**Results:**\nThe survey results stem from the responses of these 171 participants, reflecting a variety of perspectives and providing insights into the understanding and challenges related to hallucinations in NLP from a diverse and cross-disciplinary audience.\n\n(Note: Specific data points, findings, and conclusions from the survey were not provided in the excerpt, so they cannot be included here.)"
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The study aims to critically analyze and categorize the dominant frameworks used to define and address hallucination in a variety of NLP research, highlighting the coherence, transparency, and sociotechnical dimensions of such definitions.",
            "experiment_process": "The experiment begins by examining 103 selected peer-reviewed publications to determine how many explicitly define hallucination. It further investigates whether these definitions reference existing frameworks. Additionally, the study audits the sociotechnical nature of hallucination definitions, accounting for the social dimensions of the phenomenon as discussed in Section 3.",
            "result_discussion": "Out of the 103 papers, only 44 (42.7%) provide a definition of hallucination, leaving the majority (57.3%) either without a definition or any framework. Only 29 papers (27%) acknowledge and adhere to established frameworks, indicating a lack of consensus and leading to disparate interpretations. Furthermore, only 3 papers recognize the sociotechnical nature, showing inadequate integration of social perspectives in current NLP research.",
            "ablation_id": "2404.07461v1.No1"
        },
        {
            "research_objective": "The study seeks to audit and categorize the metrics used for quantifying hallucination in NLP research, examining the consistency and challenges associated with these measurement methods.",
            "experiment_process": "The analysis encompasses 103 papers, out of which 87 attempt to measure hallucination. The metrics are categorized into four broad themes: Human Evaluation, Data-driven Metrics, Statistical Metrics, and Mixed Methodologies. Each methodology's adoption rate and distinct metrics are documented. Statistical Metrics include BLEU, ROUGE, BERTScore, etc., Data-driven Metrics involve creating specific datasets/models like CHAIR, Human Evaluation employs annotators, and Mixed Methods combine multiple approaches. Criticisms and gaps in these metrics are assessed through existing literature and identified through a survey of practitioners.",
            "result_discussion": "Statistical Metrics are used by 35.2% of the works employing 25 distinct metrics, Data-driven Metrics by 26.1%, Human Evaluation by 10.2%, and Mixed Methods by 28.4%. There is no standardized approach within the research community. Significant issues with instability in metrics like ROUGE and the susceptibility of human evaluation to errors are noted. The research community utilizes a broad range of methods and metrics, leading to a lack of consensus on a standardized measurement approach for hallucination.",
            "ablation_id": "2404.07461v1.No2"
        }
    ]
}