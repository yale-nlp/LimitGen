{
    "title": "WorDepth: Variational Language Prior for Monocular Depth Estimation",
    "abstract": "Three-dimensional (3D) reconstruction from a single image is an ill-posed problem with inherent ambiguities, such as scale. Predicting a 3D scene from text descriptions also presents challenges, like the spatial arrangements of objects. We explore whether these two ambiguous modalities can be combined to create metric-scaled reconstructions. Our focus is on monocular depth estimation, which involves predicting a dense depth map from a single image, enhanced with a text caption describing the scene. We encode the text caption using a mean and standard deviation within a variational framework, learning the distribution of plausible metric reconstructions for 3D scenes described by the text captions as a prior. To determine a specific reconstruction or depth map, the image is encoded through a conditional sampler that samples from the latent space of the variational text encoder, decoding to the output depth map. Our approach alternates training between the text and image branches: one step involves predicting the mean and standard deviation from the text and sampling from a standard Gaussian; the other samples using a conditional image sampler. Once trained, depth is directly predicted from the encoded text using the conditional sampler. We validate our approach on indoor (NYUv2) and outdoor (KITTI) scenarios, demonstrating that language consistently enhances performance. Code: https://github.com/Adonis-galaxy/WorDepth.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The process of imaging is a surjection from a 3D scene to the 2D image domain, where infinitely many 3D scenes can map to the same image. Its inverse problem, estimating the 3D scene structure from a single image, i.e., monocular depth estimation, is therefore ill-posed with inherent ambiguity, such as the scale of the reconstruction. Consequently, induction is necessary, and depth estimation becomes drawing a scene with maximum likelihood from the distribution of all possible scenes, conditioned on the image. This conditional scene distribution is learned by a deep neural network on a chosen training set. While an ideal training set should accurately reflect this distribution, practical challenges arise due to the scarcity of well-established large-scale depth datasets. \n\nThese priors may come in many forms, from generic priors such as local smoothness and connectivity or object orientation that may be imposed as a part of the training objective (regularizer) to specific inductive biases realized as architectural designs (layers) or a collection object shapes. While generic priors are suitable for a wide variety of scenes, they typically lack specificity, i.e., size or shape of objects within a specific 3D scene. On the other hand, specific network designs may backfire when the assumption motivating the design does not hold, i.e., using specifics about camera parameters for reconstruction. We consider a more flexible source of priors \u2013 language \u2013 that is closely tied to semantics, and often shape (and functionality). Consider a text description of \u201cA bedroom with a bed and a table\u201d: One can imagine a probable 3D scene containing a bed and a table as the primary objects. There exist infinitely many 3D scenes compatible with the description, as there are ambiguities in terms of the scene layout and the precise shape of the bed and table. Yet, one may surmise that the scale of the scene is closely related to the objects (and their typical sizes) populating it. This lends to a prior that is specific for a given scene, yet, generic enough without assumptions on the camera used or the shapes within the imaged 3D scene.\n\nHence, the question at hand becomes whether two inherently ambiguous modalities (camera image and text descriptions) can be exploited for their complementary strengths: In the image, one can observe the layout and object shapes populating the 3D scene; in a text caption, one has strong priors about the scale (and coarse shapes) of the scene. Our work aims to resolve the respective ambiguities of the two modalities by using language to reduce the solution space to yield metric-scaled reconstructions as 2.5D depth maps.\n\nTo test the feasibility of this approach, we consider the ill-posed inverse problem of monocular depth estimation, where one predicts a depth map from a single image. Instead of using just an image, we also assume a text description or caption describing the 3D scene captured within the image. Note that we do not make any assumption regarding the source of the description, i.e., it can be dictated by humans or generated by a model. But for practicality, we use an image captioner (ExpansionNet v2) to generate a brief, concise description of the image.\n\nTo exploit the inherent ambiguity of text captions, where a single description can generate infinitely many 3D scenes, we choose to encode the caption using a variational auto-encoder (VAE) as a mean and standard deviation of the plausible scene layout distribution. By sampling a noise vector from a standard Gaussian and using the reparameterization trick customary in VAEs, we can draw from the latent distribution and decode it into a metric-scaled depth map. Yet, to choose a particular depth map amongst the many possible, one must rely on the image. This is facilitated by a conditional sampler that predicts the noise vector from the given image in place of the one sampled from a Gaussian to be used in the reparameterization step. Consequently, this substitution enables one to sample the most probable depth map, adhering to the scene arrangement and object shapes observed in the image, from the learned distribution. This naturally lends to an alternating optimization process between the (text-)VAE and conditional sampler.\n\nIn one alternation, one would predict the mean and standard deviation from the text caption and optimize the text-VAE branch for depth by minimizing a loss with respect to ground truth on the depth map sampled using a standard Gaussian (similar to traditional VAEs). In the other alternation, one would still use the mean and standard deviation predicted by the text-VAE, but instead, use the conditional sampler to \u201cselect\u201d a specific depth map compatible with the image, and again, minimize a loss on the output depth. Note: that depending on the alternation, either the text-VAE or the conditional sampler is frozen. At test-time, one no longer needs to sample from the Gaussian and may directly predict depth using the text-VAE with the conditional"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Monocular depth estimation  trains by minimizing loss between depth predictions and ground-truth depth maps [7  ###reference_b7###, 2  ###reference_b2###, 17  ###reference_b17###, 52  ###reference_b52###, 80  ###reference_b80###, 78  ###reference_b78###, 46  ###reference_b46###, 35  ###reference_b35###, 54  ###reference_b54###, 61  ###reference_b61###, 86  ###reference_b86###, 84  ###reference_b84###, 66  ###reference_b66###].\nSpecifically, DORN [16  ###reference_b16###] employs a spacing-increasing discretization strategy for depth estimation as an ordinal regression problem.\nAdaBins [2  ###reference_b2###] introduces a transformer block that segments the depth range into adaptive bins.\nASTransformer [7  ###reference_b7###] incorporates an Attention-based Up-sample Block to enhance detailed texture features.\nDepthFormer [40  ###reference_b40###] employs hierarchical aggregation and heterogeneous interaction modules for effective feature affinity and modeling.\nRPSF [47  ###reference_b47###] presents a differentiable model of the aperture mask.\nHowever, deriving semantics solely from visual cues is challenging because of scale ambiguity and the limited size of fully annotated training datasets. We use language as a prior to ground predictions to metric scale. When ground-truth depth is not available, self-supervised approaches [70  ###reference_b70###, 27  ###reference_b27###, 36  ###reference_b36###, 85  ###reference_b85###, 3  ###reference_b3###, 96  ###reference_b96###, 100  ###reference_b100###, 63  ###reference_b63###, 62  ###reference_b62###, 51  ###reference_b51###, 64  ###reference_b64###, 15  ###reference_b15###, 94  ###reference_b94###] rely on geometric constraints, often established via from various modalities, including lidar [79  ###reference_b79###, 67  ###reference_b67###, 69  ###reference_b69###, 68  ###reference_b68###, 44  ###reference_b44###, 50  ###reference_b50###, 72  ###reference_b72###] and radar [59  ###reference_b59###], or through deliberate design. Arising from training, if done at a large scale, is a prior on the scene that can be exploited for semantic tasks [33  ###reference_b33###]. On the other hand, we consider language as a semantic prior to enhance the effectiveness of monocular depth estimation.\nVariational and generative methods focus on the ambiguous nature of monocular depth estimation, many involving Diffusion or VAE models for modeling this ambiguity [10  ###reference_b10###, 57  ###reference_b57###, 56  ###reference_b56###, 73  ###reference_b73###, 5  ###reference_b5###, 83  ###reference_b83###, 41  ###reference_b41###]. DepthGen [56  ###reference_b56###] uses a depth pre-trained diffusion model, which generates depth estimations conditioned on images, and shows that the model is capable of generating multiple plausible depth maps when depth is ambiguous. DDVM [57  ###reference_b57###] uses a similar approach and designed a training pipeline that can produce both depth maps and optical flow outputs with a diffusion model. [73  ###reference_b73###] trained a VAE model that outputs a probability distribution over scene depth given an image, which can then be combined with additional inputs for more accurate depth estimations. VDN [10  ###reference_b10###] models depth as a distribution with its variance interpreted as uncertainty. The CodeSLAM model [5  ###reference_b5###] also employed a VAE conditioned on image intensities for depth estimation. However, although these work explored the idea of uncertainty in depth estimation, and even combined other modalities of inputs [73  ###reference_b73###], none have experimented with language priors, and most VAE-based approaches use images to obtain the mean of the modeled distribution, which is fundamentally different from WorDepth.\nFoundation models [53  ###reference_b53###, 37  ###reference_b37###, 38  ###reference_b38###, 6  ###reference_b6###, 48  ###reference_b48###, 21  ###reference_b21###, 104  ###reference_b104###, 23  ###reference_b23###, 77  ###reference_b77###, 49  ###reference_b49###, 98  ###reference_b98###] acquire a comprehensive understanding of languages, images, and other data types through pre-training under substantial and diverse datasets, thus forming an effective baseline for downstream tasks [75  ###reference_b75###, 76  ###reference_b76###, 39  ###reference_b39###, 12  ###reference_b12###, 89  ###reference_b89###, 74  ###reference_b74###, 8  ###reference_b8###, 81  ###reference_b81###, 71  ###reference_b71###, 2  ###reference_b2###, 82  ###reference_b82###, 42  ###reference_b42###, 95  ###reference_b95###, 92  ###reference_b92###, 93  ###reference_b93###]. To leverage foundation models for monocular depth estimation, TADP [30  ###reference_b30###] uses captions created by AI to enhance the correlation between text and images in diffusion-based vision models. VPD [97  ###reference_b97###] leverages a diffusion-based pipeline with cross-attention between text and images. Dinov2 [48  ###reference_b48###] trains a ViT [11  ###reference_b11###] with 1B parameters using an automatically built image dataset under contrastive learning objectives.\nUnlike methods that rely on foundation models for feature extraction, WorDepth is potentially more efficient for industrial applications.\nVision-language models are designed to build connections between visual and language inputs.\nCLIP [53  ###reference_b53###] conducts contrastive learning between text-image pairs, empowering various tasks like few-shot image classification  [18  ###reference_b18###, 88  ###reference_b88###, 87  ###reference_b87###, 101  ###reference_b101###], image segmentation  [99  ###reference_b99###, 55  ###reference_b55###], object detection  [103  ###reference_b103###, 55  ###reference_b55###], and 3D perception [90  ###reference_b90###, 105  ###reference_b105###, 91  ###reference_b91###, 26  ###reference_b26###]. In light of the powerful emerging ability brought by recent vision-language models, some works have tried to apply the vision-language model for monocular depth estimation.\nDepthCLIP [91  ###reference_b91###] leverages the semantic depth response of CLIP [53  ###reference_b53###] with a depth projection scheme to conduct zero-shot adaptation from the semantic language response to monocular depth estimation.\nFurthermore, [26  ###reference_b26###] extends DepthCLIP with learnable prompts and depth codebook to narrow the depth domain gap among different scenes.\nLikewise, [1  ###reference_b1###] modifies DepthCLIP [91  ###reference_b91###] using continuous learnable tokens in place of discrete human-language words.\nAdditionally, VPD [97  ###reference_b97###] exploits the high-fidelity embedding of a pre-trained text-to-image diffusion model in monocular depth estimation.\nHowever, existing methods using vision-language models rely on implicit modeling. Conversely, WorDepth explicitly models language as a prior for depth estimation and exploits strong priors regarding the size of objects described in text captions to better ground monocular depth (often scaleless) to metric scale.\n###figure_2###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "Given an RGB image , monocular depth estimation aims to infer a dense depth map  using a parameterized function  realized as a neural network, i.e., . We consider a supervised dataset  with  samples, where  denotes the ground-truth depth map, and t the text caption describing the image."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Text variational auto-encoder",
            "text": "To incorporate language priors to monocular depth estimation, we first design a variational auto-encoder (VAE) to learn the latent distribution of possible depth maps as described by the text caption. This VAE is comprised of the text encoder from a pre-trained vision-language model, CLIP [53  ###reference_b53###], which by default offers a shared latent space between vision and text embeddings, followed by a multi-layer perceptron (MLP) to estimate the mean  and standard deviation  of the latent distribution of plausible scenes based on the text encoding. Note that the CLIP text encoder is frozen at all times and never updated when training WorDepth. Specifically, given a text caption , we first encode it using the CLIP text encoder and estimate the mean and standard deviation as  using a multi-layer perceptron (MLP). To sample from the distribution parameterized by  and , we first draw a noise vector  from a standard Gaussian . Then, we use  to sample from the latent distribution via the reparameterization trick [29  ###reference_b29###], . We refer to this module as a text variational auto-encoder (text-VAE). To generate a depth map  from the sample , we first duplicate  along the horizontal and vertical axes to yield a  latent (choice of design to be discussed below in Sec. 3.2  ###reference_###) and feed it through a depth decoder to yield , where we overload  as the spatially duplicated latent, and  and  denote the height and width of the depth map, preset as hyperparameters to match the desired image dimensions.\nTo train our text-VAE and depth decoder, we minimize\nwith respect to  and , where  is the scale invariant loss (Eq. 3  ###reference_###),  the KL divergence loss (Eq. 4  ###reference_###) as detailed in Section 3.3  ###reference_###, and  the weight of the KL divergence term."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Image-based conditional sampler",
            "text": "While our text-VAE can predict plausible metric-scaled depth maps from text captions, we are interested in the depth map corresponding to a specific image. To do so, we treat text-VAE as the latent prior distribution of the plausible scene layouts. Predicting depth  for a specific image  requires sampling the latent corresponding to the depth map of the 3D scene layout with the highest likelihood to be compatible with the observed image, i.e., prior conditioned on the image. To this end, we introduce an image-based conditional sampler that will predict the sample  in place of  drawn from the standard Gaussian. Using the reparameterization trick as before, we will use  to select the latent vector  to be decoded by the depth decoder.\nSpecifically, our image-based conditional sampler utilizes a Swin-L transformer backbone to encode an image . We chose this design to exploit the locality of the tokens produced by Swin-L. The tokens are then encoded into  number of local samples  to be used to sample from the latent distribution of our text-VAE; in other words, we perform \u201cpatch-wise\u201d selection from latent distribution for more granular predictions. To do so, we additionally include  and  as part of its input. We note that  and  have been detached from the computational graph and treated as input. We refer to this module as our conditional sampler , which aims to estimate the most probable latent variable of text-VAE. Thus, the scene layout latent vector is now given by , and the predicted depth . As an implementation detail, we note that skip connections from the encoder  are injected into  by concatenation; when training text-VAE (Sec. 3.1  ###reference_###), feature maps of skip connections are of the same size, but populated with zeros instead.\nTo train the conditional sampler, we minimize the same loss (Eq. 1  ###reference_###) as that of text-VAE:\nwith respect to  and . With a batch size of , the number of  is , while  and  are the sample mean and standard deviation of  over a batch. We impose a KL divergence loss as regularization so that the estimated  does not drift from the standard Gaussian, which also serves to improve training stability."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Training Loss",
            "text": "###figure_3### Scale invariant loss. We minimize a supervised loss using ground truth . To improve training stability over diverse scenes, we use the scale-invariant depth loss [13  ###reference_b13###]:\nwhere ,  denotes the image space,  the number of pixels,  the predicted depth, and  the scaling factor to control the sensitivity of the loss.\nKullback-Leibler (KL) divergence loss. Following [29  ###reference_b29###], we employ the KL Divergence loss as a regularizer, which biases the predicted latent distribution (parameterized by mean  and standard deviation ) towards a standard Gaussian distribution. We apply the Kullback-Leibler divergence loss to  and  as follows:"
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Optimizing Wordepth",
            "text": "Training Wordepth involves optimizing text-VAE with our conditional sampler: One may choose to first train text-VAE until convergence (i.e., optimize for ), then freeze , and finally train the image-based conditional sample (i.e., optimize for ). However, we find that doing so often results in the conditional sampler being trapped in a suboptimal local minimum. Moreover, this introduces the inconvenience of an extra stage of training. Instead, we propose an alternating optimization scheme to train the text-VAE with conditional sampler. In one alternating step, we freeze the conditional sampler and train the text-VAE and depth decoder following the procedure in Sec. 3.1  ###reference_###, i.e., predicting  and  from text caption t and using the reparameterization trick with an  drawn from a standard Gaussian to sample the latent vector. In the next alternating step, we freeze text-VAE and train the conditional sampler with the depth decoder following Sec. 3.2  ###reference_###, i.e., predicting  and  using the frozen text-VAE and sample from the latent distribution using  predicted from the image. These alternating steps are repeated with a ratio of  (for optimizing text-VAE) to  (for optimizing the conditional sampler).\nInference. Once trained, we no longer require drawing  from a standard Gaussian. Instead, at test time, the inference step simply follows Sec. 3.2  ###reference_###. In another mode, if one wants to generate depth maps from text captions, one can discard the conditional sampler branch and directly sample from a standard Gaussian instead."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "Datasets.\nWe evaluate our method on indoor (NYU Depth V2 [58]) and outdoor (KITTI [20]) scenarios. NYU Depth V2 contains 480640 images with depth values from to 10 meters. We follow [34] for the dataset partition, which contains 24,231 train images and 654 test images. KITTI contains 3521216 images where depth values from to 80 meters. We adopt the Eigen Split [13] consisting of 23,488 training images and 697 testing images. Following [86, 2], after cleaning out samples without valid ground truth, we have 652 valid images for testing.\n\nNetwork Architecture. We use the ResNet-50 [24] version of CLIP [53] text encoder to extract text features. We use ExpansionNet-v2 [25] for captioning images for efficiency. We set the dimension of the latent space of the text-VAE and image-based conditional sampler to be 128. As for the image-based conditional sampler, we use a Swin-L Transformer backbone [45] pre-trained on ImageNet [9]. For the text-VAE, given CLIP features of size 1024, we use a 3-layer MLP with hidden dimensions of 512, 256, and 128 to encode text features. For the depth decoder, there are 3 convolutional up-sampling and refinement layers. For depth prediction, we attach 3 skip connections from the conditional sampler to the depth decoder between corresponding layers. When optimizing for text-VAE by our alternating optimization scheme (Sec. 3.4), we sample from a standard Gaussian; as an implementation detail, all values passed from the skip connections are set to be zero.\n\nHyperparameters. We use the Adam [28] optimizer without weight decay. The learning rate is reduced from to by a cosine learning rate scheduler. The model is trained for 50 epochs for both KITTI and NYU Depth V2 under this scheduler. for scale-invariant loss is set to 0.85, and the weights and for KL-Divergence are set to. We set the probability to optimizing text-VAE branch to 1%. Data augmentation includes random gamma within, random brightness within for NYU Depth V2 [58] and for KITTI [20], random color intensity within for each channel, random horizontal flipping with 50% probability, and random rotations within degrees.\n\nEvaluation metrics.\nFollowing [43, 7], we evaluate WorDepth and baseline methods quantitatively using mean absolute relative error (Abs Rel), root mean square error (RMSE), absolute error in log space, logarithmic root mean square error () and threshold accuracy. The evaluation metrics are summarized in the Supp. Mat. For qualitative results and comparisons, see Fig. 3 and 4, where the error map shows the absolute relative error.\n\nQuantitative results.\nWe show results on NYU Depth V2 in Tab. 1, where we improve over the baseline and existing works across all evaluation metrics. We want to highlight that WorDepth significantly excels in terms of the threshold accuracy, which measures the proportion of predictions deviating from the ground truth within a specific range. We note that while existing methods often produce high fidelity shapes (i.e., ordinal relationships of points) in depth maps, the scale tends to be off \u2013 leading to lower. Our gain in the metric indicates that a greater proportion of depth estimations align closely with the ground truth, thanks to better scale estimated based on objects that populate the scene, thereby yielding depth values in ranges closer to that of ground truth. Tab. 2 shows the results on the KITTI dataset, using the Eigen Split [13] partition. WorDepth also achieves state-of-the-art performance. Like NYU Depth V2, WorDepth improves the threshold accuracy, however, the relative performance gain on this metric is not as pronounced as on NYU Depth V2. This difference can be due to the wider range of object sizes and shapes that may populate an outdoor scene that are attributed to the same equivalence class of an object. For example, the term \u201ccar\u201d may refer to a sedan, a coupe, or a hatchback \u2013 all exhibit different sizes (coupes are smaller than sedans) and shapes (hatchbacks have an elevated and connect trunk). While text captions give flexibility between generality and specificity as a prior, in cases where captions tend to be vague, the explicit reliance (by modeling as a conditional prior) on them may backfire, leading to incorrect shapes and sizes. Nonetheless, conditioning on the image resolve such cases to a degree and usage of the prior leads to more benefits than harm.\n\nQualitative comparisons.\nWe show representative visual examples comparing WorDepth with a baseline method on the NYU Depth V2 dataset in Fig. 3, to highlight the benefit of the language prior. From the error map where brighter regions indicate larger errors,"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "In this study, we seek to answer the question of whether language can be used to calibrate the learned scene distribution to true real-world statistics. The answer is yes, which is valuable for\ncircumventing the long-standing problem of scale ambiguity in monocular depth or structure-from-motion problems. The approach is a first in leveraging complementary properties of two modalities with inherent ambiguities for the 3D reconstruction, to address the deficits in one another. We show that by exploiting the layout/scene ambiguity in language as a strength via our variational approach, we can ground predictions to metric scale. This opens up new avenue in how one can address the issue of scale in 3D reconstruction as well as provide a direct framework to extending the many works that currently are limited to relative or scaleless depth predictions.\nMethod\n\n\n\nAbsRel\n\nRMSE\n\n\n\nAdabins\n0.771\n0.944\n0.983\n0.159\n0.068\n0.476\n\nDepthFormer\n0.815\n0.970\n0.993\n0.137\n0.059\n0.408\n\nBaseline\n0.803\n0.965\n0.990\n0.141\n0.062\n0.427\n\nWorDepth\n0.833\n0.976\n0.994\n0.123\n0.054\n0.376\nLimitations. Generic regularizers typically yield little gains, but do little harm; specific regularizers can provide larger boosts but are limited in their applications. While using language as a prior gives flexibility between the two, specificity in the caption controls the degree of regularization imposed. Naturally, vague captions give little to no information on object shape or size, so there is little to be gained; specific, but incorrect captions may misfire, barring any malicious intent. As WorDepth relies on the quality of the caption, it is susceptible to inaccuracies stemming from descriptions provided by the image captioner. Its ease of use also opens up vulnerabilities from malicious users who may choose captions to steer predictions incorrectly.\nAcknowledgements. This work was supported by NSF 2112562 Athena AI Institute."
        }
    ],
    "url": "http://arxiv.org/html/2404.03635v4",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4"
        ]
    },
    "research_context": {
        "paper_id": "2404.03635v4",
        "paper_title": "WorDepth: Variational Language Prior for Monocular Depth Estimation",
        "research_background": "### Paper's Motivation:\nThe primary motivation for this paper arises from the inherent ambiguity and ill-posed nature of the monocular depth estimation problem, which involves predicting 3D scene structures from a single 2D image. Given that numerous 3D scenes can map to the same 2D image, the estimation process is fraught with uncertainty, especially regarding the scale of the reconstruction. This challenge is compounded by the scarcity of large-scale depth datasets, making it difficult to accurately train models solely based on visual data. The authors thus pose a crucial question: Can additional priors, beyond the training set, be harnessed to better calibrate the learned scene distribution to reflect real-world statistics?\n\n### Research Problem:\nTo address this challenge, the paper aims to explore whether two inherently ambiguous modalities\u2014camera images and text descriptions\u2014can be combined to enhance monocular depth estimation. While images provide detailed layouts and object shapes, text descriptions offer strong priors about the scale and coarse shapes of the scene. The central problem is to determine if these two modalities can be exploited for their complementary strengths, resulting in metric-scaled reconstructions captured as 2.5D depth maps. Specifically, the research investigates the feasibility of using text descriptions to guide and improve the depth maps derived from single images, leveraging a variational framework to do so.\n\n### Relevant Prior Work:\nThe paper builds on various strands of prior research:\n1. **Generic Priors in Depth Estimation:** Prior approaches have incorporated generic priors such as local smoothness and connectivity ([19, 22, 102, 67]), or object orientation ([15]) into the training objectives as regularizers, and have used specific inductive biases realized as architectural designs ([65]) or collections of object shapes ([14]). These methods, while useful, often lack specificity.\n  \n2. **Language and Semantics in Machine Learning:** The idea of using language for insights into scenes and object shapes, as well as functionality, is acknowledged in works ([31, 32, 4]). This research extends these insights by using text explicitly as a prior in depth estimation.\n\n3. **Variational Autoencoders (VAEs):** The paper's approach leverages the VAE framework, encoding the caption as a mean and standard deviation of the plausible scene layout distribution. This technique draws from established practices in VAEs, using the reparameterization trick to sample and decode into depth maps.\n\n4. **Monocular Depth Estimation Datasets and Benchmarks:** The study evaluates its approach on established benchmarks like NYU Depth V2 ([58]) and KITTI ([20]), drawing on their conventional use as standards for assessing depth estimation performance.\n\nThis framework not only complements the existing body of work by introducing language as a variational prior but also sets a new state-of-the-art in indoor and outdoor benchmarks, thereby underscoring the potential of integrating textual context with visual data in solving monocular depth estimation.",
        "methodology": "**WorDepth: Variational Language Prior for Monocular Depth Estimation**\n\n**Methodology:**\n\nIn the context of monocular depth estimation, the goal is to derive a detailed depth map \\( D \\) from a single RGB image \\( I \\). This is achieved using a parameterized function \\( f \\), implemented as a neural network such that \\( D = f(I) \\). \n\nThe method is applied to a supervised dataset \\( \\mathcal{D} \\), which consists of \\( N \\) samples of the form \\( \\{I_i, D_i, t_i\\}_{i=1}^N \\). Here:\n- \\( I_i \\) denotes the RGB image,\n- \\( D_i \\) represents the ground-truth depth map,\n- \\( t_i \\) is the text caption describing the image \\( I_i \\).",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Datasets**\n---\nWe evaluate our method on two prominent datasets:\n1. **NYU Depth V2 [58]**: An indoor dataset with 480 x 640 resolution images and depth values ranging from 0 to 10 meters. The dataset partition follows [34], with 24,231 training images and 654 testing images.\n2. **KITTI [20]**: An outdoor dataset with 352 x 1216 resolution images and depth values ranging from 0 to 80 meters. We adopt the Eigen Split [13], consisting of 23,488 training images and 697 testing images, with 652 valid test images after removing samples without valid ground truth following [86, 2].\n\n**Baselines**\n---\nWe compared WorDepth against several methods:\n- **DepthCLIP [91]** with CLIP (zero-shot)\n- **CLIPMDE [1]** with CLIP\n- **GeoNet [52]** with ResNet-50\n- **DORN [16]** with ResNet-101\n- **Yin et al. [80]** with ResNeXt-101\n- **TransDepth [78]** with ViT-B\n- **ASN [46]** with HRNet-48\n- **Big to Small [35]** with DenseNet-161\n- **DPT-Hybrid [54]** with ViT-B\n- **ASTransformer [7]** with ViT-B\n- **AdaBins [2]** with EffNet-B5 + ViT-mini\n- **NeWCRFs [86]** with Swin-L\n- **Yu et al. [84]** with Swin-L\n- **DepthFormer [40]** with Swin-L\n\n**Evaluation Metrics**\n---\nWe used the following metrics for evaluation:\n- Mean Absolute Relative Error (Abs Rel)\n- Root Mean Square Error (RMSE)\n- Logarithmic Root Mean Square Error (log RMSE)\n- Threshold accuracy at \u03b4 = 1.25, 1.252, and 1.253\n\n**Results**\n---\n### NYU Depth V2\n\n<table>\n  <tr>\n    <th>Method</th>\n    <th>Backbone</th>\n    <th>\u03b4<sub>1</sub></th>\n    <th>\u03b4<sub>2</sub></th>\n    <th>\u03b4<sub>3</sub></th>\n    <th>Abs Rel</th>\n    <th>log<sub>10</sub></th>\n    <th>RMSE</th>\n  </tr>\n  <tr>\n    <td>Baseline</td> <td>Swin-L</td> <td>0.910</td> <td>0.990</td> <td>0.998</td> <td>0.098</td> <td>0.043</td> <td>0.351</td>\n  </tr>\n  <tr>\n    <td>WorDepth</td> <td>Swin-L</td> <td>**0.932**</td> <td>**0.992**</td> <td>**0.998**</td> <td>**0.088**</td> <td>**0.038**</td> <td>**0.317**</td>\n  </tr>\n</table>\nWorDepth shows significant improvements in all metrics, particularly in threshold accuracy (\u03b4), highlighting better alignment of depth estimations with ground truth.\n\n### KITTI\n\n<table>\n  <tr>\n    <th>Method</th>\n    <th>Backbone</th>\n    <th>\u03b4<sub>1</sub></th>\n    <th>\u03b4<sub>2</sub></th>\n    <th>\u03b4<sub>3</sub></th>\n    <th>Abs Rel</th>\n    <th>log<sub>10</sub></th>\n    <th>RMSE</th>\n  </tr>\n  <tr>\n    <td>Baseline</td> <td>Swin-L</td> <td>0.969</td> <td>0.996</td> <td>0.999</td> <td>0.054</td> <td>0.085</td> <td>2.343</td>\n  </tr>\n  <tr>\n    <td>WorDepth</td> <td>Swin-L</td> <td>**0.979**</td> <td>**0.998**</td> <td>**0.999**</td> <td>**0.049**</td> <td>**0.074**</td> <td>**2.039**</td>\n "
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate how different ratios of alternating optimization steps between text-VAE and conditional sampler affect the performance of WorDepth in monocular depth estimation.",
            "experiment_process": "Optimized WorDepth with different alternation ratios ranging from 0% to 100%, by alternating optimization steps between the text-VAE and conditional sampler. Setup includes varying the percentage of updates on the text-VAE branch while maintaining a fixed training length. The performance was evaluated using the standard depth estimation metrics: mean absolute relative error (AbsRel), root mean square error (RMSE), logarithmic root mean square error (logRMSE), and threshold accuracy (\u03b4) on NYU Depth V2 and KITTI datasets.",
            "result_discussion": "It was found that optimizing text-VAE at a lower ratio (e.g., 1%) resulted in the best empirical performance. Higher ratios exceeding 10% reduced performance due to fewer updates to the sampler and increased training time. A 100% ratio\u2014a configuration in which the model does not condition on the image\u2014still yielded reasonable results but without leveraging the full benefits of image conditioning. Conversely, without the variational prior (0%), performance degraded, highlighting the necessity of alternating optimization.",
            "ablation_id": "2404.03635v4.No1"
        },
        {
            "research_objective": "Evaluate the zero-shot generalization ability of the model by leveraging language priors for improved depth estimation across diverse data domains.",
            "experiment_process": "Trained the WorDepth model on the NYU Depth V2 dataset and tested it on the Sun-RGBD dataset without fine-tuning. The goal was to assess transferability and robustness by comparing performance metrics such as mean absolute relative error (AbsRel), root mean square error (RMSE), and threshold accuracy (\u03b4) against other baseline methods. The experimental setup did not involve any additional training or domain adaptation for the Sun-RGBD test set.",
            "result_discussion": "WorDepth outperformed baseline methods by a substantial margin, demonstrating superior zero-shot generalization capability. This result indicates that leveraging language priors helps handle scene variability better than direct visual signals, suggesting enhanced robustness and transferability across different datasets.",
            "ablation_id": "2404.03635v4.No2"
        }
    ]
}