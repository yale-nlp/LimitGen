{
    "title": "OVEL: Large Language Model as Memory Manager for Online Video Entity Linking",
    "abstract": "In recent years, multi-modal entity linking (MEL) has garnered increasing attention in the research community due to its significance in numerous multi-modal applications. Video, as a popular means of information transmission, has become prevalent in people\u2019s daily lives. However, most existing MEL methods primarily focus on linking textual and visual mentions or offline videos\u2019 mentions to entities in multi-modal knowledge bases, with limited efforts devoted to linking mentions within online video content. In this paper, we propose a task called Online Video Entity Linking (OVEL), aiming to establish connections between mentions in online videos and a knowledge base with high accuracy and timeliness. To facilitate the research works of OVEL, we specifically concentrate on live delivery scenarios and construct a live delivery entity linking dataset called LIVE. Besides, we propose an evaluation metric that considers timeliness, robustness, and accuracy. Furthermore, to effectively handle the OVEL task, we leverage a memory block managed by a Large Language Model and retrieve entity candidates from the knowledge base to augment LLM performance on memory management. The experimental results prove the effectiveness and efficiency of our method.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Videos, showcased by platforms like TikTok and YouTube, have become a dominant medium for communication. As their significance grows, so does the breadth of academic research into understanding them. Beyond the well-studied areas of video retrieval and captioning, scholars are exploring aspects like pre-training, cross-modal fusion, and more, striving for a comprehensive grasp of video content. However, these existing studies mainly concentrate on understanding the holistic content of videos and often overlook the significance of specific entities within them. \n\nConsider a live streaming example: where a video captioning model might merely state \u201ca host explaining a product\u201d, however, for viewers, specific details like \u201cNike Air Jordan 37th Generation Mid-Top Basketball Shoes\u201d might be the critical information they seek. Therefore, in such scenarios, discerning specific entities can be more vital than a broad overview of the video content. Video entity linking refers to linking mentions that appear in a video to their corresponding entities in a knowledge base. Related research on this task is still relatively limited.\n\nThere have been some studies that have conducted video entity linking, but with certain limitations. On the one hand, they link to coarse-grained entities like \u201cbird\u201d or \u201chuman\u201d, which becomes overly simplistic due to the broad granularity. On the other hand, they don\u2019t demand real-time processing. With the rise of network terminals, there is an increasing demand for improved online performance in certain scenarios. For instance, in online sports live broadcasts, if specific athletes can be identified, comments and even real-time explanations can be generated based on the career of the athletes. These scenarios put forward higher requirements for online video entity linking.\n\nIn this paper, we propose the task of Online Video Entity Linking (OVEL) on dynamic video streams. The objective of this task is to link important entities appearing in online videos to a corresponding knowledge base. Furthermore, to advance the research on OVEL, we construct a dataset for LIVE stream product recognition based on live streaming scenarios called LIVE, which includes 82 live streams and nearly 250 hours of video. Based on the LIVE dataset, to better evaluate the accuracy and efficiency of entity linking on video streams, we introduce a time-weighted decay metric named RoFA, which comprehensively considers the accuracy and robustness of model predictions while also imposing requirements on online performance.\n\nConsidering the OVEL task and LIVE dataset, as shown in figure 1, we analyze the OVEL task, which poses several key challenges:\n\n1. Much Noise. Real-time scenarios often exhibit a multitude of visual scenes and various sounds, which can introduce interference in entity recognition. For instance, in live-streaming e-commerce scenarios, hosts tend to use a significant number of interjections, engage in interactions with viewers, or interact with other hosts. These can cause substantial interference in the recognition of entities.\n\n2. Timeliness. In online scenarios, which are characterized by strict time constraints, the prompt identification of salient entities and their timely recommendation to potential users often results in enhanced economic benefits. The expeditious identification of significant entities entails a challenging prerequisite for timeliness.\n\n3. Domain knowledge. Recognizing certain products requires a certain level of domain knowledge, and individuals unfamiliar with the domain may struggle to make accurate identifications. For instance, it might be challenging for some people to distinguish the specific generation and specific superstar\u2019s basketball shoes.\n\nConsidering these challenges of the OVEL task, we propose several methodologies to address these challenges. Firstly, to address the issue of high noise levels in online scenarios, we propose adopting a LLM-based information extraction approach, aiming to extract information from videos that are more relevant to the entities. Secondly, to address the issue of timeliness, we utilize a memory block to store information before the current inference moment. For the subsequent moment, only the information within the time interval and the memory block before this moment need to be inputted, ensuring real-time performance. And we delegate the management of the memory block to the LLM. Furthermore, to tackle the domain-specific nature of live recognition, we propose utilizing a model retrieval to provide examples to LLM, enabling the LLM to possess a broader background knowledge.\n\nIn summary, the main contributions of this paper are as follows:\n- To the best of our knowledge, we introduce the task of online entity linking (OVEL) for the first time, focusing on improving the accuracy and efficiency of entity recognition in online videos.\n- Building upon live streaming scenarios, we have created a dataset for live stream product recognition, comprising 82 live stream videos, approximately 250 hours of video, and nearly 3,000 data instants. And a corresponding metric named RoFA.\n- To better address the task of OVEL, we propose a framework for the comprehensive management of video stream information based on LLM as a memory manager. Additionally, we leverage retrieval for LLM to manage memory better and employ a two-stage approach for entity linking. Subsequent experiments validate the effectiveness of our framework."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Multi-modal Entity Linking",
            "text": "Multimodal Entity Linking (MEL) is an extension of entity linking that links mention in multi-modal information (e.g., images, audio, or videos) to a corresponding knowledge base. Existing research primarily focuses on static image-text pairs. ResearchersAdjali et al. (2020a  ###reference_b1###, b  ###reference_b2###); Zhou et al. (2021  ###reference_b42###); Wang et al. (2022b  ###reference_b34###, a  ###reference_b33###); Gan et al. (2021  ###reference_b11###); Sun et al. (2022  ###reference_b30###); Chengmei et al. (2023  ###reference_b7###); Xing et al. (2023  ###reference_b36###); Shi et al. (2023  ###reference_b29###); Yao et al. (2023  ###reference_b39###); Zhang et al. (2021  ###reference_b40###) constructed multiple datasets for different scenarios or proposed various multimodal representation methods, integrating features from different modalities to facilitate entity mention and entity matching.\nThese studies primarily focus on static textual and graph data and have not been extended to the domain of videos. In the realm of entity linking in videos, Li et al. (2015  ###reference_b16###) introduced a dataset for entity linking in videos and linked prominent entities from the videos to the knowledge base. For example, they linked highlights of Kobe Bryant\u2019s career to the entity \u201cKobe Bryant\u201d. Venkitasubramanian et al. (2017  ###reference_b31###) established a dataset for documentary video linking, utilizing video descriptions and content recognition to identify corresponding animals such as lions, birds, and others. These methods have two limitations. Firstly, the granularity of entities in videos is often too coarse, lacking fine-grained entity identification. Secondly, they primarily focus on pre-stored videos, linking them to the knowledge base with the whole video information, without considering real-time entity linking for online video streams."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "LLM as Memory Controller",
            "text": "With the development of large language models (LLMs) Devlin et al. (2019  ###reference_b8###); Radford et al. (2018  ###reference_b27###, 2019  ###reference_b28###); Brown et al. (2020  ###reference_b5###), LLMs that have been pre-trained on massive corpora have demonstrated remarkable capabilitiesOuyang et al. (2022  ###reference_b25###); Wei et al. (2022  ###reference_b35###). With the advent of powerful generative models such as GPT-4OpenAI (2023  ###reference_b24###), these models have demonstrated exceptional capabilities in generation, conversation, and the comprehension of human instructions, finding applications across a variety of downstream tasks. Recently, numerous researchers have integrated Memory with Large Language Models (LLMs), proposing frameworks to address resource constraints such as input length limitations inherent in LLMs. Liang et al. (2023  ###reference_b17###) proposed the utilization of memory to enhance the ability of LLMs to handle long texts, while Zhong et al. (2023  ###reference_b41###) introduced a customized memory mechanism specifically designed for LLMs. These frameworks offer potential for downstream applications of LLM-based agents. In this paper, we employ the LLM for memory block management and entity linking."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Retrieval Augment Generation",
            "text": "Despite the impressive capabilities demonstrated by models trained on large-scale corpora, they still suffer from phenomena such as hallucinations, long-tail problems, and knowledge decay. Retrieval augmentation, as a form of external corpora and knowledge enhancement, can alleviate these limitations of large models. In recent years, retrieval augmentationLewis et al. (2021  ###reference_b15###); Guu et al. (2020  ###reference_b12###); Lin et al. (2023  ###reference_b18###) has been employed in various stages of model training, fine-tuning, and inference, leading to improved performance of models on downstream tasks.\nIzacard et al. (2022  ###reference_b14###) utilized knowledge retrieval as a few-shot paradigm to enhance the performance of large language models in tasks such as knowledge question answering and fact-checking. Vu et al. (2023  ###reference_b32###) leveraged search engine retrieval to augment large language models, mitigating the issue of factual inaccuracies resulting from outdated knowledge. Asai et al. (2023  ###reference_b3###) improved the quality and factuality of model-generated outputs through retrieval and reflection.\nIn this paper, we utilize retrieval augmentation to alleviate the issue of insufficient knowledge using LLM in domain-specific scenarios."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Benchmark Construction and Evaluation",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Problem Formulation",
            "text": "Online Video Entity Linking (OVEL) is a task designed for live video data streams. The goal of this task is to accurately identify salient entities in a live video stream, like the products highlighted by the anchor in the live broadcast scene. Given a live video, for instance, within the first 3 seconds, the host first mentions a specific pair of Nike shoes, followed by another 3 seconds of detailed introduction of it, then 3 seconds of answering questions from the live audience, and another 3 seconds of introduction to Nike shoes, and followed by a 3 seconds of Adidas\u2019s competitive shoes. The prominent entities in these video streams should be Nike shoes, OVEL should predict the Nike shoes for each 3 seconds input accuracy and robustness. This uneven distribution of information poses significant challenges to the OVEL task.\n\nThe input of OVEL should be a sequence of frames that accumulate with time. Given a live video consisting of a list of video clips, where each element represents a clip of the video, a predefined knowledge base is available, where each entity in the knowledge base has corresponding multimodal information. Below is the formal formulation of OVEL at a timestamp:\n\nAn entity should be predicted at each timestamp with the video information before that timestamp. Hence, a list of entities will be predicted in the video. Each entity in the prediction list should be as similar as the ground truth. This places significant challenges on the robustness and accuracy of the algorithm."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Dataset Construction and Analysis",
            "text": "To advance the research on OVEL task, we have built an e-commerce video stream entity linking dataset based on live streaming scenarios. The construction of the dataset consists of three main steps. Firstly, the initial raw videos and their corresponding multimodal knowledge base are obtained. The second step involves segmenting the corresponding live videos into data instances and manually annotating the entities in the knowledge base. The third step entails simulating online input by dividing each data instance into a list of video clips based on their playback time. The details of dataset construction and dataset Analysis can be found in Appendix A."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Evaluation For OVEL",
            "text": "Evaluating the OVEL task is not inherently straightforward and presents certain challenges. In the domain of live streaming, early identification of entities is increasingly effective for recommendation algorithms, potentially leading to greater economic benefits. The simplest approach involves assigning higher scores to instances where the correct location of real entities is identified earlier in the video. However, there is a possibility of correct recognition in the first minute but misidentification after one and a half minutes, which puts forward requirements for the robustness of the algorithm. Based on these characteristics, we propose a comprehensive metric that considers accuracy, online performance, and robustness, referred to as Robust online Fast Accuracy (RoFA). Below is the formulation of RoFA:\n\nGiven a list of prediction results in the temporal sequence, where the scores for predictions made later should be lower, we have devised a weighted decay mechanism that is proportional to the size of the prediction results. We initialize a linearly decreasing weight. For example, the weight of the first prediction is set to 1, and the weight of the last prediction is set to 0.2. The weights between these two windows decrease linearly, which aims to evaluate the fast and robust performance of algorithms. As we only recommend the best matching product to users, when considering the prediction result for each video clip, if the prediction is correct, the score should be 1. Meanwhile, if the prediction is incorrect, the score is 0. The final metric is calculated as the sum of scores divided by the sum of weights, representing the average score. The calculation method of RoFA is as follows: \n\nwhile the score is calculated as below, with the actual entity denoted by the ground truth of the video and the predicted entity."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "In this section, we will first present the overall framework of the methodology, followed by an introduction to the summary modules that constitute the methodology and an overview of the main components of the LLM as the memory controller. Finally, we will introduce the two-stage entity linking methods."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Overview of the Framework",
            "text": "###figure_2### Figure 2  ###reference_### illustrates the entire workflow of our Framework. When the input is an online video, we initialize the initial memory block using the summary module. Then we leverage the memory block and image information from video clips to perform the initial retrieval of candidate products. At each time t, the LLM manager gets the current video information, accesses the content within the memory, and refers to the results obtained from the retrieval model to make decisions and update the memory from the previous time step. To better use LLM\u2019s capacity, we also employed a two-stage entity linking method. First is the retrieval model to retrieve the candidate entities, and give candidates to LLM for fine-grained entity disambiguation. Below we will provide a detailed description of each module."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Summary Module",
            "text": "For a given input of video clips, while  represents the video clip at time , transcribed speech text , and keyframe sequences  over time. The task of OVEL is to predict ground truth entities at every moment as accurately as possible.\n\nAs a task of multimodal entity linking, the fundamental model should be a multimodal retrieval model. The multimodal retrieval model aims to maximize the similarity between real-time videos and their corresponding entities while minimizing the similarity between non-matching entities. This can be represented by the following equation:\n\nIn the equation,  denotes the similarity calculation, while  represents the encoder component for both the video and the entities in the knowledge base. The video contains two multi-modal information: speech text and images.  contains all the information before time .\n\nHowever, in the context of live streaming, real-time videos present dynamic and evolving information, accompanied by a substantial amount of irrelevant noise, such as the host\u2019s habit of introducing \u201call girls\u201d and engaging with the audience. To address this issue, we first propose an approach that leverages a LLM for extracting textual content from speech. Equation 7  ###reference_### is replaced with the following formulation:\n\nWe utilize speech text following summaries for multimodal retrieval, which forms the summary module of our proposed method."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Memory Controller Module",
            "text": "However, online entity linking poses a challenge in terms of responsiveness. As the video progresses in time, we encounter a more important challenge. The length of the textual content extracted from speech increases over time, resulting in longer summaries. At this point, using the summary module cannot meet the real-time requirements. To address this issue, we propose utilizing a memory block to store past extracted information.\n\nThe memory block module is designed to record entity-related attributes from previous video clips. When processing new video segments, only the current memory information needs to be updated, thereby avoiding linear growth in the number of tokens required for inference per clip. From the equation, it can be observed that at each time step, only the memory from the previous time step and the textual information of the current clip are required as inputs.\n\nHowever, in the live-streaming scenario, there are limitations. The granularity of products in live streaming is relatively fine, requiring domain-specific knowledge. Additionally, there is a significant amount of irrelevant information present in the videos. If we solely rely on an LLM trained in a general domain to manage the memory block, there is a risk of extracting a large amount of irrelevant information. To ensure that the memory block is primarily filled with information related to the products, we combine it with the retrieval model. The products obtained through multimodal retrieval are simultaneously considered by the LLM, which acts as guidance for better memory block management.\n\nFrom the equation, it can be observed that at each time step, the inputs consist of the memory from the previous time step, the textual information of the current slice, and the retrieval results from the retrieval model. This not only fulfills the requirements of real-time inference but also alleviates the issue of insufficient domain-specific knowledge in LLM."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Two-stage Entity Linking",
            "text": "The LLM demonstrates remarkable capability, which we desire to use for entity linking. However, in real-time scenarios, it is challenging to provide all the candidate entities to the LLM due to its limited context length, and fine-grained non-deterministic generation is also difficult. Drawing from previous approaches, we divide the linking process into two steps: the first step involves the retrieval model to get entity candidates, and the second step involves the entity disambiguation made by the powerful LLM.\n\nThis approach not only leverages the powerful background knowledge of LLM but also reduces the time-consuming inference capacity. Above is the comprehensive presentation of our proposed framework. The following experiments show that our method ensures real-time performance while effectively enhancing overall performance."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we primarily focus on analyzing the experimental results conducted on the constructed dataset. Firstly, we present the main experimental results of the overall framework. Secondly, we examine the comparative analysis of different approaches in terms of temporal performance. Lastly, we compare the performance of traditional metrics on static videos. Furthermore, several intriguing phenomena emerged during our experimental process, which can be observed in Appendix B."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Experiments setting",
            "text": "Model selection. We employed Qwen-14B-ChatBai et al. (2023  ###reference_b4###) as the LLM. We also utilized powerful models such as ChatGPT. However, due to a lack of training data in the e-commerce domain, these models performed poorly on the task. We also attempted to translate the Chinese memory block and product database into English and used English text-image retrieval, however, due to the transcription errors in the text converted from voice in the live streaming scenario, including brand names, it is challenging to achieve fine-grained product recognition. Our static experimental results also demonstrate the underperformance of other models on the dataset.\n\nImplement Details. For the method proposed in this article is designed for online performance analysis, all experiments are performed on the same machine. Our local machine has four 3090 GPUs. To facilitate better inference, we deployed Qwen-14B-Chat on an A100 80G machine and used API calls to manage memory blocks through the LLM controller. Due to limitations in local inference memory, we randomly sampled a product database approximately 10 times larger than the test set from the knowledge base. We fixed this subset of 3,000 products as the candidate pool, and the test set consisted of 275 video samples. We assume that the model begins generating outputs after processing 10 video clips, indicating that the model starts linking from the 10th video clip. To better utilize the sequential information in memory, except for the Base method, all other approaches perform inference once every 5 video clip sizes. The inference results are then replicated for all five video clips. All methods are finetuned on the training set."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Main result",
            "text": "In this section, we added our framework to two multi-modal retrieval models. To compare the effectiveness of different modules, we denote the model that directly employs multimodal retrieval as \u201cBase\u201d, and our proposed LLM as memory controller as \u201cOurs\u201d. From the table, it can be observed that the approach combining retrieval model retrieval with LLM achieved the highest performance. Particularly, our method combining the CN-CLIP_L model achieved the best results, likely due to CN-CLIP_L\u2019s superior performance on the test sets compared to the other retrieval models. In most cases, using a single memory management approach yields slightly inferior results compared to using full summaries, as the structure of memory management lacks complete information, leading to some information loss. The results also demonstrate that our method provides substantial improvements when the retrieval model performs poorly. This suggests that in low-resource scenarios where the retrieval model lacks training data, leveraging the combination of the LLM can serve as a viable solution."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Online performance analysis",
            "text": "In this section, we analyze the online performance of different methods. We evaluated the time performance of various methods on the test set. We assessed the time taken to give a predicted entity and recorded the inference time on the test set at intervals of every five video clips. After calculating the average time for each method, the smoothed results are presented in Figure 3.\n\nFrom Figure 3, it can be observed that \u201cBase\u201d utilizes the retrieval model and yields the best time performance. The time cost of the Ours-M and Our Ours-R significantly increases as the number of video clips grows. It needs to be mentioned that when the window size exceeds 200, these methods surpass the recommended inference time, thereby potentially failing to provide meaningful linked entities within the given time interval. On the opposite, Our method initially exhibits a rapid increase in time cost, followed by a tendency toward stability.\n\nAnalyzing the reasons behind this situation: as the number of video clips increases, the length of the memory block also increases, resulting in an information increase in all methods. In the later stages of inference, due to its domain knowledge from the retrieval model, Ours method tends to have content that is more related to specific products and remains fixed. On the other hand, the Ours-R may continue to accumulate irrelevant information as it lacks related knowledge. And Ours-M method exhibits some instability due to variations in the length of text in different video clips, the reason may be the lack of a complete memory, the extracted information may be inconsistent in format, and there may be insufficient or redundant."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Static experiment results",
            "text": "To compare with some of the existing methods for Multimodal Entity Linking (MEL) and Video Entity Linking (VEL), we treat each video as an individual data instant and perform video entity linking across the entirety of the video\u2019s content to ascertain the applicability of our method to static data as well. In this chapter, we have selected a variety of representative approaches for evaluation. These include CLIP4ClipLuo et al. (2022) in the domain of video retrieval, a purely textual entity linking approach BLINKLogeswaran et al. (2019), and the multimodal entity linking method V2VTELSun et al. (2022). The experimental metrics primarily utilized are Recall and Mean Reciprocal Rank (MRR) at K. The experimental outcomes are as exhibited in Table 2.\n\nFrom Table 2, it can be observed that our method achieves the best performance, once again demonstrating the effectiveness of our approach. Furthermore, the performance of the CLIP4clip and V2TVEL approaches compared to pure text-based BLINK is poor, indicating that text plays a more significant role in our scenario."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we propose an Online Video Entity Linking (OVEL) task for online videos, construct the LIVE dataset based on live streaming scenarios, and introduce the RoFA metric, which considers robustness, timeliness, and accuracy. Based on the dataset, we present a method that combines LLM with a retrieval model for memory management, which handles the OVEL task efficiently. Experimental results demonstrate the effectiveness of our approach. However, OVEL is a highly challenging task, and we earnestly invite researchers to join us in the field of online video entity linking."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Dataset Construction and Analysis",
            "text": "###figure_4###"
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Memory Block Analysis",
            "text": "Memory is a very important module proposed in this paper, and its format and management form are also particularly important. In the next two sections, we will discuss the impact of memory format and memory management on experimental results."
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T1\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T1.4\" style=\"width:168.0pt;height:81pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-9.3pt,4.5pt) scale(0.9,0.9) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T1.4.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T1.4.4.5.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T1.4.4.5.1.1\">method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T1.4.4.5.1.2\">AltCLIP</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T1.4.4.5.1.3\">CN-CLIP_B</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T1.4.4.5.1.4\">CN-CLIP_L</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T1.1.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T1.1.1.1.2\">2.32</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T1.1.1.1.3\">23.16</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T1.1.1.1.4\">36.68</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T1.2.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T1.2.2.2.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.2.2.2.2\">4.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.2.2.2.3\">42.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.2.2.2.4\">56.60</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.3.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T1.3.3.3.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.3.3.3.2\">4.85</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.3.3.3.3\">35.30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.3.3.3.4\">47.02</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.4.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"S5.T1.4.4.4.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T1.4.4.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.4.4.4.2.1\">13.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T1.4.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.4.4.4.3.1\">48.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T1.4.4.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.4.4.4.4.1\">60.20</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>RoFA results of proposed methods.While  represents the removal of the retrieval module, while  represents the removal of the memory block module.</figcaption>\n</figure>",
            "capture": "Table 1: RoFA results of proposed methods.While  represents the removal of the retrieval module, while  represents the removal of the memory block module."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T2\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T2.1\" style=\"width:147.6pt;height:113.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-8.2pt,6.3pt) scale(0.9,0.9) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T2.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T2.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.1.1.1\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T2.1.1.1.1.2\">R@1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T2.1.1.1.1.3\">R@5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T2.1.1.1.1.4\">MRR@3</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T2.1.1.1.1.5\">MRR@5</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T2.1.1.2.1.1\">CLIP4clip</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.2.1.2\">1.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.2.1.3\">8.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.2.1.4\">2.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.2.1.5\">3.10</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.3.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T2.1.1.3.2.1\">AltCLIP</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.3.2.2\">8.95</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.3.2.3\">20.62</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.3.2.4\">12.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.3.2.5\">13.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.4.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T2.1.1.4.3.1\">V2VTEL</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.4.3.2\">9.09</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.4.3.3\">24.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.4.3.4\">13.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.4.3.5\">14.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.5.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T2.1.1.5.4.1\">BLINK</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.5.4.2\">42.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.5.4.3\">72.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.5.4.4\">53.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.5.4.5\">54.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.6.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T2.1.1.6.5.1\">CN-CLIP</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.6.5.2\">55.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.6.5.3\">75.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.6.5.4\">62.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.6.5.5\">63.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.7.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\" id=\"S5.T2.1.1.7.6.1\">Ours</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S5.T2.1.1.7.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.7.6.2.1\">57.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S5.T2.1.1.7.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.7.6.3.1\">82.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S5.T2.1.1.7.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.7.6.4.1\">66.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S5.T2.1.1.7.6.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.7.6.5.1\">66.8</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Static results of proposed methods.</figcaption>\n</figure>",
            "capture": "Table 2: Static results of proposed methods."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T3\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A1.T3.5\" style=\"width:136.4pt;height:32.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-7.6pt,1.8pt) scale(0.9,0.9) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A1.T3.5.5\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A1.T3.5.5.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"A1.T3.5.5.5.6\">Dataset</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T3.1.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T3.2.2.2.2\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T3.3.3.3.3\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T3.4.4.4.4\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T3.5.5.5.5\"></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T3.5.5.6.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\" id=\"A1.T3.5.5.6.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T3.5.5.6.1.1.1\">LIVE</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A1.T3.5.5.6.1.2\">2870</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A1.T3.5.5.6.1.3\">4.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A1.T3.5.5.6.1.4\">155.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A1.T3.5.5.6.1.5\">3.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A1.T3.5.5.6.1.6\">23519</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Overview of LIVE dataset. While  represents the number of video instances,  represents the average minutes per instance,  represents the average number of video clips per instance,  represents the average seconds of a single clip data, and  represents the number of entities included in the knowledge base.</figcaption>\n</figure>",
            "capture": "Table 3: Overview of LIVE dataset. While  represents the number of video instances,  represents the average minutes per instance,  represents the average number of video clips per instance,  represents the average seconds of a single clip data, and  represents the number of entities included in the knowledge base."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T4\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A2.T4.1\" style=\"width:150.5pt;height:32.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-8.4pt,1.8pt) scale(0.9,0.9) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A2.T4.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T4.1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A2.T4.1.1.1.1.1\">Format</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.1.1.1.1.2\">Struct</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.1.1.1.1.3\">Semi-struct</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.1.1.1.1.4\">Human</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.1.1.1.1.5\">LLM</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\" id=\"A2.T4.1.1.2.2.1\">RoFA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T4.1.1.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.1.1.2.2.2.1\">60.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T4.1.1.2.2.3\">50.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T4.1.1.2.2.4\">57.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T4.1.1.2.2.5\">59.36</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>RoFA results of different memory formats.</figcaption>\n</figure>",
            "capture": "Table 4: RoFA results of different memory formats."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.01411v1_figure_1.png",
            "caption": "Figure 1: The task of OVEL in the live delivery scene. The upper represents an online delivery video. At time t,\nit takes information before time t as input and identifies salient entities from the video. Relevant entities are pushed to specific persons for recommendation."
        },
        "2": {
            "figure_path": "2403.01411v1_figure_2.png",
            "caption": "Figure 2: Overview of framework structure. The initialized memory block is obtained through the summary module and used alongside keyframes extracted from the video by MEL to get initial retrieval candidates. At time t, the LLM memory controller acquires video information within the current input time interval, the memory block before time t, and incorporates retrieval results to update the content within the memory block."
        },
        "3": {
            "figure_path": "2403.01411v1_figure_3.png",
            "caption": "Figure 3: Inference time of different method. The Recommended time is determined based on the optimal inference time consumption provided by the actual application scenario."
        },
        "4": {
            "figure_path": "2403.01411v1_figure_4.png",
            "caption": "Figure 4: The procedure of LIVE dataset construction."
        },
        "5": {
            "figure_path": "2403.01411v1_figure_5.png",
            "caption": "Figure 5: Overview of entity distribution."
        },
        "6": {
            "figure_path": "2403.01411v1_figure_6.png",
            "caption": "Figure 6: Different prompt for Memory Management."
        }
    },
    "references": [
        {
            "1": {
                "title": "Building a multimodal entity linking dataset from tweets.",
                "author": "Omar Adjali, Romaric Besan\u00e7on, Olivier Ferret, Herv\u00e9 Le Borgne, and Brigitte Grau. 2020a.",
                "venue": "In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4285\u20134292.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Multimodal entity linking for tweets.",
                "author": "Omar Adjali, Romaric Besan\u00e7on, Olivier Ferret, Herv\u00e9 Le Borgne, and Brigitte Grau. 2020b.",
                "venue": "In European Conference on Information Retrieval, pages 463\u2013478. Springer.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Self-rag: Learning to retrieve, generate, and critique through self-reflection.",
                "author": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2310.11511"
            }
        },
        {
            "4": {
                "title": "Qwen technical report.",
                "author": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2309.16609"
            }
        },
        {
            "5": {
                "title": "Language models are few-shot learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.",
                "venue": "Advances in neural information processing systems, 33:1877\u20131901.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Altclip: Altering the language encoder in clip for extended language capabilities.",
                "author": "Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, and Ledell Wu. 2022.",
                "venue": null,
                "url": "http://arxiv.org/abs/2211.06679"
            }
        },
        {
            "7": {
                "title": "Mmel: A joint learning framework for multi-mention entity linking.",
                "author": "YANG Chengmei, Bowei He, Yimeng Wu, Chao Xing, Lianghua He, and Chen Ma. 2023.",
                "venue": "In The 39th Conference on Uncertainty in Artificial Intelligence.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.",
                "venue": null,
                "url": "http://arxiv.org/abs/1810.04805"
            }
        },
        {
            "9": {
                "title": "An image is worth 16x16 words: Transformers for image recognition at scale.",
                "author": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021.",
                "venue": null,
                "url": "http://arxiv.org/abs/2010.11929"
            }
        },
        {
            "10": {
                "title": "Multi-modal transformer for video retrieval.",
                "author": "Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. 2020.",
                "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part IV 16, pages 214\u2013229. Springer.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Multimodal entity linking: a new dataset and a baseline.",
                "author": "Jingru Gan, Jinchang Luo, Haiwei Wang, Shuhui Wang, Wei He, and Qingming Huang. 2021.",
                "venue": "In Proceedings of the 29th ACM International Conference on Multimedia, pages 993\u20131001.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Realm: Retrieval-augmented language model pre-training.",
                "author": "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020.",
                "venue": null,
                "url": "http://arxiv.org/abs/2002.08909"
            }
        },
        {
            "13": {
                "title": "Deep residual learning for image recognition.",
                "author": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015.",
                "venue": null,
                "url": "http://arxiv.org/abs/1512.03385"
            }
        },
        {
            "14": {
                "title": "Atlas: Few-shot learning with retrieval augmented language models.",
                "author": "Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022.",
                "venue": null,
                "url": "http://arxiv.org/abs/2208.03299"
            }
        },
        {
            "15": {
                "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks.",
                "author": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2021.",
                "venue": null,
                "url": "http://arxiv.org/abs/2005.11401"
            }
        },
        {
            "16": {
                "title": "Semantic video entity linking based on visual content and metadata.",
                "author": "Yuncheng Li, Xitong Yang, and Jiebo Luo. 2015.",
                "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pages 4615\u20134623.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system.",
                "author": "Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. 2023.",
                "venue": "arXiv preprint arXiv:2304.13343.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Ra-dit: Retrieval-augmented dual instruction tuning.",
                "author": "Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2310.01352"
            }
        },
        {
            "19": {
                "title": "Roberta: A robustly optimized bert pretraining approach.",
                "author": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.",
                "venue": null,
                "url": "http://arxiv.org/abs/1907.11692"
            }
        },
        {
            "20": {
                "title": "Zero-shot entity linking by reading entity descriptions.",
                "author": "Lajanugen Logeswaran, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, and Honglak Lee. 2019.",
                "venue": "arXiv preprint arXiv:1906.07348.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning.",
                "author": "Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. 2022.",
                "venue": "Neurocomputing, 508:293\u2013304.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Multi-grained multimodal interaction network for entity linking.",
                "author": "Pengfei Luo, Tong Xu, Shiwei Wu, Chen Zhu, Linli Xu, and Enhong Chen. 2023.",
                "venue": "In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1583\u20131594.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Howto100m: Learning a text-video embedding by watching hundred million narrated video clips.",
                "author": "Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019.",
                "venue": "In Proceedings of the IEEE/CVF international conference on computer vision, pages 2630\u20132640.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Gpt-4 technical report.",
                "author": "OpenAI. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2303.08774"
            }
        },
        {
            "25": {
                "title": "Training language models to follow instructions with human feedback, 2022.",
                "author": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.",
                "venue": "URL https://arxiv. org/abs/2203.02155, 13.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Robust speech recognition via large-scale weak supervision.",
                "author": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022.",
                "venue": null,
                "url": "http://arxiv.org/abs/2212.04356"
            }
        },
        {
            "27": {
                "title": "Improving language understanding by generative pre-training.",
                "author": "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018.",
                "venue": null,
                "url": null
            }
        },
        {
            "28": {
                "title": "Language models are unsupervised multitask learners.",
                "author": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.",
                "venue": "OpenAI blog, 1(8):9.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Generative multimodal entity linking.",
                "author": "Senbao Shi, Zhenran Xu, Baotian Hu, and Min Zhang. 2023.",
                "venue": "arXiv preprint arXiv:2306.12725.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Visual named entity linking: A new dataset and a baseline.",
                "author": "Wenxiang Sun, Yixing Fan, Jiafeng Guo, Ruqing Zhang, and Xueqi Cheng. 2022.",
                "venue": "arXiv preprint arXiv:2211.04872.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Entity linking across vision and language.",
                "author": "Aparna Nurani Venkitasubramanian, Tinne Tuytelaars, and Marie-Francine Moens. 2017.",
                "venue": "Multimedia Tools and Applications, 76:22599\u201322622.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Freshllms: Refreshing large language models with search engine augmentation.",
                "author": "Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2310.03214"
            }
        },
        {
            "33": {
                "title": "Multimodal entity linking with gated hierarchical fusion and contrastive training.",
                "author": "Peng Wang, Jiangheng Wu, and Xiaohang Chen. 2022a.",
                "venue": "In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 938\u2013948.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Wikidiverse: a multimodal entity linking dataset with diversified contextual topics and entity types.",
                "author": "Xuwu Wang, Junfeng Tian, Min Gui, Zhixu Li, Rui Wang, Ming Yan, Lihan Chen, and Yanghua Xiao. 2022b.",
                "venue": "arXiv preprint arXiv:2204.06347.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Emergent abilities of large language models.",
                "author": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022.",
                "venue": "arXiv preprint arXiv:2206.07682.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Drin: Dynamic relation interactive network for multimodal entity linking.",
                "author": "Shangyu Xing, Fei Zhao, Zhen Wu, Chunhui Li, Jianbing Zhang, and Xinyu Dai. 2023.",
                "venue": "In Proceedings of the 31st ACM International Conference on Multimedia, pages 3599\u20133608.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Msr-vtt: A large video description dataset for bridging video and language.",
                "author": "Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016.",
                "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Chinese clip: Contrastive vision-language pretraining in chinese.",
                "author": "An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, and Chang Zhou. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2211.01335"
            }
        },
        {
            "39": {
                "title": "Ameli: Enhancing multimodal entity linking with fine-grained attributes.",
                "author": "Barry Menglong Yao, Yu Chen, Qifan Wang, Sijia Wang, Minqian Liu, Zhiyang Xu, Licheng Yu, and Lifu Huang. 2023.",
                "venue": "arXiv preprint arXiv:2305.14725.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Attention-based multimodal entity linking with high-quality images.",
                "author": "Li Zhang, Zhixu Li, and Qiang Yang. 2021.",
                "venue": "In Database Systems for Advanced Applications: 26th International Conference, DASFAA 2021, Taipei, Taiwan, April 11\u201314, 2021, Proceedings, Part II 26, pages 533\u2013548. Springer.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Memorybank: Enhancing large language models with long-term memory.",
                "author": "Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang. 2023.",
                "venue": "arXiv preprint arXiv:2305.10250.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Weibo-mel, wikidata-mel and richpedia-mel: multimodal entity linking benchmark datasets.",
                "author": "Xingchen Zhou, Peng Wang, Guozheng Li, Jiafeng Xie, and Jiangheng Wu. 2021.",
                "venue": "In Knowledge Graph and Semantic Computing: Knowledge Graph Empowers New Infrastructure Construction: 6th China Conference, CCKS 2021, Guangzhou, China, November 4-7, 2021, Proceedings 6, pages 315\u2013320. Springer.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.01411v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "research_context": {
        "paper_id": "2403.01411v1",
        "paper_title": "OVEL: Large Language Model as Memory Manager for Online Video Entity Linking",
        "research_background": "### Motivation\n\nThe paper is motivated by the growing significance of video content, particularly on platforms like TikTok and YouTube. Despite extensive research into various aspects of video understanding, there's a gap in recognizing specific entities within videos. The authors argue that in many scenarios, such as live streaming, it is more critical to identify specific entities (e.g., specific products or athletes) than to merely understand the general content of the video. This gap leads to a pertinent need for systems that can precisely link mentions in videos to corresponding entities in a knowledge base in real-time, thereby improving the viewer experience and potentially enhancing economic benefits through timely and relevant information delivery.\n\n### Research Problem\n\nThe primary research problem addressed by the paper is the development of a system for **Online Video Entity Linking (OVEL)** on dynamic video streams. The goal is to link mentions of important entities in online videos to their corresponding entries in a knowledge base. This problem involves several sub-challenges:\n1. **Noise Management**: Real-time video streams contain a lot of visual and auditory noise, which can interfere with accurate entity recognition.\n2. **Timeliness**: The need for real-time processing to identify and link entities quickly in online scenarios.\n3. **Domain Knowledge**: Effective entity recognition may require specific domain knowledge that general viewers or models might lack.\n\n### Prior Work\n\nThe related work section outlines prior research in areas adjacent to video entity linking:\n- **Multimodal Entity Linking (MEL)**: Studies by Adjali et al. (2020a, b); Zhou et al. (2021); Wang et al. (2022a, b); Gan et al. (2021); Sun et al. (2022); Luo et al. (2023); Xing et al. (2023) have focused on linking mentions in static visual-textual pairs to a knowledge base.\n- **Video Entity Linking**: Previous initiatives like those by Li et al. (2015); Venkitasubramanian et al. (2017) explored linking entities in video data but faced limitations such as linking to coarse-grained entities and lacking real-time processing capabilities.\n\n### Paper's Approach\nTo address the outlined challenges and advance research in OVEL, the authors propose:\n- A novel dataset, **LIVE**, tailored for live stream product recognition, consisting of 82 live streams and approximately 250 hours of video.\n- A new metric, **RoFA** (time-weighted decay metric) to evaluate the accuracy and efficiency of entity linking in video streams.\n- A robust framework leveraging large language models (LLMs) for information extraction and memory management to handle noise and domain-specific challenges.\n- A two-stage entity linking process where multimodal entity linking methods serve as candidate retrieval followed by LLM-based entity disambiguation to optimize both accuracy and resource consumption.\n\nThese novel contributions aim to push the boundaries of entity linking in video content, especially in real-time online scenarios.",
        "methodology": "**Methodology:**\n\nIn this section, we will present the overall framework of our proposed methodology, starting with an overview of the entire process. We will then provide detailed introductions to the summary modules that form the core of our approach and examine the main components of the Large Language Model (LLM) acting as the memory controller. We will conclude our discussion by outlining the two-stage entity linking methods employed in our model.\n\n### 1. Overall Framework:\nOur methodology revolves around the integration of a Large Language Model (LLM) to manage memory effectively in an online video entity linking system. The comprehensive framework is designed to capture, process, and link entities that appear in online video content in a coherent and dynamic manner.\n\n### 2. Summary Modules:\nThe summary modules serve as the building blocks of our framework. Each module plays a significant role in the processing pipeline:\n\n- **Ingestion Module:** This module is responsible for capturing video content and generating text representations of the spoken dialogue and visual text.\n- **Segmentation Module:** It divides the ingested content into manageable segments based on temporal or topical boundaries.\n- **Preprocessing Module:** It cleans and normalizes the textual data to ensure consistency and accuracy in subsequent analysis.\n\n### 3. LLM as Memory Controller:\nThe innovation in our model lies in leveraging a Large Language Model as the memory controller. The LLM enriches the memory management process by:\n\n- **Dynamic Contextual Understanding:** The LLM maintains a dynamic understanding of the context in which entities appear, allowing for more accurate linking.\n- **Scalability:** The model scales efficiently with the volume of data, ensuring consistent performance even as the size of the video content increases.\n- **Memory Optimization:** The LLM optimizes memory allocation by prioritizing significant entities and discarding irrelevant information.\n\n### 4. Two-Stage Entity Linking Methods:\nOur entity linking approach is divided into two distinct stages:\n\n- **Stage 1 - Candidate Generation:** In this stage, potential candidates for each entity mention are identified. This involves analyzing the contextual data provided by the LLM and fetching entities from a pre-defined knowledge base.\n- **Stage 2 - Candidate Ranking:** The second stage involves ranking the candidate entities based on their relevance and context. The LLM again plays a crucial role by evaluating the contextual compatibility of the candidates with the video content.\n\nBy combining these components and innovations, our proposed methodology sets a robust foundation for accurately linking entities in online video content, leveraging the advanced capabilities of a Large Language Model to enhance memory management and contextual understanding.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### Main Experiment Setup\n1. **Dataset**: The main experimental results are conducted on the constructed dataset specifically designed for evaluating the video entity linking task.\n\n2. **Baselines**: The proposed framework is compared against several existing state-of-the-art approaches to provide a benchmark and validate its effectiveness.\n\n#### Evaluation Metrics\n- **Temporal Performance**: Evaluates how well the framework can maintain and manage entity information over time in online video streams.\n- **Traditional Metrics for Static Videos**: Assesses the performance using conventional metrics established for static video analysis, providing a comparative basis for evaluating improvements or deficiencies.\n\n#### Main Experimental Results\n- The proposed framework shows significant improvements over baseline methods in both temporal performance and traditional metrics for static videos.\n- The experiments indicate that the novel approach of using a large language model as a memory manager for video entity linking enhances both the precision and recall of entity retrieval and linking processes.\n\nIn summary, the main experimental results demonstrate the efficacy of the new framework in managing real-time entity linking tasks in online videos, outperforming existing methods in both dynamic and static video scenarios. Additional detailed observations and phenomena related to these experiments are provided in Appendix B."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Assess the effectiveness of Large Language Model (LLM) as a memory controller compared to traditional multi-modal retrieval models for the Online Video Entity Linking (OVEL) task.",
            "experiment_process": "The study integrates the proposed LLM memory controller into two multi-modal retrieval models: AltCLIP and CN-CLIP (variants B and L). The models are tested on a specific test set with the 'Base' model representing the direct use of the retrieval models and 'Ours' representing the models enhanced by the LLM. The evaluation metric for performance is presented in RoFA results in Table 1.",
            "result_discussion": "The combination of the retrieval models with LLM achieved the highest performance, particularly with CN-CLIP_L which outperformed other models. The most significant improvement was observed with AltCLIP, showcasing a nearly 300% enhancement. This suggests that dividing the task and providing sufficient candidate options to the LLM helps it perform optimally. The results indicate substantial improvements in scenarios where the retrieval model performs poorly, implying the effectiveness of the LLM enhancement in low-resource situations.",
            "ablation_id": "2403.01411v1.No1"
        },
        {
            "research_objective": "Evaluate the online performance of the proposed LLM-based method compared to traditional retrieval models in terms of time efficiency.",
            "experiment_process": "The inference time of various methods was evaluated on the test set. The time taken to predict an entity was recorded at intervals of every five video clips, and the average time for each method was calculated and presented in Figure 3.",
            "result_discussion": "The 'Base' retrieval model exhibited the best time performance. As the number of video clips increased, the time cost for the LLM-based methods (Ours-M and Our Ours-R) increased significantly, surpassing the recommended inference time with a window size exceeding 200. The LLM-based method initially showed a rapid increase in time cost which stabilized later, whereas Ours-R continued to accumulate irrelevant information. The Ours-M method displayed instability due to variations in text length of different video clips, leading to inconsistencies.",
            "ablation_id": "2403.01411v1.No2"
        },
        {
            "research_objective": "Compare the performance of the proposed method against existing Multimodel Entity Linking (MEL) and Video Entity Linking (VEL) methods using static data.",
            "experiment_process": "The study treats each video as an individual data instance and performs video entity linking across its content. Representative approaches such as CLIP4Clip, BLINK, V2VTEL, AltCLIP, and Chinese-CLIP were selected for evaluation. The primary metrics used were Recall and Mean Reciprocal Rank (MRR) at K, as shown in Table 2.",
            "result_discussion": "The proposed method achieved the best performance, validating its effectiveness. The performance of CLIP4clip and V2TVEL approaches was inferior to the text-based approach BLINK, underscoring the significance of textual data in this scenario. Among multimodal inputs, CN-CLIP and AltCLIP exhibited favorable results, justifying their selection as multimodal retrieval models for the study.",
            "ablation_id": "2403.01411v1.No3"
        }
    ]
}