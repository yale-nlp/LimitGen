{
    "title": "Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property for Perplexity in Generative Language Models",
    "abstract": "We propose a new asymptotic equipartition property for the perplexity of a large piece of text generated by a language model and present theoretical arguments for this property. Perplexity, defined as an inverse likelihood function, is widely used as a performance metric for training language models. Our main result states that the logarithmic perplexity of any large text produced by a language model must asymptotically converge to the average entropy of its token distributions. This means that language models are constrained to only produce outputs from a \u201ctypical set\u201d, which we show, is a vanishingly small subset of all possible grammatically correct outputs. We present preliminary experimental results from an open-source language model to support our theoretical claims. This work has possible practical applications for understanding and improving \u201cAI detection\u201d tools and theoretical implications for the uniqueness, predictability and creative potential of generative models.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction and Motivation.",
            "text": "Consider a generative model, defined as an algorithm that takes a user input and produces an output that statistically resembles data from a natural source. A specific type of generative model is a large language model (LLM) whose output is a body of text and the input is a text user prompt. State-of-the-art LLMs (Anthropic, 2024 ###reference_b1###; OpenAI, 2024 ###reference_b21###) are now able to produce detailed and information-rich text outputs such as entire screenplays and book-length manuscripts from a short and simple user prompt. Furthermore, LLMs produce text that can imitate human language well enough to pass the Turing test (Pinar Saygin et al., 2000 ###reference_b22###), i.e., resembles text created by humans well enough to be convincing to human observers. In this work, we argue that an LLM is strongly constrained by statistical laws. Specifically, we show that the logarithmic perplexity of any large text produced by a language model must asymptotically converge to the average entropy of its token distributions. This means that any language model is constrained to only output text strings from a typical set, which is an exponentially vanishing subset of all possible grammatically correct strings."
        },
        {
            "section_id": "1.1",
            "parent_section_id": "1",
            "section_name": "Contribution: Equipartition Property for Perplexity",
            "text": "Perplexity, defined as an inverse likelihood function, is widely used as a performance metric for language models (Meister and Cotterell, 2021). It is closely related to the information-theoretic concepts of surprisal (Levy, 2008) and cross-entropy (Cover and Thomas, 2005b), and it also appears to capture linguistic (Miaschi et al., 2021; Gamallo et al., 2017) and cognitive (Demberg and Keller, 2008; Cohen and Pakhomov, 2020) phenomena at least partially. Many \u201cAI detection\u201d tools for identifying synthetic text from language models are based on observed differences between the perplexity of synthetic and natural text (Mitchell et al., 2023; Gehrmann et al., 2019).\n\nOur main result is a generalization of the well-known Asymptotic Equipartition Theorem (AEP) (Cover and Thomas, 2005a) from information theory. The simplest version of the AEP states that a long sequence of independent and identically distributed (iid) random symbols is likely to be a typical sequence (Csiszar, 1998) defined by the property that the empirical distribution of the occurrence of different symbols within the sequence is very close to the distribution from which the symbols were drawn.\n\nThe AEP itself can be thought of as a variant of the Law of Large Numbers (LLN) (Idele, 2018) applied to a log likelihood function, and just like the LLN, there is an extensive literature on extending the AEP to a sequence of nearly-independent but not identically distributed random variables (Nishiara and Morita, 2000; Timo et al., 2010). The outputs of non-trivial language models, however, cannot be reasonably modeled as a sequence of nearly-independent symbols."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background and Definitions",
            "text": "Let M be a generative model described by , where the output  consisting of a (potentially infinite) string of tokens  is a deterministic function of user prompt  and a pseudo-random sequence . Each token is chosen from a finite set of tokens . The model M can produce a number of different output strings  for the same user prompt  corresponding to different values of the pseudo-random sequence . This defines a probability distribution , or more formally, a sequence of probability distributions  over  where  is the substring of  consisting of the first  tokens."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Sequential LLM",
            "text": "Practical implementations of LLMs specify the probability distribution iteratively:\nand so on. Thus, the model M first draws a random value for the first token by sampling from the distribution. Then the model determines a distribution for the second token as a function of the initial prompt and the randomly chosen first token. Thus, the second token is randomly sampled from a distribution and so on. We can write:\n\nGiven a string, open-source LLMs can be programmed to print out the distributions from which its tokens were selected. Specifically, given a user prompt and a string of tokens, it is possible to get a complete listing of the distributions. Note that complete knowledge of the distributions is not the same as complete knowledge of the full model even for a fixed user prompt. As an example, the former requires knowledge of the distributions for different values of the tokens, but only conditioned on the specific values contained in one specific string.\n\nRemark. The equation is simply an application of the Bayes rule of probability theory and it always holds for any generative model regardless of whether the tokens are sequentially generated. However, the conditional distributions are not in general easily accessible, so while the equation is true for all generative models, it may only be useful for sequential models.\n\nThe perplexity of a (finite length) text string for a model M is defined as the per-token inverse likelihood of the string. It is usually more convenient to work with the log-perplexity."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "A Toy Problem",
            "text": "Let  be two fixed probability distributions over the (discrete) set  of tokens. Consider a toy problem involving two language models A and B, that each generate a string of tokens  where each token is generated iid from the distribution  and  respectively. The iid assumption implies that the tokens  can be thought of as being generated by a stationary and ergodic random process.\n\nConsider a long string  randomly generated from model A. Let  denote the empirical distribution of  in the string :\nwhere  denotes the indicator function and  is the relative frequency of token  in the (long) string . The log-perplexity of string  for model A is:\nwhere  is the cross-entropy between two distributions  over .\n\nIt is well-known with equality when , where  is the entropy of distribution . The simplest version of the classical Asymptotic Equipartition Theorem (AEP) from information theory states that the log-perplexity of a long string  of iid symbols is almost always very close to the entropy  of the distribution  the symbols are drawn from.\n\nSimple AEP. For a long text string  of iid tokens  drawn from a distribution , the log perplexity  as defined is close to the entropy of the distribution  of the distribution  with high probability:\n\nProposition 1 can be thought of as a direct consequence of the (weak) Law of Large Numbers applied to the random variable :\n\nFrom this, we see that the empirical distribution  for long strings  is very close to  with high probability. Putting these observations together, we have: \n\nIntuitively, this states that a long string  generated from model A is likely to have a lower perplexity for model A than for any other model B. This means that a model trained to have low perplexity over a set of reference texts will generate text strings that are statistically similar to the reference texts. This is the theoretical justification for using perplexity as a loss function for training language models, and it is an excellent justification\u2014provided that we accept the Shannon model of language, i.e., the idea that languages can be reasonably modeled as a stochastic sequence of tokens."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "A Naive Method for Detecting Text Generated by a Language Model",
            "text": "Given a text string, we seek to determine whether or not it was generated by a given model M. This is the problem of \u201cwhite-box\u201d detection of AI-generated text where we assume knowledge of the specific language model that potentially generated the text. A naive method for doing this is as follows: compare the log-perplexity of the text for model M with a threshold: if it exceeds the threshold, we determine that the text is likely to have been generated by model M.\n\nConsider a long string randomly generated from model B. Using the same argument as in Section 2.2, we have:\n\nNote, however, that we have no basis to assert any relationship between two different perplexities. Indeed if one perplexity is significantly lower than the other, it cannot be ruled out that a text string generated by a different model has lower perplexity for a model than a text string generated by itself.\n\nThe mathematical properties of cross-entropy provide a rigorous basis for using perplexity to compare the compatibility of different models with a given text string. However, this theory does not support using perplexity to compare different text strings for compatibility with a given model."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "A Modest Generalization",
            "text": "This theoretical analysis relies on Proposition 1 ###reference_position1### which only applies to toy models that generate strings of iid tokens. \n\nGeneralized AEP. Consider a language model A\u2019 that generates a text string where the tokens are drawn independently from a sequence of distributions. Assuming the entropies of distributions are uniformly bounded i.e., the log perplexity of the string as defined in (3 ###reference_###) is close to the average entropy of the distributions with high probability:\n\nThe regularity condition that the entropies are uniformly bounded is trivially true if the token dictionary is a finite set: for any distribution over.\n\nWhile Proposition 2 ###reference_position2### does not lend itself to an intuitive interpretation in terms of the empirical distribution of the tokens, it too is a direct consequence of the Law of Large Numbers applied to the log-perplexity random variable. Much of the analysis in Section 2.2 ###reference_### can be extended to generalized models like A\u2019 in Proposition 2 ###reference_position2### that allows each token to be drawn from different distributions. However, the tokens in these models must be drawn from fixed distributions independent of past tokens. This is still quite trivial compared to modern language models where the output tokens depend in highly complex and sophisticated ways on past tokens."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "An Equipartition Property for Perplexity",
            "text": "We propose a generalization of the theory described in Section 2.2 to a broader class of models. Consider a language model \\( M \\) and a random infinitely long text string \\( S \\) generated by \\( M \\), whose probabilities for a given prompt \\( P \\) are described by a sequence of probability distributions \\( \\{ p_i \\} \\) over the set of tokens \\( V \\). We assume that the prompt \\( P \\) is fixed.\n\nThe empirical entropy \\( H_{emp}(S) \\) of model \\( M \\) for string \\( S \\) is defined as the average entropy over the sequence of distributions. Note that the empirical entropy \\( H_{emp}(S) \\) is a random variable, whereas the entropy \\( H(p_i) \\) is a constant number for a fixed distribution. The expectation of \\( H_{emp}(S) \\) is taken over random text strings \\( S \\).\n\nThe log-deviation of a probability distribution \\( p \\) over tokens \\( V \\) is defined as the standard deviation of the log-likelihood random variable under distribution \\( p \\). The log-deviation for a string \\( S \\) for model \\( M \\) is then the average log-deviation over the sequence of distributions.\n\nWe interpret the mean and standard deviation of the log-likelihood as the empirical entropy and log-deviation, respectively. If the log-deviations of the distributions for a string \\( S \\) are uniformly upper-bounded, the log-deviation asymptotically vanishes.\n\nOur main result suggests that while the log-deviation of a fixed probability distribution is deterministic, the log-deviation for a string is a random variable that can vary for different random text strings. This randomness arises because the distributions differ for each string due to the dependence of future tokens on earlier ones. This dependency is a key feature of modern language models that enables the generation of sophisticated text outputs. We introduce an auxiliary language model as a conceptual device to manage this complexity."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Concept of an Auxiliary Language Model",
            "text": "Let us consider a fixed string . For this particular string  and model M, we have a fixed set of distributions . Now define an auxiliary generative model  that generates a random string  where the tokens  are generated independently from the distributions . This construct of the auxiliary model  for a fixed string  allows us to work with fixed distributions  which are not themselves functions of a random string.\n\nIn particular, note that the empirical entropy  and log-deviation  for model  are both deterministic functions of  independent of the string . Specifically, we have . Similarly, we have  which, since we have fixed our special string , is now also fixed. Indeed, it is easy to show that  is the standard-deviation of the random variable  the log-likelihood of the random string . Then, by the Chebyshev Inequality (Cohen, 2015  ###reference_b4###), we have:\n\nClearly, for substrings  of our special string  that defines the auxiliary model , we have by definition . In addition, the entropy  of the random strings  output by auxiliary model  is by definition .\n\nThe auxiliary model  is much simpler than the general language model M. In particular, because the tokens  are generated independently of other tokens, the AEP in Proposition 2  ###reference_position2### applies to strings  generated by the auxiliary model . Indeed we note from (15  ###reference_###) that the log-deviation  vanishes for large  under some regularity conditions on the distributions , in which case, the AEP for model  follows from (16  ###reference_###)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Ensemble of Auxiliary Models",
            "text": "We can think of a generative model M as a statistical ensemble of a large number of (simpler) auxiliary models. Each auxiliary model is capable of generating a random text string consisting of a sequence of independent tokens. We can then imagine choosing randomly from one of the auxiliary models. We now make this idea more precise. Consider an enumeration of the possible text strings. Let denote the auxiliary model associated with model M and string. A sequential generative model can be represented by a probability tree where each branch in the tree represents a new token being generated. The model M as well as each of the auxiliary models can each be represented by such a probability tree. An example of a probability tree representing a generative model and a sample random string represented by a sequence of branches through the tree are shown in Fig. 1, which also shows the probability tree for one possible auxiliary model.\n\nA generative model uses a pseudo-random number generator (PRNG) that produces a sequence of random symbols to randomly choose a sequence of branches through its probability tree. As an example, suppose the symbols are independent and uniformly distributed in and a generative model uses it to generate the n\u2019th token according to the distribution using a sampling algorithm such as the classical inversion method. Once we fix the PRNG and the sampling method, we can in principle write down a deterministic function to describe the generative model M as where recall that is the fixed initial prompt. Each of the auxiliary models can also similarly be described by deterministic functions.\n\nWe can then define a new generative model that we call the ensemble model as follows. Pick a pseudo-random sequence and generate a string from model M. Now consider the auxiliary model corresponding to the string. Generate a pseudo-random string from model where is another pseudo-random sequence. The ensemble model is defined by.\n\nThe following observation is crucial to our main result: a good random number generator will almost always generate pseudo-random sequences that belong to a typical set i.e. that satisfies the simple AEP in Proposition 1.\n\nTypical String. An infinite text string (or more precisely a sequence of text strings) is said to be typical for a model M if the log-perplexity of sufficiently long sub-strings converges to their empirical entropy for model M.\n\nWe illustrate these ideas for a simple Markov language model.\n\nAEP for Perplexity. For a long text string i.e. generated from a language model with a given prompt, with high probability, the log-perplexity is close to the empirical entropy. More precisely:\n\nLet denote the pseudo-random sequence that generated our string from model M. Thus we have. With high probability, is a typical sequence. Consider the ensemble model for model M with pseudo-random sequences and let. The classical AEP in Proposition 2 for model selected by the ensemble model requires that random strings generated by model must almost always satisfy:\n\nSince is almost always a typical sequence, we can set, in which case the output of the ensemble model is exactly the same as the output of model M. Setting we get and\n\nFinally setting gives and\n\nInserting into proves the result.\n\nProposition 3 asserts that with high probability a sufficiently long text string will be typical for the model that it was generated from. Note the crucial role in the above proof of the assumption that the pseudo-random sequence is typical. We can take this construct further to establish some non-asymptotic results as well. We expect from that the log-perplexity will be within a few standard deviations of its mean e.g. even when is not large enough for asymptotics to kick in. Finally, we will show that the set of typical strings is vanishingly small compared to all grammatically correct strings.\n\nLet denote the set of substrings of typical strings for a model M. The size of the \u201ctypical set\u201d is an exponentially vanishing fraction of all grammatically correct sequences.\n\nIf we use top-k sampling, the model M generates possible grammatically correct values for each token. The number of grammatically correct strings is then lower-bounded as. Under the (mild) assumption that for a long string, at least some fraction of the token distributions are non-uniform, we have. By definition, the probability of model M generating string is, so we have for the probability of a typical string generated by model M:\n\nThe total number of typical strings cannot exceed.\n\nProposition 4 suggests that the typical sets of two different models, each set being vanishingly small, likely have zero overlap."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments with Open-Source Language Model",
            "text": "We performed a series of experiments to verify the ideas described in Section 3. The basic idea is to evaluate the empirical perplexity of generating a given string using a model\u2019s conditional probability distributions and compare the result with the log-perplexity of the string calculated using the model\u2019s probability distributions. Software code for the experiments is available at Bell and Mudumbai (2024)."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Perplexity for GPT-2 of a String Generated by GPT-2",
            "text": "We initialized the prompt string and the seed of the random number generator with fixed values to allow for later reproduction. We then used the top-k sampling with some fixed value to generate the probability distribution for the next token. We normalized the probabilities of the top-k tokens to sum to unity. We then created and saved a table consisting of token ID, token string, and selection probability of each of the top tokens. Finally, a random next token is chosen by sampling from the normalized distribution, and the new token is appended to the prompt to repeat the process for a total number of tokens. The random token chosen at each step along with the probability distribution from which they were chosen were saved into a json file for later processing. We ran this experiment many times for different seed values for the random number generator, different choices for the top-k sampling, and a number of different initial prompts. For each generated string, we can then analyze the information archived in the json file to test our claim in Proposition 3. Specifically, for each token, we calculated the log-perplexity, empirical entropy, and the log-deviation of the substring consisting of the first tokens of the generated string."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Perplexity for GPT-2 of an Arbitrary String",
            "text": "We also used a slightly modified version of the procedure described in Section 4.1 to calculate the log-perplexity and empirical entropy of an arbitrary string. We first choose a text string that we wish to analyze. We fix an initial fragment of the string as our prompt. We tokenize the remaining portion of the string excluding the prompt. Let denote the resulting sequence of tokens. Starting with, we list the probability distribution of the next token for the model using top-k sampling for the \u2019th token. We can then calculate the empirical entropy and log-deviation as in Section 4.1. However, to calculate the log-perplexity, we also need the probability of the actual next token from the string which may not be included in the list of top-k tokens. When this happens, we replaced the lowest probability entry in the list of top-k tokens with the actual next token from our string along with its probability. The normalized probability of each of the top-k tokens along with their token-ID and token strings, as well as the probability and ID of the actual next token are saved in a json file. This json file is then processed in exactly the same way as in Section 4.1 to compute the log-perplexity, empirical entropy and the log-deviation of the sub-string."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Results and Discussion",
            "text": "Figures 3(a) and 3(b) show the log-perplexity and empirical entropy for sub-strings for a string not generated by the GPT-2 model. The string in Fig. 3(a) is a paragraph written by the authors for an early draft of this manuscript, and the string in Fig. 3(b) was generated by the Claude 3 Opus model from Anthropic (2024). While our theory does not require non-convergence of to , we do see from Fig. 3(a) and Fig. 3(b) that the log-perplexity is several standard deviations away from the empirical entropy. The contrast between Figs. 3 and 2 has obvious implications for AI text detection, model fingerprinting, and so on."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Anomalies and Limitations",
            "text": "While the results from our experiments described in Section 4 were always consistent with Proposition 3, they did reveal certain limitations of our setup that we now discuss.\n\nFundamentally, these limitations arise from cost and computational constraints. More powerful LLMs can generate longer texts and be less likely to get stuck in bad states that produce degenerate distributions and so on. In this work, all experiments were ran on a single desktop machine (Windows 11, Intel Core i9-14900K 3.20 GHz, 96.0 GB RAM, NVIDIA GeForce RTX 3090 Ti). Additional plots and more details about these experiments are presented in Section A.1."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "We proposed an asymptotic property that must be satisfied by the perplexity of any long string generated by a language model and provided theoretical and experimental arguments in support of the property. While this paper focused narrowly on an exposition of the AEP property, it opens up many questions for further study. For instance, Proposition 4 suggests that different language models have distinct and unique \u201cmodel signatures.\u201d The contrast between the analyses implies potential applications for understanding and enhancing AI detection algorithms. As reported in our experimental section, our results are limited by computational constraints and can be strengthened by improved hardware and software. A broader question is to explore other statistical laws and constraints that generative models are bound by."
        }
    ],
    "url": "http://arxiv.org/html/2405.13798v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "1.1",
            "2"
        ],
        "methodology_sections": [
            "2.1",
            "2.2",
            "2.3",
            "2.4",
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2405.13798v1",
        "paper_title": "Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property for Perplexity in Generative Language Models",
        "research_background": "### Introduction:\n\nGenerative models, including large language models (LLMs), aim to produce outputs that statistically resemble data from natural sources given some user input. LLMs, a specific type of generative model, take a text prompt as input and generate a detailed and information-rich body of text (Anthropic, 2024; OpenAI, 2024). These models can create extensive outputs, such as screenplays or book-length manuscripts, from simple prompts. Remarkably, they produce text that can convincingly mimic human language, passing the Turing test (Pinar Saygin et al., 2000).\n\nThe core argument of this work is that LLMs are fundamentally constrained by statistical laws. Specifically, it shows that the logarithmic perplexity of any large text produced by an LLM must asymptotically converge to the average entropy of its token distributions. This constraint implies that a language model can only generate text strings from a typical set, an exponentially vanishing subset of all grammatically correct strings.\n\n### Motivation:\n\nThe motivation behind this research stems from understanding the constraints that govern the performance and output of LLMs. While current LLMs have demonstrated astonishing capabilities, producing text that closely mimics human writing, there remain fundamental statistical principles that dictate these models' behavior. A deeper understanding of these constraints could provide insights into the limitations and potential improvements of LLM technology.\n\n### Research Problem:\n\nThe research problem addressed by this paper is to analytically establish that the logarithmic perplexity of any large text produced by a language model asymptotically converges to the average entropy of its token distributions. The study aims to demonstrate that LLMs are inherently restricted to generate text within a specific typical set, which is a significantly limited subset of all possible grammatically correct text strings.\n\n### Relevant Prior Work:\n\n- **Language Model Capabilities:**\n  - Anthropic, 2024; OpenAI, 2024: These references highlight recent advancements in LLMs, showcasing their ability to produce detailed, information-rich texts like screenplays and manuscripts from simple prompts.\n  - The Turing Test: Pinar Saygin et al., 2000 outlines the ability of LLMs to produce text convincingly similar to human-created text, passing this classic test of machine intelligence.\n  \n- **Theoretical Foundations:**\n  - While not explicitly detailed in the provided sections, it is implied that foundational principles from information theory, specifically concerning entropy and perplexity, underpin the analysis presented in this work.\n\nIn summary, this paper delves into the constraints imposed by statistical laws on the performance of LLMs, aiming to provide a rigorous theoretical foundation to understand why and how these models can generate text resembling human language within certain limitations.",
        "methodology": "### Methodology: Iterative Specification of Probability Distribution in Generative Language Models\n\nGenerative Language Models (LLMs) implement their probability distributions iteratively, as demonstrated in work by Radford et al. (2018). The model, denoted , generates text in a sequential fashion. Here is a structured outline of the described process:\n\n1. **Initial Token Sampling**: \n   The model begins by selecting an initial token, , by sampling from the distribution . Each token is selected based on a defined probability distribution characteristic of the model.\n\n2. **Subsequent Token Sampling**:\n   Once the first token is determined, the model then governs the probability distribution for the second token. This distribution is influenced by both the initial user prompt, , and the first token, . Thus, the second token, , is randomly sampled from the distribution . This process continues iteratively, with each new token's distribution depending on the initial prompt and all previously chosen tokens.\n\n3. **Formal Representation**:\n   The probability distribution from which a string of tokens is generated can be denoted as:\n   \n   Given a string , an open-source LLM can list out the distributions  from which each token  was selected, taking into account the user prompt and the preceding tokens.\n\n4. **Distinction Between Knowledge of Distributions and Full Model**:\n   Complete knowledge of the individual conditional distributions, , does not equate to complete knowledge of the overarching model distribution for a prompt . For example, knowing every distribution requires understanding how the model would behave for different values of , conditioned on the specific sequence  found in one specific string.\n\n5. **Bayesian Application**:\n   The methodology mentions that the formulation adheres to the Bayes rule of probability theory, which is universally applicable to generative models. However, obtaining the conditional distributions, , is usually challenging. This limitation may only be overcome using sequential models where the iterative accommodation of prior tokens is straightforward.\n\n6. **Perplexity**:\n   - **Definition**: The perplexity, , of a finite text string for a model  is defined as the per-token inverse likelihood of the string .\n   - **Log-Perplexity**: It is often more practical to work with the logarithm of the perplexity, which is detailed in the methodology but not fully expanded here.\n\nThis methodology provides a formal and practical approach to understanding how generative language models (LLMs) construct sequences of text probabilistically and highlights the distinction between having complete knowledge of token-level distributions versus the overall model behavior. The use of iterative token sampling based on conditional distributions is crucial for replicating the generative process.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\nTo validate the concepts detailed in Section 3, we conducted a series of experiments utilizing a local instance of the pre-trained GPT-2 model (Radford et al., 2019). The core approach involved evaluating the empirical perplexity of generating specific strings via the model\u2019s conditional probability distributions and contrasting these results with the log-perplexity of the same strings, recalculated using the model's probability distributions.\n\n**Main Experiment Setup:**\n- **Model:** Pre-trained GPT-2 (Radford et al., 2019).\n- **Task:** Evaluate and compare empirical perplexity and log-perplexity of generating specific strings.\n- **Process:**\n  1. Evaluate the empirical perplexity of generating a string using GPT-2's conditional probability distributions.\n  2. Recalculate the log-perplexity of the same string using GPT-2\u2019s probability distributions for comparison.\n\n**Datasets:** The dataset specifics are not provided in the excerpt, but the focus appears to be on strings of text generated and evaluated within GPT-2.\n\n**Baselines:** Specific baseline models are not explicitly mentioned. However, the intrinsic comparison of empirical perplexity against log-perplexity serves as a form of internal benchmarking.\n\n**Evaluation Metrics:** \n- **Empirical Perplexity:** The perplexity calculated based on the actual output probabilities from GPT-2.\n- **Log-Perplexity:** The perplexity recalculated using the given string's probability distribution data.\n\n**Main Experimental Results:** \n- The results from the comparison validate the theoretical ideas proposed, indicating a consistency between empirical perplexity and log-perplexity as determined by GPT-2\u2019s probability distributions.\n\nAdditional details such as specific numerical results, charts, or statistical significance are not included in the provided text. The original publication might contain these specifics. The software code for these experiments is accessible at Bell and Mudumbai (2024).\n\nThe stated conclusion demonstrates the practical alignment of theoretical constructs around perplexity with observational data generated from the GPT-2 model."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the log-perplexity, empirical entropy, and log-deviations of long text strings generated by GPT-2.",
            "experiment_process": "Experiments involved repeatedly generating long text strings from GPT-2, initializing the prompt string and the seed of the random number generator with fixed values. Top-k sampling was used to generate the next token's probability distribution, normalizing the probabilities. Random tokens were chosen and appended to the prompt to form a maximum number of total tokens. The token selection along with their probabilities were saved into a JSON file. The process was replicated under different seed values, choices of k, and initial prompts. For each generated string, log-perplexity, empirical entropy, and log-deviation were calculated for each token.",
            "result_discussion": "Results showed that the log-perplexity consistently remains within a range around the empirical entropy for all test cases. The long-term average empirical entropy reveals that for top-100 sampling, there are approximately 100 alternative tokens to choose from, whereas for top-4 sampling, about four alternative tokens.",
            "ablation_id": "2405.13798v1.No1"
        },
        {
            "research_objective": "To analyze the log-perplexity and empirical entropy of arbitrary strings not generated by GPT-2 when processed by the GPT-2 model.",
            "experiment_process": "The procedure involved selecting arbitrary text strings and fixing an initial fragment as the prompt, followed by tokenizing the remaining portion. Using top-k sampling, the probability distribution for the next token was listed. The log-perplexity and empirical entropy were calculated, noting that when the next token wasn't in the top-k, it replaced the lowest probability entry. The data, including top-k tokens, were saved into a JSON file for analysis in the same method as strings generated by GPT-2.",
            "result_discussion": "Compared to strings generated by GPT-2, arbitrary strings showed log-perplexity several standard deviations away from the empirical entropy, indicating a difference that implies potential applications for AI text detection and model fingerprinting.",
            "ablation_id": "2405.13798v1.No2"
        }
    ]
}