{
    "title": "Is In-Context Learning Sufficient for Instruction Following in LLMs?",
    "abstract": "Recently, Lin et al. (2024) proposed Urial, a method using only three in-context examples to align base LLMs, achieving non-trivial instruction following performance. We show that, while effective, ICL alignment with Urial still underperforms on established benchmarks such as MT-Bench and AlpacaEval 2.0 (LC), especially with more capable base LMs. For tasks such as classification, translation, or summarization, adding more ICL demonstrations for long-context LLMs does not systematically improve instruction following performance. To address this limitation, we derive a greedy selection approach for ICL examples that noticeably improves performance. Finally, we provide a series of ablation studies to better understand the reasons behind the remaining gap and show how some aspects of ICL depart from the existing knowledge and are specific to the instruction tuning setting. Our work advances the understanding of ICL as an alignment technique.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The large-scale pre-training phase allows Large Language Models (LLMs) to acquire extensive knowledge and capabilities. However, these base models struggle to follow instructions directly from prompts, necessitating further fine-tuning to be used as conversational models. Inspired by Brown et al. (2020) who showed that LLMs can learn from demonstrations provided as part of the input\u2014a concept known as in-context learning (ICL)\u2014Lin et al. (2024) have recently studied the feasibility of in-context alignment. They found that including merely three carefully selected question-answer pairs in the prompt is sufficient to make base models follow instructions and interact with users at a similar level to instruction-finetuned models on their own benchmark. The possibility of customizing base models via ICL without changing weights is potentially game-changing due to its simplicity and flexibility.\n\nIn this work, we evaluate the Urial prompt strategy proposed by Lin et al. (2024) across several base models, including GPT-4-Base, on established instruction following benchmarks like MT-Bench (Zheng et al., 2023) and AlpacaEval 2.0 (Li et al., 2023). Although Urial achieves reasonable performance, it still lags behind instruction-finetuned models. In response, we test various strategies to improve Urial, particularly leveraging models with extensive context windows which allow for longer in-context prompts.\n\nFirst, we study the effect of many-shot in-context learning by adding high-quality demonstrations from existing instruction datasets. This approach, however, leads only to limited improvements compared to Urial, without a clear improvement trend for a large number of in-context examples. This is in contrast to many-shot ICL for tasks like summarization, translation, or classification, where providing many examples is clearly beneficial (Agarwal et al., 2024; Bertsch et al., 2024).\n\nThen we propose a simple greedy algorithm to select the in-context examples which optimize the MT-Bench score. This selection scheme allows us to nearly close the gap between Mistral-7b-v0.2 and Mistral-7b-Instruct-v0.2 by adding two additional demonstrations to the Urial prompt. Finally, we revisit the findings of Min et al. (2022) regarding what makes ICL effective: unlike for classification and multiple choice tasks, providing questions with correct answers is crucial for the effectiveness of ICL on instruction-following tasks. Furthermore, adding incorrect demonstrations further degrades the model performance.\n\nOverall, our work provides a more nuanced picture of ICL as an alignment technique compared to previous works."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Systematic evaluation of Urial",
            "text": "First, we compare the performance of several base models with the Urial in-context prompt. Table 1 shows the results on MT-Bench (Zheng et al., 2023), which is one of the most popular benchmarks for instruction following ability of LLMs. \n\nWe compare several LLMs of different sizes and capabilities, ranging from Llama-2-7B (Touvron et al., 2023) to the base GPT-4 model (OpenAI, 2023). \n\nWe observe that base models with Urial achieve competitive performance on the 1st-turn score, but cannot match the performance of their instruction fine-tuned counterparts in all cases except for Llama-2-70B and Mistral-7B-v0.1 (note that both were originally included in Lin et al. (2024), unlike most others). Conversely, the 2nd-turn score with Urial is significantly worse than for instruction-tuned LLMs. \n\nBecause of this, in the rest of the paper, we focus on the 1st-turn score and single-round conversations. We detail the breakdown of the results from Table 1 over categories in Table 5."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Many-shot ICL for instruction following",
            "text": "In the following, we explore whether we can close the gap between ICL and fine-tuning on instruction following tasks. In particular, we focus on Mistral-7B-v0.2 since this base model has a 32k context window which allows us to use many ICL examples. Moreover, we measure performance with both the MT-Bench 1st-turn score and the AlpacaEval 2.0 win rate, and report the results in Table 2.\n\nFirst, we test adding random demonstrations (after simple pre-filtering) to the three already included in Urial. For this, we score the quality of the examples in Evol-Instruct-70k with GPT-3.5-Turbo, and randomly sample demonstrations among the highest quality instructions (those with score at least 4.5 on a 0-5 scale). Table 2 shows that adding 17 or 37 random examples (which makes 20 and 40 examples in total) provides only moderate improvements, and such an increase quickly reaches a plateau, without clear benefits from scaling up to 50 demonstrations. We also test a more refined strategy: we sample instructions from OpenHermes, which includes annotations on the topic of each example, to ensure diversity between categories. This approach is however worse than Urial, regardless of the number of ICL examples used.\n\nFinally, inspired by the unsupervised ICL of Agarwal et al., we re-use the OpenHermes examples, this time without the answers (questions only) and adding instead the three Urial complete demonstrations. This strategy significantly improves the MT-Bench score and is slightly better than using the random examples from Evol-Instruct.\n\nNext, we try to greedily maximize the MT-Bench score by testing 100 high-quality instructions from Evol-Instruct-70k as the 4th additional example to Urial. For each resulting ICL prompt, we compute the MT-Bench score with GPT-4-Turbo instead of GPT-4 as the former is faster, cheaper, and mitigates potential overfitting to the exact benchmark score. In most cases, there is an improvement. Computing the true MT-Bench (GPT-4 as judge) with the best performing point as the 4th ICL example yields a significant improvement over Urial.\n\nWe then repeat this procedure sequentially to find a 5th and a 6th demonstration while restricting the search space at each round (by keeping only the instructions leading to a high enough MT-Bench score) to reduce the computational cost. This strategy achieves 7.69 MT-Bench score, almost closing the gap with Mistral-7B-Instruct-v0.2. In addition, it even improves AlpacaEval 2.0 win rate. This improvement indicates that the found IC prompt does not completely overfit to the MT-Bench score.\n\nFinally, we use the scores obtained in the first round of the greedy search to scale-up the number of ICL examples: the Evol-Instruct-Greedy curve is obtained by progressively adding the instructions with the highest scores. However, adding demonstrations in this way beyond the 6 examples does not consistently improve the MT-Bench score.\n\nTakeaways. These results show that, unlike in the setting of Agarwal et al.; Bertsch et al., simply scaling the number of ICL examples is not sufficient to consistently improve the instruction following performance. However, a greedy search approach can help to bridge the gap to the results of fine-tuned models. We highlight that our greedy search is performed with a limited computational cost, i.e., the set of candidate examples is small and the judge is slightly suboptimal (GPT-4-Turbo instead of GPT-4). We expect that more effective ICL prompts could be found with additional resources."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Understanding ICL for instruction following",
            "text": "Our analysis delves into the intricacies of alignment with a focus on the inherent challenges. We explore the diverse methodologies and their roles in addressing alignment issues. Specifically, we emphasize the significance of enhancing data quality and the selection process as pivotal factors in achieving improved alignment.\n\nFurthermore, the impact of model architecture and training regimes on alignment outcomes is explored. We identify key factors that contribute to performance variations across models, emphasizing the need for tailored architectural choices based on specific alignment objectives. The role of evaluation metrics is also discussed, highlighting how different metrics can skew the perceived success of alignment strategies.\n\nLastly, potential future directions for research are outlined, encouraging a continued focus on refining alignment strategies through innovative methodologies and robust evaluation techniques. This ongoing research is vital for advancing our understanding and capability in this complex area."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Importance of question-answer matching for in-context demonstrations",
            "text": "Min et al. (2022) showed that using demonstrations with random labels does not significantly impair the results of ICL on classification and multiple choice tasks. We conduct a similar study for instruction following and report the results of the 1st-turn MT-Bench score in Table 3. Denoting the set of in-context examples, with the query and the corresponding ground-truth answer, we test several configurations on two sets of examples: the 3 Urial examples and the 3 Urial examples with the 3 examples found by our greedy search. First, we do not use any demonstration, i.e., we directly use the original base model (Mistral-7B-v0.2-32k). As expected, this configuration performs poorly (2.93). Second, providing the questions without the answers leads to surprisingly good results (6 MT-Bench score). Third, we permute the answers with a circular shift of one position, so that all correct answers are still contained in the prompt but matched with the wrong question. This procedure significantly degrades the performance, especially with more examples (Urial + Greedy Search) where only a score of 1.80 is obtained (note that the minimum score is 1).\n\nNext, for each question, we sample a new answer from those provided for other instructions in the same category (in-domain). For example, a question about coding is paired with an answer from a different coding question. This approach ensures that while the content of the answer may be incorrect, its style and format remain appropriate. Although it is worse than the original one, this configuration achieves quite a good score (6.24 and 5.35). Finally, we sample answers from those of instructions from different, out-of-domain categories, so that even their style is incorrect compared to what implied by each question: this sampling leads to the worst instruction following results. These results show that not all the conclusions from Min et al. (2022) apply to ICL used for instruction following. Most importantly, using answers with correct content and, especially, correct style is crucial for the success of ICL. This property becomes even more evident when increasing the number of examples (to 6 instead of 3). In that case, not using the correct answers significantly degrades the performance. In turn, this result suggests that ICL for instruction following works differently compared to other less open-ended tasks."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Transferability of in-context examples",
            "text": "In Table 4, we report the performance of applying the in-context examples found by greedy search (added to Urial) on Mistral-7B-v0.2-32k to Llama-2-7B-80k and Mixtral-8x22B-v0.1-4bit. Adding the new examples does not provide a consistent improvement: while it can increase the MT-Bench score (+0.47 on Llama-2-7B-80k, +0.30 on Mixtral-8x22B-v0.1-4bit), it can also, in some cases, give worse results than the original Urial. Similarly, it yields mixed results when measured by win rate on AlpacaEval 2.0, which was not optimized by the greedy search. This analysis suggests that the most effective in-context learning demonstrations might vary across base LLMs, potentially because of differences in their pre-training. This could further explain why Urial underperforms on multiple models in Table 1, especially on those which have become available only recently and were not used for selecting the Urial examples."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusions and discussion",
            "text": "In this work, we have illustrated that In-Context Learning (ICL) via Urial provides a good baseline for instruction-following alignment, but presents a few limitations: it mostly performs slightly worse on single-turn conversations and does not generalize well to multi-round ones. \n\nFocusing on single-turn evaluation, we have shown that simply adding more in-context demonstrations is not sufficient to reliably improve performance, unlike what happens for less open-ended tasks. We have provided several analyses of why this could be the case, showing that ICL for instruction following presents different behavior compared to other tasks: in our case, the demonstrations need to be carefully chosen and of high quality, with correct answers to each question. \n\nWe conjecture that via ICL, the LLM can learn to infer the response style, but the overall learning signal is not sufficiently strong to benefit from a large amount of examples, despite the very long context windows which have become available only recently. Interestingly, a few incorrect ICL demonstrations are sufficient to severely degrade the performance. \n\nIn general, LLMs still require many different post-training steps beyond simple instruction following: domain-specific continued pre-training, long-context fine-tuning, fine-tuning for function calling, safety training against jailbreaks, prevention of prompt injections, etc. The difficulty of achieving even the baseline instruction-following alignment through ICL raises questions about the feasibility of obtaining more refined capabilities without fine-tuning."
        }
    ],
    "url": "http://arxiv.org/html/2405.19874v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "3"
        ],
        "main_experiment_and_results_sections": [
            "2",
            "3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.19874v1",
        "paper_title": "Is In-Context Learning Sufficient for Instruction Following in LLMs?",
        "research_background": "The paper investigates whether in-context learning (ICL) is sufficient for instruction following in large language models (LLMs). The motivation stems from the promising finding by Lin et al. (2024) that base LLMs can follow instructions with merely a few question-answer pairs included in the prompt. This method holds potential due to its simplicity and flexibility, enabling the customization of base models without modifying their weights.\n\n**Research Problem:**\nThe study evaluates the effectiveness of the Urial prompt strategy proposed by Lin et al. (2024) in comparison to traditional alignment methods like Supervised Fine-Tuning (SFT) and reinforcement learning (RLHF). Although Urial shows reasonable performance, it does not match the performance of instruction-finetuned models, prompting an investigation into strategies for improvement.\n\n**Relevant Prior Work:**\n1. **Traditional Alignment Methods:** \n   - Supervised Fine-Tuning (SFT) on instruction datasets (IFT, Chung et al., 2022).\n   - Fine-tuning using preference data (DPO and KTO, Rafailov et al., 2023; Ethayarajh et al., 2024).\n   - Reinforcement Learning from Human Feedback (RLHF), (Ouyang et al., 2022).\n\n2. **In-Context Learning (ICL):**\n   - Brown et al. (2020) demonstrated that LLMs could learn from demonstrations in the input (ICL).\n   - Lin et al. (2024) studied the feasibility of in-context alignment, showing base models can follow instructions using just three QA pairs in prompts but also highlighting limitations in performance.\n\n3. **Many-Shot ICL:**\n   - Research indicates that many-shot ICL generally benefits tasks like summarization, translation, and classification (Agarwal et al., 2024; Bertsch et al., 2024).\n\n4. **Effectiveness of ICL:**\n   - Min et al. (2022) identified that providing correct questions and answers is crucial for ICL\u2019s success in instruction tasks, and incorrect examples degrade performance.",
        "methodology": "The paper investigates the potential of in-context learning (ICL) to improve instruction-following capabilities in language models and to narrow the performance gap between ICL and fine-tuning. Specifically, the authors focus on the Mistral-7B-v0.2 model, primarily due to its competitive performance and its large context window (32k tokens), which makes it suitable for ICL with numerous examples.\n\nThe methodology includes evaluating the performance using two metrics\u2014MT-Bench 1st-turn score and AlpacaEval 2.0 win rate. The authors test different strategies for selecting in-context examples:\n\n1. **Random Demonstrations**: Initially, they experiment with adding random demonstrations from the Evol-Instruct-70k dataset. Here, the examples are filtered based on their quality scores, as judged by GPT-3.5-Turbo, and only high-quality instructions (scores \u2265 4.5) are selected.\n\n2. **Diverse Topic Sampling**: The authors attempt a more refined strategy by selecting examples from the OpenHermes dataset, ensuring a diverse range of topics.\n\n3. **Unsupervised Examples**: Inspired by previous unsupervised ICL methods, the authors reuse OpenHermes examples but include only the questions, paired with three complete demonstrations from Urial.\n\n4. **Greedy Search**: The authors use a greedy search method to iteratively add high-quality instructions to maximize the MT-Bench score.\n\nThe study concludes that simply increasing the number of ICL examples is not sufficient for consistent performance improvements. However, a greedy search approach can significantly enhance instruction-following capabilities, bridging the gap with fine-tuned models but with limited computational resources.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Experiment Setup:**\n\n1. **Datasets:**\n   - The primary dataset used for evaluation is MT-Bench (Zheng et al., 2023), a benchmark specifically designed to measure the instruction-following capabilities of large language models (LLMs).\n\n2. **Comparison Groups:**\n   - Various LLMs are compared, including models of different sizes and capabilities, such as Llama-2-7B (Touvron et al., 2023) and the base GPT-4 model (OpenAI, 2023).\n   - The models are compared in two settings:\n      - Base models with an in-context learning prompt referred to as Urial.\n      - Their instruction fine-tuned versions.\n\n3. **Evaluation Metrics:**\n   - The performance is measured using scores on MT-Bench, with a particular focus on the 1st-turn and 2nd-turn scores.\n   - The 1st-turn score evaluates the model's response to the initial prompt.\n   - The 2nd-turn score assesses performance in follow-up or multi-turn conversations.\n\n**Results:**\n\n- **1st-Turn Performance:**\n  - Base models with the Urial in-context prompt generally achieved competitive performance compared to their instruction fine-tuned versions, indicating that in-context learning can be effective for single-round conversations.\n  - Exceptions where base models with Urial matched or exceeded instruction fine-tuned versions in all cases include Llama-2-70B and Mistral-7B-v0.1. Notably, these models were originally included in a specific dataset tested by Lin et al. (2024), which may have influenced their performance.\n\n- **2nd-Turn Performance:**\n  - Base models with the Urial prompt performed significantly worse in 2nd-turn scores compared to instruction fine-tuned versions. This suggests that in-context learning alone is not sufficient for multi-turn conversations, which require multi-turn demonstrations to improve performance.\n\n**Conclusion:**\n- The study concludes that while in-context learning with Urial can yield competitive results for single-turn instruction following, it falls short for multi-turn conversations. Therefore, extending in-context learning for multi-round conversations remains an area for future work. The detailed breakdown of these results is provided in the supplementary tables."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To explore whether scaling the number of in-context learning (ICL) examples can close the performance gap between ICL and instruction fine-tuning on tasks using Mistral-7B-v0.2 as a base model.",
            "experiment_process": "The study uses Mistral-7B-v0.2 with a 32k context window. Performance is measured using the MT-Bench 1st-turn score and the AlpacaEval 2.0 win rate. Initial experiments involve adding random ICL examples from Evol-Instruct-70k, selected based on high quality. Additionally, examples from OpenHermes are tested with and without answers. Finally, a greedy search approach using high-quality instructions is applied, optimizing the MT-Bench score through sequential selection. The evaluation tools used include GPT-3.5-Turbo for quality scoring and GPT-4-Turbo for benchmark scoring.",
            "result_discussion": "Adding 17 or 37 random high-quality examples only provides moderate improvements that plateau beyond a certain point. The approach using OpenHermes examples was less effective. Conversely, a greedy search approach resulted in a higher MT-Bench score of 7.69, nearly closing the gap to the fine-tuned model's score of 8.06. This method also improved the AlpacaEval 2.0 win rate from 8.09 to 9.22. However, scaling up beyond the found 6 examples did not yield consistent improvement. The results indicate that simply increasing the number of ICL examples is insufficient for consistent enhancement, but a targeted greedy search can help bridge the performance gap.",
            "ablation_id": "2405.19874v1.No1"
        },
        {
            "research_objective": "To understand the importance of the matching of questions and answers in in-context demonstrations for instruction-following tasks.",
            "experiment_process": "The study evaluates several configurations of in-context examples using the MT-Bench 1st-turn score. The tested configurations include using no demonstrations, providing questions without answers, swapping answers with a circular shift, matching questions with in-domain but incorrect answers, and matching with out-of-domain answers. The examples used are the 3 Urial examples and the 3 additional examples found by the greedy search method.",
            "result_discussion": "Providing questions without answers yields unexpectedly good results, whereas misaligning answers, especially with more examples, significantly degrades performance. In-domain incorrect answers perform moderately well, but out-of-domain answers yield the worst results. These findings imply the critical importance of having correctly matched questions and answers, both in content and style, for instruction-following tasks. This suggests that ICL for instruction-following differs from more closed tasks like classification or multiple-choice.",
            "ablation_id": "2405.19874v1.No2"
        },
        {
            "research_objective": "To determine the transferability of in-context examples optimized on one base model to other large language models.",
            "experiment_process": "The study tests the ICL examples found by greedy search on Mistral-7B-v0.2-32k when applied to different models, including Llama-2-7B-80k and Mixtral-8x22B-v0.1-4bit. Performance metrics include the MT-Bench score and AlpacaEval 2.0 win rate.",
            "result_discussion": "Adding the optimized examples results in mixed performance: slight improvements in some cases (e.g., +0.47 on Llama-2-7B-80k) but also potential decreases. This suggests that the efficacy of ICL demonstrations may vary across different base models due to differences in pre-training. This variability might explain why certain methods like Urial underperform on newly available models that were not part of the initial selection.",
            "ablation_id": "2405.19874v1.No3"
        }
    ]
}