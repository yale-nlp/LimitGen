{
    "title": "Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models",
    "abstract": "We introduce Reka Core, Flash, and Edge, a series of powerful multimodal language models trained from scratch by Reka. Reka models are able to process and reason with text, images, video, and audio inputs. This technical report discusses details of training some of these models and provides comprehensive evaluation results. We show that Reka Edge and Reka Flash are not only state-of-the-art but also outperform many much larger models, delivering outsized values for their respective compute class. Meanwhile, our most capable and largest model, Reka Core, approaches the best frontier models (OpenAI, 2023; Google, 2023; Anthropic, 2024) on both automatic evaluations and blind human evaluations. On image question answering benchmarks (e.g., MMMU, VQAv2), Core performs competitively. On multimodal chat, Core ranks as the second most preferred model under a blind third-party human evaluation setup. On text benchmarks, Core not only performs competitively to other frontier models on a set of well-established benchmarks (e.g., MMLU, GSM8K) but also outperforms GPT4-0613 on human evaluation. Models are shipped in production at chat.reka.ai. A showcase of non cherry picked qualitative examples can also be found at showcase.reka.ai.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "This technical report details comprehensive evaluations of the Reka models (Core, Flash, Edge) on language and vision tasks along with discussions on development, benchmark design, and the training pipeline. \n\nReka Edge and Flash are dense models with 7B and 21B parameters, respectively. Our evaluation shows that these models are state-of-the-art for their compute class, often surpassing models much larger. Meanwhile, the current version of Reka Core approaches many of the best frontier models (OpenAI, 2023  ###reference_b22###; Google, 2023  ###reference_b16###; Google et al., 2023  ###reference_b15###; Anthropic, 2024  ###reference_b2###). It excels in both automated base model evaluations and blind third-party human evaluations. Figure 1  ###reference_### compares Reka models against proprietary large language models (LLM) APIs. We plot the price against performance, using MMLU score as an approximate indicator of model quality. All Reka models are positioned either on or beyond the Pareto frontier.\n\nOn language benchmarks, Reka Core achieves MMLU score and competitive GSM8K, HumanEval, and GPQA scores compared to other frontier models. On text-only chat, blind human evaluation shows that Reka Core ranks third on our internal ELO leaderboard. \n\nMeanwhile, our Edge (7B) model surpasses the current state-of-the-art models of this compute class, outperforming both Gemma 7B (Gemma et al., 2024  ###reference_b14###) and Mistral 7B (Jiang et al., 2023  ###reference_b20###). Additionally, the Flash (21B) model, aside from outperforming GPT-3.5 Turbo, also outperforms much larger state-of-the-art models such as Grok-1 (xAI, 2023  ###reference_b40###), Mistral Medium (Touvron et al., 2023  ###reference_b37###) and Gemini Pro 1.0 (Google, 2023  ###reference_b16###). On multimodal evaluations, Flash matches the Sonnet model on MMMU (Yue et al., 2024  ###reference_b41###). All in all, the Edge & Flash models are extremely powerful models on a compute-class basis.\n\nIn addition to comprehensive evaluations and benchmark evaluations on both language and vision (video + image) tasks, this report also shares some interesting technical details and behind-the-scenes of training large multimodal models as a startup. Areas discussed include infrastructure, data pipeline, compute, annotation pipelines, and more. Finally, artifacts of our models (playground/chat, developer platform) can be found in the following resource table (Table 1  ###reference_###)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Model",
            "text": "This section briefly describes the technical details behind these models. Our overall architecture (Figure 2) is a modular encoder-decoder architecture supporting text, image, video, and audio inputs. For now, our model only supports text outputs. The backbone Transformer model is based on the \u2019Noam\u2019 architecture, i.e., it uses SwiGLU (Shazeer, 2020), Grouped Query Attention (Ainslie et al., 2023; Shazeer, 2019), Rotary positional embeddings (Su et al., 2021) and RMSNorm (Zhang and Sennrich, 2019). Architecturally, this is similar to the PaLM architecture (Chowdhery et al., 2022) but without parallel layers. Reka Flash and Edge uses a sentencepiece vocab of 100K based on tiktoken. We add sentinel tokens for masking spans, i.e., <extra_id_0> and other special use cases such as tool-use that are beyond the scope of this technical report. Pretraining uses a curriculum that goes through multiple stages with different mixture distributions, context lengths, and objectives. The current version of this model is a dense model. Models are trained with bfloat16.\n\nOur standard models have a context length of 8K for our regular models. Reka Flash and Reka Core have K for long context models for retrieval and long document tasks. All our models pass needle-in-the-haystack (passkey retrieval) for the context they support. Based on these tests, our 128K models seem to extrapolate to K context length (but not beyond). For long context training, in addition to instruction tuning data we collect, we synthetically create supervised fine tuning data using our own suite of models by conditioning on long documents found in pretraining corpus using a technique we call reverse instruction tuning from long documents.\n\nAside from variances across clusters and providers, providers could also have high variance across time periods. For example, many compute providers have clusters that behave very differently in the first few weeks of handover or whenever the cluster undergoes a big change. Hence, we also compare the node failure rates during both the early phase and stable phase. More often than not, aside from early phase of handing over a cluster, provisioning new nodes can also introduce a new chaotic era that can last a few days or weeks. In general, we determined that a key factor influencing the difference between the early and stabilized phases is whether the cluster was actively used for distributed training by previous customers.\n\nWe built a custom inference stack for text and multi-modality running on a combination of A10s and A100s. We use Kubernetes as the underlying orchestration engine and manage several large clusters across different regions.\n\nAfter pretraining, our models are then instruction tuned (Wei et al., 2021; Ouyang et al., 2022; Chung et al., 2024) for multiple epochs using strong regularization. As for SFT data, we train on a mixture of datasets that include our proprietary and publicly available data. After SFT, models are then aligned with RLHF, specifically PPO (Schulman et al., 2017), using the same family of Reka models as the reward model. Our models go through a couple of rounds of RLHF in total. Moreover, our post-training process considers tool-use, function calling, and web search, which is out of scope for this technical report.\n\nWe collect data using external data collection companies and provide them with a user interface for annotating both text-only and multimodal data. We create an annotation UI for both collecting data and/or sending examples to human raters for blind human evaluation. This software also supports annotating for individual pointwise quality and also side-by-side (pairwise) evaluations. Our annotation software supports images, videos, and text-only prompts and responses. It also supports the annotation of multi-turn dialogues."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Training Data",
            "text": "The training data comprises a mixture of publicly available and proprietary/licensed datasets with a dataset knowledge cutoff of November 2023. The dataset ingested by our model comprises of text, images, videos, and audio clips. Reka Flash and Reka Edge were trained on approximately trillion and trillion extensively deduplicated and filtered language tokens, respectively. While the classification of corpora is not strictly defined to one class or category, approximately of our pretraining data is code related, and are STEM related. Approximately of the data is web crawl. About of our data has some relation to math. Overall mixture rates generally follow a principle of prioritizing unique tokens but are hand-adjusted using signal from a limited number of small scale ablations.\n\nMultilingual Data: Approximately of our pretraining data is explicitly (and deliberately) multilingual, comprising diverse languages tier-weighted (roughly by frequency in the wild). Beyond these explicitly up-weighted languages, we also train on the entire multilingual Wikipedia comprising of 110 languages so we expect a baseline performance for most languages. It is worth noting that these tiers reflect pretraining capability and not necessarily downstream post-training induced capabilities of the final model. To be concrete, these are meaningful to estimate the potential of a particular language, given suitable supervised fine tuning data. Languages included during pretraining are shown below.\n\nMultimodal Data: The multimodal training data comprises large collections of images, videos, documents, and webpages. The chosen data mixture is carefully optimized for quality, diversity, and scale."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Architecture & Modeling",
            "text": "This section introduces training details, model architecture, and context length details. Our overall architecture is a modular encoder-decoder design supporting text, image, video, and audio inputs, currently producing text outputs only. The backbone Transformer model utilizes the 'Noam' architecture, incorporating SwiGLU, Grouped Query Attention, Rotary positional embeddings, and RMSNorm. Architecturally, it is akin to the PaLM architecture but without parallel layers. Reka Flash and Edge employ a sentencepiece vocabulary of 100K, based on tiktoken. We include sentinel tokens for masking spans and other special use cases beyond this report's scope. Pretraining involves a curriculum with various stages, mixture distributions, context lengths, and objectives, and the model is dense. Training is conducted with bfloat16 precision.\n\nOur standard models have a context length of 8K, while Reka Flash and Reka Core extend to longer contexts for tasks like retrieval and handling long documents. All models successfully pass the needle-in-the-haystack test for their respective context lengths. For long context training, alongside instruction tuning data, we generate synthetic supervised fine-tuning data using our models. This process involves conditioning on long documents from the pretraining corpus in a method termed reverse instruction tuning from long documents."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Compute & Infrastructure",
            "text": "Our family of Reka models was trained predominantly on Nvidia H100s using Pytorch (Paszke et al., 2019). Our setup comprises clusters from a mixture of vendors with our peak compute being approximately K Hs and K As. Our peak number of clusters is . About more than of our compute came online in mid-December 2023. Reka Flash and Edge were trained on several hundreds of H100s across a period of several weeks. Our pretraining process was relatively smooth with very few loss spikes despite very aggressive learning rates even for much larger models. Figure 3 shows the training loss for Reka Core. To improve the I/O of our clusters, especially for scalable training with multimodal inputs, we used the Ceph filesystem for distributed and scalable data storage across nodes which improved I/O substantially but came with maintenance overheads.\n\nAside from variances across clusters and providers, providers could also have high variance across time periods. For example, many compute providers have clusters that behave very differently in the first few weeks of handover or whenever the cluster undergoes a big change. Hence, we also compare the node failure rates during both the early phase and stable phase. More often than not, aside from early phase of handing over a cluster, provisioning new nodes can also introduce a new chaotic era that can last a few days or weeks. In general, we determined that a key factor influencing the difference between the early and stabilized phases is whether the cluster was actively used for distributed training by previous customers.\n\nWe built a custom inference stack for text and multi-modality running on a combination of A10s and A100s. We use Kubernetes as the underlying orchestration engine and manage several large clusters across different regions."
        },
        {
            "section_id": "2.3.1",
            "parent_section_id": "2.3",
            "section_name": "2.3.1 Hardware lottery and node stability.",
            "text": "Generally, we find great unreliability when it comes to GPU nodes which often fail due to hardware errors or connection issues. Moreover, reliability among providers is generally of high variance. For more details, refer to Tay (2024  ###reference_b36###). To expand upon Tay (2024  ###reference_b36###), we report the average number of node failures across four anonymized providers, as shown in Table 4  ###reference_###. Since the likelihood of node failures is influenced by the number of nodes concurrently used for training, we report estimated failure rates for different configurations.\n\nAside from variances across clusters and providers, providers could also have high variance across time periods. For example, many compute providers have clusters that behave very differently in the first few weeks of handover or whenever the cluster undergoes a big change. Hence, we also compare the node failure rates during both the early phase and stable phase. More often than not, aside from early phase of handing over a cluster, provisioning new nodes can also introduce a new chaotic era that can last a few days or weeks. In general, we determined that a key factor influencing the difference between the early and stabilized phases is whether the cluster was actively used for distributed training by previous customers.\n\n###table_6### We built a custom inference stack for text and multi-modality running on a combination of A10s and A100s. We use Kubernetes as the underlying orchestration engine and manage several large clusters across different regions."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Post-Training",
            "text": "This section describes the post-training process which involves aligning and instruction tuning the model. After pretraining, our models are then instruction tuned for multiple epochs using strong regularization. As for SFT data, we train on a mixture of datasets that include our proprietary and publicly available data. After SFT, models are then aligned with RLHF, specifically PPO, using the same family of Reka models as the reward model. Our models go through a couple of rounds of RLHF in total. Moreover, our post-training process considers tool-use, function calling, and web search, which is out of scope for this technical report.\n\nWe collect data using external data collection companies and provide them with a user interface for annotating both text-only and multimodal data. We create an annotation UI for both collecting data and/or sending examples to human raters for blind human evaluation. This software also supports annotating for individual pointwise quality and also side-by-side (pairwise) evaluations. Our annotation software supports images, videos, and text-only prompts and responses. It also supports the annotation of multi-turn dialogues."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Evaluation",
            "text": "This section discusses the results of extensive evaluations of Reka models. We compare our models on four language model evaluations: 1) MMLU (general language understanding and question answering) (Hendrycks et al., 2021), 2) GSM8K (reasoning and arithmetic) (Cobbe et al., 2021), HumanEval (code generation) (Chen et al., 2021), and GPQA (graduate-level question answering) (Rein et al., 2023). All numbers from baselines are reported numbers in other works. MMLU is evaluated with 5-shot direct prompting for all models. For GSM8K, most models use 8-shot chain-of-thought (Wei et al., 2022) and majority voting (maj@8). For HumanEval, this is evaluated in 0-shot setup. All results from other models are reported from other works.\n\nWe compare our models using visual question answering datasets, i.e., MMMU (Yue et al., 2024), VQAv2 (Goyal et al., 2017), and Perception-Test (P\u0103tr\u0103ucean et al., 2023) for video question answering. For Reka models, all results are 0-shot. Table 5 reports comparisons of Reka Core against other frontier-class models. Overall, Reka Core performs competitively with other frontier-class models. Reka Core is still improving so we expect better results in the near future."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Base Model Evaluation",
            "text": "We conduct a series of language-only and multimodal (image, video input) evaluations. We compare our models on four language model evaluations: 1) MMLU (general language understanding and question answering), 2) GSM8K (reasoning and arithmetic), HumanEval (code generation) and GPQA (graduate-level question answering). All numbers from baselines are reported numbers in other works. MMLU is evaluated with 5-shot direct prompting for all models. For GSM8K, most models use 8-shot chain-of-thought and majority voting (maj@8). For HumanEval, this is evaluated in 0-shot setup. All results from other models are reported from other works.\n\nWe compare our models using visual question answering datasets, i.e., MMMU, VQAv2, and Perception-Test for video question answering. For Reka models, all results are 0-shot. Table 5 reports comparisons of Reka Core against other frontier-class models. Overall, Reka Core performs competitively with other frontier-class models. Reka Core is still improving so we expect better results in the near future."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Chat Model Evaluation",
            "text": "We conduct a blind evaluation with human raters from a third-party data provider company. We consider two setups: 1) multimodal chat, where the user asks a question about an image, and 2) text-only chat. We next detail our evaluation protocol and present results for each setting."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Evaluation Setup",
            "text": "For each annotation instance, human raters are given a prompt along with a maximum of 4 generations from different models, and asked to rate the answers according to the guidelines provided. Given that the number of models in our evaluation is higher than 4, we collect multiple such annotations for each prompt, each with a different subset of models. The pairing of models is decided randomly for each prompt, with all combinations being equally likely. We compute ELO scores following Askell et al. (2021), where we only consider pairwise comparisons where annotators express a preference stronger than the weakest available.\n\nWe design our evaluation dataset to cover a diverse set of prompts. The following table details the composition of our text-only evaluation set, which comprises 1K+ prompts:\n\nSimilarly, the following table reports the categories covered by our multimodal evaluation set:"
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Multimodal Chat Evaluation",
            "text": "We next report the results of our multimodal chat evaluation in comparison with Gemini Pro, IDEFICS 80B, Adept Fuyu 8B, and the strongest Llava 1.6B model: ###table_11### We find that Reka Core outperforms all models except by a substantial margin. Reka Flash ranks next, performing marginally better than Opus. Reka Edge outperforms IDEFICS 80B and Adept Fuyu 8B by a large margin, approaching the performance of Gemini Pro and the largest Llava 1.6 model."
        },
        {
            "section_id": "3.2.3",
            "parent_section_id": "3.2",
            "section_name": "3.2.3 Text-only Chat Evaluation",
            "text": "We compare our models against different versions of GPT, Llama 2 Chat, and Gemini Pro (API version), and report our results next:\n###table_12### We find that Reka Core ranks competitively on our ELO leaderboard, and it is only surpassed by GPT-4 Turbo and Claude 3 Opus. Reka Flash obtains strong results for its size, beating GPT-3.5 Turbo and the much larger Llama 2 Chat 70B."
        },
        {
            "section_id": "3.2.4",
            "parent_section_id": "3.2",
            "section_name": "3.2.4 Model development and automatic evaluation using Reka Core",
            "text": "We leverage the frontier-class capabilities of Reka Core for model selection and development and show an example of how we use it for multimodal chat. We ask Reka Core to simulate human judgment by rating a response with respect to a prompt and a reference answer. We find that Reka Core rankings across models correlate to human judgment despite the gap between pointwise and pairwise (arena style) evaluations. Our general workflow is that we perform lightweight and simple pointwise evaluations for continuous sanity checks before sending our models for third party blind human evaluations.\n\nFigure 4 reports the Reka Core scores we obtain right before producing Table 8. Despite Reka Core evaluations being pointwise, we find that it is able to accurately approximate the final rankings. Here, the only key difference is that Reka Flash and Claude Opus have flipped rankings. In practice, these models may be very similar in performance that it could go either way. In Table 8, we also note that Reka Flash and Claude Opus have very similar win rates and ELO scores, which is well reflected by their Reka Core scores being very close as well. Overall, we find that Reka Core is quite a good approximator of final human evaluation outcomes."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Cross-lingual Evaluations",
            "text": "We conduct experiments on a suite of general multilingual benchmarks such as multilingual commonsense (XStoryCloze), causal reasoning (XCOPA), question answering (Belebele, XQuAD, TydiQA). For all datasets, we report the mean across all languages. We compare our models with Llama 2 70B and GPT-3.5. All evaluations are zero-shot generative except XStoryCloze which uses log-likelihood evaluation.\n\nGenerally, we find that Reka Core outperforms all baselines reliably on most tasks. Specifically, Reka Core outperforms XCOPA, XQuAD, and TydiQA but is outperformed on XWinograd and TydiQA (w/o context). Meanwhile, Core outperforms Flash on all benchmarks. Both Flash and Core outperform Llama-2 70B and GPT-3.5."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Long Context Question Answering",
            "text": "We conduct a series of evaluations on long context question answering. We use internal benchmarks in two domains: (1) movie plots and (2) ToS (terms-of-service) contracts with contexts in the ballpark of K tokens. Both datasets are question answering tasks where the task is to answer questions given a long document. \n\nResults on long context question answering using internal evaluation datasets show that Flash and Core are competitive models."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Medical Reasoning",
            "text": "We compare our Reka models against state-of-the-art domain-specific medical models such as Meditron (Chen et al., 2023) and Med-PaLM-2 (Singhal et al., 2023). We compare on three benchmarks: MedMCQA, PubMedQA, and MMLU (Medical). MMLU medical is a macro-average over clinical knowledge, medical genetics, anatomy, professional medicine, college biology, and college medicine. \n\nTable 13 reports results on medical tasks. Meditron and Med-PaLM-2 are specialized models for medicine. Our results show that Reka Core is competitive with some of the best frontier models and specialized models in medicine. Firstly, Reka Flash and Core outperform the Meditron series. Secondly, Reka Core outperforms both Med-PaLM-2 on MedMCQA. However, it is outperformed on PubMedQA. Finally, on MMLU (Medical), Reka Core outperforms Med-PaLM-2. Overall, on average, Reka Core outperforms Med-PaLM-2 on medical tasks."
        },
        {
            "section_id": "3.6",
            "parent_section_id": "3",
            "section_name": "Detailed comparisons of Edge and Flash",
            "text": "We report detailed results of Reka Edge and Flash against other models of similar compute class. Notably, both Edge and Flash have been improved quite substantially since the initial release in Feb. Hence, numbers have been upgraded since their first appearances."
        },
        {
            "section_id": "3.6.1",
            "parent_section_id": "3.6",
            "section_name": "3.6.1 Reka Edge results",
            "text": "We report results of Reka Edge against other 7B models such as Llama 2 (Touvron et al., 2023 ###reference_b37###), Mistral (Jiang et al., 2023 ###reference_b20###) and Gemma (Gemma et al., 2024 ###reference_b14###).\n\nTable 14 ###reference_### reports results of Reka Edge against other 7B models (Gemma, Mistral, Llama). We observe that Reka Edge has an edge against all other models (no pun intended). It outperforms Mistral 7B and Llama 7B on all 8 benchmarks. As for Gemma, it outperforms Gemma for all benchmarks except MATH. Overall, Reka Edge is a super strong model at 7B scale."
        },
        {
            "section_id": "3.6.2",
            "parent_section_id": "3.6",
            "section_name": "3.6.2 Reka Flash results",
            "text": "Given that there are not many good models around the same compute class as Reka Flash, we compare Reka Flash with models that are much larger. Specifically, Llama 2 70B (Touvron et al., 2023  ###reference_b37###), Gemini Pro 1.0 (Google, 2023  ###reference_b16###), Mistral Medium (Touvron et al., 2023  ###reference_b37###) and Grok-1 (xAI, 2023  ###reference_b40###).\n\n###table_18### Table 15  ###reference_### reports results of Flash (21B) against other models of larger compute class. All competitors are approximately around 70B parameters with the exception of Grok-1 which is a sparse model with 314 billion parameters. We see that Flash outperforms (or is competitive to) all competitors on most benchmarks despite being much smaller."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduce a new series of powerful multimodal models, namely Reka Core, Flash, Edge. Reka Flash and Edge sets a new state-of-the-art on a compute-class basis, often delivering massive outsized value for their scale. Our Core model approaches frontier-class models on both human evaluation and automatic benchmarks. Reka Core is still improving so we expect to see even more improvements in the medium term. The field of large language models (Radford et al., 2018; Brown et al., 2020; Devlin et al., 2018; Raffel et al., 2019; Chowdhery et al., 2022; Hoffmann et al., 2022) is still nascent but moving very quickly. With that comes the trade-off of significant amount of noise in the landscape. We hope this technical report shows the rigor of what it takes to build frontier-class models from scratch given limited resources."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Appendix",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "MMMU breakdown",
            "text": "In Table 16  ###reference_###, we report our category-level scores in MMMU (Yue et al., 2024  ###reference_b41###) for Reka Core.\n###table_19###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Historic versioning, changelog and timeline of Reka Chat models",
            "text": "We include the version history of Reka models to easily refer to them across this tech report.\n###table_20###"
        }
    ],
    "url": "http://arxiv.org/html/2404.12387v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.3.1",
            "2.4"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.2.3",
            "3.2.4",
            "3.3",
            "3.4",
            "3.5",
            "3.6",
            "3.6.1",
            "3.6.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2.1",
            "3.6.1",
            "3.6.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.12387v1",
        "paper_title": "Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models",
        "research_background": "### Motivation\nThe paper is motivated by the need to develop high-performance multimodal language models that can effectively handle both language and vision tasks. Current state-of-the-art models often come with a significant computational cost, making them inaccessible or impractical for many users. By introducing the Reka series of models (Core, Flash, Edge), the authors aim to present powerful alternatives that provide high performance at a more efficient compute cost, thus pushing forward the frontier of what's possible with multimodal models in terms of both capability and accessibility.\n\n### Research Problem\nThe research problem addressed in this paper revolves around designing and evaluating advanced multimodal language models that can compete with, and even surpass, existing state-of-the-art models across various benchmarks and evaluation metrics. Specifically, the authors aim to:\n1. Develop multimodal models\u2014Reka Core, Flash, and Edge\u2014at different parameters sizes (7B, 21B) to assess their performance against larger and more computationally intensive models.\n2. Validate these models through rigorous benchmarking and comprehensive evaluations across language and vision tasks.\n3. Share technical insights and methodologies behind training these high-performance models as a startup, highlighting infrastructure, data pipelines, and compute resources.\n\n### Relevant Prior Work\nThe paper situates the Reka models within established research and existing state-of-the-art models by comparing their performance against several well-known models:\n1. **Frontier Models**: The Reka Core is compared to top-tier models such as GPT-4V from OpenAI and Gemini Ultra from Google, with references to prominent publications in 2023 and 2024.\n2. **Claude 3 Models**: Reka Core\u2019s performance is evaluated against various Claude 3 models (Opus, Sonnet, Haiku) from Anthropic, established as strong multimodal models.\n3. **GPT-4 Turbo, Claude 3 Opus**: The evaluation also includes robust text-only chat models, demonstrating Reka Core\u2019s high ranking on text tasks.\n4. **Compute-Class Comparisons**: Both Edge and Flash models are specifically evaluated against other models in their compute classes, such as Gemma 7B, Mistral 7B, Grok-1, and Mistral Medium, indicating that Reka models often surpass these peers.\n5. **Multimodal Performance**: Reka Flash's multimodal evaluation against models like Claude 3 Opus and Sonnet underscores its strong performance in tasks that involve both text and vision.\n\nThe references to the works of OpenAI, Google, Anthropic, among others, establish the context of high-performance language models and underscore the advanced nature of the new Reka series in this competitive landscape.",
        "methodology": "The proposed method and model, known as Reka Core, Flash, and Edge, are described as powerful multimodal language models with several innovative components and techniques.\n\n**Overall Architecture**\n- **Modular Encoder-Decoder:** The architecture supports various inputs including text, image, video, and audio, but currently only generates text outputs.\n- **Backbone Transformer Model:** \n  - Based on the 'Noam' architecture.\n  - Integrates several advanced components:\n    - **SwiGLU (Shazeer, 2020)**\n    - **Grouped Query Attention (Ainslie et al., 2023; Shazeer, 2019)**\n    - **Rotary Positional Embeddings (Su et al., 2021)**\n    - **RMSNorm (Zhang and Sennrich, 2019)**\n  - Architecturally similar to the PaLM architecture but excludes parallel layers.\n  \n**Tokenization and Sentinel Tokens**\n- Utilizes a **sentencepiece vocabulary of 100K** similar to the GPT-4 tokenizer, named tiktoken.\n- Incorporates **sentinel tokens** such as `<extra_id_0>` for masking spans and for special use cases like tool-use.\n\n**Pretraining**\n- Employs a multi-stage curriculum with varying mixture distributions, context lengths, and objectives.\n- Uses a dense model format and employs **bfloat16** for training.\n\n**Context Length**\n- Standard models support a context length of **8K**.\n- Reka Flash and Reka Core offer extended context lengths for tasks involving retrieval and long documents.\n- The models have been tested to pass needle-in-the-haystack retrieval for their respective context lengths.\n- Extended context training involves **reverse instruction tuning** from long documents gathered during pretraining.\n\n**Fault Tolerance**\n- Experiments also analyze node failure rates during early and stable phases, finding significant differences likely influenced by the cluster's distributed training history.\n\n**Inference and Training Infrastructure**\n- The custom inference stack runs on a combination of **A10s and A100s** GPUs.\n- Managed using **Kubernetes** for orchestration across large clusters in different regions.\n\n**Instruction Tuning**\n- After pretraining, models are instruction tuned using various datasets for multiple epochs with strong regularization.\n- Utilizes a combination of proprietary and publicly available datasets for Supervised Fine Tuning (SFT).\n\n**Reinforcement Learning with Human Feedback (RLHF)**\n- Following instruction tuning, models undergo a couple of RLHF rounds using PPO (Proximal Policy Optimization), with the same family of Reka models serving as the reward model.\n\n**Tool-Use and Function Calling**\n- Post-training also considers tool-use, function calling, and web search capabilities.\n\n**Data Collection and Annotation**\n- Data annotated via external companies using a custom-built annotation user interface.\n- Supports multi-modal annotation, including text, images, and videos, as well as multi-turn dialogues.\n- Enables both pointwise and side-by-side (pairwise) evaluations for quality assessment.\n\nIn summary, the Reka series combines state-of-the-art transformer components, a sophisticated pretraining and tuning regimen, extended context capabilities, detailed fault tolerance, and in-depth annotation processes to build versatile and robust multimodal language models.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets:**\n1. **Language Model Evaluations:**\n   - **MMLU** (general language understanding and question answering)\n   - **GSM8K** (reasoning and arithmetic)\n   - **HumanEval** (code generation)\n   - **GPQA** (graduate-level question answering)\n\n2. **Visual Question Answering Datasets:**\n   - **MMMU**\n   - **VQAv2**\n   - **Perception-Test** (for video question answering)\n\n**Baselines:**\n- Comparisons are made against various frontier-class models, including:\n  - GPT-4444 (likely an older version of GPT-4)\n  - Recent Claude 3 release\n  - Gemini models (Pro 1.5, Ultra)\n\n**Evaluation Metrics:**\n- **MMLU**: 5-shot direct prompting\n- **GSM8K**: 8-shot chain-of-thought and majority voting (maj@8)\n- **HumanEval**: 0-shot setup\n- **Visual Question Answering**: 0-shot setup for all datasets\n\n### Main Experimental Results:\n\n- **Reka Core Performance:**\n  - Generally competitive with other frontier-class models.\n  - Comparable to GPT-4444 (older version) in most metrics except MMLU.\n  - Positioned between Opus and Sonnet models in overall performance relative to the Claude 3 series.\n  - Mixed results when compared to Gemini models:\n    - Outperforms Gemini Pro 1.5 on MMLU, GSM8K, and HumanEval.\n    - Outperformed by Gemini Pro 1.5 on GPQA and MMMU.\n  - Outperforms Gemini Ultra (and Pro 1.5) on video question answering (MMMU, VQAv2, and Perception-Test).\n\n**Note:** The results indicate that Reka models, particularly Core and Flash, demonstrate strong performance in comparison with various competitive models and are expected to improve further."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the performance of Reka Edge against other 7B models in various benchmarks.",
            "experiment_process": "Reka Edge is compared against Llama 2 (Touvron et al., 2023), Mistral (Jiang et al., 2023), and Gemma (Gemma et al., 2024) on 8 different benchmarks. The specific models used for comparison are selected based on their 7B parameter size, making them a relevant size class for comparison.",
            "result_discussion": "Reka Edge outperforms Mistral 7B and Llama 7B on all eight benchmarks and outperforms Gemma on all benchmarks except MATH. This demonstrates that Reka Edge is a robust model at the 7B scale, showing superior performance compared to other models of the same class.",
            "ablation_id": "2404.12387v1.No1"
        },
        {
            "research_objective": "To assess the performance of Reka Flash by comparing it to models with a significantly larger compute class.",
            "experiment_process": "Reka Flash (21B parameters) is compared against several models that are much larger in compute class: Llama 2 70B (Touvron et al., 2023), Gemini Pro 1.0 (Google, 2023), Mistral Medium (Touvron et al., 2023), and Grok-1 (xAI, 2023). These competitor models have approximately 70B parameters, except for Grok-1, which is a sparse model with 314 billion parameters. Benchmarks used for comparison are unspecified but span a range of tasks to provide a comprehensive evaluation.",
            "result_discussion": "Despite being much smaller in size, Reka Flash outperforms (or is competitive with) all comparison models across most benchmarks. This highlights the efficiency and effectiveness of Reka Flash, demonstrating that smaller models can deliver substantial performance benefits.",
            "ablation_id": "2404.12387v1.No2"
        }
    ]
}