{
    "title": "PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking",
    "abstract": "Linking (aligning) biomedical concepts across diverse data sources enables various integrative analyses but is challenging due to the discrepancies in concept naming conventions. Various strategies have been developed to overcome this challenge, such as those based on string-matching rules, manually crafted thesauri, and machine learning models. However, these methods are constrained by limited prior biomedical knowledge and can hardly generalize beyond the limited amounts of rules, thesauri, or training samples. Recently, large language models (LLMs) have exhibited impressive results in diverse biomedical NLP tasks due to their unprecedentedly rich prior knowledge and strong zero-shot prediction abilities. However, LLMs suffer from issues including high costs, limited context length, and unreliable predictions. In this research, we propose PromptLink, a novel biomedical concept linking framework that leverages LLMs. It first employs a biomedical-specialized pre-trained language model to generate candidate concepts that can fit in the LLM context windows. Then it utilizes an LLM to link concepts through two-stage prompts, where the first-stage prompt aims to elicit the biomedical prior knowledge from the LLM for the concept linking task and the second-stage prompt enforces the LLM to reflect on its own predictions to further enhance their reliability. Empirical results on the concept linking task between two EHR datasets and an external biomedical KG demonstrate the effectiveness of PromptLink. Furthermore, PromptLink is a generic framework without reliance on additional prior knowledge, context, or training data, making it well-suited for concept linking across various types of data sources. The source code of this study is available at https://github.com/constantjxyz/PromptLink.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "Biomedical concept linking addresses the complex task of aligning related concepts across different data sources by using their semantic meanings and underlying biomedical knowledge (Sevgili et al., 2022  ###reference_b30###). This linking process is essential for integrating analyzes, as diverse biomedical concepts provide a multifaceted view of biomedical knowledge and data (Su et al., 2023  ###reference_b33###; Lu et al., 2023  ###reference_b20###). For instance, electronic health records (EHRs) are valuable assets for patient health analysis, containing a range of digital medical information such as tabular data and clinical notes (Abul-Husn and Kenny, 2019  ###reference_b2###; Sun et al., 2018  ###reference_b34###; Xu et al., 2022  ###reference_b40###). Similarly, knowledge graphs (KGs) play a critical role in biomedical research, offering structured knowledge like definitions of concepts and their relationships (Ma et al., 2018  ###reference_b22###).\n\nDespite their importance, cross-source biomedical linking is challenging due to variations in naming conventions across systems (Kohane et al., 2021  ###reference_b16###). For example, a KG might list a disease as \u201cEllis-Van Creveld syndrome\u201d, while an EHR may use \u201cChondroectodermal dysplasia\u201d to refer to the same condition, creating barriers to cohesive data analysis. \n\nConventional methods have targeted these challenges by establishing string-matching rules (D\u2019Souza and Ng, 2015  ###reference_b8###; Kang et al., 2013  ###reference_b14###) and using constructed thesauri (Aronson and Lang, 2010  ###reference_b4###; Savova et al., 2010  ###reference_b28###; Friedman et al., 2001  ###reference_b10###), but their dependence on fixed rules limits their adaptability in real-world conditions (Shi et al., 2023  ###reference_b31###).\n\nTo address these issues, machine learning (ML) methods have been explored, which avoid manually designing rules or thesauri by converting biomedical concepts from raw text into embeddings (latent vector representations) that are then used for computing similarity scores with learning-based scoring functions. Various models have been developed to obtain these embeddings, particularly pre-trained language models (PLMs) (Wang et al., 2023b  ###reference_b35###) that capture detailed semantic relations through extensive training on biomedical corpora (Xu et al., 2020  ###reference_b39###; Lee et al., 2020  ###reference_b17###; Alsentzer et al., 2019  ###reference_b3###; Liu et al., 2021  ###reference_b18###), and graph neural networks (GNNs) (Zhou et al., 2020  ###reference_b43###) which also understand the relationships of biomedical concepts (Bordes et al., 2013  ###reference_b5###; Grover and Leskovec, 2016  ###reference_b11###; Liu et al., 2022  ###reference_b19###).\n\nDespite successes, these ML methods demand extensive data and supervision signals, making them challenging due to expensive data annotation and training processes. Recently, large language models (LLMs) have demonstrated impressive performances in various NLP tasks due to their rich prior knowledge and language capabilities (Zhou et al., 2023  ###reference_b44###; Singhal et al., 2023  ###reference_b32###; Wang et al., 2023a  ###reference_b36###), supporting applications in zero-shot learning settings (Lu et al., 2023  ###reference_b20###).\n\nLLMs' deployment in this task continues to face challenges like designing effective prompts within context length limits (Zhang et al., 2023  ###reference_b41###), and NIL prediction capability for reliably excluding incorrect candidates (Peters et al., 2019  ###reference_b24###).\n\nThis paper presents PromptLink, utilizing LLMs for cross-source biomedical concept linking. PromptLink employs a novel two-stage prompting mechanism in GPT-4 to ensure reliable linking. The first stage filters out irrelevant candidates, minimizing token responses needed for the second stage, which generates final linking results and incorporates a self-verification prompt to improve NIL prediction accuracy. In experimental evaluation, PromptLink significantly outperforms existing methods, showing intrinsic strong biomedical knowledge and generalizing without requiring training processes. As a zero-shot and versatile framework, PromptLink adapts to various biomedical research and application needs effectively."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Biomedical Concept Linking",
            "text": "I'm sorry, but it seems like there is no specific text from the section of the scientific paper provided in your prompt. Could you please provide the specific text or section that you would like me to revise?"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. Problem Definition",
            "text": "The biomedical concept linking task aims to link biomedical concepts across various sources based on semantic meanings and biomedical knowledge, relying solely on concept names. This approach can cover a broad range of real-world applications. It differs from tasks like entity linking, entity alignment, and ontology matching, which depend on additional contextual or topological information.\n\nIn this study, we focus on linking concepts from Electronic Health Records (EHR) to corresponding concepts in a biomedical Knowledge Graph (KG). We define an EHR database and a biomedical KG, along with the linking task, as follows:\n\nAn EHR database is a relational database consisting of patient identifiers, patient attributes, and the values of these attributes. There are also multi-token biomedical concepts associated with patient attributes.\n\nA biomedical KG is a multi-relation graph comprising concepts, relation names, and the relational triples among them.\n\nThe task involves linking identified biomedical concepts from an EHR to a biomedical KG based on their semantic meanings and biomedical knowledge, thereby forming linkages. If a concept from the EHR is not found in the KG, it is linked to a special \u201cNIL\u201d entity, indicating that it is unlinkable."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. PromptLink",
            "text": "We propose PromptLink, a novel LLM-based solution for cross-source biomedical concept linking. Addressing LLMs\u2019 high cost and limited input text length, we employ a two-stage prompting mechanism with GPT-4 to generate the final linking predictions.\n\nLinking prediction using two-stage prompts.\n\nThe next step of our framework is generating linking predictions of query using the GPT-4 model, leveraging its text comprehension ability, logical reasoning ability, and prior biomedical knowledge. In this step, we design a novel two-stage prompt for our task.\n\nCombining the two prompts utilizes their strengths and mitigates weaknesses. The first stage focuses on concept pairs to filter out unrelated candidates. The second stage evaluates all candidates in a broader context to identify the closest match or reject all unmatched candidates.\n\nIn the first stage, the LLM is prompted to check if a concept pair should be linked. By defining the response structure, the LLM can return answers in specified formats. To improve the prompt response quality, we adopt the self-consistency prompting strategy that repeatedly prompts the same question to the LLM multiple times.\n\nSpecifically, we prompt each concept pair for multiple times, thus obtaining the belief score. Considering the belief scores across different candidates, we derive a comprehensive filter strategy to exclude irrelevant candidates. This approach ensures that irrelevant candidates are not considered in the next stage, optimizing both efficiency and effectiveness. The approach is described as follows:\n\nIf the similarity indicates some candidates closely align with the query concept, in such cases, candidates with belief scores of zero will be filtered out as they are deemed irrelevant to the query concept. This filtering strategy effectively removes many irrelevant candidates, thereby optimizing efficiency and effectiveness for the subsequent stage.\n\nOtherwise, the range of different candidates\u2019 belief scores is not wide enough to justify filtering. Thus, all candidates will be subjected to double-checking by the second-stage prompt.\n\nIn the second stage, the LLM evaluates the candidates retained from the first stage\u2019s filtering process using a compositional prompt that consists of two consecutive questions to perform complex reasoning.\n\nSpecifically, the LLM is asked to (1) label the relationship between the query concept and all candidate concepts as \u201cexact match\u201d, \u201crelated to\u201d, or \u201cdifferent from\u201d and (2) use self-verification prompts to either identify the closest candidate or dismiss all candidates if none are close. Thus, the final concept linking result of this prompt is usually one item from the candidates.\n\nIn this stage, we also use the self-consistency strategy that prompts the same question multiple times. Subsequently, we calculate the occurrence frequency for answers and retrieve the final linking result for the query EHR concept as follows:\n\nIf the result indicates a high probability that none of the candidates are appropriate, \u201cNIL\u201d is chosen as the final linking prediction.\n\nOtherwise, the candidate with the highest frequency is decided as the final linking result. If two candidates tie for the highest frequency, the one with higher alignment to the query concept is chosen."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Experiments & Discussions",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Implementation Details",
            "text": "Datasets.\nIn our experiments, we curate two biomedical concept linking benchmark datasets: MIID (MIMIC-III-iBKH-Disease) and CISE (CRADLE-iBKH-Side-Effect).\nMIID comprises 1,493 diagnosis concepts from MIMIC-III, which is an EHR dataset including over 53,423 hospital patient records, and 18,697 disease concepts from iBKH, which is a KG dataset with 2,384,501 entities. To construct MIID, we first remove exact matches between MIMIC-III diagnosis concepts and iBKH disease concepts. Then, we link the remaining MIMIC concepts to iBKH using ICD-9 and UMLS CUI codes. We use the linked concept pairs as ground-truth labels only for evaluation purposes.\nCISE contains 1,500 CRADLE diagnosis concepts and 4,251 iBKH drug side-effect concepts, constructed by using CUI and SNOMED CT codes. Ground-truth matched pairs are also only used for evaluation purposes.\n\nExperimental Settings.\nFollowing the definition in Sec. 2.1 and recognizing the scarcity of supervision in the biomedical domain, we mainly focus on the biomedical concept linking under the zero-shot setting.\nAdditionally, our biomedical concept linking task solely relies on concept names for broad real-world application coverage.\nGiven this characteristic of our data input, graph-based linking methods, such as selfKG, are not applicable as they need topological information to establish concept alignment.\nSimilarly, thesauri-based methods, such as MetaMap, are unsuitable as they only establish links between EHR concepts and KG concepts existing in the pre-defined vocabulary.\nTherefore, the following baseline methods are compared:\nConventional methods: Jaccard Distance, Levenshtein Distance, Jaro-Winkler Distance, BM25. These methods measure the concept pairs\u2019 string similarity and relevance and then obtain the linking prediction result.\nMachine learning-based methods: Pre-trained language models are used to generate concept embedding and linking prediction results. Specifically, we select representative PLMs including BioGPT, BioClinicalBERT, BioDistilBERT, KRISSBERT, and ada002."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Concept Linking Experiment Results",
            "text": "###table_1### Table 1  ###reference_### shows the accuracy of our proposed PromptLink along with baseline methods, when every method links a query EHR concept with their predicted top-1 KG concept.\nAs can be seen, PromptLink outperforms competing approaches across both datasets in terms of zero-shot accuracy, underscoring the superiority of our LLM-based concept linking methodology.\nMoreover, conventional methods based on string similarity lag behind machine learning techniques, which leverage embeddings from pre-trained language models to effectively match conceptually similar but lexically distinct entities like \u201cEllis-Van Creveld syndrome\u201d and \u201cChondroectodermal dysplasia\u201d."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3. Ablation Studies",
            "text": "Prompt Effectiveness and Efficiency.\nWe conduct ablation studies to reveal the effectiveness and cost-efficiency of the prompt used in our approach, as shown in Table 2. This comparison uses the same input data and 10 linking candidates across various prompts. In the table, the \u201cBefore prompting\u201d denotes the performance of using only embedding similarity obtained from the pre-trained LM, while other methods use LLM to predict linking results based on LM-generated candidates.\nFrom Table 2, the \u201cBefore Prompting\u201d method achieves the worst accuracy, demonstrating that linking performance could be improved by using LLM. Notably, PromptLink with both two-stage prompts achieves the best accuracy with the second-highest cost (M total tokens, costing approximately $66.25), indicating that the combined effect of the prompts substantially enhances accuracy, with the costs being moderated by the first stage\u2019s proficiency in eliminating unrelated candidates.\n\nNIL Prediction.\nAnother ablation study examines PromptLink\u2019s NIL prediction ability. In our built MIID and CISE datasets, each query EHR concept is designed to have a ground-truth linking KG concept. To reflect the real-world unlinkable scenario, we extend our MIID dataset into \u201cMIID-NIL\u201d which contains a proportion of unlikable EHR concepts. In Figure 3, the overall accuracy of PromptLink in the MIID-NIL dataset is 0.8145. Specifically for the unlikable concepts, PromptLink outputs the expected \u201cNIL\u201d with 0.9290 accuracy, which validates the NIL prediction ability of our proposed method."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4. Case Studies",
            "text": "In case studies on linking EHR concepts to MIID\u2019s KG disease concepts, three scenarios are presented: (1) concepts assessed by both ground-truth labels and a clinician; (2) concepts evaluated by a clinician due to missing ground-truth labels; (3) irrelevant concepts judged by a clinician. \n\nOverall, PromptLink could link biomedical concepts more accurately and appropriately. For cases I-V, PromptLink\u2019s linking results are justified by the ground-truth label and clinician. Specifically, for cases I and II, PromptLink accurately links the EHR concepts to conceptually similar but lexically distinct KG concepts. This difference showcases the effective use of LLM\u2019s biomedical knowledge. PromptLink\u2019s predictions are more accurate and specific.\n\nFor cases VI-IX, where linking ground truth labels are lacking, PromptLink\u2019s predictions also align more accurately with EHR concepts, according to a clinician\u2019s review. In cases VI and VII, PromptLink closely matches the EHR concepts. In cases VIII and IX, PromptLink correctly and automatically identifies no matching KG disease concepts."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Conclusion",
            "text": "In this study, we introduce PromptLink, a novel framework leveraging LLMs and multi-stage prompts for effective biomedical concept linking. Compared with previous concept linking methods, PromptLink achieves better linking accuracy, attributed to LLM\u2019s intrinsic strong biomedical knowledge. PromptLink further employs multi-stage prompts to maintain cost-efficiency and handle the NIL prediction problem. Moreover, PromptLink functions as a zero-shot framework, requiring no training and demonstrating strong flexibility and generalizability across biomedical systems. Promising future work can focus on further enhancing the prompt effectiveness, reducing costs, and minimizing manual efforts, aiming to extend PromptLink\u2019s application to broader systems."
        }
    ],
    "url": "http://arxiv.org/html/2405.07500v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2.2"
        ],
        "main_experiment_and_results_sections": [
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.07500v1",
        "paper_title": "PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking",
        "research_background": "The paper \"PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking\" addresses the intricate and essential task of linking related biomedical concepts across different data sources. Here is a detailed breakdown of the paper\u2019s motivation, research problem, and relevant prior work:\n\n### Motivation\n\nThe primary motivation behind the paper is rooted in the necessity of enabling integrative analyses by linking biomedical concepts originating from diverse sources. These sources often provide multifaceted views of biomedical knowledge and data, such as Electronic Health Records (EHRs) and Knowledge Graphs (KGs). By effectively linking these concepts, researchers can harness comprehensive patient health analyses and structured biomedical knowledge, ultimately facilitating advanced biomedical research.\n\n### Research Problem\n\nThe research problem tackled in the paper revolves around the challenge of cross-source biomedical concept linking. Discrepancies in biomedical naming conventions across different systems pose a significant hurdle. For instance, the same disease might be termed differently in a KG and an EHR, creating a barrier to cohesive data analysis. Traditional methods relying on string-matching rules and thesauri have limitations related to coverage and generalizability. Machine learning-based methods, while promising, require extensive data and supervision signals, making them resource-intensive and less practical for novel downstream applications. Large Language Models (LLMs), with their rich prior knowledge and zero-shot learning capabilities, present a promising yet challenging solution due to their cost, context length constraints, and NIL prediction capability.\n\n### Relevant Prior Work\n\nThe paper builds upon various strands of prior research:\n1. **String-Matching and Thesauri-Based Methods**: Early approaches focused on string-matching rules and constructed thesauri (e.g., Aronson and Lang, 2010; Savova et al., 2010; Friedman et al., 2001). These methods, however, suffer from fixed rules and crafted thesauri, limiting their application in diverse real-world scenarios.\n   \n2. **Machine Learning-Based Methods**: The transition towards embedding-based methods involves transforming biomedical concepts into embeddings, leveraged for computing similarity scores. Notable advancements include the use of pre-trained language models (PLMs) (e.g., Wang et al., 2023b; Xu et al., 2020; Lee et al., 2020; Alsentzer et al., 2019) and graph neural networks (GNNs) (Zhou et al., 2020; Bordes et al., 2013; Grover and Leskovec, 2016). These models capture both semantics and relationships but are data-hungry and require significant supervision.\n   \n3. **Large Language Models (LLMs)**: Recent LLMs such as GPT-4 have shown impressive performance in NLP tasks, offering potential for zero-shot learning applications (e.g., Zhou et al., 2023; Singhal et al., 2023). However, they face challenges like context length limits and NIL prediction capability (e.g., Zhang et al., 2023; Peters et al., 2019).\n\nBy leveraging LLMs and introducing a novel two-stage prompting mechanism, the paper aims to overcome these challenges and enhance the cross-source biomedical concept linking task's effectiveness and efficiency.",
        "methodology": "### Methodology Section: Description of Proposed Method or Model\n\n**Proposed Method: PromptLink**\n\n*PromptLink* is a novel approach to cross-source biomedical concept linking that leverages large language models (LLMs). The method addresses two primary constraints of LLMs: high computational costs and limitations in input text length, by incorporating a two-stage process which is both efficient and effective.\n\n**Key Components and Innovations:**\n\n1. **Concept Representation and Candidate Generation**:\n    - **Preprocessing**: The input text undergoes preprocessing which includes lowercasing and removing punctuation.\n    - **Embedding Generation**: SapBERT, a biomedical-specialized pre-trained language model, is used to create embeddings for concepts extracted from both Electronic Health Records (EHR) and Knowledge Graphs (KG). For multi-token concepts, token-level embeddings are averaged.\n    - **Candidate Retrieval**: Cosine similarity is computed between EHR concept embeddings and KG concept embeddings. For each EHR concept, the top \\( k = 10 \\) KG concepts with the highest cosine similarities are selected as candidates for further linking.\n\n2. **Linking Prediction Using Two-Stage Prompts**:\n    - **First Stage**:\n        - This stage aims at filtering out unrelated candidates. GPT-4 is prompted to determine if each concept pair should be linked. The responses are obtained through a self-consistency strategy, where the same question is prompted multiple times (\\( t \\)). The belief score for each pair is computed based on the frequency of affirmative responses.\n        - A filtering strategy using parameter \\( \\tau = 0.5 \\) (threshold) is employed to discard candidates with zero belief scores, provided there are other closely aligned candidates. This reduces the number of irrelevant candidates and optimizes efficiency.\n    - **Second Stage**:\n        - The retained candidates are evaluated in a broader context using a compositional prompt with GPT-4. This consists of two questions:\n            1. Label the relationship between the query concept and candidates as \u201cexact match\u201d, \u201crelated to\u201d, or \u201cdifferent from\u201d.\n            2. Use self-verification prompts to either identify the closest match or dismiss all candidates if none are sufficiently close.\n        - A final linking result for the query EHR concept is decided based on occurrence frequency (\\( f \\)) of the outcomes and additional considerations such as embedding similarity for resolving ties.\n\nIn summary, *PromptLink* introduces an innovative two-stage prompting mechanism utilizing a specialized biomedical language model and GPT-4, significantly improving the accuracy and efficiency of cross-source biomedical concept linking.",
        "main_experiment_and_results": "### Main Experiment Setup and Results: PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking\n\n#### Datasets\nThe main datasets used in the experiments are two biomedical concept linking benchmark datasets, MIID (MIMIC-III-iBKH-Disease) and CISE (CRADLE-iBKH-Side-Effect):\n\n1. **MIID (MIMIC-III-iBKH-Disease)**:\n    - Contains 1,493 diagnosis concepts from the MIMIC-III dataset and 18,697 disease concepts from the iBKH knowledge graph.\n    - MIMIC-III is an extensive EHR corpus with over 53,423 hospital patient records.\n    - iBKH comprises 2,384,501 entities.\n    - For MIID construction, exact matches between diagnosis and disease concepts are excluded, and the remaining concepts are linked via ICD-9 and UMLS CUI codes to form ground-truth pairs.\n\n2. **CISE (CRADLE-iBKH-Side-Effect)**:\n    - Includes 1,500 diagnosis concepts from the CRADLE dataset and 4,251 drug side-effect concepts from iBKH.\n    - Uses CUI and SNOMED CT codes for constructing ground-truth pairs.\n\n#### Experimental Settings\n- **Zero-Shot Setting**: Given the limited supervision in the biomedical domain, the focus is on zero-shot biomedical concept linking.\n- **Concept Names**: The task relies solely on concept names which rules out the applicability of graph-based and thesauri-based linking methods needing topological information and predefined vocabulary, respectively.\n\n#### Baseline Methods\nThe following baseline methods are categorized into conventional and machine learning-based approaches:\n\n- **Conventional Methods**:\n    - Cosine Distance\n    - Jaccard Distance\n    - Levenshtein Distance\n    - Jaro-Winkler Distance\n    - BM25 \n\n- **Machine Learning-Based Methods**:\n    - Various pre-trained language models (PLMs) are used to generate embeddings and predict linking using embedding cosine similarity. The selected PLMs include:\n        - BioBERT\n        - BioGPT\n        - BioClinicalBERT\n        - BioDistilBERT\n        - KRISSBERT\n        - ada002\n        - SAPBERT\n\nAlthough the detailed results of each method are not provided in the query, the performance of each baseline would typically be evaluated on the two datasets using appropriate metrics such as precision, recall, and F1 score. The expected outcome would summarize the effectiveness of large language models in zero-shot cross-source biomedical concept linking, comparing them to conventional string similarity measures and highlighting their advantages in terms of accuracy and robustness."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the effectiveness and cost-efficiency of the prompts used in the PromptLink approach.",
            "experiment_process": "The experiment compares the performance of different prompting strategies using the same input data and 10 linking candidates. Specifically, the 'Before prompting' method, which uses only embedding similarity from a pre-trained language model, is compared against other methods that involve LLM-driven predictions based on LM-generated candidates. The analysis includes evaluating the accuracy and cost (measured in total tokens) of these methods, with costs approximated for LLM usage.",
            "result_discussion": "The 'Before prompting' method achieves the worst accuracy, affirming that LLM usage can improve linking performance. The PromptLink approach utilizing both two-stage prompts shows the best accuracy with the second-highest cost, indicating the significant combined effect of these prompts on enhancing accuracy. The cost is moderated by the first stage's efficiency in eliminating unrelated candidates.",
            "ablation_id": "2405.07500v1.No1"
        },
        {
            "research_objective": "To test PromptLink's ability to predict NIL (no link) outcomes for unlinkable concepts.",
            "experiment_process": "The study examines PromptLink's performance on the MIID-NIL dataset, an extension of the original MIID dataset that includes a proportion of unlinkable EHR concepts. The accuracy of PromptLink is assessed in this real-world scenario. The performance is further compared with the SAPBERT method, which uses a hard-coded cosine similarity threshold for NIL prediction.",
            "result_discussion": "PromptLink demonstrates an overall accuracy of 0.8145 on the MIID-NIL dataset. For unlinkable concepts, it outputs the expected 'NIL' with 0.9290 accuracy, validating its NIL prediction capability. In contrast, SAPBERT achieves lower accuracy (maximum 0.7920) due to its reliance on a manually set threshold, which either produces many wrong predictions (for low thresholds) or fails to link many concepts (for high thresholds).",
            "ablation_id": "2405.07500v1.No2"
        }
    ]
}