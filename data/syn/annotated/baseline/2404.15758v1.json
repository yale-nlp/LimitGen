{
    "title": "Let\u2019s Think Dot by Dot: Hidden Computation in Transformer Language Models",
    "abstract": "Transformers can use additional tokens to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, learning to effectively utilize these tokens is difficult and requires specific, dense supervision to converge. We provide a theoretical characterization of the class of problems where additional tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, such tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. This raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed inputs. Code is available at https://github.com/JacobPfau/fillerTokens",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The most widely used language model (LM) alignment methods are purely behavioral. Reinforcement learning from human feedback, constitutional AI, instruction fine-tuning, and automated red-teaming all rely on judging or comparing model output tokens.\n\nEmpirically, commercial large language models (LLMs) do not benefit from filler tokens on common QA and math benchmarks; Claude 2 and GPT-3.5 achieve the same performance without intermediate tokens. However, current LLMs' limitations cannot be extrapolated to larger scales: The empirical evidence on current LLMs does not clarify whether a failure to use filler tokens is an inherent limitation of transformer expressivity, or instead, if filler token performance may arise at larger scale. Additionally, it is unclear whether these evaluations targeted tasks where filler tokens would be beneficial.\n\nOur contributions are the following:\nWe construct two synthetic datasets, 3SUM and 2SUM-Transform, on which LLAMA transformers fail to solve the task without filler, but achieve improved accuracy when provided filler tokens.\nWe find that filler token performance increases over immediate answers as the length and complexity of inputs increase.\nWe contextualize filler tokens with respect to theoretical expressivity results, but we show empirically that they do seem to add power within certain bounds.\nWe find that learning to use filler tokens is difficult and requires specific, dense supervision to converge. Standard data is insufficient for models to learn to leverage filler tokens effectively.\n\nTaken together, these findings suggest that although current LLMs are unlikely to benefit from filler tokens, this is not an inherent limitation of current architectures. Given demonstrations of parallelizable task decompositions, we expect that current LLMs could also realize benefits from filler tokens."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Recent theoretical work establishes that transformers without additional reasoning tokens are limited to solving only highly parallelizable problems (see Strobl et al., 2023 for an overview). Formally, Merrill & Sabharwal (2023a) place log-precision transformers in the circuit complexity class, which can be equivalently understood as the class of problems definable in first-order logic with majority quantifiers (Merrill & Sabharwal, 2023b). It follows that problems outside (those that cannot be defined in first-order majority logic) cannot be solved by transformers without additional reasoning tokens. This includes canonical reasoning problems like composing permutations, graph connectivity, or evaluating boolean formulas. This suggests that\u2014without additional reasoning tokens\u2014transformers are surprisingly limited.\n\nAs long as the number of filler tokens is polynomial, the argument of Merrill & Sabharwal (2023a) goes through to show that transformers with filler tokens can only solve problems in. Merrill & Sabharwal (2023a) show for inputs of size, a transformer can be simulated by an depth, size threshold circuit. If we add polynomial filler tokens, this implies we can simulate the circuit with depth and size.\n\nThere are likely many problems in that transformers without filler tokens cannot express, including those that fundamentally require resolving many nested quantifiers at the same time (Merrill & Sabharwal, 2023b). Filler tokens make problems with deep quantifier nesting clearly solvable: with appropriate positional encodings, a problem requiring quantifier depth can be expressed with filler tokens by using the filler tokens to enumerate over quantified values. We will define such problems and show empirically that transformers cannot solve them without filler tokens, while they can learn to solve them perfectly with filler tokens.\n\nPrevious and concurrent research identified cases where token representations contribute to the prediction of tokens occurring multiple indices later showing that, in practice, such contributions both reduce loss on the average case (Janus, 2023; Wu et al., 2024) and can be mechanistically identified via probing (Pal et al., 2023). Complementing these works, we propose filler tokens as a limit case for coordinated, token-agnostic, non-myopic computation; this case is of particular interest for its expressivity and alignment properties.\n\nRecent work has also proposed training transformers to predict when further computation is needed for token predictions using pause tokens (Goyal et al., 2024) or meta-tokens (Zelikman et al., 2024). Whereas Goyal et al. (2024) and Zelikman et al. (2024) address the engineering question of how to modify the transformer architecture, language modeling objective, and tokenization process to allow adaptive, filler-like computation; our work addresses the scientific question of under what conditions standard, causal transformers on the unmodified next-token prediction task can learn to use intermediate tokens as filler tokens."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Synthetic data: 3SUM and 2SUM",
            "text": "We would like to understand why previous results found no performance increase on tested LLMs (Lanham et al., 2023). The motivation for this problem comes from two directions. Theoretically, 3SUM is of interest since it is likely not expressible with a single forward pass (as it has quantifier depth greater than 2) but is parallelizable. Intuitively, 3SUM involves simply matching triples of in-context inputs by their meaning. So a demonstration that 3SUM is learnable provides evidence of an expressivity gap between different settings for the general class of nested quantifier resolution problems. A secondary, simpler task involves matching pairs of inputs (summing to zero), but in which we obfuscate the input tokens by applying a transformation only specified in the final token of the input sequence. Leaving the input under-defined until this final transform token prevents in-place computation over input tokens forward passes. The 2SUM-Transform problem is an instance of the more general format in which a question is posed at the end of a long input, as when presenting a document followed by a question about the document."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3SUM Definition and Tokenization",
            "text": "Our sequence data consists of input e.g. \u201cA01 B10 C73 D27\u201d, intermediate tokens e.g. \u201c. . .\u201d, and 3SUM-label e.g. \u201cTrue\u201d. Here, \u201cA05\u201d denotes the tuple and marks this as the first input. Inputs are vectorized as multi-hot binary vectors passed to the model as embedding vectors followed by a learned linear layer. The input vectors have masked labels and so do not contribute to the loss. Besides the masked input tokens, all subsequent tokens are presented as one-hot labels so as to be compatible with the standard cross-entropy language modeling objective. This choice of inputs as embedding vectors and the rest as one-hot tokens is admittedly non-standard and was made to reduce the scale of compute needed.\n\nWe consider the task of predicting whether 3SUM is satisfied given input sequences. The approach is designed to uncover whether a transformer can represent and learn the underlying relationships needed to predict this task's output effectively from the input representation."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "2SUM-Transform",
            "text": "Formally the 2SUM problem is: Given  as input, predict. This can be done in a single forward pass with a standard transformer, so to demonstrate the utility of the model, we propose the 2SUM-Transform problem in which a permutation888 here denotes the -fold direct product of the cyclic group on  elements\u2013i.e. each digit of every tuple is permuted independently.  is used to obscure the input sequence. This permutation shifts every digit of the input tokens by a random offset. The resulting 2SUM-Transform input is then. We randomly sample 10 such permutations999For 2SUM, a brute-force solution, without knowledge of the transform, requires  comparisons where  is the number of possible transformations.  and uniformly at random sample a permutation to apply for each sample in the dataset. For 2SUM training, we use a binary cross-entropy loss, since both inputs are multi-hot vectors. Causal masking is applied as per the standard language-modelling objective."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "3SUM: transformers converge with filler tokens and fail without",
            "text": "Figure 2 shows that, as expected, for length-6, dimension-3 3SUM instances, 3SUM is learnable without filler tokens. However, as we scale the length of inputs up to length 12, we find increasing performance gaps: The no-filler models achieve near-random accuracy at 66%.\n\nGiven the possibility of non-linear, learned probes confounding the interpretation of representations with the probes\u2019 own computation, we compare to a control condition (Hewitt & Liang, 2019). This ensures that the observed accuracy scaling reflects the frozen model layers\u2019 representations and not the probe itself. For the control task, we take a model trained on sufficiently simple 3SUM sequences for which immediate solutions are tractable: length-10, dimension-1 data. This was determined by training another model without any intermediate tokens and observing that model achieved 100% accuracy.\n\nFigure 2 showed that 4 layer models realize maximal benefit starting at length-12 3SUM inputs, that is sequences of token length. Our experiments required to realize an expressivity gap over the no-filler response. This raises the question of whether LLMs with tens or hundreds of layers require prohibitively many tokens to see improved performance over the no-filler baseline? To answer this question, we show the effects of scaling input complexity, i.e. dimension, instead of input length, realizing performance gaps at lower token counts.\n\nFigure 5 shows that for fixed length inputs, by increasing input dimension, we can realize performance gaps between settings for even length-8 inputs. However, given that to realize this performance gap, minimally 6-dimensional inputs are required (when using a small, 34M LM) we expect integer addition tasks will not offer suitably rich structures for taking advantage of model capabilities when using large models\u2014natural-language tasks may offer alternatives. In these experiments, we used as our no-filler baseline a model trained on a 50/50 mixture of sequences, and instance-adaptive CoT (the evaluation is done on the sequences only). We use this mixed-dataset baseline rather than training on only immediate-response sequences, because the mixed-dataset models outperform the immediate-response-only models. In Appendix B, we provide further results on the effects of scaling tuple dimensionality for length-10 inputs."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "We use a 34M-parameter Llama model with 4 layers, 384 hidden dimension, and 6 attention heads (Touvron et al., 2023). This is a scaled-down, randomly-initialized version of the Llama model. Input 2SUM and 3SUM vectors are given as hard-coded, multi-hot embedding vectors which are projected through a learned dimensional linear layer. These dimensions correspond to tuple digits and hard-coded positional values. We use Adam with a learning rate of 1e-4 throughout. \n\nWe train on 10,000,000 samples and test on 2,000 samples. We train to convergence: for 25 epochs in the no-filler setting. We always report the per-run maximum validation performance across epochs, i.e. early-stop performance."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "To demonstrate that filler tokens enhance expressive capacity, allowing transformers to solve challenging tasks, we need to establish that transformers without filler tokens cannot address the 3SUM task. Specifically, we must prove that the inability to learn 3SUM without filler tokens is due to expressive capacity constraints, rather than differences in data distribution and presentation. We illustrate that for shorter inputs, 3SUM can be solved without filler tokens, but for longer inputs, it cannot. The length and dimension scaling experiments below were trained on a mix of different data types.\n\nFigure 2 shows that for length-6, dimension-3 3SUM instances, the task is learnable both with and without filler tokens. However, as input length increases to 12, a performance gap emerges: models without filler tokens achieve near-random accuracy at 66%, while those with filler tokens maintain 100% accuracy.\n\nGiven a model trained with filler tokens, we fine-tune the final attention layer to predict solutions with fewer filler tokens. Figure 4 shows that predictions improve monotonically with additional filler tokens during decoding. Each point in the figure represents a different final-layer fine-tune. The initial half of the filler tokens is crucial, achieving high performance while using only a portion of the total filler tokens, aligning with the requirements of an efficient 3SUM algorithm.\n\nTo ensure results reflect the models' representations and not the probe's calculations, we conduct a control condition inspired by previous studies. This approach confirms that the observed accuracy scaling reflects the frozen model layers\u2019 representations. For the control task, a model trained with filler tokens on simple 3SUM sequences of length-10, dimension-1 achieved 100% accuracy. We confirm that filler tokens are redundant in models capable of solving problems without them.\n\nFigure 5 illustrated that 4-layer models benefit from filler tokens for longer, more complex sequences of token length, beginning at length-12 3SUM inputs. This leads to the question of whether large-scale models need a prohibitive number of filler tokens to enhance performance over no-filler baselines. To investigate, we examine the effects of scaling input complexity via dimension, finding performance gaps at lower filler token counts.\n\nOur experiments show that increasing input dimension allows for performance gaps between different approaches, even with length-8 inputs. However, at least 6-dimensional inputs are needed to achieve these gaps with smaller models, indicating that other rich tasks may be necessary for larger models to leverage filler tokens. For these experiments, a model trained on a mixed dataset outperformed those trained exclusively on immediate-response sequences. Further results on the effects of scaling tuple dimensionality for length-10 inputs are provided in the Appendix."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Filler Tokens Only Improve Performance Given Parallelizable CoT Demonstrations",
            "text": "Despite transformers having the expressive capacity to solve certain tasks, learning poses a hard learning problem. There are two reasons for this: First, it is impossible to densely supervise solutions, because by assumption, tokens are used in precisely those cases when underlying, hidden computation decorrelates from the meaning of the corresponding tokens. Second, algorithms learned from data generically require instance-adaptive, serial computation (Merrill & Sabharwal, 2023c)\u2013such computation is incompatible with the parallel structure.\n\nTo quantify the effects of these learning obstacles, we conduct an ablation study. We train models on data using instance-adaptive sequences (described in Section 3.1). We find that models trained on instance-adaptive data fail to use certain computational strategies. The resulting models remain at, or below, baseline performance. This indicates that there is no transfer from serial, instance-adaptive demonstrations for the 3SUM problem."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "2SUM Experiments",
            "text": "In the 2SUM setting, a transformer with no filler performs well above random, but significantly below the same model when trained with filler, as shown in Table 1. Table 1 reports the maximum performance across five random initializations, because we observe significant variance across runs. We use the same hyper-parameters as are used for 3SUM, c.f. Section 4.1."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "For certain parallelizable problems, transformers achieve improved performance given additional forward passes. This performance gap demonstrates that, with adequate training data, intermediate tokens between input and answer may be used purely for their computational capacity rather than for human-like, faithful serial reasoning which human-generated text typically represents. In such cases, the intermediate tokens can sometimes be non-informative and potentially misleading if they describe reasoning unrelated to actual computations occurring in hidden representations.\n\nWe have provided a theoretical case for increased token utilization and empirical evidence that such usage can be efficiently learned. The central question is: First, to what extent do token-parallelizable algorithmic problems arise in the natural language context? Second, to what extent does natural-language text provide adequate supervision for this process, offering parallelizable supervision instead of non-parallelizable, instance-adaptive processes? If these conditions are met, increased token utilization efficiency is expected to emerge in LLMs."
        }
    ],
    "url": "http://arxiv.org/html/2404.15758v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "3.2",
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.15758v1",
        "paper_title": "Let\u2019s Think Dot by Dot: Hidden Computation in Transformer Language Models",
        "research_background": "**Motivation:**\n\nThe primary motivation of this paper is to explore and understand the potential benefits of using filler tokens, such as repeated dots ('\u2026\u2026'), in transformer language models (LMs). This motivation is driven by empirical observations that current chain-of-thought reasoning, while improving performance, often leads to intermediate reasoning steps that are not faithful to the final answer. By investigating the utility of filler tokens, the paper aims to uncover whether LMs can perform cross-token computations that are not evident from the immediate response tokens themselves.\n\n**Research Problem:**\n\nThe research problem addressed in this paper is whether transformer language models can achieve improved performance on certain tasks by using filler tokens. The paper seeks to determine if the use of filler tokens provides a computational advantage and extends the expressive power of transformers, particularly in tasks that require complex reasoning with nested quantifiers. The study further explores whether this potential benefit is constrained by the expressivity limits of transformers or if it is simply a matter of the current model scales and training approaches.\n\n**Relevant Prior Work:**\n1. **Chain-of-Thought Reasoning:** Previous studies, such as those by Wei et al. (2023), Suzgun et al. (2022), and Lanham et al. (2023), have shown that chain-of-thought reasoning improves language model performance. However, chain-of-thought answers often lack faithfulness to the intermediate reasoning steps (Lanham et al., 2023; Turpin et al., 2023).\n   \n2. **Model Alignment Methods:** Common LM alignment methods like reinforcement learning from human feedback, constitutional AI, and instruction fine-tuning rely on judging model outputs. These methods might be undermined by models capable of utilizing filler tokens since the reasoning across filler tokens is not easily judged by output tokens alone.\n   \n3. **Empirical Studies on Filler Tokens:** Empirical evidence indicates that commercial LLMs like Claude 2 and GPT-3.5 do not show performance gains with filler tokens on common QA and math benchmarks (Sachan, 2023; Lanham et al., 2023). However, this does not clarify if this limitation is due to the fundamental expressivity of transformers or if benefits might emerge at larger scales or in different tasks.\n   \n4. **Theoretical Expressivity Limits:** Prior work has pointed out that transformers, as single-token predictors, belong to the complexity class , limiting them from solving problems like permutation composition or graph connectivity (Merrill & Sabharwal, 2023a; Strobl et al., 2023). Linear or polynomial chain-of-thought steps, however, can extend transformer capabilities beyond this class, although filler tokens are still within .\n\n5. **Transformer Task Decomposition:** Sanford et al. (2024) suggested that transformers without intermediate tokens are inadequate for certain expressive tasks. This paper examines whether filler tokens can help transformers solve such conjectured inadequate tasks.\n\nThe research builds upon these works by experimentally demonstrating that transformers given filler tokens can perform significantly better on tasks requiring complex reasoning than those given immediate-answer prompts.",
        "methodology": "### Methodology\n\nThe methodology in this paper aims to investigate the impact of filler tokens on the performance of large language models (LLMs) by crafting synthetic datasets tailored to specific conditions under which these tokens might provide a benefit.\n\n1. **Objective**: \n   - **Evaluate the Effect of Filler Tokens**: Determine what types of evaluation and training data enable models to effectively use filler tokens.\n   - **Identify Beneficial Data Conditions**: Find out what constraints or situations enhance model performance through filler tokens.\n\n2. **Synthetic Datasets**: \n   - **3SUM Task**: \n     - **Theoretical Motivation**: It suggests that the 3SUM problem might surpass the expressive abilities of a single forward pass due to its quantifier depth, indicating a need for a strategy like filler tokens.\n     - **Intuitive Explanation**: Involves matching triples of context-based inputs by their semantic meaning. Success in learning the 3SUM task using filler tokens would signal a notable expressivity gap between models trained with and without filler tokens, particularly for nested quantifier resolution problems.\n   - **2SUM-Transform Task**: \n     - **Task Description**: While simpler than 3SUM, this task requires models to find pairs of inputs that sum to zero. Complexity is introduced by transforming the input tokens in a way that is only revealed in the final token of the sequence.\n     - **Transform Mechanism**: This late specification transformation obfuscates the input, making it impossible for the model to perform computations on the input tokens in a single forward pass effectively. This simulates scenarios where a question about a long document is posed at the end of the document.\n\nBy designing these two synthetic tasks, the study aims to uncover the conditions where filler tokens can meaningfully enhance LLM performance, bridging an understanding gap in previous research (Lanham et al., 2023). This approach provides a structured way to explore the hidden computational capabilities of transformer-based models through the innovative application of filler tokens.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Datasets:**\n- The experiment involves 3SUM problem instances with varying input lengths and dimensions.\n- Key datasets include:\n  - Length-6, dimension-3 instances.\n  - Length-12 instances.\n  - Control task with length-10, dimension-1 data to handle simpler 3SUM sequences.\n\n**Baselines:**\n- A model trained with filler tokens.\n- A no-filler baseline model achieving near-random accuracy at 66% on length-12 inputs.\n- For control conditions, a model trained on sufficiently simple sequences that do not require filler tokens, achieving 100% accuracy on dimension-1 data.\n\n**Evaluation Metrics:**\n- Accuracy of predicting the correct solution to the 3SUM problem.\n\n**Main Experimental Results:**\n1. **Length-6, Dimension-3 Instances:**\n   - Models, both with and without filler tokens, achieved learnability success.\n\n2. **Length-12 Inputs:**\n   - Models with filler tokens maintained 100% accuracy.\n   - No-filler models performed poorly, achieving only about 66% accuracy.\n\n4. **Control Task Results:**\n   - On length-10, dimension-1 sequences, models achieved 100% accuracy with minimal (2%) filler tokens.\n   - Demonstrated that filler tokens add redundancy when the model has sufficient expressivity without them.\n\n6. **Mixed Dataset Baseline:**\n   - Trained on a mix of filler token sequences and instance-adaptive CoT, used for evaluation.\n   - Outperformed models trained solely on immediate-response sequences.\n\nOverall, the experiments indicate the significant role of filler tokens in learning complex sequences and solving the 3SUM problem efficiently compared to no-filler models. The effects were particularly pronounced in higher-dimensional inputs and when using a mixed dataset for training and evaluation."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To understand why previous results found no performance increase from filler tokens on tested LLMs and determine what kinds of evaluation and training data can benefit from filler tokens.",
            "experiment_process": "The experiment utilized two synthetic datasets: 3SUM and 2SUM-Transform. The 3SUM dataset required models to predict if the sum of three distinct indices equaled zero, using sequences that included intermediate tokens (filler tokens, Chain of Thought (CoT) with parallelizable solutions, and CoT with instance-adaptive solutions). Filler sequences used repeated dots (e.g., '...') while CoT sequences logically decomposed the 3SUM task. Similarly, the 2SUM-Transform task involved matching pairs of inputs with permuted tokens, obfuscating the input tokens using a final transformation token to prevent in-place computation.",
            "result_discussion": "The results indicated that fillers can improve model performance on both 3SUM and 2SUM-Transform tasks. The 3SUM task saw greater performance gaps between models with and without filler tokens as input lengths increased. Fine-tuning on reduced filler tokens also showed that the models retained understanding and could make accurate predictions. These findings suggest that filler tokens can serve computational benefits by providing structure, even when the intermediate steps do not carry meaningful task information.",
            "ablation_id": "2404.15758v1.No1"
        },
        {
            "research_objective": "To determine if transformers trained with/without filler tokens can learn to solve the 3SUM problem effectively and if filler tokens confer greater expressive capacity.",
            "experiment_process": "The experimental setup used a 34M-parameter Llama model with 4 layers, hidden dimensions of 384, and 6 attention heads. Input vectors were multi-hot embedding vectors, passed through a learned 64-dimensional linear layer. The experiments consisted of two cases: training and testing on no-filler (immediate-answer) data, and training on a mix of CoT and filler tokens while testing on filler-token sequences. The task was scaled in length and dimension to observe expressivity gaps under these conditions, utilizing standard training setups with Adam optimizer and specific hyper-parameters.",
            "result_discussion": "The results showed that the 3SUM problem is unsolvable without filler tokens beyond short inputs, revealing the impact of input length and dimensional increases on performance gaps. In complex scenarios with filler tokens, models achieved significantly higher accuracy. This highlighted the computational benefits of filler tokens and their role in enabling transformers to address more complex tasks, pointing to their utility in natural language tasks requiring extended computations.",
            "ablation_id": "2404.15758v1.No2"
        },
        {
            "research_objective": "To assess the impact of learning obstacles on filler-token performance and to evaluate the necessity of parallelizable CoT demonstrations for filler token utility.",
            "experiment_process": "Two ablation studies were conducted. First, models were trained only on filler-token sequences (length-14, dimension-3) to evaluate the difficulty in learning filler-token computations without parallelizable CoT data. Second, models were trained using instance-adaptive CoT sequences to see if they could transfer this learning to filler-token computations. Performance was evaluated across multiple initializations and compared to baseline conditions.",
            "result_discussion": "Training on filler-token-only sequences resulted in performance similar to the no-filler, immediate-answer condition, indicating significant learning difficulties. Moreover, training on instance-adaptive CoT sequences did not transfer to filler tokens, with models failing to improve on filler-token data, underscoring that parallelizable CoT data is critical for learning filler-token computations efficiently. These results demonstrate the complexity of training models to use filler tokens effectively without dense supervision and suitable problem structures.",
            "ablation_id": "2404.15758v1.No3"
        }
    ]
}