{
    "title": "Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark",
    "abstract": "The rapid development of large language models (LLMs) enables them to convey factual knowledge in a more human-like fashion. Extensive efforts have been made to reduce factual hallucinations by modifying LLMs with factuality decoding. However, they also pose risks of hindering knowledge updates, as they make models overly confident in known facts.\n\nIn this work, we revisited the current factuality decoding methods and verified their effectiveness in enhancing factual accuracy. Subsequently, we conducted further evaluation of several strong factuality decoding methods on the knowledge editing benchmark. All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3%. This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of preserving the flexibility for knowledge editing. Therefore, our work suggests that research into factual alignment should simultaneously focus on the effectiveness of knowledge editing.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) (OpenAI, 2022; 2023; Touvron et al., 2023a; b) have demonstrated remarkable capabilities in various NLP tasks, owing to the knowledge memory acquired during pre-training. Despite the generated text often appearing correct, careful observation reveals that they sometimes exhibit factually incorrect statements, i.e., \u201dhallucinations.\u201d Such hallucinations significantly undermine the reliability of LLMs in real-world scenarios.\n\nFactual hallucinations have received widespread attention due to their significant side effects, as LLMs generate content that deviates from established world knowledge. Furthermore, with the widespread changes in the world\u2019s circumstances and the continuous expansion of large model scales, it\u2019s paramount to efficiently keep factual knowledge up-to-date. Knowledge Editing has been proposed to address this issue, achieving efficient modifications to model facts while ensuring no adverse effects on other unrelated knowledge.\n\nAn excellent text generation without hallucinations demands that an LLM is capable of \u201dknowing\u201d correctly and \u201dtelling\u201d accurately, which means it needs to keep factual knowledge up-to-date and convey it accurately. Some emerging works focus on narrowing the gap between \u201dknowing\u201d and \u201dtelling\u201d in LLMs, guiding them to accurately \u201dtell\u201d the facts they know. In particular, various factuality decoding methods can directly generate answers that better align with factuality, which is highly convenient as it does not require the infusion of extensive new factual knowledge through supervised fine-tuning (SFT) or RLHF.\n\nOur experimental results on the TruthfulQA and FActScore benchmarks have demonstrated the effectiveness of factuality decoding in LLMs. These diverse decoding strategies lead to varying improvements in LLMs\u2019 factual metrics compared to their original decoding methods. Although modifying the decoding methods of LLMs enables them to more accurately \u201dtell\u201d the factual knowledge they have learned, they overlook a crucial aspect: Can these modified LLMs still be efficiently edited for updated knowledge?\n\nFactuality decoding encourages LLMs to produce answers that better align with pre-training facts, leading them to believe that the facts they have learned are accurate. Therefore, we suspect that current decoding methods for factuality may potentially inhibit the inherent simplicity and generalization ability of pre-trained language models. This overconfidence could result in rigidity in knowledge, making it challenging to update outdated facts using recent efficient knowledge editing methods.\n\nHence, we find that current existing decoding methods still cannot perfectly address the factual hallucinations of LLMs, as they overlook the importance of preserving the flexibility for knowledge editing. Altogether, our study highlights the potential risks associated with current factuality decoding methods and validates their apparent decline in knowledge editing. Consequently, we strongly advocate that a proficient LLM with factual accuracy should prioritize both the efficient update of factual knowledge and the accurate conveyance of factual information, thereby reducing the likelihood of factual hallucinations. Therefore, we recommend that research exploring factual alignment should simultaneously focus on the effectiveness of knowledge editing."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "This section revisits decoding approaches (Section 2.1  ###reference_###) and factual knowledge editing (LABEL:{ssec:factKE}), aiming to delve deeper into the subsequent exploration of the impact of factuality decoding on knowledge editing for LLMs.\nModel editing (Mitchell et al., 2022  ###reference_b21###; Yao et al., 2023  ###reference_b40###) aims to efficiently adjust the behavior of the original base model  on specific editing descriptors , without affecting the model\u2019s behavior on other samples.\nThe editing descriptors  describes a desired change in model behavior and can be represented as , where  is a input-output pair like Who is the president of US? Joe Biden.\nThe ultimate objective of model editing is to generate an edited model, denoted as . Consequently, given an edit descriptor , the post-edit model  is anticipated to predict the edited output answer, formally represented as , where .\nA factual knowledge can be represented using a triplet , where  represents the subject,  represents the relation, and  represents the object (Petroni et al., 2019  ###reference_b26###; Zhong et al., 2023  ###reference_b44###).\nConsequently, if LLMs can predict the masked entity expressing this fact in a cloze-style question, such as in The president of the United States is_ , which is built from the triplet , and the object can be predicted as \u201cJoe Biden,\u201d then it indicates that LLMs possess knowledge of this fact.\nFact editing (De Cao et al., 2021  ###reference_b8###; Zhong et al., 2023  ###reference_b44###) is an indispensable aspect of the continuous development of LLMs. This is because the factual knowledge within models cannot always remain correct over time, which should become outdated as time progresses.\nBased on the preceding context, in fact editing,  can be represented by a tuple  while  can be represented by , denoting the edited factual answer for the original . Thus, the edit descriptor  can be represented as: , and the post-edit model  satisfies  while .\nConsequently, given a collection of fact edits , fact editing involves learning a function  satifying .\nAs the scale of LLMs continues to expand, adjusting model parameters through retraining becomes increasingly challenging. Consequently, efficient  methods without requiring training are receiving more attention. This is also why our work chooses to evaluate using a simple yet efficient knowledge editing method in Section 4  ###reference_###."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Decoding Approaches",
            "text": "This paper focuses on decoding strategies used in open-ended language generation tasks, which entail language models receiving input prompts and generating fluent and coherent continuations.\nThe objective is to anticipate the succeeding word within a given contextual sequence, which is a fundamental pre-training goal extensively employed in state-of-the-art large language models (Radford et al., 2018  ###reference_b27###; Brown et al., 2020  ###reference_b4###; Anil et al., 2023  ###reference_b1###; Touvron et al., 2023b  ###reference_b33###).\nFormally, given an prompt sequence of length , denoted as , where  is a token in the vocabulary . We compute the next token probability distribution from a pre-trained autoregressive language model  conditioned on the previous context:\nwhere  represents a temperature parameter regulating the precision of the subsequent-token distribution.\nIn text generation, the language model samples from the conditional distribution  to generate the next token , iterating this process continuously until the sequence generation reaches the end token.\nAt decoding time, various decoding strategies can be applied at each step  to select the next token , with the given predicted distribution of the next token .\nThe most prevalent strategy involves sampling-based decoding, where  is randomly sampled from the distribution. Another prevalent method involves searching for the most probable text sequence through either greedy decoding or beam search (Wu et al., 2016  ###reference_b37###). However, these approaches often leads to repetitive and monotonous outputs, thus giving rise to numerous variants.\nFor instance, nucleus sampling (Holtzman et al., 2019  ###reference_b10###) selects tokens from the top- percentile of the next token distribution, while top- sampling  (Fan et al., 2018  ###reference_b9###) chooses tokens from the top- candidates in the next token distribution."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Factual Knowledge Editing",
            "text": "Model editing (Mitchell et al., 2022  ###reference_b21###  ###reference_b21###; Yao et al., 2023  ###reference_b40###  ###reference_b40###) aims to efficiently adjust the behavior of the original base model  on specific editing descriptors , without affecting the model\u2019s behavior on other samples.\nThe editing descriptors  describes a desired change in model behavior and can be represented as , where  is a input-output pair like Who is the president of US? Joe Biden.\nThe ultimate objective of model editing is to generate an edited model, denoted as . Consequently, given an edit descriptor , the post-edit model  is anticipated to predict the edited output answer, formally represented as , where .\nA factual knowledge can be represented using a triplet , where  represents the subject,  represents the relation, and  represents the object (Petroni et al., 2019  ###reference_b26###  ###reference_b26###; Zhong et al., 2023  ###reference_b44###  ###reference_b44###).\nConsequently, if LLMs can predict the masked entity expressing this fact in a cloze-style question, such as in The president of the United States is_ , which is built from the triplet , and the object can be predicted as \u201cJoe Biden,\u201d then it indicates that LLMs possess knowledge of this fact.\nFact editing (De Cao et al., 2021  ###reference_b8###  ###reference_b8###; Zhong et al., 2023  ###reference_b44###  ###reference_b44###) is an indispensable aspect of the continuous development of LLMs. This is because the factual knowledge within models cannot always remain correct over time, which should become outdated as time progresses.\nBased on the preceding context, in fact editing,  can be represented by a tuple  while  can be represented by , denoting the edited factual answer for the original . Thus, the edit descriptor  can be represented as: , and the post-edit model  satisfies  while .\nConsequently, given a collection of fact edits , fact editing involves learning a function  satifying .\nAs the scale of LLMs continues to expand, adjusting model parameters through retraining becomes increasingly challenging. Consequently, efficient  methods without requiring training are receiving more attention. This is also why our work chooses to evaluate using a simple yet efficient knowledge editing method in Section 4  ###reference_###  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Factuality Decoding for LLMs",
            "text": "Our work focuses on potential pitfalls in current factuality decoding strategies for LLMs. Before evaluating the modified LLMs with various decoding methods, it is necessary to thoroughly understand them. Therefore, this section first revisits several strong decoding methods for LLMs\u2019 factuality, then evaluates and analyzes their performance in enhancing the factuality of LLMs.\n\nInference-Time Intervention (ITI) first identifies a sparse set of attention heads with high linear probing accuracy for truthfulness, as defined by the TruthfulQA benchmark. Then, during inference, it shifts activations along these truth-correlated directions. This process is repeated autoregressively until the entire answer is generated.\n\nZhang et al. first construct a factually weak LLM by inducing hallucinations from the original LLMs, and then penalize these induced hallucinations during decoding to enhance the factuality of the generated content. Specifically, they determine the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Decoding Methods",
            "text": "To avoid the resource-intensive nature of existing methods like RLHF (Ouyang et al., 2022; Bai et al., 2022a; Menick et al., 2022) and RLAIF (Bai et al., 2022b), which require significant annotation and computational resources, factuality decoding aims to modify the decoding architecture of LLMs solely to narrow the gap between \"knowing\" and \"telling\". We have selected several representative strong decoding methods, which are introduced individually as follows.\n\nInference-Time Intervention (ITI) (Li et al., 2024b) first identifies a sparse set of attention heads with high linear probing accuracy for truthfulness, as defined by the TruthfulQA benchmark. Then, during inference, it shifts activations along these truth-correlated directions. This process is repeated autoregressively until the entire answer is generated.\n\nDoLa (Chuang et al., 2023) leverages a modular encoding of knowledge to magnify factual knowledge within an LM through a contrastive decoding approach. In this method, the next-word probability output is derived from the disparity in logits between a higher layer and a lower layer. By accentuating the knowledge from higher layers and diminishing that from lower layers, LoRa aims to reduce factual hallucinations.\n\nZhang et al. first constructs a factually weak LLM by inducing hallucinations from the original LLMs, and then penalizes these induced hallucinations during decoding to enhance the factuality of the generated content. Specifically, ICD determines the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Factuality Evaluation",
            "text": "We evaluate the factual enhancement of the decoding method on the TruthfulQA (Lin et al., 2021) and FActScore (Min et al., 2023) benchmark. Evaluation on both benchmarks adheres to the settings of previous studies. For TruthfulQA, we employ multiple-choice-based metrics, specifically MC1, MC2, and MC3 scores. For FActScore, assessment is conducted through retrieve+chatGPT methodology. We conduct the evaluations on two sizes of llama-2-chat (7B,13B) models as base models. The results are presented in Table 1.\n\nThrough observation of the experimental results, we first note that increasing the model size does not lead to a significant improvement in factual accuracy for the llama2 model. When compared to the base llama-2-chat model, the majority of decoding methods enhanced the factual performance of the llama2 model to some extent across both benchmarks.\n\nIn conclusion, the aforementioned factual decoding methods enhance the factual accuracy of base LLMs, guiding LLMs to infer more authentic facts from existing knowledge."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Multi-hop Knowledge Editing Benchmark",
            "text": "We conduct experiments on the multi-hop knowledge editing benchmark to assess whether factuality decoding methods remain effective in updating knowledge. Changing one fact should result in cascading changes to the model\u2019s associated knowledges. For instance, if we modify the UK Prime Minister to be Rishi Sunak, the response to Who is married to the British Prime Minister? should differ. Therefore, this section introduces the concept of multi-hop facts and presents a multi-hop knowledge editing dataset, along with efficient editing methods tailored to it. In our experiments, we design prompts for fact editing and prompt LLMs to answer multi-hop knowledge questions through a chain of thought approach."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Multi-hop Fact Editing",
            "text": "Multi-hop fact editing aims to edit not only a single-hop fact but also all the facts within the multi-hop context that are affected by this edited fact. Formally, we consider two chains of facts, and , which have the same relation set. When editing a single-hop fact in the first fact chain with an edit descriptor, the factual memory of the large model regarding it should be edited to. For instance, regarding the two-hop question Who is married to the British Prime Minister? mentioned above, the original answer should be Carrie Johnson, and the corresponding chain of facts can be described as follows: (United Kingdom, head of government, Boris Johnson), (Boris Johnson, spouse, Carrie Johnson). With a fact edit and an additional fact chain, the edited LLMs should respond with the new rippling answer: Akshata Murthy.\n\nEdits  \n2-hop  \n3-hop  \n4-hop  \nTotal\n\n1  \n2,454  \n855  \n446  \n3,755\n\n2  \n2,425  \n853  \n467  \n3,745\n\n3  \n-  \n827  \n455  \n1,282\n\n4  \n-  \n-  \n436  \n436\n\nAll  \n4,879  \n2,535  \n1,804  \n9,218"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "We conduct experiments on the recent knowledge editing dataset MQuAKE-CF-3k (Zhong et al., 2023) for multi-hop fact editing. MQuAKE-CF-3k comprises 3,000 instances derived from paths extracted from Wikidata (Vrande\u010di\u0107 & Kr\u00f6tzsch, 2014), which consists of fact triples associated with millions of entities. Table 2 presents the statistics of the MQuAKE-CF-3k dataset. The dataset provides multi-hop fact questions with fact chains, along with the answers before and after editing, which are used to evaluate knowledge editing on counterfactual edits."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Editing Methods",
            "text": "We consider efficient and convenient knowledge editing methods that can be flexibly applied to all black-box LLMs, sidestepping the computational burden associated with retraining models. In our experiments, we design prompts for fact editing and prompt LLMs to answer multi-hop knowledge questions through a chain of thought approach."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Knowledge Editing Evaluation for Factuality Decoding",
            "text": "In this section, we evaluate the knowledge editing of factuality decoding methods on the benchmark introduced earlier to explore the impact of factuality decoding on the factual updates of LLMs."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Implementation Details",
            "text": "We employ two different sizes of LLMs, the llama-2-chat 7B and 13B, with the unchanged decoding strategy as the baseline. Following previous studies, we apply two strong factuality decoding strategies, ITI and DoLa, on the llama-2-chat models in our experiments. During inference with llama2 models, we align all decoding methods with all the baseline, including settings such as temperature=0.9, top-=0.95, and others."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Main Results",
            "text": "The experimental results demonstrate that factuality decoding methods lead to a significant decrease in accuracy of the question answering task for knowledge editing, indicating that Factuality Decoding severely impacts the knowledge flexibility of LLMs. Moreover, by comparing the editing results using default decoding and DoLa on llama2 models of different sizes, it is apparent that the impact of model size on knowledge editing is opposite. When the size of the llama2 model is increased, the accuracy of DoLa decreases. This further reveals that as the model size increases, the current factuality decoding exacerbates the detrimental effects on the factual updates of LLMs."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Factuality Accuracy vs Knowledge Flexibility",
            "text": "We consider the performance of LLMs in knowledge editing as indicative of their knowledge flexibility. As shown in Figure 2, we visually present the performance of different factuality decoding methods in knowledge editing alongside the baseline and ChatGPT. It can be observed that ChatGPT, with a particularly large parameter size, achieves the highest accuracy, implying its strong knowledge flexibility. However, the factuality decoding reduces the knowledge flexibility of the llama2 model compared to its original state, suggesting potential pitfalls of factuality decoding for knowledge flexibility in developments of LLMs.\n\nFigure 2 further illustrates the comparison between the decline in knowledge flexibility and the improvement in factuality accuracy. It can be easily observed that the proportion of the decline in knowledge flexibility far exceeds the improvement in factuality accuracy. This prompts us to ponder whether such improvements in factuality accuracy are worth the significant loss in knowledge flexibility. Therefore, we recommend that research on factuality should simultaneously consider both aspects of factuality accuracy and knowledge flexibility."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Case Study",
            "text": "Figure 3 provides a qualitative comparison example for baseline and factuality decoding. Factuality decoding may lead to errors even when knowledge is correctly edited in the baseline. Taking a closer look, the output of factuality decoding consistently reflects answers prior to editing, while the baseline generates correct new answers based on the edits. This indicates that LLMs using factuality decoding exhibit excessive confidence in their own knowledge, thus failing to adjust their answers based on editing prompts."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we evaluate existing factuality decoding methods that enhance the factual accuracy of large language models on the knowledge editing benchmark. The results indicate that while these methods enhance factual accuracy to some extent, they lead to significant loss of knowledge flexibility. This excessive confidence in large language models makes it difficult to carry out knowledge editing. Therefore, our work advocates that research into factual alignment should not overlook the importance of the effectiveness of knowledge editing."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Discussion and Future Work",
            "text": "This paper proposes a new concept regarding the\nfactuality of large language models: LLMs with good factuality should simultaneously balance factuality accuracy and knowledge flexibility.\nThis implies that while accurately conveying factual information, it should also ensure that keeping the knowledge up-to-date is easily achievable for LLMs.\nBased on the validation results of existing factuality decoding methods in this paper, achieving the desired balance seems challenging.\nNot only decoding, but we also hold a skeptical stance regarding methods such as RLHF and SFT for injecting new knowledge or self-alignment in terms of knowledge editing.\nIt appears to present a natural paradox where we strive for LLMs to simultaneously maintain a strong belief in existing knowledge while also being capable of facile modification when necessary.\nTherefore, for future work, we plan to incorporate more methods aimed at improving the factuality of LLMs into our repertoire of knowledge editing evaluation criteria.\nWe aim to further validate their factual updating capabilities and, in the process, observe the inherent relationship between factuality accuracy and knowledge flexibility.\nWe envision establishing a comprehensive validation framework for the factuality of LLMs that integrates both accuracy and flexibility, which will be of paramount practical significance for the long-term development of future large language models."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Ethics Statement",
            "text": "Ethical considerations are of utmost importance in our research endeavors. In this paper, we conscientiously adhere to ethical principles by exclusively utilizing open-source datasets and employing models that are either open-source or widely recognized in the scientific community. Moreover, our proposed method is designed to ensure that the model does not produce any harmful or misleading information. We are committed to upholding ethical standards throughout the research process, prioritizing transparency, and promoting the responsible use of technology for the betterment of society."
        }
    ],
    "url": "http://arxiv.org/html/2404.00216v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "4",
            "5",
            "5.3",
            "5.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.00216v1",
        "paper_title": "Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark",
        "research_background": "### Motivation:\nThe paper is motivated by the need to improve the reliability and factual accuracy of large language models (LLMs). While LLMs have shown impressive performance in various natural language processing tasks, they often produce factually incorrect statements, known as \"hallucinations\". These hallucinations can significantly undermine the reliability of LLMs in real-world scenarios. Therefore, there is a pressing need to find methods that ensure LLMs can generate factually correct information while also retaining the ability to have their knowledge efficiently updated to reflect new and accurate facts.\n\n### Research Problem:\nThe main research problem addressed in the paper is the tension between using factuality decoding methods to improve the accuracy of the factual information LLMs generate and the need for these models to retain the flexibility for efficient knowledge updates through knowledge editing techniques. The paper investigates whether current advanced factuality decoding methods, which encourage LLMs to produce answers that better align with pre-training facts, may potentially hinder the models' ability to be efficiently updated with new factual information.\n\n### Relevant Prior Work:\n1. **LLMs and Hallucinations**:\n   - OpenAI (2022, 2023), Touvron et al. (2023a, b), Wei et al. (2022), Chang et al. (2023), and Liu et al. (2023) demonstrate LLM capabilities across various NLP tasks.\n   - Zhang et al. (2023), Huang et al. (2023), and Tonmoy et al. (2024) discuss the issue of hallucinations, leading to unreliable outputs in LLMs.\n \n2. **Knowledge Editing**:\n   - Sinitsin et al. (2020) and De Cao et al. (2021) propose knowledge editing to allow for efficient modifications to model facts while minimizing adverse effects on unrelated knowledge.\n\n3. **Factuality Decoding**:\n   - Li et al. (2023b, 2024a, b), Chuang et al. (2023), and Zhang et al. (###reference_b42###) explore different factuality decoding methods to generate more factually aligned answers.\n   - Yang et al. (2023), Ovadia et al. (2023), Ouyang et al. (2022), and Bai et al. (2022a) focus on approaches like supervised fine-tuning (SFT) and RLHF to enhance factual accuracy.\n\n4. **Benchmarks for Evaluation**:\n   - Lin et al. (2021) and Min et al. (2023) created the TruthfulQA and FActScore benchmarks to assess the effectiveness of factuality decoding in LLMs.\n   - Zhong et al. (2023) introduced the MQuAKE benchmark, suitable for evaluating knowledge editing in black box LLMs.\n\n### Key Takeaways:\nThe paper highlights the significant trade-offs associated with current factuality decoding methods and underscores the importance of balancing factual accuracy with the flexibility of knowledge updating. The study provides empirical evidence showing that while factuality decoding improves fact-telling, it may hinder the ability to update knowledge efficiently, leading to a rigidity that poses challenges for maintaining up-to-date and accurate information within LLMs. The paper advocates for a dual focus on factual accuracy and knowledge editability in future research to holistically address the issue of hallucinations in LLMs.",
        "methodology": "### Methodology\n\nOur work focuses on potential pitfalls in current factuality decoding strategies for LLMs. Before evaluating the modified LLMs with various decoding methods, it is necessary to thoroughly understand them. Therefore, this section first revisits several strong decoding methods for LLMs\u2019 factuality, then evaluates and analyzes their performance in enhancing the factuality of LLMs.\n\n**Contrastive decoding (CD):** According to Li et al. (2023b), contrastive decoding leverages the distinctions between expert and amateur Language Models (LMs) of varying sizes. It selects tokens that maximize the difference in their log-likelihoods. Consequently, factual knowledge that remains unlearned by the weaker amateur model is highlighted by contrastive decoding in the stronger expert model to enhance factuality.\n\n**Inference-Time Intervention (ITI):** As described by Li et al. (2024b), ITI first identifies a sparse set of attention heads with high linear probing accuracy for truthfulness, as defined by the TruthfulQA benchmark. During inference, it shifts activations along these truth-correlated directions. This process is repeated autoregressively until the entire answer is generated.\n\n**DoLa:** Chuang et al. (2023) propose DoLa, which leverages a modular encoding of knowledge to magnify factual knowledge within a Language Model (LM). The next-word probability output is derived from the disparity in logits between a higher layer and a lower layer. By accentuating the knowledge from higher layers and diminishing that from lower layers, DoLa aims to reduce factual hallucinations. \n\n**Zhang et al.:** In this method, a factually weak LLM is first constructed by inducing hallucinations from the original LLMs. These induced hallucinations are then penalized during decoding to enhance the factuality of the generated content. Specifically, ICD determines the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions.\n\nIn summary, these methods offer distinct mechanisms to enhance the factuality of LLM outputs, whether by leveraging differences across models and layers or by employing targeted interventions during the inference process.",
        "main_experiment_and_results": "**Main Experiment Setup:**\n\n- **Datasets:** The primary dataset used for evaluating the knowledge editing capabilities of the factuality decoding methods on large language models (LLMs) is the Knowledge Editing Benchmark. This benchmark comprises diverse tasks designed to assess a model's ability to update its factual knowledge base effectively.\n\n- **Baselines:** The baselines against which the factuality decoding methods are compared include traditional knowledge editing methods. These methods likely involve fine-tuning and prompt-based approaches, although they are not explicitly detailed in the provided text.\n\n- **Evaluation Metrics:** The evaluation metrics used to assess performance focus on the accuracy and reliability of the factual updates. These metrics gauge how well the LLMs maintain factual correctness post-editing and measure consistency across different factuality decoding methods.\n\n**Main Experimental Results:**\n\nAlthough specific numerical results or performance are not provided in the context, the main findings rely on comparing the effectiveness of factuality decoding methods against baseline approaches. The results reveal significant insights into how factuality decoding influences the ability of LLMs to handle factual updates and retain factual accuracy."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The goal of this ablation study is to assess whether factuality decoding methods remain effective while updating knowledge, particularly considering their impact on the knowledge editing flexibility of large language models (LLMs).",
            "experiment_process": "The experiments are conducted on the multi-hop knowledge editing benchmark. This benchmark tests whether changing one fact can result in cascading changes to associated knowledge. Two knowledge editing methods, In-context editing (ICE) and MeLLo, are employed. ICE generates responses based on new facts as conditions without altering model parameters. On the other hand, MeLLo decomposes multi-hop questions into subquestions during inference, obtaining tentative answers and self-checking their compatibility with edited facts. Prompts are designed for fact editing, and the LLMs are prompted to answer multi-hop knowledge questions through a chain of thought approach.",
            "result_discussion": "The experimental results demonstrate that current advanced factuality decoding methods lead to significant declines in the performance of LLMs on edited factual question-answering tasks. For instance, the accuracy of llama2-7b using the original decoding is 36.8 with MeLLo, while it drops to 6.9 with ICD, representing a significant decrease of 81.3%. This suggests that while factuality decoding helps LLMs convey factual information more accurately, it also poses challenges in updating outdated facts, making it difficult to efficiently edit new knowledge. Consequently, the study highlights the potential risks of current factuality decoding methods and suggests that research into factual alignment should balance both factual accuracy and the flexibility for knowledge editing.",
            "ablation_id": "2404.00216v1.No1"
        },
        {
            "research_objective": "This ablation study aims to explore the trade-off between improved factuality accuracy and the loss of knowledge flexibility resulting from the application of factuality decoding methods in LLMs.",
            "experiment_process": "The evaluation involves comparing the performance of different factuality decoding methods against a baseline and ChatGPT in terms of knowledge editing capability. Knowledge flexibility is measured by the ability of an LLM to adapt and update its knowledge accurately following edits. Various factuality decoding methods are visually presented alongside the baseline and ChatGPT in a performance comparison, as shown in Figure 2.",
            "result_discussion": "The findings indicate that ChatGPT, with its large parameter size, achieves the highest accuracy, suggesting strong knowledge flexibility. However, the llama2 model with factuality decoding demonstrates a reduced knowledge flexibility compared to its original state. The decline in knowledge flexibility far exceeds the improvement in factuality accuracy, raising questions about whether such improvements are worth the significant compromise in flexibility. The study concludes that research on factuality should consider both factuality accuracy and knowledge flexibility.",
            "ablation_id": "2404.00216v1.No2"
        },
        {
            "research_objective": "This case study aims to provide a qualitative comparison between the baseline and factuality decoding to highlight the impact of factuality decoding on knowledge editing outcomes.",
            "experiment_process": "The study qualitatively examines the outputs produced by LLMs when using either the baseline or factuality decoding methods. The comparison is presented in Figure 3, illustrating specific examples where the baseline successfully edits knowledge while factuality decoding fails to do so.",
            "result_discussion": "The case study reveals that factuality decoding consistently generates pre-edit answers, demonstrating excessive confidence in the model's own knowledge. In contrast, the baseline method produces correct new answers based on the edits. This overconfidence with factuality decoding leads to errors, as the output fails to adjust according to the edited prompts, highlighting a fundamental limitation in the approach.",
            "ablation_id": "2404.00216v1.No3"
        }
    ]
}