{
    "title": "IMO: Greedy Layer-Wise Sparse Representation Learning for Out-of-Distribution Text Classification with Pre-trained Models",
    "abstract": "Machine learning models have made incredible progress, but they still struggle when applied to examples from unseen domains. This study focuses on a specific problem of domain generalization, where a model is trained on one source domain and tested on multiple target domains that are unseen during training. We propose IMO: Invariant features Masks for Out-of-Distribution text classification, to achieve OOD generalization by learning invariant features. During training, IMO learns sparse mask layers to remove irrelevant features for prediction, ensuring that the remaining features remain invariant. Additionally, IMO includes an attention module at the token level to focus on tokens useful for prediction. Our comprehensive experiments show that IMO substantially outperforms strong baselines such as prompt-based methods and large language models in terms of various evaluation metrics and settings.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "When deploying natural language processing (NLP) models trained on labeled data in the wild, it is well known that they suffer from poor predictive performance on the samples drawn from distributions different than their training. Although various domain adaptation (DA) methods have been proposed, they assume the availability of labeled/unlabeled data from target domains and/or the target domain information. However, for many real-world applications, especially for early-stage businesses, users may apply their models to arbitrary data so the test data may well be Out-of-Distribution (OOD). Hence the domain information may not be available for DA. In addition, training datasets are often expensive to acquire so they are available only in one domain. Therefore, this work focuses on single-source domain generalization (DG) for text classification, which aims to enable classifiers trained in one source domain to robustly work on the same classification tasks in any unseen OOD data without any model tuning.\n\nPre-trained large language models (LLMs) have drawn a lot of attention due to their strong predictive performance on a variety of tasks. Although generative models or classifiers built on top of pre-trained LLMs outperform prior models in multiple domains, their performance is still not robust on tasks when the testing distribution differs substantially from the training distribution. Recent works show that one of the key reasons is spurious correlations, which refer to the correlations between features and model outputs that are not based on causal relationships.\n\nTo take a step towards \u201ctrain it once, apply it anywhere\u201d, we propose a novel greedy layer-wise Invariant Masking technique for OOD text classification, coined IMO, which selects domain-invariant features and key token representations from appropriate layers of a pre-trained deep transformer encoder to mitigate spurious correlations. The resulting hidden representations are sparse from the top layer to a specific layer of the pretrained model. We demonstrate the effectiveness of this technique through theoretical justifications and extensive experiments. Similar to studies on computer vision tasks, we shed light on how to apply sparsity as an effective inductive bias to deep pre-trained models for OOD text classification. Our contributions are:\n\nWe propose IMO, a novel top-down greedy layer-wise sparse representation learning method for pre-trained text encoders for robust OOD classification by sharply reducing task-specific spurious correlations. In comparison with bottom-up layer-wise and simultaneous search across all layers, we discover that the top-down greedy search is decisive for performance improvement.\n\nWe develop a theoretical framework that elucidates the relationship between domain-invariant features and causal features. Additionally, we provide an explanation of how our method learns invariant features.\n\nOur comprehensive experimental results show that:\n\n(i) Using IMO achieves better performance on the classification of topics and sentiment polarity in the majority of the target domains.\n\n(ii) Using IMO achieves improved performance on social factor classification.\n\n(iii) IMO achieves similar OOD performance with varying sizes of training data. The differences in accuracy between using 1k and 3.5 million training instances for models trained with IMO are less than 6%, and for those trained without IMO is more than 16%. 111Codes are available at https://github.com/WilliamsToTo/IMO."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Domain Generalization. Numerous DG methods have been proposed in the past decade, and most of them are designed for multi-source DG Chattopadhyay et al. (2020); Zhao et al. (2020); Ding et al. (2022); Zhang et al. (2022); Lv et al. (2022). Existing DG methods can be roughly classified into two categories: invariant representation learning and data augmentation. The key idea of the former is to reduce the discrepancy between representations of source domains Muandet et al. (2013); Li et al. (2018a, b); Shao et al. (2019); Arjovsky et al. (2020). The key idea of data augmentation is to generate out-of-distribution samples, which are used to train the neural network with original source samples to improve the generalization ability Xie et al. (2020); Wei and Zou (2019); Volpi and Murino (2019). This paper focuses on single-source DG, where the model is trained on a single source domain, then evaluated on multiple unseen domains. Wang et al. (2021c) proposes a style-complement module to synthesize images with unseen styles, which are out of original distributions. Qiao et al. (2020) proposes adversarial domain augmentation to encourage semantic consistency between the augmented and source images in the latent space. Ouyang et al. (2023) uses a causality-inspired data augmentation approach to encourage network learning domain-invariant features. In terms of text classification, Ben-David et al. (2022); Jia and Zhang (2022) apply prompt-based learning methods to generate a prompt for each sample, then use large language models to predict labels. Causal Representation Learning (CRL). CRL addresses OOD generalization by exploring causal features that lead to labels. It is based on the assumption that causal features are stable across different environments or data selections. Since CRL is very ambitious and even infeasible in real application, a more practical method is invariant representation learning. Peters et al. (2016) investigated that invariant features, to some extent, infer the causal structure. Arjovsky et al. (2020) also assumes that prediction conditioned on invariant features is stable under different environments. Following such assumption, a strand of methods tries to learn invariant features by mitigating spurious correlated features, which vary across environments Muandet et al. (2013); Chattopadhyay et al. (2020); Asgari et al. (2022); Izmailov et al. (2022); Hu et al. (2022b). This paper also follows this thread of methods, where we treat features that don\u2019t affect prediction as spurious correlated features."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Learning Sparse Domain-Invariant Representations",
            "text": "LLMs are pre-trained on large-scale corpora so that they can capture rich correlations between tokens across various domains. To enable trained models incorporating LLMs to work across domains, our key idea originates from the Invariance Assumption that the conditional distributions of labels conditioned on invariant features do not change across domains (Peters et al., 2016  ###reference_b38###). Zhang et al. (2021  ###reference_b67###) show that the assumption can hold, and there is a subnetwork inside a full network that can achieve better OOD performance than the full network. For a specific classification task, such as sentiment polarity analysis, the assumption indicates that there are certain sparse representations that are potential causes of labels (Wang and Jordan, 2022  ###reference_b57###) across domains. Our method IMO realizes this idea by constructing sparse domain-invariant representations from the hidden representations of the selected layers of pre-trained transformer-based encoders.\nLet  be the input space and  be the label space, a domain is characterized by a joint distribution  on . In the context of a single source DG, we have access to the data of one source domain  drawn from its joint distribution . The goal is to learn a predictive model  using only the data sampled from  to minimize the prediction error on  unseen target domains, each of which is associated with a joint distribution . Due to domain drifts, .\nFollowing (Quinzan et al., 2023  ###reference_b41###), we make the same assumptions that (i) , where  denote the features that directly cause , (ii)  is exogenous noise, independent of any features, and (iii)  has no direct causal effect on any features because classification labels are assigned after observing the corresponding texts. Although , we show in \u00a73.3  ###reference_### that under all above assumptions, there is a sparse representation  such that the function  exists in both source and target domains. We empirically study the presence of invariant representations and influence of spurious correlations in \u00a74.3  ###reference_###.\nAs illustrated in Figure 1  ###reference_###, our method constructs sparse domain-invariant representations at both feature and token levels in a top-down manner. At the feature level, given embeddings produced by the transformer block of the top layer, a parametric mask layer identifies invariant features from the embeddings. Then, the mask layer is frozen and the algorithm learns the mask layer for the lower layer. The process is repeated until a pre-specified layer is reached. At the token level, a soft attention mechanism incorporates the selected features from the top layer to identify the tokens strongly correlated with  and use attention weights to create aggregated sparse representations based on the selected features for binary classification. For multi-class classification tasks, a sparse representation is created for each class so that each of them can focus on class-specific information. The model is regularized during training to increase the divergences of the representations between classes.\n###figure_1### For binary classification, we treat the filtering vector  from the last layer  as the query vector and compute the attention weight by performing the matrix product between  and each token embedding from the last layer : .\nHere, the filtering vector and token embeddings are interpreted as matrices, with  and . For an input token sequence, we aggregate the masked token embeddings to obtain a sequence representation , where . Finally, the sequence representation is fed into a fully-connected layer, followed by generating a distribution over the label space as follows: .\nFor the multi-class classification task, we propose using multiple mask layers  in the last layer  to capture corresponding features and tokens for labels . The number of mask layers equals the number of labels. Each label has its own attention weights , and its own representation . Instead of using a fully-connected layer, we use a learnable weight vector per class to project  to a scalar: , where  and . The rationale behind this is that each class should have its own weight vector and hidden representations for encoding class-specific information. Then, we concatenate these scalars to a vector , and compute the predictive distribution by .\nTo encourage mask layers to extract label-specific features, we propose the following regularization term to penalize pairwise cosine similarities between the corresponding mask layers (where  is the number of label-specific mask layers):\nRather than training all mask layers simultaneously, we adopt a layer-wise training procedure to train them sequentially from the top layer to the bottom layer. As illustrated in Figure 1  ###reference_###, for each layer, a new filtering layer, , is introduced on the top of the -th transformer layer, with . Crucially, during this phase, the previously trained mask layers remain frozen to preserve their learned parameters. Upon each layer\u2019s training completion, the model is stored as . This iterative procedure continues until the training of the most bottom filtering vector, , is completed. Consequently, a suite of models, ranging from  to , is collected. We empirically determine the model\u2019s efficacy by evaluating its performance on the validation set from the source domain. The best-performing model is chosen as the model to test on the target domains.\nDuring training, the overall objective for binary classification is to (1) have good predictive performance on classification tasks and (2) maximize sparsity in mask layers to only keep invariant features. When training mask at layer , the loss function is:\nwhere  denotes the cross entropy loss and  denotes the predictive model. , where , is a hyperparameter that controls the balance between predictive performance and sparsity in mask layers.  is the sparse regularization term for mask at layer .\nFor multi-class classification, we add a distance regularization term:\nThe hyperparameter  serves to calibrate the equilibrium between features specific to individual labels and those shared across multiple labels.\n###figure_2### ###figure_3### ###figure_4###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Extraction of Invariant Features",
            "text": "Given a text input , where  is a token in , a transformer-based pre-trained language model is employed to convert  to a continuous token representation. We use hidden states produced by each transformer layer  as token representations, denoted as .  embeds both invariant features (useful for prediction in different domains) and spuriously correlated features (irrelevant for prediction) produced by layer . Based on the Invariance Assumption, the invariant features  ensure  to be the same for each domain .\nIn a transformer layer , the spuriously correlated features are filtered out by performing element-wise multiplication between token representation  and a learnable mask .\nA parametric filtering vector  contains zero and non-zero elements, where we define a trainable weight vector  and a trainable pruning threshold vector . A unit step function\n\nis applied to get a binary mask . By applying element-wise multiplication , the zero elements of  remove corresponding features in token embeddings , while non-zero elements characterize the importance of corresponding features.\nAs the unit step function  is not differentiable, we approximate its derivative by using the derivative estimator proposed in Xu and Cheung (2019  ###reference_b64###) such that all parameters of a mask layer are trainable by using back-propagation and the family of stochastic gradient descent algorithms,\nFollowing Xu and Cheung (2019  ###reference_b64###), we add a sparse regularization term  to the training loss to encourage the sparsity of mask layers:\nwhere  encourages high (but not extremely large) thresholds. A higher threshold leads to removal of more features. During inference, we retain the mask layers to retain invariant features while discarding irrelevant ones."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Identification of Invariant Tokens",
            "text": "Given a long token sequence, not all information is useful for target tasks. For example, function words, such as \u2018the\u2019, or \u2018that\u2019, provide little information for predicting sentiment polarity. Thus, we employ a token-level attention mechanism to focus on important tokens. Instead of using all features of a token representation, we compute attention scores by using only the invariant features. The proposed attention mechanism differs slightly between binary and multi-class classification.\nFor binary classification, we treat the filtering vector  from the last layer  as the query vector and compute the attention weight by performing the matrix product between  and each token embedding from the last layer : .\nHere, the filtering vector and token embeddings are interpreted as matrices, with  and . For an input token sequence, we aggregate the masked token embeddings to obtain a sequence representation , where . Finally, the sequence representation is fed into a fully-connected layer, followed by generating a distribution over the label space as follows: .\nFor the multi-class classification task, we propose using multiple mask layers  in the last layer  to capture corresponding features and tokens for labels . The number of mask layers equals the number of labels. Each label has its own attention weights , and its own representation . Instead of using a fully-connected layer, we use a learnable weight vector per class to project  to a scalar: , where  and . The rationale behind this is that each class should have its own weight vector and hidden representations for encoding class-specific information. Then, we concatenate these scalars to a vector , and compute the predictive distribution by .\nTo encourage mask layers to extract label-specific features, we propose the following regularization term to penalize pairwise cosine similarities between the corresponding mask layers (where  is the number of label-specific mask layers):\nRather than training all mask layers simultaneously, we adopt a layer-wise training procedure to train them sequentially from the top layer to the bottom layer. As illustrated in Figure 1  ###reference_###  ###reference_###, for each layer, a new filtering layer, , is introduced on the top of the -th transformer layer, with . Crucially, during this phase, the previously trained mask layers remain frozen to preserve their learned parameters. Upon each layer\u2019s training completion, the model is stored as . This iterative procedure continues until the training of the most bottom filtering vector, , is completed. Consequently, a suite of models, ranging from  to , is collected. We empirically determine the model\u2019s efficacy by evaluating its performance on the validation set from the source domain. The best-performing model is chosen as the model to test on the target domains.\nDuring training, the overall objective for binary classification is to (1) have good predictive performance on classification tasks and (2) maximize sparsity in mask layers to only keep invariant features. When training mask at layer , the loss function is:\nwhere  denotes the cross entropy loss and  denotes the predictive model. , where , is a hyperparameter that controls the balance between predictive performance and sparsity in mask layers.  is the sparse regularization term for mask at layer .\nFor multi-class classification, we add a distance regularization term:\nThe hyperparameter  serves to calibrate the equilibrium between features specific to individual labels and those shared across multiple labels.\n###figure_5### ###figure_6### ###figure_7###"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Theoretical Analysis",
            "text": "Based on our assumptions,  exists, when  are the parent nodes of  in the underlying causal graph. Because  are a subset among all possible hidden representations correlated with , there should be a subset of hidden representations serving as parents of , otherwise the invariance assumption does not hold. Due to the widely used faithfulness assumption stating that statistical independences imply the corresponding causal structures (Neal, 2020  ###reference_b35###), we aim to find out , where  is any feature set non-overlapped with .\nWe start our theoretical analysis by introducing a sparsity regularization term , which counts the number of edges between  and the random variables of features in an underlying causal graph, where  is the variable for labels and  denotes the random variable of the feature . Then we introduce a loss function , analogous to Eq. (4  ###reference_###).\nConsidering the simplest case that there is only a causal feature  and a non-causal feature , the corresponding random variables are denoted by  and . From any causal graphs in Fig. 2  ###reference_###, we conclude that  so that the cross entropy term in  remains the same when using the term , but the loss decreases after removing the non-causal feature from the loss due to the regularization term .\nThe two feature case can be easily extended to the case having more than two features. It is trivial that excluding a non-causal feature from the loss  leads to the decrease of  due to the Markov property of causal graphs (Peters et al., 2017  ###reference_b39###).\nIf there is no edge between  and  in a causal graph, then .\nDuring training, we start with a loss  with a complete set of features. If a non-causal feature  is removed,  decreases according to Corollary 1. In contrast, if a causal feature  is removed, the cross entropy term increases because the mutual information . Namely,  adds additional information for predicting . However, in that case,  may still decrease if the increase of  is smaller than the decrease of the regularization term , where . The exceptional case can be mitigated if  is sufficiently small. As a result, the loss  provides an effective way to guide the search for the features serving as the causes of the labels, although we cannot recover the underlying true causal graphs. Herein, the loss (4  ###reference_###) is a surrogate of  by using a deep neural network."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We show that our approach significantly outperforms the competitive baselines in almost all settings, followed by empirically verifying that domain-invariant sparse representations indeed exist and spurious features deteriorate model performance, as well as justifying the effectiveness of top-down greedy search strategy and individual modules in the ablation study.\n\nWe evaluate our method on binary and multi-class classification tasks. Herein, we adopt accuracy as the metric for binary sentiment polarity classification and macro-F1 for multi-class classification tasks. All models are trained with five different random seeds to assess the statistical significance.\n\nThe datasets for binary sentiment analysis include Amazon Review Polarity, Yelp Review Polarity, IMDB, TweetEval Sentiment, and Yahoo! Answers Sentiment. For multi-class classification, we consider topic classification task in the AG News dataset and social factor prediction task in SocialDial. More details about datasets can be found in Appendix A.2.\n\nFor domain generalization, we include models like PADA, an example-based autoregressive prompt learning algorithm, and PDA, a prompt-based learning algorithm for domain generalization.\n\nLarge Language Models such as Alpaca-7B, Alpaca-7B-LoRA, and ChatGLM-6B are used, applying few-shot in-context learning. The specific query templates used for the LLMs can be found in Appendix A.3.\n\nData augmentation techniques are utilized as they benefit domain generalization tasks. EDA uses four operations to augment text data. UDA employs back-translation to generate diverse paraphrases while preserving the semantics of the original sentences.\n\nTable 1 reports the comparisons between our method and the baselines on sentiment polarity classification. Interestingly, our method achieves superior performance over baselines in various settings. Data augmentation methods show slightly inferior performance in comparison to the simple fine-tuning of models in terms of average accuracy, suggesting that simply back-translating or paraphrasing instances within source domains does not enhance performance on target domains.\n\nAs shown in further tables, our method outperforms all baselines in terms of average macro-F1 on AG News and SocialDial, showcasing a substantial performance gap when handling difficult tasks.\n\nWe inspect shared representations at both feature and token levels. Invariant features are expected to have non-zero values across domains. Taking the best performing model in sentiment analysis as an example, we train the model in each domain respectively and visualize its masks in each domain. Observations confirm the presence of shared features selected by the masks.\n\nTo study whether our proposed masking mechanism indeed identifies robust features, we compare the performance using the selected features with the non-selected ones. Specifically, we run additional experiments by replacing the learned binary masks, training a model using those non-selected features. The results show that models using the non-selected features have a significant accuracy reduction in source domains and perform worse than using all features. In target domains, the corresponding performance drop using non-selected features is significant, affirming that our masks mitigate the use of spurious features."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "We evaluate our method on binary and multi-class classification tasks. We adopt accuracy as the metric for binary sentiment polarity classification and macro-F1 for multi-class classification tasks. All models are trained with five different random seeds to assess the statistical significance.\n\nThe datasets for binary sentiment analysis include Amazon Review Polarity Zhang et al. (2015a), Yelp Review Polarity Zhang et al. (2015a), IMDB Maas et al. (2011), TweetEval Sentiment Barbieri et al. (2020), and Yahoo! Answers Sentiment Li et al. (2019). \n\nFor multi-class classification, we consider the topic classification task in the AG News dataset Gulli (2005); Del Corso et al. (2005); Zhang et al. (2015b) and the social factor prediction task in SocialDial Zhan et al. (2023). More details about datasets can be found in Appendix A.2.\n\nDomain Generalization Models.\nPADA Ben-David et al. (2022) is an example-based autoregressive prompt learning algorithm for domain generalization based on the T5 language model Raffel et al. (2020). PDA Jia and Zhang (2022) is a prompt-based learning algorithm for domain generalization.\n\nLarge Language Models.\nAlpaca-7B Taori et al. (2023) is a baseline, which is finetuned from LLaMA 7B Touvron et al. (2023) on 52K instruction-following data generated by self-instruct Wang et al. (2022). Alpaca-7B-LoRA is a finetuned Alpaca-7B model using low-rank adaptation Wang (2023); Hu et al. (2022a). ChatGLM-6B THUDM (2023) is an open large language model based on General Language Model Du et al. (2022), optimized for Chinese question-answering and dialogue. All LLMs use few-shot in-context learning. The specific query templates used for the LLMs can be found in Appendix A.3.\n\nData Augmentation.\nWiles et al. (2022); Gokhale et al. (2022) find data augmentation benefits domain generalization tasks. EDA Wei and Zou (2019) uses four operations (i.e., synonym replacement, random insertion, random swap, and random deletion) to augment text data. UDA Xie et al. (2020) uses back-translation to generate diverse paraphrases while preserving the semantics of the original sentences. PGB Shiri et al. (2023) generates syntactically and lexically diversified paraphrases using a fine-tuned BART."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Domain Generalization Results",
            "text": "Table 1 reports the comparisons between our method and the baselines on sentiment polarity classification. Our method achieves superior performance over all baselines in 7 of 12 settings. Data augmentation methods show slightly inferior performance in comparison to the simple fine-tuning in terms of average accuracy. This suggests that simply back-translating or paraphrasing instances within source domains does not enhance performance on target domains.\n\nAs shown in Table 2 and Table 3, our method outperforms all baselines in terms of average macro-F1 by 3.22% and 5.16% on AG News and SocialDial respectively. Among baselines, the strongest performance surpasses Alpaca-7B, Alpaca-7B-LoRA, and ChatGLM by a large margin. This superior performance shows that current open-source large language models still have a substantial performance gap when handling difficult tasks."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Analysis of Spurious Features",
            "text": "We inspect shared representations at both feature and token levels. Invariant features are expected to have non-zero values across domains. Taking the best performing model in the sentiment analysis as an example, we train the model in each domain respectively and visualize its masks of the top layer in each domain. As depicted in Fig. 3, there are indeed a set of features shared across domains selected by the masks. We further compute Cosine similarities between the filtering vectors of the top layer trained on different source domains. As shown in Table 8 in the Appendix, their similarities range from 0.68 and 0.85. At the token level, we inspect the shared attention weights visualized in Fig.4 (see Appendix A3), which indicate the keywords shared across domains in sentiment analysis, such as \u201cgreat\u201d and \u201cslow.\u201d\n\nTo study whether our proposed masking mechanism indeed identifies robust features, we compare the performance of using the selected features with the non-selected ones. Specifically, we run additional experiments by replacing the learned binary masks, followed by freezing all parameters except the classification head and training a model using those non-selected features. The results in Table 4 show that models using the non-selected features have an approximate 6% accuracy reduction in source domains and perform worse than using all features. In target domains, the corresponding performance drop using non-selected features is significantly higher than that using both our method as well as using all features. Hence, our masks indeed mitigate the use of spurious features."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Ablation Study",
            "text": "We compare top-down greedy search with alternative methods: bottom-up layer-wise search (B2T), simultaneous search (w/o sq), and only applying a mask on the last layer (last). From Table 1, 2, and 3, we can tell that top-down greedy performs significantly better than the alternative competitors. We conjecture that top-down layer-wise learning serves as a regularization method that reduces the risk of losing crucial features that are well correlated with , and the corresponding optimization problem is easier to solve than learning all mask layers simultaneously. Representations from higher layers are shown to be more context-specific than lower layer representations Ethayarajh (2019). In contrast, the bottom-up approach may drop key features in lower layers that significantly contribute to important higher layer features.\n\nWe compare variants of IMO by using varying backbone models and removing the corresponding components. For backbones, we compare T5, denoted as IMO-T5. To study the contribution of each component in our approach, we conduct experiments where we exclude the mask layers, attention mechanisms, or both. These models are denoted by w/o , w/o , and w/o , respectively. The corresponding results are reported in Table 5, Table 11, and Table 10 in Appendix. For comparison between backbones, we find that encoder-decoder neural architectures (i.e., T5) consistently achieve better performance. Compared with variants that remove both the attention module and mask layers, IMO with the attention module or mask module has a significant performance improvement in terms of accuracy or F1 on average, which justifies the usefulness of both modules.\n\nAdditionally, we compare IMO with various sparsity methods to implement mask layers, including STR Kusupati et al. (2020), STE Bengio et al. (2013); Liu et al. (2020), and Scalar, which uses a learnable single scalar instead of the threshold vector. All those alternative methods lead to a significant drop, as seen in Table 5.\n\nTo explore the influence of source domain training data size on performance within target domains, we train models based on T5 with and without our method on the Amazon review dataset with varying sizes of training data (i.e., 1k, 10k, 100k, 1M, and 3.6M). The results in Table 6 show that our method depends significantly less on training data size, though more training data can improve the performance overall. Notably, 1k training data yields a remarkable decline for the models without using IMO, while the corresponding performance reduction is significantly less by using our method."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper presents a novel method, coined IMO, which is a greedy layer-wise representation learning method aiming to improve single-source domain generalization on pre-trained deep encoders for text classification tasks. The key idea is to retain invariant features through trainable mask layers and incorporate a token-level attention module to focus on the tokens that directly lead to the prediction of labels. Through extensive experiments, we demonstrate that IMO achieves superior OOD performance over competitive baselines on multiple datasets. The visualization of masks and attention weights empirically justifies the effectiveness of identified invariant sparse representations."
        }
    ],
    "url": "http://arxiv.org/html/2404.13504v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.13504v1",
        "paper_title": "IMO: Greedy Layer-Wise Sparse Representation Learning for Out-of-Distribution Text Classification with Pre-trained Models",
        "research_background": "### Paper's Motivation:\nThe motivation behind this paper is to address the challenge of deploying natural language processing (NLP) models in real-world scenarios where these models encounter out-of-distribution (OOD) data, which significantly differs from the training data. Traditional domain adaptation (DA) methods are inadequate for these scenarios because they assume the availability of labeled or unlabeled data from target domains, which is often not feasible. Therefore, there is a strong need for models that can generalize robustly to unseen OOD data without requiring target domain information or further model tuning.\n\n### Research Problem:\nThe core research problem is improving text classification performance in single-source domain generalization (DG) settings, particularly in the presence of OOD data. The paper seeks to develop a method that can enhance the ability of pre-trained large language models (LLMs) to generalize well across different domains by mitigating spurious correlations and selecting domain-invariant features.\n\n### Relevant Prior Work:\nThe paper builds on several strands of prior research:\n- Understanding the limitations of NLP models in OOD settings and the resulting performance degradation (Wang et al., 2021b).\n- Domain adaptation techniques that work with access to target domain data (Liu et al., 2022; Saunders, 2022).\n- Recognition that pre-trained LLMs, despite their performance, are not inherently robust to distribution shifts (Bang et al., 2023).\n- Insights into the issue of spurious correlations causing performance issues in variable domains (Wang et al., 2021a; Feng et al., 2023; Veitch et al., 2021).\n- Prior work on leveraging sparsity for domain generalization, particularly in computer vision, providing a foundation for applying similar concepts to NLP (Zhang et al., 2021).\n\n### Proposed Solution:\nThe proposed solution, IMO (Invariant Masking technique for OOD text classification), is a greedy layer-wise sparse representation learning method that selects domain-invariant features and key token representations from different layers of a pre-trained deep transformer encoder. This method aims to significantly reduce task-specific spurious correlations, providing a way to improve the robustness of text classifiers in OOD settings.\n\n### Contributions:\n- Introduction of IMO, a top-down greedy layer-wise sparse representation learning method for robust OOD classification.\n- Development of a theoretical framework explaining the relationship between domain-invariant and causal features.\n- Comprehensive experimental results demonstrating the effectiveness of IMO, including outperforming competitive baselines like ChatGPT with fewer parameters and achieving consistent OOD performance regardless of training data size variations.",
        "methodology": "The methodology section describes a novel approach, named **IMO (Iterative Mask Optimization)**, for out-of-distribution (OOD) text classification using pre-trained large language models (LLMs). The key idea stems from the **Invariance Assumption**, which asserts that the conditional distributions of labels given invariant features remain consistent across different domains.\n\n### Key Components and Innovations:\n\n1. **Invariance Assumption**:\n   - Based on the principle that certain invariant features, which truly cause labels, remain constant across domains, resulting in a stable prediction performance on out-of-distribution data.\n\n2. **Subnetwork Exploration**:\n   - Inspired by the findings of Zhang et al. (2021), which suggest that a specific subnetwork within a full network can yield better OOD performance by focusing on domain-invariant features.\n\n3. **Sparse Representation Construction**:\n   - IMO's core approach involves creating sparse, domain-invariant representations from the hidden layers of pre-trained transformer encoders. This involves a **greedy layer-wise process**, sequentially identifying and freezing invariant features, starting from the top-most layer down to a pre-specified lower layer.\n\n4. **Top-Down Mask Filtering**:\n   - At each layer, parametric mask layers are used to filter out non-invariant features. These masks are trained to focus on invariant features at both the feature and token levels:\n     - **Feature Level**: Mask layers identify and retain only the invariant features produced by the transformer's top layers.\n     - **Token Level**: A soft attention mechanism uses the identified invariant features to select tokens strongly correlated with the labels.\n\n5. **Binary and Multi-Class Classification**:\n   - **Binary Classification**: Uses a filtering vector at the final layer to compute attention weights for token embeddings, aggregating them into a sequence representation which is then fed into a fully-connected layer to predict the label probabilities.\n   - **Multi-Class Classification**: Incorporates multiple mask layers at the final layer, each dedicated to a specific class, to capture class-specific features. Each class has its own set of attention weights and representation vector, projected to scalars and concatenated to form the final predictive distribution.\n\n6. **Regularization and Sparsity**:\n   - To encourage the masks to focus on invariant features, a regularization term penalizes the pairwise cosine similarities between the masks, promoting sparsity and invariance.\n   - Addition of a distance regularization term in multi-class classification to balance the label-specific features and shared features.\n\n7. **Layer-wise Training Procedure**:\n   - Instead of training all masks simultaneously, IMO employs a sequential training method from the top layer to the bottom layer. This iterative process ensures that the previously trained mask layers retain their learned parameters, allowing the model to build progressively more refined sparse representations.\n\n8. **Loss Functions**:\n   - The objective for binary classification includes terms for predictive accuracy and sparsity, combining cross-entropy loss with a sparsity regularization term.\n   - In multi-class classification, an additional distance regularization term is included to manage the balance between unique and shared features.\n\n9. **Empirical Validation**:\n   - Various models are evaluated on a validation set drawn from the source domain to determine the best-performing model, which is then tested on unseen target domains for final evaluation.\n\n### Summary:\nIMO is an innovative technique that effectively leverages pre-trained LLMs and the principles of invariant representation to enhance OOD text classification. Its core strength lies in constructing sparse, domain-invariant representations via a top-down, layer-wise process, with rigorous regularization to ensure feature invariance and minimize spurious correlations.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Main Experiment Setup\n\n**Tasks:** \nThe main experiment involves both binary and multi-class classification tasks:\n- **Binary sentiment polarity classification**\n- **Multi-class topic classification** on AG News dataset and social factor prediction tasks on SocialDial.\n\n**Evaluation Metrics:**\n- **Accuracy** for binary sentiment polarity classification.\n- **Macro-F1** for multi-class classification tasks.\n\n**Datasets:**\n- **Binary sentiment analysis:** \n  - Amazon Review Polarity\n  - Yelp Review Polarity\n  - IMDB\n  - TweetEval Sentiment (modified for binary classification)\n  - Yahoo! Answers Sentiment\n- **Multi-class classification:**\n  - AG News\n  - SocialDial\n\n**Baselines:**\n- Pre-trained models: BERT, BERT-zh, BART, BART-zh, ChatYuan\n- Domain generalization models: PADA, PDA\n- Large Language Models (LLMs): ChatGPT (gpt-3.5-turbo), Alpaca-7B, Alpaca-7B-LoRA, ChatGLM-6B\n- Data augmentation methods: EDA, UDA, PGB\n\n**Training Setup:**\n- All models were trained with five different random seeds to ensure statistical significance.\n  \n#### Main Experimental Results\n\n**Binary Sentiment Polarity Classification:**\n- **Performance Comparison:** Our method using BART as the backbone (IMO-BART) outperforms all baselines, achieving superior performance in 7 out of 12 settings and surpassing the best baseline, ChatGPT, by an average of 2.63%.\n\n**Multi-class Classification:**\n- **AG News Dataset:** Our method exceeds all baselines in terms of average macro-F1 by 3.22%.\n- **SocialDial:** Outperforms all baselines by 5.16% in macro-F1.\n- **Baseline Comparison:** Among baselines, ChatGPT exhibits the highest performance on both datasets, significantly surpassing other models like Alpaca-7B, Alpaca-7B-LoRA, and ChatGLM, highlighting a notable performance gap between current open-source LLMs and ChatGPT."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the effectiveness of the top-down greedy search method for learning sparse masks in comparison with alternative search methods for Out-of-Distribution (OOD) text classification.",
            "experiment_process": "The study compares the top-down greedy search method with alternative search methods including bottom-up layer-wise search (B2T), simultaneous search (w/o sq), and applying a mask only on the last layer (last). The performance of these methods is evaluated via experiments detailed in Table 1, Table 2, and Table 3. Additionally, the study compares different backbone models (BART, T5, BERT) and examines the contribution of each component (mask layers, attention mechanisms) in the IMO approach. The results are stated in Table 5, Table 10, and Table 11 in the appendix. Various sparsity methods (STR, STE, Scalar) are also tested for implementing mask layers. Lastly, the study explores the impact of source domain training data size on performance in target domains using the Amazon review dataset with training data sizes ranging from 1k to 3.6M.",
            "result_discussion": "The results indicate that the top-down greedy search significantly outperforms alternative methods, likely due to better regularization and contextual feature retention. Encoder-decoder models (BART, T5) perform better than encoder-only models (BERT). The presence of either the attention or mask module in IMO leads to significant performance improvements, highlighting their importance. Alternative sparsity methods result in a notable performance drop. Regarding training data sizes, models using IMO are less dependent on the quantity of training data compared to those not using IMO, although more data generally enhances performance.",
            "ablation_id": "2404.13504v1.No1"
        },
        {
            "research_objective": "To investigate the contribution of individual components (mask layers and attention mechanisms) within the IMO approach for OOD text classification with different backbone models.",
            "experiment_process": "Variants of IMO are evaluated by removing specific components (mask layers, attention mechanisms, or both). These experiments are conducted using backbone models BART, T5, and BERT, denoted as IMO-T5 and IMO-BERT. The results for these component-exclusion experiments are presented in Table 5, Table 10, and Table 11 in the appendix.",
            "result_discussion": "IMO variants that include either the attention module or the mask module demonstrate significant improvements in accuracy and F1 scores compared to models lacking these components, justifying the utility of both modules. Encoder-decoder models consistently outperform encoder-only models.",
            "ablation_id": "2404.13504v1.No2"
        },
        {
            "research_objective": "To assess the robustness of the IMO method across different training data sizes within the source domain.",
            "experiment_process": "Models based on BART, trained with and without IMO, are evaluated on the Amazon review dataset with training data sizes of 1k, 10k, 100k, 1M, and 3.6M. The results of these experiments are shown in Table 6.",
            "result_discussion": "The study finds that models using IMO depend significantly less on the size of training data for source domains. While increased training data generally improves performance, models utilizing IMO exhibit a notably smaller performance decrease with very low training data sizes (1k).",
            "ablation_id": "2404.13504v1.No3"
        }
    ]
}