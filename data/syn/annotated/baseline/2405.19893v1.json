{
    "title": "Similarity is Not All You Need: Endowing Retrieval-Augmented Generation with Multi\u2013layered Thoughts",
    "abstract": "In recent years, large language models (LLMs) have made remarkable achievements in various domains. However, the untimeliness and cost of knowledge updates coupled with hallucination issues of LLMs have curtailed their applications in knowledge-intensive tasks, where retrieval-augmented generation (RAG) can be of help. Nevertheless, existing retrieval-augmented models typically use similarity as a bridge between queries and documents and follow a retrieve-then-read procedure. In this work, we argue that similarity is not always the \u201cpanacea\u201d and totally relying on similarity would sometimes degrade the performance of retrieval-augmented generation.\n\nTo this end, we propose MetRag, a Multi\u2013layEred Thoughts enhanced Retrieval-Augmented Generation framework. To begin with, beyond existing similarity-oriented thought, we embrace a small-scale utility model that draws supervision from an LLM for utility-oriented thought and further come up with a \u201csmarter\u201d model by comprehensively combining the similarity- and utility-oriented thoughts. Furthermore, given the fact that the retrieved document set tends to be huge and using them in isolation makes it difficult to capture the commonalities and characteristics among them, we propose to make an LLM as a task-adaptive summarizer to endow retrieval-augmented generation with compactness-oriented thought. Finally, with multi-layered thoughts from the precedent stages, an LLM is called for knowledge-augmented generation.\n\nExtensive experiments on knowledge-intensive tasks have demonstrated the superiority of MetRag.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, large language models (LLMs) such as GPT4 have made remarkable achievements in a variety of tasks due to their marvelous capability in language comprehension and generation. However, the untimeliness and cost of knowledge updates, together with hallucinations issues of LLMs, have curtailed their applications in knowledge-intensive tasks to a large extent, where retrieval-augmented generation (RAG) approaches that prepend documents to the query without updating the underlying language models would come in handy. Existing retrieval-augmented generation approaches are typically similarity-based, i.e., they retrieve documents from an external corpus based on similarity. The retrieved documents are prepended as context for LLMs independently for generation augmentation. These approaches have been found to outperform purely parametric LLMs, especially in some knowledge-intensive generation tasks.\n\nIn this work, we argue that similarity is not always the \"panacea\" for retrieval-augmented generation and totally relying on similarity would sometimes degrade the performance. As is shown in an example, when a user types in a query about an author, a similarity-driven retrieval system might rank documents according to similarity metrics, which may not always provide the most informative content. Given the limitations in current approaches, beyond similarity, we aim to endow retrieval-augmented generation with multi-layered thoughts (i.e., utility- and compactness-oriented thoughts) for performance boosting.\n\nHowever, the solution is quite non-trivial and needs to tackle essential challenges. The first challenge (C1) is training a model that perceives utility-oriented thoughts rather than solely similarity, requiring external labeled data, which is hard to obtain. Though LLMs can serve as data annotators and come up with a high-quality corpus for model training, their innate uncontrollable characteristics can sometimes deteriorate performance. The second challenge (C2) involves document summarization to reduce the burden imposed by numerous documents on LLMs and better capture commonalities between them. Simple summarization may not retain the most important information relevant to the input query, necessitating a summarization model aligned with the task itself and possessing compactness-oriented thoughts.\n\nTo this end, we propose MetRag. With an LLM serving as supervision on document utility regarding the input query, we develop a utility model aligning with the LLM\u2019s feedback, equipping it with utility-oriented thoughts. Combining similarity- and utility-oriented thoughts, we consider the outputs of both a similarity model and a utility model. Further, to endow the summarization model with compactness-oriented thoughts, we distill summary ability from a strong teacher model and utilize a subsequent reward model to refine the summarization model's alignment with the end task.\n\nWith the multi-layered thoughts derived from the precedent stages, an LLM is used for knowledge-augmented generation. We evaluate the proposed MetRag on multiple knowledge-intensive tasks, with extensive experiments and analysis demonstrating its superiority."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) approaches tend to enhance LLMs by following a retrieve-then-read pipeline when given the input query. This involves first retrieving a set of documents from an external corpus, then utilizing the retrieved documents as side information for an LLM to make the final prediction. This pipeline has demonstrated superior performance, especially in knowledge-intensive tasks by fine-tuning or directly prepending to LLMs.\n\nHowever, the essence of RAG is the quality of retrieved passages, yet many existing approaches rely on similarity-based retrieval that tends to ignore the underlying utility of associated passages. While some methods have made progress in introducing the power of LLMs in augmenting retrieval ability and even achieving adaptive retrieval, purely depending on the capability of LLMs can be risky since their innate \u201cuncontrollable\u201d characteristics can sometimes degrade performance. Additionally, these methods often ignore the potential inner relationships among retrieved passages and use them in isolation, which can deteriorate performance."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Task-Oriented Summarization",
            "text": "Large language models (LLMs) have made remarkable achievements in a variety of domains such as question answering and summarization. However, calling large-scale commercial LLMs is costly and may induce data leakage issues. To address this, many approaches focus on distilling the abilities of large-scale LLMs to smaller models to enhance their capabilities for downstream tasks, such as generating high-quality summaries through distillation. For knowledge-intensive tasks, simple summarization is not optimized, as it may not retain the most important information relevant to the input query. Therefore, it is necessary to generate summaries associated with downstream tasks. Current methods often design intricate samples and perform distillation without a further aligning strategy for performance boosting, which limits their effectiveness in applications."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "The proposed approach",
            "text": "I'm sorry, but it seems like you've mentioned a placeholder or an incomplete request related to Figure 2. Could you please provide the specific section of the text you would like me to revise or any particular details you would like me to address?"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Overview",
            "text": "In this work, we propose MetRag, as shown in Figure 2. MetRag endows retrieval-augmented generation with multi-layered thoughts by firstly embracing utility-oriented thoughts and combining similarity and utility of documents for performance boosting (detailed in Section 3.2) and further pursuing compactness-oriented thoughts via a task-adaptive summarizer (detailed in Section 3.3), finally incorporating the derived multi-layered thoughts for answer generation (detailed in Section 3.4)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "A Tale of Two \u201cModels\u201d",
            "text": ""
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Similarity Model as an Off-the-shelf Retriever",
            "text": "Given an input query, an off-the-shelf dense retriever is incorporated to map the input to a low-dimensional embedding such that we can efficiently retrieve relevant documents with respect to query from a given corpus via a predefined similarity metric (which could be cosine similarity and so on), where the similarity score between the query and document can be computed. With the derived similarity scores, the Top- documents that have the highest similarity scores with respect to input query are retrieved for enhancing follow-up tasks."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 LLM\u2019s Supervision Empowered Utility Model",
            "text": "In this work, we argue that similarity is not always the ideal solution for information retrieval, and solely relying on it can sometimes degrade the performance of retrieval-augmented generation approaches. As illustrated in Figure 1, documents with higher information gain relative to the input query may rank lower due to inadequately low similarity scores. Inspired by the success of large language models (LLMs) in various tasks, we aim to incorporate an LLM for supervision on document utility. In this paper, we define the utility of a document concerning a question by its usefulness in assisting an LLM to answer the question, modeled by the normalization of the probability of generating correct answers. \n\nTo begin, given the top documents set that a retriever considers most similar to the input query, the approximated similarity likelihood of each document can be formalized using a temperature hyperparameter. This parameter controls the smoothness of the probability distribution, with higher values producing a softer distribution and lower values resulting in a \"harder\" max operation. \n\nTo enhance the retriever with LLM insights on retrieval utility, we incorporate an LLM as the supervision signal on document utility concerning the input query. This approach allows the trained utility model to consider both similarity and the utility a document provides. \n\nWe bridge the gap between similarity distribution and LLM-supervision enhanced utility distribution by minimizing the KL-divergence of these two distributions. During training, the parameter of the LLM is frozen, and only the parameters of the retriever are updated, resulting in the development of the utility model. This optimization process remains lightweight. Additionally, we introduce an empty string to the training process to enable the utility model to assess whether introducing a document for a query can improve utility, thus achieving selective retrieval."
        },
        {
            "section_id": "3.2.3",
            "parent_section_id": "3.2",
            "section_name": "3.2.3 Reunion of Similarity- and Utility- oriented Thoughts",
            "text": "Considering that an LLM would sometimes go out of control and produces unreliable supervision signals which degrade the model performance sometimes yet dense retrievers trained on accurately labelled corpus remain stable with relevance guarantee, though sometimes useless, we further combine similarity- and utility-oriented thoughts and take the output of a similarity model into consideration via an integration strategy. Therefore, the final score between document and the input query can be defined as follows:\n\n\\[ \\text{FinalScore}(d, q) = \\alpha \\cdot \\text{Sim}(d, q) + \\beta \\cdot \\text{Util}(d, q) \\]\n\nwhere \\(\\text{Sim}(d, q)\\) and \\(\\text{Util}(d, q)\\) find value of the top-k largest similarity score and top-n largest utility score among retrieved documents through similarity and utility, respectively. Finally, only documents with \\(\\text{FinalScore}(d, q) > \\theta\\) are permitted to proceed to the subsequent stages. With the derived final score, we can easily obtain the current documents set \\(\\mathcal{D}_c\\) w.r.t. input query \\(q\\) for follow-up tasks."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Pursuit of Compactness-oriented Thoughts",
            "text": "Given that retrieved documents are often more than one, using them in isolation due to the context limitation of LLMs or simply aggregating the top documents without considering the relationships between them makes it difficult to capture the commonalities and characteristics among them. This can confuse LLMs, thus incurring information loss and potentially degrading performance, where text summarization can be helpful. \n\nHowever, simple summarization cannot ensure that the most important information relevant to the input query is retained. Therefore, it is necessary to train a summarization model that aligns with the task itself. We propose the Task-adaptive Summarizer, which not only reduces the computational costs in end tasks but also alleviates the burden on LLMs to identify relevant information in a large set of retrieved documents, thereby facilitating compactness-oriented thoughts."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 Distilling from Strong Teacher Models",
            "text": "To initiate the summarization process, we design instructions via randomly sampled queries and retrieved documents. Given a query, we harness the expertise of a sophisticated teacher model, e.g., GPT-4, to extract summarization proficiencies that facilitate the creation of a preliminary summary and compile an initial corpus consisting of Instruction-Summary pairs, where the instruction and summary are defined as follows: where the summarization instruction, Template is the summarization prompt template (detailed in Section A.1), and is the -th document retrieved for the query from the former stage."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Alignment via End-task Feedback",
            "text": "To ensure the faithfulness of the summarizer to the end task, inspired by the principles of the DPO Rafailov et al. (2023), we incorporate the LLM\u2019s performance on the end task as a reward for the summarizer. Given a query, we adeptly produce concise summaries of the retrieved documents to generate a prompt for the end task response, with a label indicating whether the response is correct. This process accords with a specific distribution. \n\nFormally, we define the training corpus of this aligning process as consisting of triplets for each query. With a reward model parameterized to estimate, the binary classification loss can be defined with the sigmoid function. Furthermore, we follow the concept of DPO that eschews the need to explicitly estimate the reward model by solving it as a function of the language model policy."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Knowledge-augmented Generation",
            "text": "With the input query and the external knowledge derived from the former stages, we can directly call an LLM (which can be fine-tuned in a supervised manner using question answering datasets), where its knowledge-augmented generation of answer can be formalized as follows:\n\n\\[ \\hat{a} = \\arg\\max_a P(a \\mid q, K) \\]\n\nwhere \\( P(a \\mid q, K) \\) is the probability of the answer \\( a \\) given the query \\( q \\) and the external knowledge \\( K \\), and \\(\\arg\\max\\) denotes the argument of the maximum, i.e., the answer \\(\\hat{a}\\) for which \\( P(a \\mid q, K) \\) is maximized."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We split all train, validation and test sets of datasets following Izacard et al. (2023); Asai et al. (2023b).\nOur training data includes randomly sampled instruction-following input-output pairs. We randomly sample 50k pairs from training sets of NQ, TriviaQA and HotpotQA to train our model in different stages.\nAll experiments are conducted using 4 NVIDIA A100 GPUs.\nWe train the utility model for 5 epochs with a learning rate of 1e-5, a batch size of 16 for each device, a warm-up ratio of 0.2, the passage window size of 50 and the temperature parameter set to 0.05.\nFor task-adaptive summarizer and generation model, we utilize open-source Llama Factory and adopt Lora tuning for 1 epoch with a learning rate of 5e-5, a batch size of 4 and a cosine learning rate scheduler.\nFor NQ, TriviaQA and HotpotQA, we use 2018 English Wikipedia as the external retrieval source while for PopQA, since the 2018 Wikipedia sometimes lacks articles about entities that have been added to Wikipedia recently, we use December 2020 preprocessed Wikipedia corpus as the external retrieval source. We preprocess the external corpus following Izacard et al. (2023).\nAs for retriever, we incorporate BGE owing to its superior performance in a variety of benchmark leaderboards and retrieve up to 5 documents for each input query for testing, where similarity is defined as cosine similarity.\nWe examine the effectiveness of each component in MetRag by preparing the following variants:\ni) MetRag w/o Comb, which removes the combination of similarity model and utility model and degrades to the original similarity based retriever;\nii) MetRag w/o AS, which removes the task-adaptive summarization for information integration.\nWe plot the performance comparison in Figure 3, from which we can observe that the overall performance would drop significantly when either component is discarded, thus verifying the effectiveness of our dedicated design.\nIn particular, we find that MetRag w/o Comb performs worst among all variants, showcasing that the augmented information, which serves as the cornerstone in retrieval-augmented generation, deserves more attention.\nSince the utility model plays a key role in bringing the supervision of LLMs into the passage selection process, we take a closer look at how the passage window size influences the final performance by directly incorporating the utility model trained under different settings to the final task. Due to space limits, we average the metrics in four datasets and present the result in Figure 4 and find that the model performance improves as the window size grows. This demonstrates that the growing passage window size endows more LLMs\u2019 powers for distinguishing passage significance among diverse inputs to the utility model, thus improving performance on downstream tasks. However, due to the computational burden (the model training time grows linearly with the passage window size) that a large passage window size imposes during training, there is a need to balance the trade-off between performance and cost.\nWhen optimizing the utility model, we add an empty string in the training process to leverage the knowledge of LLMs to achieve selective retrieval. Here, we conduct some case studies to examine this mechanism.\nAs we can see from Table 3, there are example queries that the utility model deems no retrieval and in total there are 20.9% queries which the utility model ranks the empty string higher than other documents. Many of the listed queries are commonsense knowledge that has been memorized in LLMs\u2019 parameters and we can easily call an LLM for the required answer instead of retrieval. This illustrates that in retrieval-augmented generation, despite the further knowledge external corpus introduces, the inherent knowledge of LLMs when deploying retrieval-based techniques is worth investigation and we believe that our work has provided a straightforward and plausible solution.\nHowever, this design still has certain limitations, i.e., the LLM used for the end task needs to be the same or stronger than the one utilized during utility model training, otherwise, this mechanism may not work.\n[]\\VerbContentQueriesD\nIs Cartagena or Chess more popular around the world?\nIn what country is Chalhuacocha?\nWhat was Ian Fleming\u2019s first Bond book?\nWhen was catch me if you can made?\nCathay is a poetic name for which country?\nWho sings sugar sugar you are my candy girl?\nWho used to present I\u2019m a celebrity now?\nWhat county is Icknield Walk First School located in?\nWhere does the cell spend most of its time in the cell cycle?\nWhat country lies to north of the Republic of Chad?\nAre Vintage Life and InStyle both US-based magazines?\nWho is the first president to be impeached?\nAre Andrew Stevens and Charles Burnett both American?\nWhich Genre of Television did Charles Quinton Murphy act for?\nHow was President Kennedy assassinated?\n"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "We split all train, validation, and test sets of datasets following Izacard et al. (2023); Asai et al. (2023b). Our training data includes randomly sampled instruction-following input-output pairs. We randomly sample 50k pairs from training sets of NQ, TriviaQA, and HotpotQA to train our model in different stages. All experiments are conducted using 4 NVIDIA A100 GPUs.\n\nFor NQ, TriviaQA, and HotpotQA, we use 2018 English Wikipedia as the external retrieval source, while for PopQA, since the 2018 Wikipedia sometimes lacks articles about entities that have been added recently, we use a December 2020 preprocessed Wikipedia corpus as the external retrieval source. We preprocess the external corpus following Izacard et al. (2023).\n\nAs for the retriever, we incorporate BGE owing to its superior performance in a variety of benchmark leaderboards and retrieve up to 5 documents for each input query for testing, where similarity is defined as cosine similarity."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Tasks and Datasets",
            "text": "We evaluate our proposed MetRag on a variety of knowledge-intensive public datasets (including general Open-Domain QA: NQ, TriviaQA-unfiltered, HotpotQA and entity-centric QA datasets: PopQA) and evaluate the performance via metrics EM and F1. All experiments are conducted in a zero-shot manner, where we provide instructions and retrieved information about tasks without few-shot demonstrations."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Baselines",
            "text": "Baselines without retrievals.\nWe evaluate strong publicly available pre-trained LLMs.\n\nBaselines with retrievals.\nWe evaluate models augmented with retrieval only at test time or during training. The first category includes standard RAG baselines, where an LM generates output given the query prepended with the top retrieved documents. The latter category includes approaches that are trained with retrieved passages."
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Settings",
            "text": "We split all train, validation and test sets of datasets following Izacard et al. (2023); Asai et al. (2023b). Our training data includes randomly sampled instruction-following input-output pairs. We randomly sample 50k pairs from training sets of NQ, TriviaQA, and HotpotQA to train our model in different stages. All experiments are conducted using 4 NVIDIA A100 GPUs. We train the utility model for 5 epochs with a learning rate of 1e-5, a batch size of 16 for each device, a warm-up ratio of 0.2, the passage window size of 50, and the temperature parameter set to 0.05. \n\nFor task-adaptive summarizer and generation model, we utilize open-source Llama Factory and adopt Lora tuning for 1 epoch with a learning rate of 5e-5, a batch size of 4, and a cosine learning rate scheduler. For NQ, TriviaQA, and HotpotQA, we use the 2018 English Wikipedia as the external retrieval source while for PopQA, since the 2018 Wikipedia sometimes lacks articles about entities that have been added to Wikipedia recently, we use the December 2020 preprocessed Wikipedia corpus as the external retrieval source. We preprocess the external corpus following Izacard et al. (2023). \n\nAs for retriever, we incorporate BGE owing to its superior performance in a variety of benchmark leaderboards and retrieve up to 5 documents for each input query for testing, where similarity is defined as cosine similarity."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results and Analysis",
            "text": "We examine the effectiveness of each component in MetRag by preparing the following variants:\ni) MetRag w/o Comb, which removes the combination of similarity model and utility model and degrades to the original similarity based retriever;\nii) MetRag w/o AS, which removes the task-adaptive summarization for information integration.\nWe plot the performance comparison in Figure 3, from which we can observe that the overall performance would drop a lot when either component is discarded, thus the effectiveness of our dedicated design is verified.\nIn particular, we find that the MetRag w/o Comb performs worst among all variants, showcasing that the augmented information, which serves as the cornerstone in retrieval-augmented generation, deserves more attention.\n\nSince the utility model plays a key role in bringing in the supervision of LLMs into the passage selection process, we take a closer look at how the passage window size would influence the final performance by directly incorporating the utility model trained under different settings to the final task. Due to the space limit, we average the metrics in four datasets and present the result in Figure 4 and find that the model performance improves as the window size grows, which demonstrates that the growing passage window size endows more LLMs\u2019 powers for distinguishing passage significance among diverse inputs to the utility model thus improving performance on downstream tasks. However, due to the computational burden (the model training time grows linearly with the passage window size) that a large passage window size imposes during training, there is a need to balance the trade-off between performance and cost.\n\nWhen optimizing the utility model, we add an empty string in the training process to leverage the knowledge of LLMs to achieve selective retrieval. Here, we conduct some case studies to examine this mechanism.\nAs we can see from Table 3, there are example queries that the utility model deems no retrieval and in total there are 20.9% queries which the utility model ranks the empty string higher than other documents. As we can see from Table 3, many of the listed queries are commonsense knowledge that has been memorized in LLMs\u2019 parameters and we can easily call an LLM for the required answer instead of retrieval, which illustrates that in retrieval-augmented generation, despite the further knowledge external corpus introduces, the inherent knowledge of LLMs when deploying retrieval-based techniques is worth investigation and we believe that our work has provided a straightforward and plausible solution.\nHowever, this design still has certain limitations, i.e., the LLM used for the end task needs to be the same or stronger than the one utilized during utility model training, otherwise this mechanism may not work.\n\nWe introduce a case study to illustrate the virtues of our task-adaptive summarizer in Table 4, as we can see from the table, since the original information is quite long and with a lot of distracting information, an LLM tends to get lost in a large chunk of words. However, with our task-adaptive summarizer, the most relevant information for this query is extracted thus an LLM can easily answer the question given the extracted knowledge.\n\nIn sum, the virtues of task-adaptive summarizer are two-fold. On the one hand, by using a small-scale LLM (i.e., a 13B Llama model) as the summarizer it could speed up the inference for downstream tasks. The summary ratio across the four datasets are 8.25%, 7.35%, 6.77%, and 7.32% respectively, which might increase a small amount of inference cost by adding the summary stage, nevertheless, since the inference cost of LLMs is linearly correlated with token number, with the knowledge derived from the summarizer, it can potentially decrease the inference cost for high-cost commercial LLMs. On the other hand, it further enhances the performance by extracting the most relevant information for a given task so as to alleviate distracting information, which improves the EM and F1 metrics across the four datasets on average by 1.4% and 1.7% respectively, which further justifies our model design."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Main Results",
            "text": "From the empirical results across multiple knowledge-intensive datasets, the major findings can be summarized as follows:\n\nRetrieval largely improves performance.\nWhen compared against approaches without retrieval, the retrieval-augmented approaches demonstrated superiority in EM metric, showing the tremendous power and potential of retrieval-augmented generation. Besides, with retrieval-augmentation, even small-scale LLMs (e.g., 7B LLMs) can achieve comparable performance w.r.t. large-scale LMs (e.g., 13B LLMs) in terms of EM metric, showing their power in pursuit of true knowledge.\n\nLong-tail queries benefit more from retrieval.\nAs depicted in Table 1, retrieval-augmented approaches achieve most performance gain in PopQA dataset with long-tail queries, illustrating that performance degradation due to knowledge updating issues can be alleviated to a great extent by retrieval.\n\nSupervised Fine-tuning improves instruction following.\nWith regard to F1 metric, we find that approaches trained with retrieved passages perform better, showcasing their ability in instruction following for abstracting concise answers. However, for approaches without supervised fine-tuning, there exists a seesaw effect between EM and F1 with one focuses on answer accuracy while other focuses on the balance between exactness and conciseness.\n\nThe way of incorporating external information matters!\nDifferent from approaches that directly incorporate the retrieved passages for answer generation, our proposed MetRag endows multi-layered thoughts to \u201ctake the essence and discard the dross\u201d so that the most useful information of retrieved passages can be abstracted and distraction information can be dropped for end tasks. Experimental results in four different datasets illustrate and verify the rationale of our proposed MetRag."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Analysis",
            "text": "We examine the effectiveness of each component in MetRag by preparing the following variants:\ni) MetRag w/o Comb, which removes the combination of similarity model and utility model and degrades to the original similarity based retriever;\nii) MetRag w/o AS, which removes the task-adaptive summarization for information integration.\nThe overall performance drops significantly when either component is discarded, verifying the effectiveness of our dedicated design.\nIn particular, MetRag w/o Comb performs worst among all variants, highlighting the importance of augmented information in retrieval-augmented generation.\n\nThe utility model plays a key role in incorporating the supervision of LLMs into the passage selection process. We analyze how the passage window size influences final performance by incorporating the utility model trained under different settings. The model performance improves as the window size grows, demonstrating that a larger passage window size enhances the ability to distinguish passage significance among diverse inputs, thus improving performance on downstream tasks. However, the model training time grows linearly with the passage window size, necessitating a trade-off between performance and cost.\n\nWhen optimizing the utility model, we add an empty string in the training process to leverage LLM knowledge for selective retrieval. Case studies show 20.9% of queries are ranked with an empty string higher than other documents, reflecting that commonsense knowledge memorized in LLMs' parameters can be accessed without retrieval. This mechanism relies on using an LLM for the end task that is the same as or stronger than the one used during utility model training.\n\nWe introduce a case study to illustrate the virtues of our task-adaptive summarizer. With our task-adaptive summarizer, the most relevant information is extracted, enabling an LLM to easily answer the question based on the extracted knowledge.\n\nIn sum, the task-adaptive summarizer has two main benefits. First, using a small-scale LLM as the summarizer speeds up inference for downstream tasks. The summary ratio is low across datasets, which might increase inference cost slightly, but the derived knowledge can decrease inference cost for high-cost LLMs. Second, it enhances performance by extracting relevant information, improving EM and F1 metrics across datasets by 1.4% and 1.7% on average, justifying our model design."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we propose MetRag, a multi-layered thoughts enhanced retrieval-augmented generation framework. MetRag first utilizes LLM\u2019s supervision to obtain utility-oriented thoughts and combines the similarity and utility of documents for performance enhancement. Furthermore, it seeks compactness-oriented thoughts through a task-adaptive summarizer. Finally, with multi-layered thoughts from the previous stages, an LLM is employed for knowledge-augmented generation. Extensive experiments on knowledge-intensive tasks have demonstrated the superiority of the proposed MetRag."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "One of the limitation of our work is that the effectiveness of utility model is highly hinges on a strong LLM\u2019s supervision, although we have find that an LLM like LLama 7B or 13B is enough for training a satisfactory utility model. In sum, our work opens up a fresh perspective to reconsider retrieval-augmented generation, but more complex situation that require reading a large amount of material to\nanswer (e.g., legal or medical documents) is still unresolved. Hence, extending our framework in the super-long contexts is one of the future work."
        }
    ],
    "url": "http://arxiv.org/html/2405.19893v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.2.3",
            "3.3",
            "3.3.1",
            "3.3.2",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "4.2",
            "4.2.1",
            "4.2.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "4.2",
            "4.2.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.19893v1",
        "paper_title": "Similarity is Not All You Need: Endowing Retrieval-Augmented Generation with Multi\u2013layered Thoughts",
        "research_background": "### Paper's Motivation\n\nThe paper is motivated by the limitations of large language models (LLMs) in knowledge-intensive tasks due to issues such as the untimeliness and cost of knowledge updates, as well as hallucination problems. Although retrieval-augmented generation (RAG) approaches have shown promise by leveraging external documents to supplement the LLMs without updating the underlying models, these methods predominantly rely on similarity-based retrieval. The authors argue that relying solely on similarity can degrade performance, particularly when the retrieved documents have high similarity scores but low information utility for the task at hand. They aim to enhance RAG approaches by addressing these limitations and introducing multi-layered thoughts that consider factors beyond mere similarity.\n\n### Research Problem\n\nThe primary research problem addressed in this paper is how to improve the performance of retrieval-augmented generation by moving beyond purely similarity-based approaches. The challenges that need to be addressed include:\n1. Training a model that can perceive utility-oriented thoughts in addition to similarity.\n2. Reducing the burden on LLMs imposed by multiple retrieved documents and capturing commonalities and characteristics between these documents to avoid information loss and performance degradation.\n\n### Relevant Prior Work\n\nThe paper builds upon and differentiates itself from previous work in several key areas:\n- **LLMs and Knowledge-Intensive Tasks**: Previous work highlighted the remarkable achievements of LLMs like ChatGPT and GPT4 in various tasks, but also pointed out their limitations in timely knowledge updates and hallucinations (Zhou et al., 2023; Kasai et al., 2022; Rawte et al., 2023).\n- **Retrieval-Augmented Generation (RAG)**: Existing RAG approaches enhance LLMs by prepending retrieved documents to the query. However, these are typically similarity-based and do not consider the utility or compactness of the information, leading to issues when the most useful documents are not the most similar ones (Jiang et al., 2023; Ram et al., 2023; Asai et al., 2023).\n- **Document Similarity and Summarization**: The limitations of similarity-based retrieval and simple summarization in capturing the most critical information regarding a query have been previously discussed (Robertson and Zaragoza, 2009; Shi et al., 2023; Mallen et al., 2023).\n- **Combining Similarity with Utility and Compactness**: The authors propose combining similarity-oriented thoughts with utility-oriented and compactness-oriented thoughts to better align with the task requirements. This brings in concepts from both dense retrievers and LLM-based feedback (Kwiatkowski et al., 2019).\n\nBy addressing these limitations and leveraging both similarity and utility-oriented retrieval, combined with advanced summarization techniques, the authors propose MetRag, a novel approach to retrieval-augmented generation.",
        "methodology": "I'm sorry, but without the actual methodology text or a description of the proposed method or model, I can't provide a description of the proposed method or model. Could you provide the specific details or text from the methodology section?",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets:**\n- Natural Questions (NQ)\n- TriviaQA\n- HotpotQA\n- PopQA\n\n**Training Data:**\n- 50,000 instruction-following input-output pairs sampled from the training sets of NQ, TriviaQA, and HotpotQA.\n\n**Model:**\n- Base Large Language Model (LLM): Llama2\n\n**External Retrieval Source:**\n- 2018 English Wikipedia for NQ, TriviaQA, and HotpotQA\n- December 2020 preprocessed Wikipedia corpus for PopQA\n\n**Training Setup:**\n\n1. **Utility Model Training:**\n   - Epochs: 5 \n   - Learning Rate: 1e-5 \n   - Batch Size: 16 per device\n   - Warm-up Ratio: 0.2 \n   - Passage Window Size: 50 \n   - Temperature Parameter: 0.05\n\n2. **Task-Adaptive Summarizer and Generation Model:**\n   - Using open-source Llama Factory, with Lora tuning \n   - Epochs: 1 \n   - Learning Rate: 5e-5\n   - Batch Size: 4 \n   - Learning Rate Scheduler: Cosine\n\n**Retriever:**\n- BGE for retrieving up to 5 documents per input query, using cosine similarity.\n\n**Infrastructure:**\n- All experiments were conducted on 4 NVIDIA A100 GPUs.\n\n### Evaluation Metrics\n\n- Exact Match (EM) and F1 scores across the four datasets.\n\n### Main Experimental Results\n\nOverall, the results demonstrate the importance of each advanced component in the MetRag model and highlight the trade-offs between performance and computational cost."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the individual contribution of combining similarity and utility models, and the task-adaptive summarization component in MetRag's performance.",
            "experiment_process": "The researchers prepared two variants of MetRag for testing: (i) MetRag w/o Comb, which removes the combination of similarity model and utility model, and (ii) MetRag w/o AS, which removes the task-adaptive summarization for information integration. Experiments were conducted using 4 NVIDIA A100 GPUs. They evaluated the performance via metrics EM and F1 on NQ, TriviaQA, HotpotQA, and PopQA datasets.",
            "result_discussion": "The results showed a significant drop in overall performance when either component was removed, verifying the effectiveness of the dedicated design. MetRag w/o Comb performed the worst, highlighting the importance of the augmented information derived from combining similarity and utility models.",
            "ablation_id": "2405.19893v1.No1"
        },
        {
            "research_objective": "To assess the impact of the passage window size in the utility model on the final performance of MetRag.",
            "experiment_process": "The utility model was trained with different passage window sizes. The metrics from the four datasets (NQ, TriviaQA, HotpotQA, and PopQA) were averaged to assess performance. The experiments included evaluating the balance between increasing passage window size and the associated computational costs.",
            "result_discussion": "Performance improved as the window size grew, showing that a larger passage window size helps the utility model better distinguish passage significance among diverse inputs. However, there is a need to balance performance improvements with the increased computational burden.",
            "ablation_id": "2405.19893v1.No2"
        },
        {
            "research_objective": "To examine the efficacy of leveraging the inherent knowledge of LLMs during the selective retrieval process in MetRag.",
            "experiment_process": "An empty string was added during the utility model training process to leverage LLM knowledge for selective retrieval. Example queries were examined where the utility model ranked this empty string higher than documents, indicating no need for retrieval. Metrics and case studies from four datasets (NQ, TriviaQA, HotpotQA, and PopQA) were analyzed.",
            "result_discussion": "20.9% of the queries indicated that the utility model preferred the LLM's inherent knowledge over retrieved documents. This approach showcased a straightforward solution; however, the LLM utilized for the end task needs to be as strong or stronger than the one used during utility model training.",
            "ablation_id": "2405.19893v1.No3"
        },
        {
            "research_objective": "To compare the impact of different base LLMs on utility modeling in MetRag.",
            "experiment_process": "The utility model was trained using different base LLMs (Llama2, Baichuan) and incorporated directly into the final task. Performance was evaluated across four datasets (NQ, TriviaQA, HotpotQA, and PopQA) to analyze any differences.",
            "result_discussion": "Larger LLMs typically outperformed smaller ones, with some anomalies such as Llama2 scoring higher than Llama2 on PopQA, which was consistent with findings from Self-RAG. Baichuan underperformed compared to the Llama series possibly due to its emphasis on specific languages, like Chinese.",
            "ablation_id": "2405.19893v1.No4"
        },
        {
            "research_objective": "To illustrate the advantages of the task-adaptive summarizer in MetRag.",
            "experiment_process": "A case study was conducted to highlight the effectiveness of the task-adaptive summarizer. The summarizer's impact on inference speed and performance was analyzed across four datasets, with the summarizer using a small-scale LLM (13B Llama model).",
            "result_discussion": "The task-adaptive summarizer improved inference speed and reduced the inference cost. Furthermore, it enhanced performance by extracting the most relevant information, thus improving EM and F1 metrics on average by 1.4% and 1.7% respectively, justifying the model design.",
            "ablation_id": "2405.19893v1.No5"
        }
    ]
}