{
    "title": "Training LLMs over Neurally Compressed Text",
    "abstract": "In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text na\u00efvely compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale. While our method delivers worse perplexity than subword tokenizers for models trained with the same parameter count, it has the benefit of shorter sequence lengths. Shorter sequence lengths require fewer autoregressive generation steps and reduce latency. Finally, we provide extensive analysis of the properties that contribute to learnability, and offer concrete suggestions for how to further improve the performance of high-compression tokenizers.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Today\u2019s large language models (LLMs) are almost exclusively trained over subword tokens. The tokenizers used to produce these tokens\u2014often BPE [23, 56] or Unigram [37]\u2014are compressors that typically achieve ~4 compression over natural language text [74]. While these tokenizers \u201chide\u201d the character-level makeup of each token from the LLM [74, 44], this downside is widely seen as outweighed by the significant benefits of compression. Compared to raw byte-level models, an LLM trained over subword tokens sees ~4 more text per token, allowing it to model longer-distance dependencies, ingest more pretraining data, and predict more text at inference time, all without increasing compute.\n\nGiven these advantages, it raises the question, could we compress text further to achieve even greater gains? It is well known that autoregressive language models can be turned into lossless text compressors, and recent work has shown that LLMs can easily achieve 12 compression over English text [16]. \n\nIn this paper we explore various options for doing so, focusing primarily on the idea of using Arithmetic Coding (AC) [73], which is known to reach the near-optimal compression rate for a particular model that assigns probabilities to text continuations. Figure 1 presents our high-level approach. First, a small language model \u201cM1\u201d is trained over raw byte sequences. Next, this frozen model is used to compress pretraining corpus text by applying a standard compression algorithm like AC. The resulting compressed bitstream is then chunked into tokens, which are used to train \u201cM2\u201d, a language model that directly reads and writes neural-compressed text.\n\nGiven a perfect probabilistic model of the raw byte sequence, the compression step would output a fully-compressed bitstream that would be indistinguishable from random noise, and hence unlearnable by M2. In reality, M1 can never be perfect [78], so the M1-compressed output will still contain learnable patterns. We explore whether using compression powered by a relatively small M1 is able to \u201cremove\u201d the simple structure that M1 understands from the input\u2014e.g., patterns of spelling, word frequency, and basic grammar\u2014while retaining any higher-level structure that M1 fails to model\u2014e.g., patterns requiring \u201cdeeper\u201d reasoning and long range coherence. A larger M2 would then learn to model this higher-level structure, without needing to relearn the low-level structure removed by M1. \n\nIn practice, we find that text compressed via Arithmetic Coding is not readily learnable by a standard transformer-based LLM, with resulting models predicting tokens at chance. Interestingly, this result holds even when M1 is reduced to a context-free unigram model, suggesting that the challenge of modeling AC-compressed text stems from the difficulty of learning the AC compression and decompression process itself. We verify this hypothesis by showing that even the sub-tasks of AC-compressing and AC-decompressing text are not learned well beyond a few initial tokens.\n\nTo aid learnability, we propose compression via Equal-Info Windows, a simple technique that breaks text into contiguous windows and compresses them via Arithmetic Coding independently. Rather than splitting text into windows of equal text length, we track the number of bits output by the compressor, and close each window just before it exceeds a set information threshold (e.g., 32 bits of information). This has the advantage that when chunking the subsequent bitstream into M2 tokens, there is a stable mapping from N tokens to one window (e.g., four 8-bit tokens\u2009\u2009one 32-bit window). At each window boundary, we reset both AC algorithm and the M1 model context. This ensures that each window may be mapped back onto raw text without any additional information.\n\nThrough ablations on window size and M2 vocabulary size, we find that Equal-Info Windows make learning of AC-compressed text possible across a range of settings. However, we also observe that learning progresses gradually, starting with tokens at the left edge of each window, and for longer windows, the model learns little about the tokens near the right edge. Our best-performing setting uses short 16-bit windows that each correspond to a single 16-bit M2 token.\n\nOur main contributions are as follows:\n(1) Outline advantages and challenges of training over neural compressed text.\n(2) Compare LLMs trained over different tokenizers along two axes: bits/byte and FLOPs/byte.\n(3) Show that standard LLMs can\u2019t learn to model vanilla AC-compressed text.\n(4) Propose compression via Equal-Info Windows, and show that it enables learning over neural compressed text."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Motivation and Background",
            "text": "The most straightforward advantage is efficiency. By compressing the same text into a shorter token sequence, the model can process more text for the same computational cost. In particular, a model trained over  compressed text will see  more text during training compared to a model trained over raw text, given an equal compute budget. Increasing the amount of data seen in pretraining is often an effective means of improving performance [35  ###reference_b35###, 30  ###reference_b30###]. Processing text more efficiently also confers benefits at inference time, reducing the serving cost for handling a request of a given prompt and continuation length. In addition to reducing the raw compute needed for inference, compression can also improve inference latency, since generating better-compressed output requires fewer sequential autoregressive steps.\nA second advantage is that working with compressed text allows modeling longer contextual dependencies. In vanilla transformer-based models, computation for the self-attention layer scales quadratically with the sequence length, . This has limited the sequence lengths used by such models in practical settings to ~k tokens.666Exploring sub-quadratic attention mechanisms is an area of active research [1  ###reference_b1###, 70  ###reference_b70###, 36  ###reference_b36###, 75  ###reference_b75###, 5  ###reference_b5###, 9  ###reference_b9###, et alia]. However, regardless of the cost of attention, compressing the input increases the effective context \u201cfor free\u201d.\nIf, via compression, each token represents (on average)  bytes of raw text, then the resulting LLM can model dependencies across  longer distances compared to a raw text model operating over the same token sequence length. While the benefits of modeling longer context (beyond ~ bytes) are modest when viewed merely as perplexity gains [51  ###reference_b51###], the ability to condition on long context is critical for many applications, such as retrieving content from a document, or answering a coding question provided documentation.\nA third potential advantage of training over compressed text is that information will be spread more uniformly across the sequence. By the nature of compression, a text span that is relatively predictable (e.g., a boilerplate notice) will be more compressible than a span with high perplexity (e.g., a unique product serial number). When an LLM is trained over well-compressed text, each token will represent roughly an equal amount of information. Since the LLM allocates equal compute to each token, this amounts to allocating more compute for \u201charder\u201d text spans. This adaptivity is similar in spirit to \u201cAdaptive Computation Time\u201d (ACT) [27  ###reference_b27###], which learns to allocate additional compute at some sequence positions in an end-to-end manner, but with the advantage that in our case the computation remains \u201cdense\u201d\u2014identical operations are applied at each position.777It should be noted that ACT learns to allocate more compute where it is useful, as opposed to merely where the predictions are hard. For example, ACT learns to not waste compute on inherently unpredictable text spans. We expect that as a heuristic, allocating more compute to higher-perplexity text spans is valuable, but leave this to future work to verify.\nIt is not at all obvious what types of compression are \u201ctransparent\u201d enough to be learnable through a standard LLM training process. Strong compression can be seen as removing as much redundant or predictable information from a sequence as possible. Consequently, the bitstream output by a good compressor is inherently hard to distinguish from random noise. In this work, we explore the setting where M2\u2014the model trained over compressed text\u2014has a larger capacity than M1, the model used for compression. In principle, this setup should allow M2 to extract additional information from the signal even after M1 has compressed it. However, for strong enough M1 compression, the resulting bitstream may be too noisy to detect any signal.\nAs a prerequisite for M2 to effectively predict continuations of compressed text, we anticipate that it is necessary for M2 to have the ability to decompress bits\u2009\u2009text and compress text\u2009\u2009bits. These sub-tasks are challenging in their own right. First, M2 needs to accurately \u201csimulate\u201d M1 in order to know the probabilities it assigns to the text, which determine the output of compression.888For Arithmetic Coding, not only would M2 need to know the probabilities M1 assigns to the observed text, but it would also need to know the probabilities assigned to many unobserved symbols. This is because Arithmetic Coding operates over cumulative probabilities, i.e., the probability that the next symbol is e or any alphabetically preceding symbol. Training models to mimic other models can be difficult [41  ###reference_b41###], and even in settings where models do learn to copy the behavior of another network [29  ###reference_b29###], this is often only when looking at which symbol was assigned the highest probability\u2014the actual probabilities assigned often differ [60  ###reference_b60###]. Second, M2 needs to learn the compression procedure itself. In our case, this means tracking the Arithmetic Coding algorithm, which requires maintaining high-precision numerical state across long contexts. We investigate these sub-tasks in detail in Section 5.2  ###reference_###.\nA further learnability challenge is the high level of context sensitivity needed to interpret a bitstream of compressed text. When chunked into tokens, a particular bit subsequence (e.g., 10111001) can map onto the same token despite having no stable \u201cmeaning\u201d across occurrences. We show examples in Section 6.1  ###reference_###, where a token maps to many different underlying text forms, necessitating strong contextual understanding. While LLMs are robust to some level of polysemy, as highlighted by the success of Hash Embeddings [62  ###reference_b62###] where multiple unrelated words share a single token representation, we suspect this has its limits.\nAn additional technical challenge is that compression methods can be sensitive to the precise model probabilities used. To achieve lossless compression in our setup, it is critical that the M1 probabilities match during compression and decompression. This can be hard to guarantee in practice, as there are many sources of numerical noise in LLM inference, especially when running on parallel hardware. An expanded discussion of numerical stability issues can be found in Section 3.7  ###reference_###.\nFinally, a specific challenge of training over neural compressed text is that multiple models need to be stored and run side-by-side in order to perform inference. We assume that if M1 is relatively small, this additional overhead is not a significant drawback compared to a standard tokenizer, which is also a separate model that is needed to tokenize text input and detokenize LLM outputs. In evaluating our approach, we include M1 compute in our calculations of total inference cost (FLOPs/byte)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Advantages of Training over Neural-Compressed Text",
            "text": "Training LLMs over compressed text is appealing for many reasons. We discuss three advantages in detail below.\nThe most straightforward advantage is efficiency. By compressing the same text into a shorter token sequence, the model can process more text for the same computational cost. In particular, a model trained over  compressed text will see  more text during training compared to a model trained over raw text, given an equal compute budget. Increasing the amount of data seen in pretraining is often an effective means of improving performance [35  ###reference_b35###  ###reference_b35###, 30  ###reference_b30###  ###reference_b30###]. Processing text more efficiently also confers benefits at inference time, reducing the serving cost for handling a request of a given prompt and continuation length. In addition to reducing the raw compute needed for inference, compression can also improve inference latency, since generating better-compressed output requires fewer sequential autoregressive steps.\nA second advantage is that working with compressed text allows modeling longer contextual dependencies. In vanilla transformer-based models, computation for the self-attention layer scales quadratically with the sequence length, . This has limited the sequence lengths used by such models in practical settings to ~k tokens.666Exploring sub-quadratic attention mechanisms is an area of active research [1  ###reference_b1###  ###reference_b1###, 70  ###reference_b70###  ###reference_b70###, 36  ###reference_b36###  ###reference_b36###, 75  ###reference_b75###  ###reference_b75###, 5  ###reference_b5###  ###reference_b5###, 9  ###reference_b9###  ###reference_b9###, et alia]. However, regardless of the cost of attention, compressing the input increases the effective context \u201cfor free\u201d.\nIf, via compression, each token represents (on average)  bytes of raw text, then the resulting LLM can model dependencies across  longer distances compared to a raw text model operating over the same token sequence length. While the benefits of modeling longer context (beyond ~ bytes) are modest when viewed merely as perplexity gains [51  ###reference_b51###  ###reference_b51###], the ability to condition on long context is critical for many applications, such as retrieving content from a document, or answering a coding question provided documentation.\nA third potential advantage of training over compressed text is that information will be spread more uniformly across the sequence. By the nature of compression, a text span that is relatively predictable (e.g., a boilerplate notice) will be more compressible than a span with high perplexity (e.g., a unique product serial number). When an LLM is trained over well-compressed text, each token will represent roughly an equal amount of information. Since the LLM allocates equal compute to each token, this amounts to allocating more compute for \u201charder\u201d text spans. This adaptivity is similar in spirit to \u201cAdaptive Computation Time\u201d (ACT) [27  ###reference_b27###  ###reference_b27###], which learns to allocate additional compute at some sequence positions in an end-to-end manner, but with the advantage that in our case the computation remains \u201cdense\u201d\u2014identical operations are applied at each position.777It should be noted that ACT learns to allocate more compute where it is useful, as opposed to merely where the predictions are hard. For example, ACT learns to not waste compute on inherently unpredictable text spans. We expect that as a heuristic, allocating more compute to higher-perplexity text spans is valuable, but leave this to future work to verify."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Challenges of Training over Compressed Text",
            "text": "It is not at all obvious what types of compression are \u201ctransparent\u201d enough to be learnable through a standard LLM training process. Strong compression can be seen as removing as much redundant or predictable information from a sequence as possible. Consequently, the bitstream output by a good compressor is inherently hard to distinguish from random noise. In this work, we explore the setting where M2\u2014the model trained over compressed text\u2014has a larger capacity than M1, the model used for compression. In principle, this setup should allow M2 to extract additional information from the signal even after M1 has compressed it. However, for strong enough M1 compression, the resulting bitstream may be too noisy to detect any signal.\nAs a prerequisite for M2 to effectively predict continuations of compressed text, we anticipate that it is necessary for M2 to have the ability to decompress bits\u2009\u2009text and compress text\u2009\u2009bits. These sub-tasks are challenging in their own right. First, M2 needs to accurately \u201csimulate\u201d M1 in order to know the probabilities it assigns to the text, which determine the output of compression.888For Arithmetic Coding, not only would M2 need to know the probabilities M1 assigns to the observed text, but it would also need to know the probabilities assigned to many unobserved symbols. This is because Arithmetic Coding operates over cumulative probabilities, i.e., the probability that the next symbol is e or any alphabetically preceding symbol. Training models to mimic other models can be difficult [41  ###reference_b41###  ###reference_b41###], and even in settings where models do learn to copy the behavior of another network [29  ###reference_b29###  ###reference_b29###], this is often only when looking at which symbol was assigned the highest probability\u2014the actual probabilities assigned often differ [60  ###reference_b60###  ###reference_b60###]. Second, M2 needs to learn the compression procedure itself. In our case, this means tracking the Arithmetic Coding algorithm, which requires maintaining high-precision numerical state across long contexts. We investigate these sub-tasks in detail in Section 5.2  ###reference_###  ###reference_###.\nA further learnability challenge is the high level of context sensitivity needed to interpret a bitstream of compressed text. When chunked into tokens, a particular bit subsequence (e.g., 10111001) can map onto the same token despite having no stable \u201cmeaning\u201d across occurrences. We show examples in Section 6.1  ###reference_###  ###reference_###, where a token maps to many different underlying text forms, necessitating strong contextual understanding. While LLMs are robust to some level of polysemy, as highlighted by the success of Hash Embeddings [62  ###reference_b62###  ###reference_b62###] where multiple unrelated words share a single token representation, we suspect this has its limits.\nAn additional technical challenge is that compression methods can be sensitive to the precise model probabilities used. To achieve lossless compression in our setup, it is critical that the M1 probabilities match during compression and decompression. This can be hard to guarantee in practice, as there are many sources of numerical noise in LLM inference, especially when running on parallel hardware. An expanded discussion of numerical stability issues can be found in Section 3.7  ###reference_###  ###reference_###.\nFinally, a specific challenge of training over neural compressed text is that multiple models need to be stored and run side-by-side in order to perform inference. We assume that if M1 is relatively small, this additional overhead is not a significant drawback compared to a standard tokenizer, which is also a separate model that is needed to tokenize text input and detokenize LLM outputs. In evaluating our approach, we include M1 compute in our calculations of total inference cost (FLOPs/byte)."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Compression",
            "text": "In this work, we focus on lossless compression, which aims to encode a sequence of input symbols, , into a bitstream while minimizing the expected length of the bitstream. Compression methods are often factored into a \u201cmodeling\u201d component and a \u201ccoding\u201d component [45  ###reference_b45###]. The input sequence can be viewed as a sample from a true distribution , , with a standard autoregressive decomposition, . The \u201cmodeling\u201d component aims to approximate  with . While some compression algorithms assume static probabilities for each symbol, stronger algorithms are \u201cadaptive\u201d, meaning that symbol probabilities may change based on context. In this work, we use context-aware transformer-based language models to represent .\nThe \u201ccoding\u201d component of a compression algorithm converts the input sequence to a bitstream of length . To maximize compression, we want a coding algorithm that minimizes the expected number of bits in the bitstream, . This is done by assigning shorter bit sequences to common symbols and longer sequences to less common ones.999This process can result in extremely uncommon sequences becoming longer under compression, as no algorithm can compress all possible input strings [45  ###reference_b45###]. In practice, natural language inputs are highly compressible and these edge cases are inputs that one would not recognize as natural language. The expected length is lower bounded by  where  [57  ###reference_b57###]. This means that, given a near-optimal coding algorithm, the achievable level of compression derives from how well the model  approximates ."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Arithmetic Coding",
            "text": "Arithmetic Coding [53  ###reference_b53###, 49  ###reference_b49###] uses a model  to compresses a sequence  to a bitstream, which is the binary expansion of a float . The float  is found by assigning successively smaller sub-intervals to each symbol , with the final interval enclosing . An interval is made of an upper and lower bound,  and its size is given by . Starting with , at each step of encoding, the interval for the symbol  is created by partitioning the interval  based on the cumulative distribution of  given the previous context, . The size of this interval is given by . Thus:\nwhere  is the symbol before  in a strict ordering of , i.e.,  is the previous token in the vocabulary. Finally, the bitstream of minimal length that represents the binary expansion of a number inside the final interval  is used as the compressed representation.\nEquivalently, the binary expansion can be seen as maintaining a bitstream prefix  and creating successive intervals  by partitioning the current interval in half. If the first interval is chosen, a  bit is appended to the bitstream prefix , while choosing the second interval appends a .\nOnce the final interval  is computed, smaller and smaller bit intervals are created until reaching a bit interval  that is fully enclosed by . At this point, the corresponding bitstream  is the final compressed representation.\nThe coding component of Arithmetic Coding is nearly optimal: the output bitstream will have a length of  bits when using infinite precision. In the finite precision setting using  bits, an extra  bits are added [31  ###reference_b31###]. See [73  ###reference_b73###] for an example implementation. In our experiments, we use precision . The practical effect of using a finite precision implementation of Arithmetic Coding is that the model\u2019s cumulative distribution gets quantized to integers using  bits. This results in a minimum probability of  being assigned to all tokens."
        },
        {
            "section_id": "2.5",
            "parent_section_id": "2",
            "section_name": "Related Work",
            "text": "Recent work has looked at using large language models for compression, but has not to our knowledge attempted to train subsequent models over the resulting compressed output. Works like [16  ###reference_b16###] use a transformer language model as the modeling component of Arithmetic Coding, but they do not train over compressed output nor do they make modifications to the compression algorithm to facilitate learnability by downstream models. Additionally, they focus on the setting of compressing fixed-size sequences of bytes. By contrast, our models operate over input sequences of fixed token length. This allows for models with higher compression rates to leverage longer contexts, as more bytes are included in the input.\n[63  ###reference_b63###] proposes changes to Arithmetic Coding to make it more amenable to use with LLMs\u2014namely, they rank sort the logits from the model before creating text intervals, . This could help alleviate issues stemming from errors in M2\u2019s simulation of M1. However, they do not train models on top of their compressed output.\nSome approaches to \u201ctoken-free\u201d (i.e., purely character- or byte-level) language modeling down-sample the input sequence via convolutions [13  ###reference_b13###, 61  ###reference_b61###], which could be seen as a form of end-to-end neural tokenization. However one important distinction is that the resulting tokenization is \u201csoft\u201d\u2014outputting high-dimensional vectors and not implying a discrete segmentation\u2014in contrast to our tokenization that outputs discrete tokens.\nMethods for learning discrete tokenization end-to-end have also been proposed [11  ###reference_b11###, 25  ###reference_b25###]. In the case of MANTa [25  ###reference_b25###], the learned segmentation appears to be fairly semantic (i.e., respecting word and morpheme boundaries), which could be an advantage over our approach. However, they lack our bias towards encoding an equal amount of information per token.\nIn modeling audio, it is common practice to use learned tokenizers that compress the raw input signal to discrete tokens from a fixed-size codebook [64  ###reference_b64###, 3  ###reference_b3###, 12  ###reference_b12###, 6  ###reference_b6###]. However, this compression is lossy, whereas we focus on lossless compression.\nOther recent work focuses on using the \u201cmodeling\u201d component from well-known compressors to do other tasks. [34  ###reference_b34###] uses the model from GZip to perform text classification. [68  ###reference_b68###] uses the Arithmetic Decoding algorithm with an LLM as the model to do diverse parallel sampling from that LLM. One could imagine that the \u201cmodel\u201d of our compressors (M1) is a teacher for M2, but unlike these other applications, the M1 values are not used outside of compression.\n[40  ###reference_b40###] also explores learning over compressed text, but with several key differences. First, they use n-gram language models [57  ###reference_b57###] while we use LLMs. Second, their model is conditioned on compressed bitstreams but produces a distribution over the raw, uncompressed, bytes while our M2 models predict directly in the compressed space. Additionally, they only consider static Huffman coding [32  ###reference_b32###] as the algorithm to compress model inputs. While this avoids the context sensitivity issues we outline in Section 2.2  ###reference_###, it results in a far worse compression rate compared to the adaptive compression methods we use. One important distinction is that their equal-information windows are overlapping, and used as a sliding window to provide context to their n-gram language model. By contrast our equal-information windows are non-overlapping, and used to segment text\ninto a series of equal-length bitstrings that can be interpreted independently by M2, and whose boundaries are easily identifiable, as they map to a fixed number of M2 tokens.\nConcurrently, [26  ###reference_b26###] explores how the compression performance of a tokenizer correlates with downstream model performance. They find that tokenizers that compress better perform better, which generally aligns with our findings, particularly in the large vocabulary setting, see Fig. 6  ###reference_###. However, we find that using the strongest compressors is detrimental to learnability, as seen in the AC line in Fig. 3  ###reference_###. These conflicting results likely stem from differences in tokenization strategy. Their work is restricted to BPE-based compressors while we explore stronger compressors built on LLMs and Arithmetic Coding. The qualitative differences between these classes of tokenizers are explored more in Section 6.1  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "For each experiment, we compress long contiguous sequences of training data using different methods. We then chunk the compressed output into tokens and train M2 models over those tokens."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Training Data",
            "text": "All training data used is English web text from C4 (en 3.1.0) [52  ###reference_b52###]. After tokenization, each document in C4 has an <EOS> token appended to it. We concatenate documents together to generate a long sequence of text. These long sequences are then split into individual examples, which are shuffled using the deterministic dataset functionality from SeqIO [54  ###reference_b54###]."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Training M1",
            "text": "The model used for compression is a decoder-only Transformer model. It uses the model size seen in Table 4 and a context length specified therein. We use a batch size appropriate for our computational resources, an rsqrt decay learning rate schedule starting at a specified rate with a set number of warmup steps, and a z-loss incorporated into the training. The model is trained for a specified number of steps using the Adafactor optimizer. The feed-forward layers adopt ReLU activations, and distinct learnable relative attention embeddings are utilized at each layer. The model training employs a deterministic SeqIO dataset and is implemented using Jax, Flax, and T5X frameworks. The final validation performance of the M1 model is measured in bits per byte, providing a standard measure of perplexity as discussed in Section 3.8. M1 and M2 models are both trained on the C4 training data. However, the final validation data used for evaluating M2 is not seen during M1 training, ensuring no information leakage. This mirrors the usual practice where LLM tokenizers are trained on the same dataset that their corresponding LLM is trained on subsequently."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Compression Methods",
            "text": "When compressing C4 training data, we use an example length of bytes and apply one of the following compression techniques. This results in compressed examples that are, on average, much longer than our target sequence length of M2 tokens. Thus, each example fills or nearly fills the model\u2019s context window with a compressed sequence made from contiguous raw bytes. We compress examples using each method, allowing us to train each M2 model for steps without repeating data.\n\nArithmetic Coding: In this setting, we use a decoder-only transformer language model to model , that is, when creating the interval , the partitions for each possible character, , are calculated using the probabilities for the next token output by the transformer. \n\nThe compressor model is run over contiguous text sequences of bytes. The generated logits are used as the model distribution for Arithmetic Coding. We use the Range Encoding (a finite-precision implementation of Arithmetic Coding) implementation from TensorFlow Compression with a precision of . The range encoding implementation uses integers with precision\u2009 bits. This is enough to encode -bit float logits, so should not cause numerical issues as our models are trained using bfloat16. While the compressor model is only trained on sequences of length , it uses relative position embeddings in its attention layers. Thus, it can be applied to longer sequences. Some works observe decreased performance as inputs are scaled to lengths beyond those seen in training, but we find that compression performance is similar in the two settings. Compressing sequences of length  yields a compression ratio of while compressing sequences of length  yields a ratio of . This suggests the performance drop from long sequences has minimal effect on compression, or that the increased contextual information makes up this difference.\n\nWe will see that text compressed in this straightforward manner is not readily learnable by M2. Thus, we explore alternative compression methods that modify the \u201cmodeling\u201d and \u201ccoding\u201d components for better learnability. Table 2 shows how our different approaches affect the compression ratio.\n\nStatic Logits Arithmetic Coding: One potential difficulty of learning over compressed text is that the \u201cmodeling\u201d component of the compression algorithm is hard to learn\u2014that is, the second language model (M2) has trouble learning to simulate the probabilities the compressor model (M1) assigns to bytes. \n\nTo weaken the compressor model, we replace the context-sensitive LM model with a static byte unigram model\u2014that is, the model\u2019s distribution is the same for all byte tokens in the input, i.e., . This distribution is estimated using the byte unigram statistics from the C4 training data.\n\nEqual Information Windows: The difficulty in modeling compressed text could also be because the \u201ccoding\u201d component of the compression algorithm is hard to learn. That is, the language model is not able to track the state variables used in Arithmetic Coding.\n\nOur proposed method of weakening the coding component of Arithmetic Coding compression is to reset the AC encoder once it has output a set number of bits, creating windows of fixed size where each window is an independently AC-compressed sequence. Windows will represent a variable amount of text, but as each window is created via compression, we expect roughly the same amount of information per window.\n\nIn addition to resetting the AC encoder, we also reset the M1 model\u2019s context. This means that each  bits of output can be decoded independently, at the cost of a weaker M1 model due to the lack of context. As each window is fully self-contained, the model no longer has to learn to track Arithmetic Coding state variables over long distances.\n\nIn cases where \u201cspare bits\u201d are available at the end of a window (but not enough to add an additional symbol of text), we pad with zeros. This complicates the decoding algorithm, but the compression scheme remains lossless. See Appendix I for further discussion and an alternative padding approach that gives similar results. \n\nWhen compressing an additional character would result in a bitstream that is greater than  bits long, i.e., more than  binary expansions are needed to create an interval that is enclosed by , the bitstream (padded to  bits as necessary) representing the input up to and including character  is emitted. Then both the AC encoder and M1 model are reset. That is,  is calculated as if ; the bit interval is also reset to . Similarly, M1 is only conditioned on inputs that are part of the current window, the inputs after . That is, . \n\nWe use  to denote the bits per window, and  for the vocabulary size of M2. For example, EqualInfoAC represents AC encoding with 16-bit Equal Info Windows and 8-bit M2 tokens (vocabulary )."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Tokenization of Compressed Text",
            "text": "Most compression methods output a bitstream, but training M2 directly over bits would not be ideal. As M1 was trained over UTF-8 bytes, the bit-level output of compression would result in M2 being applied to much longer sequences. Additionally, models are generally trained with vocabulary sizes much larger than two. Thus, we need a method to segment the bitstream into tokens, creating a more standard sequence for training language models.\n\nWe convert the bitstream into a token sequence by grouping every several bits into a token\u2014resulting in a vocabulary size based on the chosen grouping. This method of creating tokens from the compressed bitstream is expected to result in a more uniform distribution of tokens than the usual Zipfian distribution of word or subword tokens. This allows us to use larger vocabularies without encountering issues of rare or unattested tokens.\n\nThroughout this work, we focus on the \u201ctoken compression ratio\u201d\u2014the ratio between the input and output token sequence lengths. It is important to note that the meaning of \u201ctoken\u201d can differ between the input and output sequences. Generally, the input sequence is one byte per token, while output tokens represent multiple bytes. This differs from the more standard \u201cbit compression ratio\u201d\u2014the ratio of input bits to output bits. Our goal is to reduce the computational overhead of running LLMs by training them on compressed input, hence our concern with reducing the number of tokens that M2 consumes. This is elucidated in our analysis.\n\nWhen creating -bit tokens from the output of Arithmetic Coding, the bit compression ratio remains unchanged\u2014the total number of bits is unchanged. However, the number of tokens in the sequence is reduced, thus reducing the number of tokens the LLM must process. We compute compression ratios over the C4 dev set, which is unseen during M1 training.\n\nTo highlight the differences between the tokenization methods above, we measure the performance (as bits/byte on a sample of the C4 validation set) of two trivial models for our compression-based tokenizers in Table 3. The \u201cuniform\u201d model na\u00efvely assigns equal probability to each token, regardless of context. The \u201cunigram\u201d model also ignores context, but assigns probabilities based on the global token frequencies observed in the training data. We observe that our compression-based tokenizers (AC and EqualInfoAC) output a near-uniform distribution of tokens across their vocabulary, which is reflected in the near-zero gain over \u201cuniform\u201d achieved by modeling unigram statistics."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Training M2 on Compressed Data",
            "text": "Each M2 model is trained for a specific number of steps with a batch size and a fixed sequence length. This results in each model training on a significant number of billion tokens, with the vast majority being non-padding tokens. Detailed statistics can be found in the corresponding appendix and table that outline the exact size of each dataset. Dataset shuffling is controlled through seeding, and the dataset state is checkpointed during training, ensuring that each model sees every example exactly once during a training run.\n\nThe models are trained at varying sizes, with parameters ranging from millions to billions, as listed in a specific table. These sizes exclude embedding parameters. When using 8-bit tokens, the vocabulary size is a specific number, and it increases with 16-bit tokens. The sequence length for all M2 models is consistent at a set number of tokens. This consistency implies that when training on 16-bit tokens, there is double the byte exposure per example and throughout the overall training process compared to 8-bit tokens. All other hyperparameters are aligned with those utilized in the M1 model."
        },
        {
            "section_id": "3.6",
            "parent_section_id": "3",
            "section_name": "Baselines",
            "text": "We compare our M2 models against baseline models using various hyperparameters. All configurations, including sequence length, align with those utilized for our M2 training."
        },
        {
            "section_id": "3.7",
            "parent_section_id": "3",
            "section_name": "Numerical Stability",
            "text": "Arithmetic Coding relies on creating \"intervals\" that map to each symbol in a vocabulary based on the quantized cumulative distribution of a model's logits while predicting the subsequent token. A minor change in logits due to numerical noise can lead to significantly different output bitstreams, posing challenges for the practical application of neural language models in compression tasks. Common sources of noise include variations in batch size, parallel computation, hardware changes (such as CPU versus GPU versus TPU, and different TPU topologies), variations in inference processes (like computing logits for an entire sequence simultaneously versus one token at a time using KV caches), and alterations to the longest sequence length in the batch.\n\nTechniques like the rank-sorted algorithm utilized in LLMZip [63] may mitigate these challenges since only the token order needs consistency across different settings. Developing alternative methods for LLM-based compression should consider numerical stability issues and aim to reduce these concerns through algorithm design. Increasing quantization levels might also help lower numerical noise issues, though this approach could compromise the compression ratio."
        },
        {
            "section_id": "3.8",
            "parent_section_id": "3",
            "section_name": "Evaluation",
            "text": "As the tokenization scheme varies across the approaches we consider, models cannot be directly compared on \u201cper-token\u201d metrics such as negative log likelihood loss. Rather, following previous work, we report perplexity in terms of \u201cbits-per-byte,\u201d which scales the model\u2019s loss by the token-level compression rate.\n\nWe also compare models on how much computation (FLOPs) is required to perform inference over a given length of raw text (bytes). More specifically, we calculate M2\u2019s expected FLOPs/byte by scaling FLOPs/token\u2014approximated by excluding embedding parameters\u2014by the token-level compression rate (as tokens/byte). For methods using an M1 model during compression, the FLOPs/byte cost of M1 is added. For more details on the evaluation metrics see Appendix G.\n\nWe evaluate models on a sample of the C4 validation set. During evaluation, the model is run over batches or ~ million tokens. These tokens represent different amounts of text based on the compression method, making it impractical to run evaluation on the same sequence of bytes for all methods. To confirm that our validation samples are large enough to be representative, for each method, we train five models with different seeds. We find the final performance to be extremely stable, with the largest standard deviation in bits/byte being negligible. See Appendix B for more information about variance."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "As seen in Fig. 3, the most obvious approach\u2014compression using Arithmetic Coding with M1 assigning next-token probabilities\u2014fails to learn anything. Regardless of scale, the model only learns to output a uniform distribution over tokens, the performance of which is denoted by the dashed line. As the Arithmetic Coding procedure is near optimal, the compression ratio is essentially determined by the loss of M1. Thus, even though the M2 model learns nothing useful, when scaled by the compression rate, this setting ends up with the same performance as the M1 model. Similarly, models trained over data compressed with StaticAC\u2014where M1 is replaced with a static unigram model\u2014fail to learn. This result suggests that the difficulty in learning stems from the complexity or brittleness of the Arithmetic Coding process itself, rather than from M2\u2019s inability to model M1. Note that the weak \u201cmodeling\u201d component of this compression scheme results in a much lower compression rate and thus worse bits/byte performance, despite the model also learning a uniform distribution.\n\nFig. 3 shows that EqualInfoAC outperforms the byte-level baseline at most model sizes, with the gains increasing with scale. In addition to better bits/byte performance, training over compressed data has the advantage of using fewer FLOPs/byte for a given model size\u2014seen in the leftward shift of the EqualInfoAC curve compared to the Bytes curve\u2014due to shorter sequence lengths.\n\nUsing -bit tokens (k vocabulary) increases performance further. EqualInfoAC outperforms the Bytes baseline at all model sizes.\n\nHowever, EqualInfoAC outperforms the SentencePiece baseline in terms of tokens/byte. Models using EqualInfoAC take fewer autoregressive steps to generate the same text than models using SentencePiece encoding. This has the potential to reduce generation latency, at the cost of reduced compute efficiency. This is a tradeoff that is often worth making in production. For instance, speculative decoding is a popular approach that performs redundant computation in order to potentially accelerate auto-regressive steps.\n\nIt is noteworthy that the EqualInfoAC M2 models learn well despite being trained on data that has nearly uniform unigram statistics, as we saw in Table 3. In the best case, our billion parameter M2 model achieves bits/byte. This is a large gain over the na\u00efve uniform and empirical unigram models from Table 3, and approaches the performance of a parameter-matched SentencePiece model, despite using fewer FLOPs/byte. It is apparent from Fig. 3 that if FLOPs/byte were held constant, SentencePiece would achieve slightly better bits/byte than EqualInfoAC. However there is another axis along which EqualInfoAC may still be preferred. Setting aside inference FLOPs, all our SentencePiece models require longer sequences to encode the same text when compared to our best EqualInfoAC setting. This means that regardless of FLOPs used, the SentencePiece models will take more decoder steps at inference time. It is up to the practitioner whether it is \u201cworth it\u201d to trade off some bits/byte performance in order to achieve shorter sequences. In many serving scenarios, decoder steps are a practical bottleneck for determining system latency, and there are cases where one may be willing to incur extra inference cost to reduce latency (e.g., speculative decoding). To this end, it may be advantageous to scale up an EqualInfoAC model to recover bits/byte performance while retaining the reduced latency. This can be seen visually in Fig. 4.\n\nWe see a similar effect in Fig. 5, which ablates the EqualInfoAC window size. In terms of bits/byte, the shortest -bit windows perform the best. However, the next-best setting is the longest -bit windows, despite the fact that these M2 models fail to learn almost anything beyond the uniform distribution. This unintuitive trend stems from the fact that longer windows translate to better compression rates. If we remove the effect of compression rate by looking at bits-per-token (Fig. 5(b)), we see a clearer monotonic trend\u2014increasing window length makes it harder to learn, as we move closer to simply running Arithmetic Coding over the whole sequence. For and -bit windows, performance improvements with scale are small, but present; see Table 10 for exact numbers.\n\nTokenizing compressed text using a larger -bit vocabulary (k) results in a higher token compression rate, seen in the leftward shift of each curve in Fig. 6. The same trend holds for larger and -bit windows, but the performance increase with scale is so slight that we omit them from the graph. See Table 10 for the exact values. For Arithmetic Coding methods, larger vocabulary also improves bits/byte, seen as a downward shift in the curves.\n\nGiven the recent findings, we anticipate that continuing to scale models beyond billion parameters is unlikely to deliver an \u201cemergent\u201d ability to learn over AC-compressed text, since"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Additional Experiments",
            "text": "At this point, we have established that while the simplest approaches to training over compressed text fail, there are alternate compression schemes that are learnable. In this section, we conduct additional experiments to shed light on which aspects of different compression methods are difficult to learn and what contributes to their learnability."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Bitstream tokenization is not the main source of difficulty",
            "text": "The compression algorithms we consider output a bitstream, which we later chunk into tokens of a fixed bit depth (e.g., -bit tokens). As such, it is common for the bits representing a single character or UTF-8 byte to be split across multiple tokens. Compounding this issue is that the value of these tokens are contextually determined and may differ depending on the surrounding bytes.\nThe fact that both -bit and -bit token chunking strategies work suggests that this is not too much of an issue for the model. To further investigate this, we train two models\u2014one m and one m\u2014on the raw bitstream output by Arithmetic Compression, i.e., each token is either a  or a  and the vocabulary has a size of . We use the same hyperparameters as in Section 3  ###reference_###. Working at the bit level means that the output sequence is now longer than the input sequence, which was UTF-8 bytes. As such, this setting is not practical in the real world.\nWhen trained to convergence, the two models have cross entropy losses of  for the m parameter model and  for the m model\u2014not meaningfully better than the na\u00efve uniform distribution, which yields a loss of . This failure mode is the same as in Fig. 3  ###reference_###, which suggests that AC encoding itself is the main source of difficulty, as opposed to any issue around tokenization or vocabulary size."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Transformers struggle to learn Arithmetic Coding",
            "text": "Arithmetic Coding is a sequential algorithm that involves tracking multiple state variables as the input (byte) sequence is consumed. Each token in the output sequence represents multiple transformations of these variables, e.g.,  transformations when using -bit token chunking. Theoretically, only  transformer layers are needed to have a computational path through the model layers that can process a sequence of  tokens as a chain, where each token conditions on the previous one. While most of our transformers have the capacity to model these sequences\u2014only our m model has fewer layers\u2014we see in practice that the Arithmetic Coding algorithm is still difficult to learn.\nTo directly diagnose the ability to track Arithmetic Coding, we format AC compression and decompression as sequence-to-sequence tasks. The input provides the model with the true text, so we expect a model that is able to learn Arithmetic Coding should achieve an accuracy of . We compress sequences of  bytes using M1 and Arithmetic Coding.131313We use shorter raw text sequences to keep the final sequence length of inputs + targets manageable. We concatenate the bytes and AC output tokens to create the compression task. For the decompression task, we simply flip the order\u2014AC output tokens first and then bytes. The target tokens (bytes or tokens) are shifted by the input vocabulary size, ensuring that they have distinct values. We use a decoder-only transformer as our model with a causal attention mask, i.e., even during the input sequence, future tokens are hidden from the model. We train models with m parameters. Loss, gradients, and evaluation metrics are only computed on the target tokens.\nIn the decompression task, the target tokens are bytes. By ignoring the inputs and just modeling the outputs, the decompression model can achieve decent performance without actually leveraging the input data. To control for this, we also train a byte-level language model baseline on the same sequence-to-sequence data, excluding the input tokens. If the decompression model is actually learning to decompress Arithmetic Coding, we would expect stronger performance than the byte-level baseline. As we see in Table 5  ###reference_###, the baseline model, which does not see the input tokens, has the same performance as the decompression model.141414The slight gain is statistically insignificant, . Clearly, the models trained for decompression are not actually learning to do decompression.\nThe model trained for compression actually shows some signs of learning. Training a language model directly on the compressed output results in the model learning a uniform distribution over tokens, see Fig. 3  ###reference_###. When the model is able to attend to the input text, we see that the performance in Table 5  ###reference_### is better than the uniform distribution (which would have a cross entropy loss of ). While this method shows some hope for the learnability of Arithmetic Coding, the need to include the input sequence negates the main advantage of compression, i.e., applying the model to a shorter sequence. Additionally, the compressor\u2019s performance is far from the  it should be able to achieve.\nWe also find training on these sequence-to-sequence datasets to be less stable than training on the language modeling datasets. In our experiments, large performance swings and divergence were relatively common."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Larger vocabulary helps beyond increasing the compression ratio",
            "text": "Our best results training over compressed text use EqualInfoAC with -bit windows and vocabulary size at either k (best) or  (second-best). One clear advantage of the  model is that it has a  better token compression rate, so sees twice as much raw text during training. To assess whether its performance gain is due entirely to this advantage, we train a m parameter M2 model over the same dataset, but reduce its sequence length from \u2009\u2009. This model trains on half as many tokens, but sees the same amount of underlying text as the  model.151515To compensate for the smaller number of tokens in a sample of  batches from validation set when each example is  tokens, we compute our evaluation metrics over  batches.\nTable 6  ###reference_### shows that even in this setting, the model with larger vocabulary is stronger.161616It may be possible to achieve further gains by increasing the token bit depth further. However, most deep learning frameworks do not support using unsigned data types for inputs, and the resulting large vocabulary size can cause a computational bottleneck in the final softmax layer. In fact, most of the bits/byte gain (% absolute) is due to the structural change in tokenization, as opposed to the additional text seen. One possible explanation for its strong performance is that the  model uses exactly one token to represent each equal-info window. We\u2019ll see in the next section that in EqualInfoAC settings with multiple tokens per window, any non-initial tokens are highly context-dependent, and learning proceeds on a curriculum from the \u201ceasy\u201d window-initial tokens to the \u201charder\u201d window-final tokens."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "In this section, we examine how neural compression-based tokenizers differ from standard tokenizers and conduct additional analysis on training dynamics and the learnability of compressed data. This analysis leads us to several recommendations for future work developing new compression schemes that aim to be learnable by transformer models while delivering stronger compression than existing methods."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "EqualInfoAC is less stable and less semantic than SentencePiece",
            "text": "When we move to versions of EqualInfoAC that contain multiple tokens per window, such as EqualInfoAC, this transparency is destroyed for all non-initial tokens within a window. This is illustrated in Table 8. When the same token appears window-initially in different contexts, we see the window text has a stable prefix\u2014e.g., token # always maps to the prefix \u201cle-\u201d. However, when occurring as the second token within a two-token window, there are no apparent correspondences between window text. As EqualInfoAC window length increases, the proportion of tokens that are stable decreases. This may explain the observed difficulty of learning over longer windows. The window text for all instances of these tokens can be seen in Appendix M.\n\nNote that Table 8 examines window\u2009\u2009text, as opposed to token\u2009\u2009text correspondences. This is because for multi-token windows, the mapping from tokens to text is not well defined. More specifically, each character maps to a particular subsequence of the compressed bitstream, but these may not align with token boundaries. Fig. 7 illustrates the mapping between characters, bits, and tokens. We find that many windows contain a character (shown in purple) whose bits are split across two -bit tokens.\n\nFig. 7 also highlights that window-initial characters are not being well compressed, with the window-initial token often only covering one or two characters. This is due to our EqualInfoAC procedure fully resetting M1\u2019s context at every window boundary. With no context, M1 cannot make confident predictions, leading to more bits being needed to represent the initial character. On the positive side, this setup guarantees that a window can be decoded in isolation, which should aid learning. However, it is worth exploring in future work whether maintaining some M1 context across windows could improve the compression ratio without hurting learnability."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "AC decoding is learned step-by-step",
            "text": "As Arithmetic Coding is a sequential (left-to-right) and contextual algorithm, the text represented by a given token will differ based on the previous token. As such, a model should perform better on a token if it has a strong understanding of the token before it. When using EqualInfoAC compression, each window represents an independent Arithmetic Coding document. As we move deeper into the window, more and more AC decompression must be done to understand the token.\n\nTo understand how a token\u2019s position within a window affects learning, we track across training the average accuracy at each position within the -token windows of a m parameter EqualInfoAC model. The absolute accuracy of the EqualInfoAC model is relatively poor, but its relatively long window provides the clearest illustration of the positional trends. We observe similar trends for shorter windows where the model has stronger performance. Fig. 8 shows both raw accuracy (left) as well as the increase over \u201ctrivial\u201d accuracy (right), which we define as the maximum accuracy achieved in the first steps of training. Looking at accuracy increase highlights the \u201csequential learning\u201d trend by discounting any part of accuracy that is text independent. In particular, we note that window-final tokens have a non-uniform distribution due to the use of window-final padding bits (see our EqualInfoAC formulation in Section 3.3), which can be learned without any understanding of the text.\n\nWe observe two interesting trends. First, there is a clear ordering as to when the model starts to make meaningful (non-trivial) progress on a given position. The initial token (#1) is learned first, followed fairly quickly by #2 and then #3. Later tokens are only \u201cunlocked\u201d after training steps, suggesting that the ability to model these tokens builds on a foundation of understanding the preceding tokens within the window.\n\nThe second trend concerns the accuracy reached at each position. Here, we observe an increase in accuracy from #1 < #2 < #3, followed by a decrease from #3 < #4 < #5 and so on. The final token #8 also fits this trend when looking at the increase over non-trivial accuracy. The raw accuracy in this position is higher than previous tokens #4\u20137, due to the skewed distribution introduced by window-final padding. We interpret the increase across the first three positions as due to the benefit of extra leftward context. This is akin to the initial byte in a word being harder to predict than the following bytes. The decreasing performance at tokens #4 and beyond suggests the model is unable to track AC decompression indefinitely. While the model clearly learns to decompress longer sequences as training progresses, reliably decoding past bits of AC output appears to be a challenge."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Learnable distributions are less uniform",
            "text": "A well-known result in the compression literature is that there can be no recursive compression. The compression algorithm removes information captured by its model, resulting in a uniform output that appears random to the original model. However, our setting is not recursive compression. Instead, a separate and larger model is trained on the compressed output, which should be able to capture new patterns in the bitstream. Despite this, the output of compression using M1 appears very uniform, as evidenced by the minimal gains from modeling the unigram token distribution in Table 3. Therefore, it seems reasonable that this uniformity could make it hard for M2 to learn (as all patterns must be contextual). We investigate this by plotting the KL divergence between the observed empirical distribution and a uniform distribution for different segmentations of the bitstream. If the underlying distribution of bits was truly random and independent, then the distribution of unigrams for some bitstream segmentation should remain uniform, and therefore the KL divergence should remain close to zero. On the other hand, if the distribution diverges from uniform, there is contextual information to be learned when training an LLM to model the bitstream.\n\nWe segment the bitstream either into bit n-grams, where successive n-grams are allowed to overlap, or into n-bit tokens, following our M2 tokenization procedure\u2014see Section 3.4. We only plot tokenization into n-bits that are factors of the bitstream length, otherwise tokens would cross window boundaries in the EqualInfoAC setting.\n\nAs a baseline, we used the cryptographic secrets package in Python to generate bitstreams that should be truly random and independent. As such, the KL divergence should remain at zero when segmented in the same way as the compressed data. The reason this does not hold in some figures is that the maximum likelihood estimate of entropy is negatively biased. When using a Miller-Madow estimator to correct for this bias, the expected KL divergence is well within sampling noise bounds. To account for noise in the entropy estimation, we plot percentile intervals of the KL divergence between the observed entropy from disjoint samples of the data and the uniform distribution. As the number of bits in a segmentation grow, the vocabulary size increases exponentially, requiring many more samples. Thus, we expect noise in the entropy estimate to grow with the number of bits. This holds, but it is obfuscated by the log scaling in some figures. In fact, the magnitude of the noise for settings such as EqualInfoAC is larger than for Arithmetic Coding (AC) or random number-generated (RNG) data. This noise behavior is seen in various analyses.\n\nThe AC and RNG lines in the plots are very similar, and their sampling noise intervals have large overlaps. This suggests that the data generated by AC compression with M1 is difficult to distinguish from random data. For some specific settings, the AC entropy is statistically significantly less than the RNG entropy; however, differences in the mean entropy only start to appear after several decimal places. This is a possible explanation for why M2 models trained on AC data only learn to output a uniform distribution.\n\nEqualInfoAC has the most information among the Arithmetic Coding approaches. Given that this is the most learnable setting, it suggests that non-uniformity of the bitstream may be important for learning. We also see a large increase when moving to larger bit tokens, providing a further possible explanation for why larger vocabulary is helpful. Finally, we note that StaticAC has less information than EqualInfoAC, suggesting that weakening the \"coding\" component of Arithmetic Coding is a more effective way to retain information and increase learnability for M2."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We have shown there is promise in the idea of training LLMs over neural-compressed text. In the best case, this will allow training over text that is better compressed than standard subword token sequences, while maintaining learnability. This is an appealing prospect, as models that read and write more text per token are more efficient to train and serve, and can model longer dependencies.\n\nWhile bespoke compression methods have developed around different modalities (e.g., text, audio, images, video) and different applications, to our knowledge, no efficient compression methods have been designed specifically for use as LLM tokenizers. We are optimistic that future work will create such methods. Compared to today\u2019s subword tokenizers, we expect these methods (i) will deliver higher compression rates, (ii) will come closer to equal information per token, thus allocating compute more effectively, and (iii) will give models a more direct view of the underlying raw text, thus helping on spelling and pronunciation tasks. As a tradeoff, we expect these neural tokenizers will be somewhat less stable in their text\u2009\u2009token mapping, but perhaps not so unstable as our approach here. In particular, we think it is worth exploring methods under which a given word typically maps to a relatively small number (tens not thousands) of relatable token sequences.\n\nOne direction we left unexplored is the idea of passing information between the compressing model (M1) and the LLM trained over compressed text (M2). Some additional signal of M1\u2019s internal state or output may be helpful for M2 to accurately simulate M1, which is a prerequisite to flawlessly encoding and decoding M1-compressed text.\n\nFor hill-climbing in this space, we found it useful to iterate on the sequence-to-sequence sub-tasks of compression and decompression, which should, in theory, be learnable with high accuracy. Specifically, if future work can devise a strong (~) compressor that a transformer can be trained to accurately encode and decode, we expect that this will be an ideal candidate for tokenizing text for LLMs."
        }
    ],
    "url": "http://arxiv.org/html/2404.03626v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4",
            "2.5"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5",
            "3.6",
            "3.7",
            "3.8"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "6",
            "6.1",
            "6.2",
            "6.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "3",
            "5"
        ]
    },
    "research_context": {
        "paper_id": "2404.03626v1",
        "paper_title": "Training LLMs over Neurally Compressed Text",
        "research_background": "### Paper's Motivation\nThe motivation behind this paper is to explore whether compressing text further than the current subword tokenization methods, like BPE or Unigram, might lead to even greater efficiencies in training large language models (LLMs). This stems from recognizing that subword tokenizers, while effective, still leave room for potential improvements in compression rates and hence LLM performance, especially as LLMs trained over subword tokens describe seeing approximately four times more text per token, enabling them to model longer-range dependencies and consume more data without increased computational cost. The authors are curious whether neural compression might squeeze out more gains by removing simpler patterns from the text, thereby requiring the LLM to learn more complex structures.\n\n### Research Problem\nThe research problem investigated in the paper is determining if LLMs can be effectively trained using neurally compressed text. Specifically, it seeks to understand whether an LLM can learn from bitstreams compressed by previous LLMs using techniques like Arithmetic Coding (AC), thus compressing text efficiently while preserving learnability. The paper examines if compressing text with smaller LLMs (to remove simpler structures) can make the resulting compressed text easier to model for larger LLMs by focusing on more complex structures. \n\n### Relevant Prior Work\nThe paper builds on several related areas of research:\n\n1. **Subword Tokenizers**: Existing tokenizers like BPE and Unigram used in libraries like SentencePiece, achieve about 4x compression over natural language text. These tokenizers balance the trade-off between hiding character-level makeup and the significant benefits derived from compression.\n\n2. **Autoregressive Models as Compressors**: It is a well-known concept that autoregressive language models can serve as lossless text compressors. The authors reference work demonstrating that LLMs, like Chinchilla 70B, can achieve 12x bit-level compression over English text.\n\n3. **Arithmetic Coding (AC)**: The technique referred to is used for achieving near-optimal compression rates by assigning probabilities to text continuations. The paper explores using AC for pretraining corpus compression to potentially train a more advanced LLM on these compressed sequences.\n\n4. **Challenges with Neural Compression**: The text notes that initial attempts at training LLMs on AC-compressed text fail, a result attributed to the difficulty in learning the AC compression and decompression process itself.\n\n5. **Alternative Compression Techniques**: The paper proposes \"Equal-Info Windows,\" inspired by GZip-like methods, to divide the text into learnable chunks, resetting compression algorithms and model contexts at these boundaries.\n\n6. **Previous Comparisons**: The paper situates its exploration within the context of comparing subword tokenizers' effectiveness, noting that subword tokenizers tend to produce stable mappings between text and tokens which is advantageous for LLM training.\n\nThe synthesis of these areas aims to push the boundary of what's possible with LLM training, potentially leading to more efficient and scalable models by using neural compression methods.",
        "methodology": "**Methodology:** The proposed method involves using a two-stage process to train Language Learning Models (LLMs) on neurally compressed text. Here are the key components and steps of the proposed method: 1. **Compression Process:** - **M1 Model for Compression:** Long contiguous sequences of the training data are compressed using M1, which is a byte-level language model. - **Output Chunking:** The compressed output from the M1 model is then chunked into smaller units called tokens. 2. **Training with Compressed Data:** - **Training Models (M2):** These tokens generated from the compressed output are used to train different M2 models. - **Baselines and Tokenization:** - **Baseline Training on UTF-8:** For certain baselines, the models directly train over UTF-8 bytes. The byte tokenizer used for these baselines is taken from ByT5. - **SentencePiece Tokenization:** Other baselines involve training on text that is tokenized using a SentencePiece vocabulary. The SentencePiece vocabulary consists of a specified number of tokens from T5. 3. **Dataset and Metrics:** - The models, in both baselines, process a total of billion bytes. **Innovations:** - **Neural Compression:** Using a byte-level language model (M1) for the initial compression of training data sequences. - **Model Training Over Compressed Tokens:** Introducing a novel training strategy where models (M2) are trained on tokens derived from the compressed output, potentially making training more efficient. By leveraging these innovations, the methodology aims to explore the benefits and efficiencies of training large language models on neurally compressed data.",
        "main_experiment_and_results": "In the main experiment of the study, the researchers compared various compression methods and their impact on Large Language Model (LLM) training performance. Below is the detailed breakdown of their setup and findings:\n\n### Datasets:\n- The researchers used datasets compressed using different methods: Arithmetic Coding (AC) with model M1, Static Arithmetic Coding (StaticAC), EqualInfoAC, GZip, and SentencePiece.\n\n### Baselines:\n1. **Bytes**: A baseline where raw bytes of text data are used.\n2. **SentencePiece**: A subword tokenization method.\n3. **EqualInfoAC**: A compression method that combines AC with sentence token segmentation, aiming to equalize the information each segment carries.\n\n### Evaluation Metrics:\n- **Bits per Byte**: Measures the efficiency of the text compression.\n- **Perplexity**: Assesses the performance of language models on sequences of the same length they were trained on.\n- **Tokens/Byte**: Indicates how many tokens are needed to represent the same text, impacting generation latency.\n- **FLOPs/Byte**: The number of floating point operations required per byte for training and inference.\n\n### Key Findings:\n1. **Arithmetic Coding with M1**:\n   - Models trained on data compressed with Arithmetic Coding using M1 fail to learn anything meaningful, outputting a uniform distribution over tokens.\n   \n2. **StaticAC**:\n   - Similarly, models trained on data compressed using a static unigram model (StaticAC) also fail to learn, suggesting that difficulties stem from the brittleness or complexity of the coding process.\n\n3. **EqualInfoAC**:\n   - Models using EqualInfoAC outperform the Bytes baseline at most model sizes, showing better bits/byte performance and requiring fewer autoregressive steps to generate the same text.\n   - Although EqualInfoAC underperforms compared to SentencePiece in terms of bits/byte, it excels in tokens/byte, reducing generation latency due to fewer autoregressive steps.\n\n4. **SentencePiece**:\n   - SentencePiece baseline models outperformed all other methods, including Bytes, across all model sizes in terms of bits/byte.\n   - SentencePiece tends to achieve slightly better bits/byte performance compared to EqualInfoAC if FLOPs/Byte are held constant but take more decoder steps at inference time.\n\n5. **GZip**:\n   - Training over GZip-compressed text was relatively ineffective. Despite some learning, weak compression and poor bits/byte performance mean it lags behind the AC-compressed text models.\n\n6. **Effectiveness Across Model Sizes**:\n   - The advantages of EqualInfoAC, including in terms of generation latency and tokens/byte, become more pronounced with model scale.\n   - EqualInfoAC models with larger vocabularies (e.g., 256k) yield higher token compression rates and modest performance gains in bits/byte.\n\nOverall, the study suggests that EqualInfoAC exhibits potential for improved training efficiency and reduced latency in LLMs, especially in larger scales, despite being outperformed by SentencePiece in terms of bits/byte performance. The research highlights trade-offs in efficiency and latency, which are crucial for practical deployment scenarios."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate the effectiveness of the Equal-Info Windows technique for learning over neurally compressed text using Arithmetic Coding.",
            "experiment_process": "Compress long contiguous sequences of training data using various methods. For the primary experiments, use a byte-level language model (M1) in the compression algorithm. Chunk the compressed output into tokens and train the M2 models over these tokens. Conduct ablations on window size and M2 vocabulary size. The M2 models are trained with a fixed computation budget (FLOPs/byte) and evaluated based on perplexity benchmarks (bits/byte). Observations include standard LLMs trained over AC-compressed text, GZip-compressed text, and Equal-Info Windows-compressed text.",
            "result_discussion": "The Equal-Info Windows make learning of AC-compressed text possible across various settings. Remarkably, M2 models trained with this technique outperform byte-level baselines on perplexity benchmarks for fixed computational budget. However, the M2 models underperform compared to subword baselines due to relatively unstable mappings induced by neural tokenizers. Future research should focus on improving neural tokenizers to maintain high compression and learnability.",
            "ablation_id": "2404.03626v1.No1"
        },
        {
            "research_objective": "Determine the compressibility and learnability of neurally compressed text using GZip.",
            "experiment_process": "Compress text using GZip and train standard LLMs over this compressed text. The LLMs are evaluated to see if they can model GZip-compressed text effectively.",
            "result_discussion": "Standard LLMs are capable of learning to model GZip-compressed text. However, the performance is not competitive with other methods. The results indicate that while some compression methods produce learnable data, they might not be optimal for LLM training compared to other techniques such as Equal-Info Windows.",
            "ablation_id": "2404.03626v1.No2"
        },
        {
            "research_objective": "Analyze the learnability of text compressed via Arithmetic Coding (AC) when trained over a byte-level language model (M1).",
            "experiment_process": "Train a small language model (M1) over raw byte sequences, then use this frozen model to compress pretraining corpus text via Arithmetic Coding. Chunk the compressed bitstream into tokens and train a subsequent language model (M2) over these tokens. Evaluate the M2 models' ability to learn and predict compressed text.",
            "result_discussion": "Text compressed via Arithmetic Coding is not readily learnable by a standard transformer-based LLM, resulting in models predicting tokens at chance. This suggests that the difficulty lies in learning the AC compression/decompression process itself. Even with a context-free unigram model, AC-compressed text remains challenging to model, indicating that the compression method is not suitable for direct LLM training.",
            "ablation_id": "2404.03626v1.No3"
        }
    ]
}