{
    "title": "Will the Real Linda Please Stand up\u2026to Large Language Models? Examining the Representativeness Heuristic in LLMs",
    "abstract": "Large language models (LLMs) have demonstrated proficiency in understanding text and generating human-like responses but may exhibit biases acquired from their training data. One such bias is the representativeness heuristic, a psychological concept where the likelihood of an event is judged based on its resemblance to a well-known prototype rather than considering broader facts or statistical evidence.\n\nThis work explores how the representativeness heuristic affects LLM reasoning. We developed ReHeAT (Representativeness Heuristic AI Testing), a dataset with problems that span six common types of representativeness heuristics. Experimental results show that LLMs applied to ReHeAT exhibit biases related to the representativeness heuristic. The reasoning steps of the models often rely on stereotypes rather than the problem descriptions provided.\n\nNotably, model performance improves when the prompt includes a hint reminding the model to apply its knowledge. This suggests that the influence of the representativeness heuristic is distinct from traditional biases, as it can occur even when LLMs have the necessary knowledge but succumb to a cognitive trap. The findings underscore the need for further research into the impact of the representativeness heuristic on model reasoning and decision-making, as well as the development of solutions to address this bias.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "\u201cLinda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice and also participated in anti-nuclear demonstrations.\u201c\n\nWe have seven statements below: [1] Linda is a teacher in elementary school. [2] Linda works in a bookstore and takes Yoga classes. [3] Linda is a psychiatric social worker. [4] Linda is a member of the League of Women Voters. [5] Linda is a bank teller. [6] Linda is an insurance salesperson. [7] Linda is a bank teller and is active in the feminist movement.\n\nQuestion: Rank the seven statements associated with each description by the degree to which Linda resembles the typical member of that class.\n\nThis is one experiment known as the \u201cLinda problem\u201d devised by Tversky and Kahneman; it was demonstrated that people are influenced by specific descriptions, such as being deeply concerned with issues of discrimination and social justice and also participating in anti-nuclear demonstrations to rank [7] higher than [5]. However, [7] combines [5] and an additional event. This means that, from a statistical perspective, [5] is more likely to occur than [7] because it is more general and less restrictive.\n\nKahneman & Tversky introduced this phenomenon as the \u201crepresentativeness heuristic,\u201d which involves estimating the likelihood of an event by comparing it to an existing prototype in our minds. It offers a convenient shortcut in decision-making by aligning with intuitive thinking, leading people to rely on it frequently. Generally, this heuristic is quite beneficial, as it simplifies complex judgments. However, it is important to recognize that it can also result in significant errors, given that people are prone to assessing the likelihood of an object\u2019s category membership based on superficial similarities while neglecting actual statistical evidence. For example, people categorize individuals based on their looks, actions, or background description from language, leading to skewed perceptions and decisions. This stereotyping phenomenon is widespread among humans.\n\nLLMs trained on real-world data and instructed to emulate human behavior may capture the representativeness heuristic. Previous work has mainly focused on the biases within training data. These biases often stem from data distributions that do not reflect the desired proportions that would drive unbiased decision-making. In contrast, the representativeness heuristic represents a type of cognitive bias that has yet been thoroughly investigated in LLMs. It is unique in leading the model to make mistakes even when it possesses the knowledge necessary to solve the problem. The model is able to answer the Statistical Prototype Question (SPQ), yet it tends to fail to answer the Representativeness Heuristic Question (RHQ). The SPQ and RHQ are intrinsically equivalent, where SPQ is expressed statistically, and RHQ expresses the same statistical logic in a scenario. This indicates that LLMs can engage in erroneous cognitive reasoning even with the knowledge of statistical probability. Interestingly, providing a hint can prompt the model to use the knowledge to make a correct prediction. This indicates that the representativeness heuristic can block the model from following a correct reasoning path; instead, it relies on the cognitive shortcut to make a trivial decision.\n\nTo investigate the representativeness heuristic in LLMs, we construct a dataset, ReHeAT (Representativeness Heuristic AI Testing), which contains 202 RHQs that span six types of representativeness heuristics\u2014Base Rate Fallacy, Conjunction Fallacy, Disjunction Fallacy, Insensitivity to Sample Size, Misconceptions of Chance, and Regression Fallacy. The questions we designed are adapted from those used in prior investigations into heuristics within the field of psychology. To the best of our knowledge, our dataset is the first to offer extensive and comprehensive coverage of RHQs, enabling exploration of LLMs\u2019 capabilities in countering the cognitive bias.\n\nOur findings indicate that these LLMs exhibit behaviors that closely mirror human heuristic behavior. Additionally, advanced prompting techniques such as chain of thought (CoT), in-context learning, and self-consistency prompting, offer marginal improvements. Nevertheless, when explicitly prompted to recall its knowledge, the model shows an improvement in performance. This underscores the significance for future research to address the representativeness heuristic, guiding LLMs toward correct reasoning and decision-making."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Social biases in natural language processing (NLP) systems and related data have been studied with respect to their fairness, inclusivity, and accuracy. For example, Bolukbasi et al. and Garg et al. are among the pioneers in demonstrating gender biases embedded in word embeddings, showing how these representations could reinforce stereotypical associations. Caliskan et al. conclude that standard machine learning methods for NLP could acquire societal biases from textual data.\n\nSome works expanded the understanding of bias sources in NLP systems, including those from data collection, annotation processes, and model architecture choices. To date, numerous efforts have been made to mitigate social biases in systems through a variety of methods, including data augmentation, changes in model architecture, and training objectives.\n\nResearchers are concerned about whether large language models (LLMs) inherit social biases from the trillions of tokens they have been trained on. Weidinger et al. provide a comprehensive taxonomy of social risks within LLMs. Although the research community has documented numerous social biases in LLMs, few LLM researchers have examined these biases from the human mind's perspective.\n\nIn this work, we study the bias issue in LLMs from a new angle, representative heuristics, a concept originating in psychology. Our current research emphasizes a more in-depth and comprehensive exploration of the application of the representativeness heuristic within LLMs. For example, we have compiled a dataset encompassing a large number of questions (a total of 202) and have benchmarked performance across a wider range of LLMs based on diverse prompting strategies."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Representativeness Heuristic",
            "text": "Drawing from prior research, we organize our research around a framework that categorizes the representativeness heuristic into six types (Kahneman & Tversky, 1973  ###reference_b13###; Kahneman et al., 1982  ###reference_b14###; Tversky & Kahneman, 1974  ###reference_b28###; 1983  ###reference_b29###).\nThese categories vary in their fundamental logical approach and their impact on decision-making processes.\n###figure_2### ###figure_3### ###figure_4### ###figure_5### ###figure_6### ###figure_7### Base Rate Fallacy occurs when individuals overlook or insufficiently account for the population base rate of an event (i.e., its overall prevalence or frequency) in favor of specific instances or recent information. Figure 2(a)  ###reference_sf1### presents an example where  represents the proportion of individuals with a symptom (all blue points within the large circle),  denotes the rate of illness (all points within the small circle), and  indicates the proportion of the sick who have the symptom (all blue points within the small circle).\n represents the illness rate among those with the symptom.\nMost people would assume that because  is high,  would also be high.\nYet, according to Bayes\u2019 theorem,\nmeaning  is greatly influenced by the base rates of  and , showing the importance of considering general prevalence in evaluating specific probabilities.\nAn example question can be seen in Table 17  ###reference_### of the Appendix.\nConjunction Fallacy occurs when people mistakenly believe that the chance of two events happening together is greater than the chance of either event happening alone.\nSee Figure 2(b)  ###reference_sf2###, consider the example at the beginning of the article.\n represents the probability that the person is a bank teller (all points within the small circle, less relevant to the description).\n is the probability that the person is active in the feminist movement (all points within the large circle, more relevant to the description).\n the probability that the person is both a bank teller and active in the feminist movement (all purple points).\n will always be no larger than  and , no matter which one is closer to the description.\nAn example question can be seen in Table 18  ###reference_### of the Appendix.\nDisjunction Fallacy occurs when people incorrectly judge the probability of a broader category to be smaller than one of its specific components.\nIn Figure 2(c)  ###reference_sf3###, we can imagine the small circle representing ice cream and the large circle representing frozen food.\nSince ice cream is a subset of frozen food, the probability of frozen food is higher than ice cream.\nHowever, when people talk about summer refreshments, they often think of ice cream rather than frozen food.\nThis choice illustrates a common tendency to differentiate specific items from their general classifications based on contextual associations.\nAn example question can be seen in Table 19  ###reference_### of the Appendix.\nInsensitivity to Sample Size occurs when people underestimate how important sample size is in data evaluation, potentially leading to incorrect conclusions.\nFigure 2(d)  ###reference_sf4### presents an example where a small group (small circle) and a large group (large circle) both have 50% blue dots and 50% red dots (2:2 and 12:12).\nIf we add one red dot to each sample, then the ratio of blue to red becomes 40% and 60% (2:3) in the small group, versus 48% and 52% (12:13) in the large group.\nIt should be recognized that smaller groups are more prone to skewed outcomes because even small changes have a larger impact on the overall dynamics of a small group.\nAn example question can be seen in Table 20  ###reference_### of the Appendix.\nMisconceptions of Chance involve misunderstanding how randomness works, especially thinking that past outcomes will affect future outcomes, in cases where the outcomes are in fact independent. For example,\nFigure 2(e)  ###reference_sf5### presents a dice-rolling example.\nPeople wrongly believe that if a specific outcome has occurred frequently, it is less likely to happen again shortly, or vice versa if it has occurred rarely.\nHowever, in truly random events, such as rolling a fair die, the probability of any given outcome (1-6) remains constant at , unaffected by the sequence of previous results.\nAn example question can be seen in Table 21  ###reference_### of the Appendix.\nRegression Fallacy occurs when individuals overlook the natural tendency for extreme situations to move back towards the average (regress to the mean) and instead, erroneously assign this regression to a particular cause, see Figure 2(f)  ###reference_sf6###.\nFor instance, if an athlete shows a lackluster performance following a perfect game, it might be incorrectly ascribed to external factors, neglecting the likelihood of natural variance in performance.\nAn example question can be seen in Table 22  ###reference_### of the Appendix."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Dataset and Experimental Setup",
            "text": "We begin by constructing a dataset intended to reflect a diverse array of questions and scenarios (\u00a74.1 ###reference_###). Following this, we introduce the models and prompting strategies (\u00a74.2 ###reference_###), along with the evaluation methods (\u00a74.3 ###reference_###)."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Data Collection",
            "text": "Our main resource for creating test questions on representativeness heuristic comes from the academic studies by Kahneman and Tversky (Kahneman & Tversky, 1973; Kahneman et al., 1982; Tversky & Kahneman, 1974; 1983). Their work introduced various question types and insights into the design of cognitive heuristic measures. Building on their foundation, we design questions that extensively explore the representativeness heuristic. Specifically, our test set contains 49 questions drawn directly from previous research and 153 new questions that have been carefully adapted. To support the usefulness of our adapted items, we pay special attention to retaining the essence of the representativeness heuristic being tested, while changing the situation and context of the original items. The total 202 examples are all in English. As shown in Table 1, each example is composed of the following characteristics: Query: Provide the question\u2019s description and requirements. Type: Indicate the question\u2019s category, such as Disjunction Fallacy. Feature: Highlight unique aspects, such as choose one, or the question\u2019s source, such as original problem in Bar-Hillel & Neter, 1993. Ground Truth: Represents the standard answer. Human Response: Document the outcomes of human responses from academic research when available, or denoting N/A otherwise."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Models and Prompting Strategies",
            "text": "We investigate LLaMA 2 (llama-2-70b-chat) (Touvron et al., 2023  ###reference_b27###). We apply four different prompting strategies to generate answers. \n\nStandard: We ask the model to directly answer the query without explicit reasoning instructions using a greedy decoding method.\n\nZero-shot Chain-of-Thoughts (CoT): We first ask the model to generate its reasoning with an instruction to think step-by-step, but not to give the answer directly, then direct it to answer the query with the context of reasoning steps.\n\nThis two-step CoT strategy is developed based on CoT (Wei et al., 2022  ###reference_b31###) and its zero-shot variation (Kojima et al., 2022  ###reference_b15###).\n\nSelf-Consistency: We prompt the model to generate ten answers with a temperature sampling parameter, using majority voting to finalize the model decision via diverse reasoning paths. This prompting strategy is known as self-consistency prompting (Wang et al., 2023  ###reference_b30###).\n\nFew-shot In-Context Learning (ICL): The model is prompted with a few selected examples from the same category of representativeness heuristic so that the model can learn a task from demonstrations. Samples used as exemplars will not participate in accuracy calculations. This ability to learn from context is known as in-context learning (Brown et al., 2020  ###reference_b6###)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Evaluation Methods",
            "text": "Automatic evaluation: All query questions in ReHeAT are either in multiple-choice or ranking format. We adopt precise prompt templates in Table 13 of the Appendix to instruct models to generate responses. For a multiple-choice question, a response is deemed correct if and only if it contains the ground-truth option. For a ranking question, the exact match between relative permutations of response options and ground-truth options qualifies for a true positive model prediction.\n\nHuman evaluation: In addition, one of the present authors with expertise in psychology conducted the human evaluation to assess the output reasoning steps from zero-shot CoT prompting. We report the proportions of the four possible LLM outcomes: both reasoning and prediction are correct; the reasoning is correct, but the prediction is incorrect; the prediction is correct, but the reasoning is incorrect; both reasoning and prediction are incorrect."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experimental Results",
            "text": "In addition to evaluating the models\u2019 performance on the ReHeAT dataset and analyzing their reasoning abilities (\u00a75.1  ###reference_###; \u00a75.2  ###reference_###), we further investigated the potential performance boost that could be achieved by providing hints to the models (\u00a75.4  ###reference_###), as well as how situational similarity influences model performance (\u00a75.3  ###reference_###).\n\nAs shown in Table 2  ###reference_###, one-shot ICL performs the best among various prompting strategies. This has piqued our curiosity: Could providing more examples further improve performance?\n\nWe also check each type of representativeness heuristic (Table 6  ###reference_### - 9  ###reference_### of the Appendix). We have observed that in most cases, most models perform relatively poorly when dealing with questions of Conjunction Fallacy and Disjunction Fallacy. Compared to other types of questions, such as Base Rate Fallacy, the challenge with Conjunction Fallacy and Disjunction Fallacy lies in the fact that the statistical reasoning required is embedded within the connotations and combinations of texts (e.g., South Africa is a subset of Africa) rather than being directly indicated by expressions like large sample size or far more than.\n\nWe examine LLMs\u2019 knowledge of statistical principles in answering SPQs. The four models possess statistical knowledge, demonstrating a comprehensive understanding across all categories of statistical principles (Table 4  ###reference_### of the Appendix; SPQs example in Table 14  ###reference_### of the Appendix). This significantly differs from our observations on the accuracy of the four models\u2019 performance on RHQs, indicating that the models indeed made errors related to the representativeness heuristic.\n\nOverall, LLMs have demonstrated a representativeness heuristic bias similar to humans, as found in the research findings reported by Bar-Hillel & Neter (1993  ###reference_b2###); Kahneman & Tversky (1973  ###reference_b13###); Kahneman et al. (1982  ###reference_b14###); Tversky & Kahneman (1974  ###reference_b28###; 1983  ###reference_b29###), where human accuracy across various tasks was within the 10% to 50% range.\n\nExample questions and model answers are in Table 17  ###reference_### - 22  ###reference_### of the Appendix."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "LLMs Exhibit the Representativeness Heuristic",
            "text": "We report the model\u2019s performance on the ReHeAT dataset with different prompting strategies. As shown in Table 2, one-shot ICL performs the best among various prompting strategies. This has piqued our curiosity: Could providing more examples further improve performance?\n\nTo explore whether LLMs can acquire knowledge related to the representativeness heuristic through ICL, we report the -shot performance of several types of LLMs. From  to , LLaMA-2 exhibits a noticeable improvement in accuracy from adding an example in context. However, such an uptrend becomes saturated and begins to fluctuate with further increases in the number of shots.\n\nWe also check each type of representativeness heuristic. We have observed that in most cases, most models perform relatively poorly when dealing with questions of Conjunction Fallacy and Disjunction Fallacy. Compared to other types of questions, such as Base Rate Fallacy, the challenge with Conjunction Fallacy and Disjunction Fallacy lies in the fact that the statistical reasoning required is embedded within the connotations and combinations of texts rather than being directly indicated by expressions like large sample size or far more than.\n\nWe examine LLMs\u2019 knowledge of statistical principles in answering SPQs. The models possess statistical knowledge, demonstrating a comprehensive understanding across all categories of statistical principles. This significantly differs from our observations on the accuracy of the models\u2019 performance on RHQs, indicating that the models indeed made errors related to the representativeness heuristic.\n\nOverall, LLMs have demonstrated a representativeness heuristic bias similar to humans, as found in the research findings reported by Bar-Hillel & Neter (1993); Kahneman & Tversky (1973); Kahneman et al. (1982); Tversky & Kahneman (1974; 1983), where human accuracy across various tasks was within the 10% to 50% range.\n\nExample questions and model answers are provided in the Appendix."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Discrepancies in Model Reasoning: Beyond Predictions",
            "text": "While testing the ReHeAT dataset, we also conducted human evaluations on the reasoning steps produced by the LLaMA2-70B model under CoT prompting. As before, we form four possible combinations of outcomes, as shown in Figure 4 and Table 10 of the Appendix.\n\nGenerally, LLaMA2-70B more frequently produces correct outcomes from incorrect reasoning processes; the reasons are often incorrectly based on a stereotype rather than the problem\u2019s description. We have conducted case studies on some interesting reasoning, which we include in Appendix A."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Improving Performance by Hinting that LLMs to use Their Knowledge",
            "text": "How can the model\u2019s cognitive process be put back on track? We test whether LLMs show enhanced performance with prompts that hint them to utilize their existing knowledge. We tested two types of hints: one general and the other based on more detailed cues for each representativeness heuristic type (Table 16 of the Appendix). These prompts aimed to hint at the model to recall the knowledge it possessed. The results are presented in Table 3. We also provide detailed results for each type of representativeness heuristic in Table 11 and Table 12 of the Appendix. We found that both types of hints provided a noticeable improvement for most models, with the specific type of hints providing a more significant boost in performance compared to the general prompts."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "The Impact of Situational Similarity on Model Performance",
            "text": "After observing the performance differences between the model on statistical SPQs and the contextual RHQ counterparts found in ReHeAT, we delve further into the transition between these two - Intermediate Questions (IQs). This type of question integrates specific situational contexts and statistical data, making it necessary to consider both concrete data and the potential impact of the situation when making decisions. For example, for Linda\u2019s problem, we assign a probability to each independent event to offer the model with more explicit statistical information. Nevertheless, the model has to infer the information based on the similarity manifested in the meaning of the statements in RHQs.\n\nThe performance of the models on IQs is reported, and compared with SPQs\u2019 results, more errors were made for IQs. This indicates that introducing scenarios interferes with the model\u2019s statistical decision-making process. This also explains why models exhibit the representativeness heuristic more frequently when responding to RHQs."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduce a novel dataset (ReHeAT) for the representativeness heuristic to assess whether models, like humans, make representativeness heuristic errors in decision-making\u2014specifically, by overlooking known information and relying on similarity for judgment. Our research reveals that even the most advanced LLMs tend to repeat human heuristic mistakes when addressing issues related to the representativeness heuristic, highlighting the necessity for a deep understanding of these biases in model decision-making processes. Furthermore, we explored how models perform differently when faced with various types of questions. For example, questions regarding the Conjunction Fallacy and Disjunction Fallacy present a significant challenge. This is due to the model\u2019s difficulty discerning the latent probabilistic relationships embedded within the text. We also found that hints to stimulate the model to recall its existing knowledge can, to some extent, enhance its performance. More specific and detailed prompts tend to lead the model to demonstrate better performance. However, although this method is effective, the model\u2019s potential is far from fully tapped, and there remains significant room for improvement."
        }
    ],
    "url": "http://arxiv.org/html/2404.01461v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.2",
            "4.3",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.01461v2",
        "paper_title": "Will the Real Linda Please Stand up\u2026to Large Language Models? Examining the Representativeness Heuristic in LLMs",
        "research_background": "### Motivations:\n\nThe paper's motivation revolves around understanding and addressing the cognitive biases present in Large Language Models (LLMs). Specifically, it investigates the representativeness heuristic, a phenomenon where individuals make judgments based on how much something resembles a known stereotype or prototype rather than on statistical probability. Given that LLMs are trained to emulate human behavior and decision-making, they might also exhibit similar heuristic biases. This could lead to instances where LLMs make decisions based on cognitive shortcuts rather than accurate statistical reasoning.\n\n### Research Problem:\n\nThe primary research problem is to evaluate the extent to which LLMs, such as GPT-3.5, GPT-4, PaLM 2, and LLaMA 2, display the representativeness heuristic when making decisions. The study aims to determine whether these models are prone to the same type of cognitive errors as humans are, even when they possess the necessary knowledge to solve the problems correctly. Moreover, it investigates how advanced prompting techniques can mitigate these heuristic biases, with the goal of guiding LLMs toward better reasoning and decision-making.\n\n### Relevant Prior Work:\n\n1. **Tversky & Kahneman (1983)**: Introduced the \"Linda problem,\" demonstrating that people are influenced by specific descriptions and tend to rank improbable but stereotypically matching scenarios higher than more likely but less descriptively fitting ones. This is pivotal in understanding the representativeness heuristic.\n\n2. **Kahneman & Tversky (1973)**: Introduced the concept of the representativeness heuristic, explaining how people estimate the likelihood of an event by comparing it to an existing prototype, which often neglects actual statistical evidence.\n\n3. **Spencer et al., (2016)**: Discussed stereotyping, a common occurrence among humans, which aligns with the concept of heuristics influencing decision-making based on superficial similarities.\n\n4. **Bender et al., (2021); Bolukbasi et al., (2016); Garg et al., (2018); Sheng et al., (2019); Zhao et al., (2019)**: Focused on biases in training data for LLMs, showing how skewed data distributions can lead to biased outcomes, setting a foundation for studying cognitive biases like the representativeness heuristic.\n\n5. **Bar-Hillel & Neter (1993); Kahneman et al., (1982); Tversky & Kahneman (1974, 1983)**: Provided foundational analyses and experiments on various heuristics and biases, crucial for developing the RHQs used in this study's newly constructed dataset.\n\n6. **Wei et al., (2022); Brown et al., (2020); Huang et al., (2022)**: Explored advanced prompting techniques such as chain of thought (CoT), in-context learning, and self-consistency, which have been used to analyze and potentially mitigate heuristic biases in LLMs.\n\nThe paper leverages this prior work to build upon the understanding of cognitive heuristics and apply it to the context of LLMs, aiming to reveal the extent of heuristic influence and suggest pathways for improvement.",
        "methodology": "In the methodology section of the paper titled \"Will the Real Linda Please Stand up\u2026to Large Language Models? Examining the Representativeness Heuristic in LLMs,\" the proposed method is outlined as follows:\n\n1. **Dataset Construction**:\n   - The authors start by creating a dataset that is designed to encapsulate a wide variety of questions and scenarios (\u00a74.1 ###reference_###). This ensures that the evaluation process comprehensively assesses the Large Language Models (LLMs) across different contexts and types of queries.\n\n2. **Models and Prompting Strategies**:\n   - The next step involves the introduction of the models and the strategies used for prompting (\u00a74.2 ###reference_###). This provides clarity on which LLMs are being tested and how they are being prompted to generate responses. It includes details on the specific models selected for the study and the tailored prompting techniques employed to elicit answers for the constructed dataset.\n\n3. **Evaluation Methods**:\n   - Finally, the authors detail the evaluation methods (\u00a74.3 ###reference_###). This section covers how the responses from the LLMs are assessed, the criteria used to judge their performance, and any metrics or benchmarks applied to measure the effectiveness of the models in handling the representativeness heuristic.\n\nBy maintaining the original phrasing and inclusion of the section references, this summary faithfully represents the key components and innovations of the proposed method described in the paper.",
        "main_experiment_and_results": "### Main Experiment Setup\n\nThe primary goal of the main experiment was to assess the performance of various Large Language Models (LLMs) on tasks related to the representativeness heuristic. The experiment utilized several datasets and evaluation metrics to gauge model accuracy and reasoning abilities:\n\n- **Datasets**: The ReHeAT dataset was used to evaluate the models' performance.\n- **Models Tested**: Four types of LLMs were tested:\n  - GPT-4\n  - GPT-3.5\n  - LLaMA-2\n  - PaLM 2\n- **Prompting Strategies**: Different in-context learning (ICL) prompt strategies were employed, ranging from zero-shot to multiple-shot setups.\n- **Evaluation Metrics**: Model accuracy was measured, and detailed analysis on specific question types such as Conjunction Fallacy, Disjunction Fallacy, and Base Rate Fallacy questions were conducted. Statistical Principle Questions (SPQs) were used to assess the models' understanding of statistical principles.\n\n### Main Experimental Results\n\n- **One-shot ICL Performance**: One-shot ICL showed the best performance among various prompting strategies.\n- **Multi-shot Performance**: \n  - Both **GPT-4 and LLaMA-2** showed improved accuracy with the addition of a single example but exhibited performance saturation and fluctuation with more shots.\n  - **GPT-3.5 and PaLM 2** did not show a clear improvement pattern with more in-context examples, suggesting a weaker or non-existent correlation between the number of examples and accuracy.\n  \n- **Question Type Performance**: \n  - Most models performed poorly on Conjunction Fallacy and Disjunction Fallacy questions compared to other types like Base Rate Fallacy questions.\n  - The difficulty with Conjunction and Disjunction Fallacies is attributed to the embedded statistical reasoning within the text, rather than explicit terms.\n\n- **Statistical Knowledge**: The four models displayed a comprehensive understanding of statistical principles across all categories of SPQs, but this knowledge did not consistently translate into accurate reasoning on representativeness heuristic questions.\n\n### Final Observations\n\n- **Representativeness Heuristic Bias**: The models exhibited a similar representativeness heuristic bias to humans, consistent with findings from past research by Bar-Hillel & Neter (1993) and Kahneman & Tversky (1973, 1983).\n- **Example Questions and Model Answers**: Detailed examples and model responses are available, providing insights into the types of errors the models made related to the representativeness heuristic.\n\nIn summary, while LLMs show some capability in understanding and applying statistical principles, their performance on representativeness heuristic tasks varies significantly, with notable difficulties in tasks involving Conjunction and Disjunction Fallacies."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To explore whether providing hints that prompt LLMs to utilize their existing knowledge can enhance their performance and reduce representativeness heuristic biases.",
            "experiment_process": "Two types of hints were tested: a general hint and more detailed cues for each representativeness heuristic type. These prompts aimed to hint at the model to recall the knowledge it possessed. Detailed results for each representativeness heuristic were presented, specifically in Table 11 and Table 12 of the Appendix.",
            "result_discussion": "Both types of hints provided a noticeable improvement for most models. The specific type of hints provided a more significant boost in performance compared to the general prompts.",
            "ablation_id": "2404.01461v2.No1"
        },
        {
            "research_objective": "To understand how situational similarity influences LLM performance and their susceptibility to the representativeness heuristic.",
            "experiment_process": "The performance on Intermediate Questions (IQs) was analyzed. These questions integrate specific situational contexts with statistical data, requiring the models to consider both explicit statistical information and situational context. For instance, for Linda's problem, a probability was assigned to each independent event to provide more explicit statistical information. The model's performance was then compared between IQs and standard SPQs, as reported in Table 5 of the Appendix.",
            "result_discussion": "More errors were made with IQs compared to SPQs, indicating that introducing scenarios interferes with the model's statistical decision-making process. This explains why LLMs exhibit the representativeness heuristic more frequently when responding to situational RHQs.",
            "ablation_id": "2404.01461v2.No2"
        }
    ]
}