<p align="center">
  <h1 style="display: inline;">
    Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers
  </h1>
</p>

<p align="center">
  <a href="https://arxiv.org/abs/">📖 Paper</a> •
  <a href="https://huggingface.co/datasets/yale-nlp/">🤗 Data</a>
</p>


## 📰 News
- **2025-xx-xx**: We are excited to release the LimitGen paper, dataset, and code!

## LimitGen Benchmark

While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations, remains understudied. LimitGen, the first comprehensive benchmark for evaluating LLMs' capability to support early-stage feedback and complement human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a synthetic dataset carefully created through controlled perturbations of papers, and LimitGen-Human, a collection of real human-written limitations.

## 🚀 Quickstart
### 1. Setup



### 2. Limitation Generation

### 3. RAG


### 4. Evaluation


## ✍️ Citation
If you use our work and are inspired by our work, please consider cite us (available soon):
```

```
